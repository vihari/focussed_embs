just look at how people actually did it . ; ) measuring shadow length at the same time at different places , comparing horizons ( as also suggested in comments ) , . . . also have a look at this list , some of which are down to earth methods . ; ) by the way , there are easy ways to prove that the earth is rotating , like the foucault pendulum , as well .
in mathematics , there is a complete symmetry between $+i$ and $-i$ . both the imaginary unit and the minus imaginary unit obey $$ i^2 = ( -i ) ^2 = -1 $$ the exchange of $i$ and $-i$ is known as the ${\mathbb z}_2$ automorphism group of the complex numbers ${\mathbb c}$ . when you introduce the complex numbers for the first time , it is a complete convention whether you call a square root of $ ( -1 ) $ as $+i$ or $-i$ . however , in physics , we have to break the symmetry between $+i$ and $-i$ because we must know whether a wave in a particular situation is $\exp ( i\omega t ) $ or $\exp ( -i\omega t ) $ , for example . in particular , $xp-px=i\hbar$ and not $-i\hbar$ . also , and the following choice of the sign is actually not independent from the previous one in the commutator , schrödinger 's equation was chosen to be $$ h |\psi \rangle = i\hbar\frac{d}{dt}|\psi\rangle $$ where $h$ is the hamiltonian that may be replaced by $h=e$ when acting on an energy eigenstate $|\psi\rangle$ . this equation is totally universal everywhere in quantum mechanics where a hamiltonian is well-defined ( it may be even quantum field theory or some descriptions of string theory ) . the equation above , with $h=e$ , is solved by $$|\psi ( t ) \rangle = \exp ( et/i\hbar ) |\psi ( 0 ) \rangle = \exp ( -iet/ \hbar ) |\psi ( 0 ) \rangle = \exp ( -i\omega t ) |\psi ( 0 ) \rangle $$ all the forms are equivalent because $1/i = -i$ – this equation is equivalent to $i^2=-1$ – and because $e=\hbar\omega$ without a minus sign . so your sign is wrong ; the sign you denounced is the right one and the sign you wanted is the incorrect one . just to be sure , in quantum field theory , we work with various objects – quantum fields – that are expanded into terms that depend on time as $\exp ( -i\omega t ) $ while there must also be terms that depend on time via $\exp ( +i\omega t ) $ . but these are terms in operators , not the time dependence of the wave function . one must be careful about the precise statements and objects . i have not made any statement of the sort that only the expression $\exp ( -i\omega t ) $ and not $\exp ( +i\omega t ) $ appears in quantum theory papers and books . of course , both of them may appear somewhere – in quantum field theory , both of them have to appear because there are both creation and annihilation operators , both particles and antiparticles . but when we are asking how an energy $e$ wave function ( and i mean the ket vector ) depends on time , it is always via $\exp ( -iet/\hbar ) $ . the bra vector has the opposite sign ( plus ) in the exponent .
great question ; i remember being so confused by this when i first took analytic mechanics . the components of the angular velocity " in the body frame " are not zero because when one writes these components , one is not referring to measurements of the motions of the particles in the body frame ( because , of course , the particles are stationary in this frame ) . instead , one is referring to angular velocity as measured in an inertial frame but whose components have simply been written with respect to a time-varying basis that is rotating with the body . in practice , we make measurements of the positions $\mathbf x_i ( t ) = ( x_i ( t ) , y_i ( t ) , z_i ( t ) ) $ of the particles in an inertial frame . then , we note that for a rigid body ( let 's consider pure rotation for simplicity ) , the position of each particle $i$ satisfies \begin{align} \mathbf x_i ( t ) = r ( t ) \mathbf x_i ( 0 ) \end{align} for some time-dependent rotation $r ( t ) $ . then we compute $\boldsymbol\omega ( t ) = ( \omega^x ( t ) , \omega^y ( t ) , \omega^z ( t ) ) $ in the standard way in terms of $r ( t ) $ . to see how this is done in detail , see , for example stackexchange-url once we have $\boldsymbol\omega$ , we can write its components with respect to any basis we like . if we write it in the standard ordered basis $\{\mathbf e_i\}$ , then we will just get $\omega_x ( t ) $ as its components . if we write it in some basis $\{\mathbf e_{i , b} ( t ) \}$ that is rotating with the body ( like one that points along the principal axes of the body ) then we get different components $\omega^i_b ( t ) $ , and these are the body components . main point reiterated . angular velocity is being measured with respect to an inertial frame , but its components can be taken with respect to any basis we wish such as one rotating with the body .
laminar flow reversibility ; gets straight from equations , but it is hard to believe it works in reality . yet : http://www.youtube.com/watch?v=p08_kltkp50
as understood by einstein 's general theory of relativity completed in 1915-16 , gravity is indeed a manifestation of ( nothing else than ) the curvature of space and i have some doubts about your implicit claim that you have made this discovery " independently " of einstein . according to the precise equations of general relativity , the so-called einstein 's equations $$ g_{\mu\nu} = \frac{8\pi g}{c^2} t_{\mu\nu} , $$ what influences the curvature of spacetime is the stress-energy tensor that knows about the density of energy and momentum and the flux of energy and momentum . terms like " flux of momentum " may sound obscure but they are described by well-defined mathematical formulae . in particular , " flux of momentum " is nothing else than the component of pressure . so pressure also influences the curvature of spacetime – and therefore the gravitational field and the behavior of objects in this field – according to general relativity . on the other hand , it is irrelevant for the curvature and gravity whether the same stress energy tensor – the density of mass , energy , momentum , and components of pressure and stress – are achieved by the electromagnetic field , one material , or another material . however , it is still impossible to " create " curvature of space without any material ( or energetic ) carrier . the equations explicitly show that the ricci tensor is zero if there is no energy/momentum density in the space . so one can not create a " black hole out of nothing " . nevertheless , black holes may suck all the material and make the spacetime around ricci-flat ; the ricci ( or einstein ) tensor is equal to zero almost everywhere in the space . this ricci-flatness is still importantly violated at the black hole singularity which is the reason why the black holes still carry a nonzero mass/energy . the question is getting increasingly impenetrable as one continues to read it so what you exactly wanted to do with the frame-dragging effect remained unknown to me ( and i guess that not only me ) . frame-dragging is a particular new gravitational effect that occurs in the gravitational field induced by rotating bodies .
i think you just forgot that the $\int_a^b f\ , dl$ is not a scalar expression . rather it should be written in a form $\int_a^b \vec{f}\cdot d\vec{l}$ . then it comes to the sign of the scalar product : $$\vec{f}\cdot d\vec{l}=f\ , dl\ , \cos\theta$$ where the angle $\theta$ is taken between the vector $\vec{f}$ and the direction of the tangent to the integration path from $a$ to $b$ . then , in your first example , $w_{a \to b}=\int_{r_a}^{r_b} f ( r ) dr = \int_{r_a}^{r_b} \left ( -\frac{gmm}{r^2} \right ) dr$ the path could go with any slope , but the gravity is always directed downwards , along the $r$ axis . that means , we can always take $ ( \pi-\theta ) $ as the angle between the vector $d\vec{l}$ and the $r$ axis , that is $$dl\ , \cos ( \pi-\theta ) =dr$$ but $\cos ( \pi-\theta ) =-\cos\theta$ and thus we have $$\vec{f}\cdot d\vec{l}=-f\ , dr=-\frac{gmm}{r^2}dr$$ for your second example : . . . we also should change the sign , because the gravitational force is always a force of attraction . what the authors actually mean is that : the coulomb 's and newton 's forces have exactly the same expressions , but the sign conventions for them are different . the newton 's force is defined that if all the quantities ( $m$ , $m$ and $r$ ) are positive , then the vector of the force is directed towards the other body . but for the coulomb 's force , if all the quantities ( $q_1$ , $q_2$ and $r$ ) are positive , then the vector of the force is directed away from the other charge . that becomes manifest if we take the vector expressions for these forces : $$\vec{f}_n=-\frac{gmm\ , \vec{r}}{r^3}\qquad\vec{f}_c=\frac{q_1q_2\ , \vec{r}}{4\pi\epsilon_0\ , r^3}$$ now the different signs are clearly seen . " . . . from point $a$ to point $b$ . . . " - . . . as i understand it - the work that i must do is always $u_b-u_a$ . however the work that the force that is being created by the field do_es_ is always $u_a-u_b$ , am i correct ? yes this is correct . the mnemonic rule is very simple : $u$ is like the height of the slope . when you go up , $u_b&gt ; u_a$ , and it is you who does the work . but when you go down , $u_a&gt ; u_b$ , and it is the field force who does the work .
quoting from my copy of the 2nd edition of jackson 's book on classical electrodynamics , section 1.2: assume that the force varies as $1/r^{2+\epsilon}$ and quote a value or limit for $\epsilon$ . [ . . . ] the original experiment with concentric spheres by cavendish in 1772 gave an upper limit on $\epsilon$ of $\left| \epsilon \right| \le 0.02$ . followed a bit later by williams , fakker , and hill [ . . . gave ] a limit of $\epsilon \le ( 2.7 \pm 3.1 ) \times 10^{-16}$ . that book was first published in 1975 , so presumably there has been some progress in the mean time .
no , op 's calculation is correct . in more detail , the paper states on page 323 ( apparently assuming that $\mu$ and $\nu$ are real numbers ) , that the result is $$ \mu^2 + 2 \nu^2 ~=~ 2\mu^2 -1~ . $$ the first expression is correct , and corresponds to op 's $3\mu^2-2$ . the second expression is wrong . in other words , the paper makes a mistake in the very last step while reducing with $\nu^2=\mu^2-1$ .
in short , the answer goes like this : all fermion masses are assumed to unify at some high scale ( e . g . ~$10^{16} gev$ ) in msugra . so , the mass differences between them at low energies are due to the running of the masses from that high scale down to the observed scale ( e . g . ~1tev at the lhc ) . the $\beta$ function for the stop mass has a positive contribution due to the top yukawa coupling , $y_t$ , which is large due to the fact that the top mass is not small compared to the higgs vacuum expectation value , $v$ , ( $m_t = y_tv$ ) . this implies that the stop mass drops more rapidly as the renormalization scale is lowered than the 1st and 2nd generation squarks , which have negligible yukawa couplings . this leads to a mass spectrum where the stop is light and the first two generation squarks are nearly degenerate ( ditto for the stau ) . the supersymmetry primer by martin has a good discussion of this ( http://arxiv.org/abs/hep-ph/9709356 ) , particularly on pg . 46 ( where you can find the $\beta$ functions ) and 75 ( which discusses the squark and slepton spectrum ) .
as a matter of fact one could also discuss commutation relations at different time : $$ [ \phi ( x ) , \phi ( y ) ] = i e ( x , y ) i\quad ( 1 ) $$ for free fields $e$ is the so-called causal propagator or advanced minus retarded fundamental solution that depends on the free field equation satisfied by $\phi$ . the point is that , passing to considering interacting fields , at least formally , equal time commutation relations remain unchanged with respect to the free case , whereas the corresponding of ( 1 ) changes into an , in practice , unknown form as they include the full dynamics . actually even this idea does not work completely , as interacting fields $\phi$ are affected by a renormalization constant $z^{1/2}$: $$\phi ( t , \vec x ) \to z^{1/2} \phi ( t , \vec x ) \quad \mbox{in weak sense as } t \to \pm \infty$$ and , dealing with naively with renormalization procedure , it arises $z=0$ . so canonical commutation relations seem to be untenable for fields $\phi ( x ) , \partial_t \phi ( y ) = \pi ( y ) $ . however all that sounds a bit academic as the renormalization procedure , in a sense , solves the problem . i would like to stress that the fact that commutation relations are taken at equal time is not in contradiction with relativistic invariance : covariance ( i.e. . , the use of tensors and taking space and time on the same footing ) is just one way to make explicit relativistic invariance , but it is by no means the unique one ! hamiltonian formalism is not covariant , though it is relativistically invariant : all equations ( including ccr ) take the same form in every inertial reference frame .
the electric field from your potential is : $$e ( r ) = {2\over r^3}$$ using gauss 's law , the total charge in a sphere of radius r is : $$q ( r ) = \oint e \cdot ds = 4\pi r^2 {2\over r^3} = {8\pi\over r}$$ the total charge is decreasing with r , so there is a negative charge cloud of density $$ \rho ( r ) = {1\over 4\pi r^2} {dq\over dr} = - {4\over r^4}$$ but the total charge at infinity is zero , so there is a positive charge at the origin , cancelling the negative charge cloud , of a divergent magnitude . if you assume this charge is a sphere of infinitesimal radius $\epsilon$ , the positive charge at the origin is $$q_0 = \int_\epsilon^\infty 4\pi r^2 {4\over r^4} = {16\pi \over \epsilon}$$ this is not a distribution in the mathematical sense , but it is certainly ok to work with , so long as you keep the $\epsilon$ around and take the limit $\epsilon$ goes to zero at the end of the day . mathematicians have not had the last word on the class of appropriate generalized solutions yet .
you are absolutely right . in fact , the uncertainty relations are a direct consequence of this wave nature and the " conjugated " variables being $p \leftrightarrow x$ and $e \leftrightarrow t$ . i will attempt a simple explanation . remember that the wavefunction gives you a probability amplitude for finding the particle at some location $x$ at some time $t$ ? the probability to find the particle at some point $x$ at time $t$ is given by $|\psi ( x , t ) |^2$ . now here 's the catch : if you put your plane wave into that equation , you will find that $|\psi ( x , t ) |^2 = a$ , independent of $x$ and $t$ . so for your plane wave , the particle has equal probability to be anywhere . in terms of uncertainty , then , we could say that the uncertainty in position is at a maximum , and so is the uncertainty in time . what about momentum and energy ? well , your plane wave has a definite momentum of $p = \hbar k$ and a definite energy of $e = \hbar \omega$ , so their uncertainty is zero . this is an extreme case of the uncertainty relation : certainty in one variable means maximum uncertainty in the other variable . any other possible form of the wave function can always be written as a sum of plane waves ( that is the fourier theorem ) : $$\psi ( x , t ) = \int de \int dk a ( k , e ) \exp ( i ( kx-e/\hbar t ) $$ you can think of $\psi ( x , t ) $ as the wavefunction in position and time domain and $a ( k , e ) $ as the wavefunction in momentum and frequency domain , and the conversion between those two is achieved via the fourier transform . you can now mathematically show how if $\psi ( x , t ) $ is " narrow " ( i.e. . position has low uncertainty ) then $a ( k , e ) $ must be broad and vice versa . the intuition behind that : you need to add up a lot of plain waves with lots of different momenta to go from the broad plain wave distribution to a narrow wave packet , but this then implies that you have high uncertainty in momentum .
ok , area $a$ is just $\pi d^2/4$ . the real question is : what is $c$ ? it depends on the shape of the orifice ( and reynolds number ) . there are some quick-and-dirty approximations here . it depends on the orifice geometry , like whether its edges are rounded or sharp .
the moon orbits around the sun , but so does the earth . they orbit together with the moon 's orbit perturbed by the nearby earth . if fact , despite their different masses they experience the same acceleration , so it should not be surprising that they are bound to the same orbit since they are bound to each other ( i.e. . at basically the same distance from the sun ) . the moon experiences motion relative to the earth and is bound to it by the earth 's gravity , and once bound , unless the tidal forces due to the sun pull them apart , they will stay bound together - accelerating towards the sun at the same rate , as essentially one object . this is the key : outside of the hill sphere the difference in gravitational force ( the so-called " tidal force" ) is great enough to break the gravitational binding . you may be interested to know that the earth-moon roche limit vis-à-vis the sun is about 33.6 million kilometers and that the earth is roughly 150 million kilometers from the sun . so we are quite safe from the danger of having our moon stolen .
it is not that the earth is expanding ; instead the primary effect is that the earth 's surface is shrinking . the effect occurs because the earth 's surface does not remain flat . instead , it gets tilted and folded . some parts of the crust get subducted ; once they disappear , what is left appears smaller . over any appreciable distance , rock has good compressive strength but negligible tensile strength . consequently , when rock is pushed together it becomes thicker and taller ( thereby decreasing the surface area ; see the demonstration here : http://en.wikipedia.org/wiki/mountain_building ) . but when rock is pulled apart it instead tends to form cracks that are obviously new crust ( and so are not counted when trying to determine if the earth 's surface is expanding ) . the overall effect is that the size of the older portion of the earth 's surface is smaller than the actual current surface of the earth but this is only an effect of standard geology .
when you write the five dimensional kaluza-klein metric tensor as $$ g_{mn} = \left ( \begin{array}{cc} g_{\mu\nu} and g_{\mu 5} \\ g_{5\nu} and g_{55}\\ \end{array} \right ) $$ where $g_{\mu\nu}$ corresponds to the ordinary four dimensional metric and $ g_{\mu 5}$ is the ordinary four dimensional vector potetial , $g_{55}$ appears as an additional scalar field . this new scalar field , called a dilaton field , is physically meaningful , since it defines the size of the 5th additional dimension in kaluza-klein theory . they are natural in every theory that hase compactified dimensions . even though such fields have up to now not been experimentally confirmed it is wrong to call such a field " unphysical " . " unphysical " are in some cases fields introduced to rewrite the transformation determinant in calculations of certain generating functionals , or the additional fields needed to make an action local , which may have conversely to such dilaton field , no well defined physical meaning .
you are not " replacing a mathematical proof " . what the statements you are referring to mean is that in tensor notation , the proof is immediate , so that nothing needs to be written down . this is because if you have a tensor equation as above , in order to prove lorentz invariance , do a lorentz transformation and go to another set of coordinates $x^{\mu'}$ . then using the usual transformation laws we get that ${\partial_{\mu}} = \lambda^{\mu'}_{\mu}\partial_{\mu'}$ and $f_{\mu\nu} = \lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}f_{\mu'\nu'} $ , we can write the maxwell equation in terms of the new coordinates to become $\lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}\lambda^{\sigma'}_{\sigma} ( f_{\mu'\nu ' , \sigma'}+f_{\nu'\sigma ' , \mu'}+f_{\sigma'\mu ' , \nu'} ) =0 . $ however , this can only hold if the thing inside the brackets is zero itself . namely maxwell 's equation in the primed coordinate system also holds . more succintly , what a " tensor equation " means is that there was nothing special about the coordinate system in which the equations were derived . you could have equally well chosen another system and derived the same equations . thus invariance under coordinate change is immediate .
the answer is yes . what you describe is the quantum mechanical picture behind the elastic rayleigh scattering . for the actual calculation , see r . loudon , the quantum theory of light , chapter 8.8 .
i recently re-derived these equations with all the dimensionful constants in place . your last statement in the " edit " is correct : $t_{00} = \rho_{e}\ , c^{2} = \rho\ , c^{4}$ . it is easy to lose track of factors of $c$ in calculations like this ; the usual culprit is mixing up $t$ and $x^{0} = c\ , t$ , and $\partial_t$ and $\partial_0 = c^{-1}\ , \partial_{t}$ . for instance , $g_{tt} = c^{2}\ , g_{00}$ .
it is not wrong , all three normalization conditions are natural and they do not contradict each other because , in fact , the first equation is nothing else than the product of the following two equations ! just substitute your formula for $\psi$ , $\psi = r y$ , to the first equation . the only mistake you have to fix to show that the first equation becomes the product of the other two is to replace the wrong ( and really nonsensical , one can not integrate over same $r$ " twice " in one integral ) $drdr$ in the first equation by the correct $r^2 dr$ . the first integral splits to the product because some factors in the integrand only depend on the radial coordinate $r$ so they can be taken out of the integral and integrated over $r$ separately while the remaining factors only depend on the angles $\theta , \phi$ . as kyle said , the general technique is called the " separation of variables " .
a matter distribution with a sinsuoidally varying monopole or dipole moment will only produce variations in the gravitational field within the matter distribution . if the quadrupole ( or a higher multipole moment ) varies sinsuoidally , you will produce gravitational waves in a way very analogous to how electromagnetic waves are produced , with the amplitude differing by just a few numeric constants . i would not expect the back-reaction of the wave to amplify the variation that created the wave in the first place , though . is this what you mean by ' a resonance ' ?
so you have two rigid bodies in contact to the ground . let us call $n_a$ and $n_b$ the contact force ( normal to ground ) and $f_a$ , $f_b$ the frictional force at a and b ( arbitrarily chosen to act in a positive x direction ) . we can call the pin forces $p_x$ and $p_y$ acting on the bar ( an reacting on the disk ) . the sum of moments about the disk center is $$ r f_a + c = i \ddot{\theta} $$ and about the bar center $$ \frac{\ell}{2} \cos\theta ( n_b-p_y ) - \frac{\ell}{2} \sin\theta ( n_b+p_x ) = 0 $$ couple with the motion in the x and y axes of the disk $$ m \ddot{x} = m r \ddot{\theta} = f_a -p_x \\ m \ddot{y} = n_a -p_y - m g = 0 $$ and the bar $$ m \ddot{x} = f_b + p_x \\ 0 = n_b+p_y - m g $$ for the condition of motionless with $\ddot{x}=\ddot{\theta}=0$ the above is 6 equations for 6 unknowns ( $p_x , p_y , n_a , n_b , f_a , f_b$ ) which is solved by elimination . next find which is the smallest $c$ value that makes $|f_a| = \mu_s n_a$ and $|f_b| = \mu_s n_b$ if you get the correct result you will have $p_x&lt ; 0$ and $f_a&lt ; 0$ and $c \propto \mu_s r m g $
the curvature radius is increasing at a rate of the speed of light . to see how fast a comoving observer is receding from you ( assuming you are a comoving observer as well ) , you need the hubble parameter and the observer 's distance . $$ v=h\cdot d $$ $$ h = \dot r/r $$ you can see that everything past the hubble radius $d_h = c/h$ recedes faster than the speed of light , independent of the content of the universe . why do not you read up on it , there is an excellent paper by davis and lineweaver about common misconceptions .
i am not sure what you mean by ' double contraction ' , but the ricci tensor in local coordinates is given by \begin{align} r_{\mu \nu} = r^\rho_{~~\mu \rho \nu} , \end{align} which is the same as $g^{\sigma \rho} r_{\sigma \mu \rho \nu}$ , exactly what you have written .
i have found an explanation . at the end of section 22.2 , working with euclidean path integrals , weinberg shows that $$-\frac{1}{32\pi^2} \int d^4x \epsilon^e_{ijkl}f_{\alpha ij} f_{\beta kl} \operatorname{tr} ( t_\alpha t_\beta ) = n_+ - n_-$$ where $t_\alpha$ are gauge group generators and $n_\pm$ is the number of zero modes of the gauge covariant dirac operator $i\gamma^\mu d_\mu$ with positive respectively negative chirality . this is the atiyah-singer index theorem at work according to weinberg . ( i have heard of this theorem before but i am not familiar with it . ) as a consequence of this , in the non-trivial sectors where the $\theta$ term is non-zero , there exists at least one zero mode . a massless field enters the path integral as $\overline\psi \gamma^\mu d_\mu \psi$ and it can be integrated out $$\mathcal z =\int d [ a_\mu , \ldots ] \det ( i\gamma^\mu d_\mu ) \exp ( s_\text{yang-mills} [ a_\mu ] + s_1 [ \ldots ] ) $$ where the dots and $s_1$ stand for other fields and their actions , and the determinant of the operator and not its inverse appears , since $\psi$ is grassmann . but if $\gamma^\mu d_\mu$ has a zero mode in every non-trivial sector . . . the determinant is 0 in all the non-trivial sectors . this is effectively the same as discarding the $\theta$-term ! ( weinberg 's proof uses that $i\gamma^\mu d_\mu$ and $\gamma_5$ anti-commute . since this is not the case for the bilinear $i\gamma^\mu d_\mu + m$ appearing for a massive field , we do not prove " too much " and rule out $p$ violations when all fermions are massive . ) i found the zero-mode argument in chapter 94 of srednicki 's book . srednicki argues that for physical reasons $i\gamma^\mu d_\mu$ must have a zero mode ; in weinberg 's book one finds the mathematical proof that this is the case .
i have just found out that it is fundamentally wrong to approximate the fermi function with chebyshev polynomials . representation of the fermi function on the real axis by chebychev polynomials might be okay , but the representation of the fermi function in the complex plane and especially close to the poles of the fermi function is certainly very poor ( no pole versus pole ) . by applying jordan 's lemma to this poor representation we miss the most important ingredients , i.e. the poles , will certainly result in a poor result .
antiparticles naturally arise when studying the dirac equation within quantum field theory . recall that we may expand a dirac spinor field as a plane wave , namely , $$\psi= \sum_{s=1}^2 \int \frac{\mathrm{d}^3 p}{ ( 2\pi ) ^3} \frac{1}{\sqrt{2e_{p}}} \left [ b^s_p u^s ( p ) e^{ipx}+c^{s\dagger}_p v^s ( p ) e^{-ipx}\right ] $$ and similarly for the conjugate field . notice the appearance of two distinct creation and annihilation operators ; these give rise to the electron and positron , the antiparticle . the dirac spinor transforms under a representation of the double cover of $sl ( 2 , \mathbb{c} ) $ which is a reducible representation . hence we may propose a decomposition or ansatz , $$\psi=u ( p ) e^{-ipx}$$ where $u ( p ) $ is a four-component dirac spinor which may be broken down into a set of two-component spinors known as weyl spinors ( and with a reality condition , majorana spinors ) : $$u ( p ) =\left ( \begin{array}{c} \sqrt{p\cdot \sigma}\ , \xi\\ \sqrt{p \cdot \sigma}\ , \xi\\ \end{array} \right ) $$ for $\xi^{\dagger}\xi=1$ . the antiparticle , a positron , corresponds to a negative frequency solution , namely , $$v ( p ) =\left ( \begin{array}{c} \sqrt{p\cdot \sigma}\ , \eta\\ \sqrt{p \cdot \sigma}\ , \eta\\ \end{array} \right ) $$ where $\psi=v ( p ) e^{+ipx}$ instead . notice both solutions have positive energy , as $$e=\int \mathrm{d}^3 x \ , t^{00}=\int \mathrm{d}^3 x \ , \bar{\psi} ( m-\gamma^i \partial_i ) \psi \geq 0$$ ( the above expression is obtained by applying noether 's theorem to the spacetime translation symmetry giving rise to energy-momentum tensor . ) both the electron and positron are fermions , obey the same quantum field theory , and satisfy fermi-dirac statistics which - roughly - dictate we quantize the theory using anti-commutation relations rather than commutation relations , otherwise we would obtain a hamiltonian unbounded from below .
there are several magazines and websites for the amateur astronomer that include calendars of astronomical events and viewing charts , etc . you might want to start at sea and sky 's astronomy calendar of celestial events for calendar year 2012 or sky and telescope 's this week 's sky at a glance .
most of the astronomy images we find online have some color modification how close to the false color images would they be this is a common misconception , that the pictures you see of galaxies and nebulae are necessarily " false color " , " modified color " , or " photoshopped " . some of them , yes . but a lot of them are quite simply true color , but taken with a sensor ( cmos , ccd ) that does not suffer from the limitations of the human eye . e.g. look at this image of the horsehead nebula : all that color is real . it is there , in the photons reaching you . but your eye cannot see it . a cmos , however , can . this is not " false color " , although saturation was likely increased in post-processing , in addition to what the sensor can do . but the hues are probably real ( e . g . , the red you see in the image , or the blue , was present in the photons hitting the sensor - albeit at a lower saturation level ) . ( an astute observer may object that the eye and the cmos do not see the exact same hues , but let 's not go down that rathole now . ) " false color " means when the image shows green where the cmos ( or the human eye , if luminosity was higher ) would see red , or something like that . this is not always the case with images of nebulae and galaxies ; in fact , if the image was taken with visible light , chances are the hues are preserved . proper false color images are those taken in uv or ir , and then artificially converted to visible light . this is an example of it , the sun in ultraviolet : now , to answer your question : unfortunately , even from a near distance , most of these objects will not look much better . they are , after all , faint , rarefied clouds of dust and gas . they are just not bright enough for the human eye to see color . there are few exceptions . a notable one would be close binary systems where the components are stars of very different temperatures . kind of like albireo , but much closer . from a starship , looking at the two stars orbiting each other , you had see very clearly a striking color difference - perhaps a large , somewhat dim , deep red star , and a blinding , crisp dot of bluish white light , the smaller and more active companion . the views from the center of a globular cluster undergoing a compression phase should be pretty spectacular , too . night would never be dark on a planet in the middle of the cluster .
your map fails to be completely positive . if you apply it to half of a maximally entangled state $ ( |0\rangle|0\rangle+|1\rangle|1\rangle ) /\sqrt{2}$ , you can easily see that $\phi ( \rho ) =\rho$ and $\phi ( \rho' ) =\rho'$ imply that $\phi ( |0\rangle\langle1| ) = \alpha |0\rangle\langle1|$ and $\phi ( |1\rangle\langle0| ) = \alpha^* |1\rangle\langle0|$ for the resulting state to be positive ( with $|\alpha|\le1$ ) . however , this is incompatible with the last condition .
let us compare thermal equilibrium radiation you have described and the black body radiation . take an object b at thermal equilibrium ( not necessarily a black body ) , and denote with $\varepsilon ( \lambda ) $ the fraction of radiation of wavelength $\lambda$ it absorbs . thus if $a ( \lambda ) $ of radiation of the wavelength $\lambda$ falls on b , it absorbs $\varepsilon ( \lambda ) a ( \lambda ) $ and reflects the rest . imagine a box with a small hole you have just described , and a close this hole with b : assume , that b has the same temperature , as the radiation in the box ( or , rather , take a box with the same temperature as b ) . then the system is at thermal equilibrium . thus absorbing $\varepsilon ( \lambda ) a_0 ( \lambda ) $ it has to emit the same amount , i.e. $\varepsilon ( \lambda ) a_0 ( \lambda ) $ ( here $a_0 ( \lambda ) $ is the thermal equilibrium radiation you have described and computed in class : radiation , going from the hole ) . object is indeed called a black body if it absorbs all the radiation , falling on it , i.e. if $\varepsilon ( \lambda ) =1$ . from the " thermal equilibrium " argument above you see , that it emits $\varepsilon ( \lambda ) a ( \lambda ) =a_0 ( \lambda ) $ , i.e. black body radiation coincides with thermal equilibrium radiation and we can now call $a_0 ( \lambda ) $ by the words " thermal radiation " . as a bonus , if the body b is not black , it still has to emit $\varepsilon ( \lambda ) a_0 ( \lambda ) $ . this is called " kirchhoff 's law of thermal radiation " .
note , that , on the third line , the $\beta$ indice of $\sigma^{\mu\nu}$ and the $\dot \beta$ indice of $\tilde \sigma^{\mu\nu}$ must be raised for indice coherence . same error for the following lines . the formula you want to demonstrate is certainly false . take $\beta = \nu = 0$ , and noting that $\sigma^{\alpha0}= -\frac{1}{2}\sigma^\alpha , \tilde \sigma^{\mu0}= \frac{1}{2}\sigma^\mu $ , if the formula was exact , it would imply ( with $g_{11}=g_{22}=g_{33}$ ) , and in short tensorial notation : $-\frac{1}{4} \vec \sigma \otimes \vec \sigma = 0\tag{1}$ which is obviously false .
the radiation from an ultrarelativistic ( $v \approx c$ ) particle on a circular path is called synchotron radiation . the total power radiated from such a particle is $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c}\gamma^4$$ where $a$ is the acceleration and $\gamma$ is the lorentz factor , $\gamma^2 = 1/ ( 1-v^2/c^2 ) $ .
the spin-statistics thing is not a problem , it is a theorem ( a demonstrably valid proposition ) , and it should not be addressed , it should be understood and celebrated . the higgs field gives us interactions between chiral fermions and the higgs , $yh\cdot \chi_\alpha\eta^\alpha$ which produces mass terms $m \chi_\alpha\eta^\alpha$ if the higgs field has a vacuum expectation value $h=v$ . we have $m=yv$ . however , in more general quantum field theories , one may write down similar ( bilinear ) mass terms manually without any higgs field ( and without cubic yukawa terms ) and none of these issues affects the fact that the spin-statistics relationship holds . the field $h$ is spin-zero field and bosonic , the fields $\chi , \eta , \psi$ ( the latter is the 4-component dirac spinor combining the previous two ) are spin-1/2 and fermionic fields .
according to quantum mechanics , every particle has a wavefunction which completely describes it . the behavior of the particle , including its time evolution and the distribution of outcomes to any measurement performed on the particle , is determined by its wavefunction ( edit : michael brown correctly notes in the comments that if the particle is part of a bigger composite system , we often cannot use a wavefunction to describe it , and we need to use a density matrix instead ) . some properties of these quantum particles are similar to properties of classical particles . some properties are similar to classical waves . the coexistence of properties of both kinds is what some people describe as " wave-particle duality " ( i have to say that personally i dislike this term ) . a composite system , like an atom which is composed of a nucleus and electrons , also has a wavefunction ( or density matrix ) , and therefore also has both kinds of properties ( particle-like and wave-like ) , and therefore can also be said to have " wave-particle duality " . the question of whether a certain elementary particle is indeed elementary or is composed of smaller particles is interesting , but unrelated . as of now mainstream physics considers electrons elementary particles , but of course we have no way of knowing whether or not this will change in the future . edit : to answer your last question - yes , in principle you can take any set of particles and consider them as one system . the only question is whether it is useful to do so . for example the nucleus and the electrons of an atom are often easy to treat as one system - an atom . you can in principle consider , like you suggest in your question , only part of the quarks in the nucleus together with the electrons as one system , but you are not likely to achieve any useful insight by doing that . the rest of the quarks in the atom interact with the quarks in your system very strongly , and any meaningful description will need to include them .
what is colloquially called ''empty space'' is not really empty - it is filled by the electromagnetic field and the gravitational field ; it is called empty only because it does not contain ( nonzero ) matter fields . the electromagnetic is the medium that carries electromagnetic waves , such as the air density field ( colloquially just called ''air'' ) carries sound waves and the water density field ( colloquially just called ''water'' ) carries water waves . indeed , electromagnetic waves are nothing else than propagating high-frequency oscillations in the electric fields , in precisely the same way as sound waves are propagating ohigh-frequency scillations in the pressure field of air ( or any other mechanical medium ) , and water waves are propagating low frequency oscillations in the mass density field of water .
no , it does not have to be numbered unless , like @dmckee comments , you have received specific instructions to do the numbering . i think your question can best be split up in 2 parts for the answer : writing a report and writing a research paper . writing a report for a report the main goal is typically to show exactly what you have done and how you have done it . quite often the report will be used by other students to continue your work . for this purpose it is convenient to have a numbered list for the methods , because it immediately attracts attention and makes it easy to follow the ' recipe ' . writing a research paper when you write a research paper your goal is in general to resolve a particular issue that exists in the scientific community . your focus will be on the research question and the conclusions you can draw from the experiments/simulations that you did . in this case a numbered list for the method will draw way too much attention to it , much more than it deserves . it is , after all , easy to spot because it disrupts the flow of the paper . in some journals it is not even allowed to use numbered lists if they are not inline ( i.e. . 1 ) . . . 2 ) . . . ) . so in conclusion , if you do not have specific instructions from someone ' higher up': use the numbered list if you want to focus on the method , use the paragraph if you want to focus on a different part of the report/paper
wikipedia says in electromagnetism , absolute permittivity is the measure of the resistance that is encountered when forming an electric field in a medium . in other words , permittivity is a measure of how an electric field affects , and is affected by , a dielectric medium . the permittivity of a medium describes how much electric field ( more correctly , flux ) is ' generated ' per unit charge in that medium . it is clear that more the permittivity of a matter , more resistance is offered to the electric field resulting in decrease if the flux in gauss law . in other words flux in inversely proportional to permittivity . you can visualize it as , suppose 10 magnetic field line are produced due to the charge but due to the dielectric matter the number of field lines passing through the gaussian surface decrease and the decrease is proportional it is permittivity . you can visualize entire electrostatics on electric field lines , this will help you a lot to grab the concepts .
the most general lorentz transformation that is connected to the identity is given by the conjugation by $\exp ( -a ) $ where $$ a = \frac 12 \omega_{\mu\nu} \gamma^\mu \gamma^\nu $$ and $\omega_{\mu\nu}$ is an antisymmetric tensor containing $d ( d-1 ) /2$ parameters . the group of all such transformations is isomorphic to $spin ( d-1,1 ) $ . if $\omega$ only contains one component $0\mu$ , then it is a boost , and the nonzero numerical value of $\omega$ is the rapidity - the " hyperbolic angle " $\eta$ such that $v/c=\tanh\eta$ . if only one doubly spatial component of $\omega$ is nonzero , then this component $\omega_{\mu\nu} = -\omega_{\nu\mu}$ is obviously the angle itself . note that the spatial-spatial terms in $a$ are anti-hermitean , producing unitary transformations ; the mixed temporal-spatial terms in $a$ are hermitean and they do not product unitary transformations on the 4-component space of spinors ( but they become unitary if they are promoted to transformations of the full hilbert space of quantum field theory ) . in 4 dimensions , a general antisymmetric matrix $4\times 4$ contains 6 independent parameters and has eigenvalues $\pm i a , \pm i b$ , so in 3+1 dimensions , one can always represent a general lorentz transformation as a rotation around an axis in the 4-dimensional space followed by a boost in the complementary transverse 2-plane . this is the counterpart of the statement that any $su ( 2 ) $ rotations in 3 dimensions is a rotation around a particular axis by an angle . if you allowed $a$ to contain something else than $\gamma^{\mu\nu}$ matrices which generate the lorentz group , you could get other groups . only for a properly chosen subset of allowed values of $\omega$ , you would get a closed group from the resulting exponentials ( under multiplication ) . in particular , if you allowed $a$ to be an arbitrary complex combination of any products of gamma matrices , well , then you would allow $a$ to be any complex $4\times 4$ matrix , and its exponentials would produce the full group $gl ( 4 , c ) $ - surprising , carl ? ; - ) it is not a terribly useful groups in physics because actions are usually not invariant under this " full group " , are they ? also , there are not too many groups in between $spin ( 3,1 ) $ and $gl ( 4 , c ) $ - i guess that there is no proper group of $gl ( 4 , c ) $ that has a proper $spin ( 3,1 ) $ subgroup . obviously , there are many subgroups of $spin ( 3,1 ) $ - such as $spin ( 3 ) $ , $spin ( 1,1 ) \times spin ( 2 ) $ , and others .
the mistake is that the trasmitted qubit is entangled with the degrees of freedom of the internal pretender/observer , so that it is not coherent when it is presented to the quantum computer from the point of view of the computer . the quantum computer will fail independently of whether the observer is internal or external , because of lack of coherence in the qubit . in order to get the coherence back , the computer will have to undo the measurement by the observer , and recohere the external qubit . but it is a nice idea anyway .
how do i calculate $m\left ( ^{10}_4\mathrm{be}\right ) $ and $m\left ( ^{11}_5 \mathrm{b}\right ) $ ? the masses and various other properties of isotopes are available freely at wolfram alpha . they are , $m\left ( ^{10}_4\mathrm{be}\right ) =10.013533818u$ $m\left ( ^{11}_5 \mathrm{b}\right ) =11.009305406u$ where $u$ denotes unified atomic mass units . notice you are already given the mass number in the superscript of the isotope . as john rennie noted , the reaction should probably be with $^{9}_4\mathrm{be}$ , $$^{9}_4\mathrm{be} + ^{2}_1 \mathrm{h} \to ^{10}_{5}\mathrm{b} + n$$ in which case the mass is $9.012182201u$ .
the heat equation is an example of a convection-diffusion equation . your problem is one-dimensional in space ( only $x$ ) , which simplifies it a bit . the term on the left hand side is the time-rate of change of the internal energy $u$ ( often a multiple of the temperature ) . the second term is a diffusion term , as , in time , it diffuses or " smooths " peaks . the last term is a convective term , i.e. your medium is moving at constant velocity $v_0$ to the right .
the sine function can be taylor expanded to yield , $$\sin x = x-\frac{1}{6}x^3 +\mathcal{o} ( x^5 ) $$ hence the argument of the sine function must be dimensionless , otherwise one would be adding quantities of differing dimension as evinced by the taylor series . therefore , $$ [ k ] =\frac{1}{ [ x ] }=\mathrm{meter}^{-1}$$ to ensure the product $kx$ has no dimensions . similarly , $\omega$ in your equation has dimensions of frequency , or inverse time to ensure $\omega t$ is dimensionless .
once a comes into contact with the spring , a 's velocity relative to b will be the rate at which the spring is contracting . when the spring achieves its maximum contaction , the rate of contraction is zero , and thereby a 's velocity relative to b is also zero .
no . frequency is defined as 2π*θ/t where theta is the angle rotated for a time t . you maybe tempted to equate frequency to angular velocity . but it is not so . angular velocity = dθ/dt . angular frequency= 2*pi* ( integral of x over time interval t ) /t
in a $d$-dimensional euclidean space ( with positive definite norm ) , one has $$ \vec{\nabla} \cdot \frac{\vec{r}}{r^d} ~=~{\rm vol} ( s^{d-1} ) ~\delta^d ( \vec{r} ) , $$ cf . the divergence theorem and arguments involving either test functions and integration by part , or $\epsilon$-regularization , similar to methods applied in this phys . se answer . here ${\rm vol} ( s^{d-1} ) $ is the surface area of the $ ( d-1 ) $-dimensional units sphere $s^{d-1}$ . by similar arguments one may show that the identity $$ \vec{\nabla} ( r^{2-d} ) ~=~ ( 2-d ) \frac{\vec{r}}{r^d} , \qquad d\neq 2 , $$ contains no distributional contributions in $d$-dimensional euclidean space . for the related questions in minkowski space , one suggestion is to introduce an $\epsilon$-regularization in the euclidean formulation , and then perform a wick rotation , and at the end of the calculation , let $\epsilon\to 0^+$ .
heat of vaporization is related to enthalpy change , while dew point is related to free energy change , i.e. enthalpy plus entropy . that is why they are very different concerning relative humidity . the enthalpy of a gas is more-or-less independent of pressure or partial pressure , because gas molecules do not really interact with each other . at insanely-high pressures there would be some effect on enthalpy of course , but the effect at everyday pressures is very low . pressure mainly affects a gas via entropy not enthalpy . the enthalpy of a liquid is somewhat dependent on total pressure : a high pressure will push the molecules closer together and therefore change their interaction energies . but obviously the enthalpy of the liquid does not depend on what the gas partial pressures are , it can only depend on the liquid 's own total internal pressure . so the answer is : heat of vaporization , being related to enthalpy not entropy , has essentially no dependence on relative humidity . ( given a constant total air pressure ) -- update -- oops , whenever i wrote " enthalpy " i should have said " enthalpy per molecule " or " enthalpy per mole " [ "molar enthalpy" ] . you can check for yourself that the enthalpy per molecule of an ideal gas is independent of pressure or partial pressure . for a real-world gas , it is approximately independent . the " per mole " quantities are what matter for dew point etc .
your problem lies in assuming that $$ \nabla^2 = \frac{\partial^2}{\partial r^2} + \cdots $$ this is not the case , you need to use $$ \nabla^2 = \frac{1}{r^2}\frac{\partial}{\partial r}\left ( r^2\frac{\partial}{\partial r}\right ) +\cdots $$ then will you obtain the correct answer of $-1/2$ .
if you can measure the input energy ( sun 's spectrum ) and compare it to the reflected energy ~30% ( earth 's reflected spectrum ) you can compute the absorbed energy ~70% ( planet 's absorption ) . isolating that absorbed energy to just the surface requires some approximations and a lot of instrument calibration . the sun ( in space ) has a spectrum of a black body at ~5700 k . when this energy hits the atmosphere , some is reflected ( called albedo ) and some is absorbed and later radiated out again at a different frequency as heat ( measured by ir spectroradiometers ) . the atmosphere is quite cool compared to the surface of the earth and it does not have a lot of mass for heating directly , but it is good at catching the longer wave heat infrared ( ir ) . so when the surface of the earth is measured from space about 50% of the ir ( heat ) energy passes through the clouds and 50% is reflected back ( green house effect ) . by comparing the sun 's input , to the measured albedo they can correct for some of that effect . it can also be calibrated against sensors on the ground which show what amount of energy makes it through the atmosphere . notice that $h_2o$ , $o_3$ ( ozone ) , $co_2$ and $o_2$ absorb noticeable amounts of the radiated energy ( absorption bands ) . in this image the yellow represents the spectrum outside the atmosphere like in the image above , with the red showing what frequencies and power strength reach the surface at sea-level . note how $o_3$ absorbs the higher frequency ( shorter wave-length ) ultra-violet and keeps us from getting too sun burnt ? so the ir radiation from space can be measured and compared to the model and ground based sensors . this image is a broad-spectrum interpretation of what the " brightness " or power would look like if we could see in this frequency band . this is not a spectrum view but rather a view of the total amount of energy seen reflected using very wide ir vision . the wide vision makes seeing details difficult as you can see there are mostly big blobs of different strengths of reflected ir power . this is where nasa 's camera is much more powerful because of a combination of filters and frequency resolution they can increase the dynamic range of the image dramatically and see much more than blobs ( see the detail in the lut desert image below ) . where the current model which is always being refined by new information looks something like this . the specific instrument that nasa uses is called moderate resolution imaging spectroradiometer ( modis ) and there is an entire team to support calibration . the modis instrument has 36 different spectral bands ( groups of wavelengths ) , including some that detect thermal radiance , or the amount of infrared energy emitted by the land surface . since the two modis instruments scan the entire surface each day , they provide a complete picture of earthly temperatures and fill in the gaps between the world’s weather stations—particularly in extreme environments where temperatures are simply not measured . the spectral band separation for the 36 modis bands is based on a complex system of dichroic beamsplitters , focal plane masks , and individual bandpass filters . there are four co-registered focal planes separated by three dichroic beamsplitters to cover the vis ( 400 nm to 550 nm ) , nir ( 650 nm to 950 nm ) , swir ( 1200 nm to 2250 nm ) , mwir ( 3600 nm to 4600 nm ) and lwir ( 6500 nm to 14500 nm ) . linear detector arrays , one for each band , are covered by bandpass filters of dimensions as small as 1 mm by 7 mm . the modis requirements include relative bandwidths as small as 1% , with tight tolerances on band center , band width , edge slopes , and out-of-band specifications . -- ieee vis -- visual spectrum , nir - near infrared , swir - short wave infrared , mwir - medium wave infrared , lwir - long wave infrared . the advantage of the modis sensor is most of the atmospheric temperatures are much lower than the surface of the earth . so using narrower ir filters and comparing different bands the near direct temperature of the earth 's skin can be measured ( assuming proper calibration ) . using this information directly they can make daily direct measurements . for example the lut desert was found to often be the hottest spot on the planet : the lut desert has an areal extent of about 80,000 km and contains several odd geomorphic distinctions making it a unique place . large areas of the lut desert regularly exceed 65.0°c , and the hottest spot on earth was detected in the lut five out of seven years . ir imaging in general is often used in industrial applications to find irregular sources of heat . it can be used to see objects in great detail where each pixel acts as a tiny thermometer allowing you to measure the temperature of every pixel on an image . this is how the modis works , but it is much more complicated . here is an example from a flir handheld device illustrating the concept with the left image in black and white with the reference spot highlighted , compared to the right image in ir with the same spot highlighted .
for simplicity of notation say $p = \frac{x - n}{x + n}$ given $\delta x$ is the uncertainty in x and $\delta n$ is the uncertainty in n then $\delta ( x - n ) $ = $\delta ( x + n ) = \sqrt {\delta ^2x + \delta ^2n}$ and therefore : $\delta p = p \sqrt{ ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x - n} ) ^2 + ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x + n} ) ^2}$ this is based upon equations 1b and 2b of the following reference : http://www.rit.edu/~w-uphysi/uncertainties/uncertaintiespart2.html
suppose you have a constant angle $\theta$ slope in the original frame ( we suppose that the transitions from horizontal movements to the slope is quasi-instantaneous ) . call $x$ and $x'$ the horizontal displacements in the original and moving frame . call $t$ the total time for going to $z=h$ to $z=0$ . then you have $x'= x- v_0 t$ with $ x= h \cot \theta$ , the angle of the slope in the moving frame is given by : $$h \cot \theta ' = h \cot \theta - v_o t \tag{1}$$ the coordinates of the normal force ( in the original and moving frames ) are $ \vec n = mg ( - \sin \theta , \cos \theta ) $ . the unit displacement vector , in the moving frame , is $\vec n = ( - \cos \theta ' , - \sin \theta' ) $ the total supplementary work is : $$w_{supp} = \vec n . \vec n \quad \dfrac{h}{\sin \theta'} = mgh\ , ( \sin\theta \cot \theta ' - cos \theta ) \tag{2}$$ using $ ( 1 ) $ , we get : $$w_{supp} = -mg v_0 t \tag{3}$$ the work due to the gravity force is : $w = ( mg \sin \theta' ) \dfrac{h}{\sin \theta'} = mgh$ so , finally , in the moving frame , we have : $$ mgh - mgv_0t = \frac{1}{2} m ( \delta v ) ^2\tag{4}$$ however , $gt$ is nothing else than $\delta v$ . the work , in the lhs of the above equation , does not depend on the slope , so we may imagine a quasi-instantaneous $90$ degrees turn from horizontal to vertical , then a quasi-vertical slope , followed by a quasi-instantaneous $90$ degrees turn from vertical to horizontal . during the quasi-instantaneous turns , the modulus of the speed is conserved ( because energy is conserved and no work is done ) . so , considering a vertical movement , it is obvious that $\delta v= gt$ so finally you have , skipping the overall $m$ factor : $$ gh - v_0 \delta v = \frac{1}{2} ( \delta v ) ^2\tag{5}$$
the differential $dp ( x ) $ is the probability of finding the body in an interval of length $dx$ centered at $x$ . the quantity $p$ you are looking for is the cumulative distribution function , $$p ( x ) =\int_{-\infty}^x \frac{dp}{dx} ( x ) dx , $$ which is the probability that the particle will be to the left of the point $x$ . since the particle cannot be to the left of $-a$ you can fix $c$ by requiring that $p ( -a ) =0$ . this will then give $p ( 0 ) =1/2$ as expected . it is just a matter of being precise as to exactly what you are calculating .
first of all , this is just a change of basis , which is up to us to make . furthermore we should always choose a basis that makes our calculations easier , and hopefully makes things more intuitive . for a simpler example - just try finding the volume of a sphere in cartesian coordinates , its just a bad choice . second of all , you do not have to use a fourier basis , to my knowledge everything -loops renormalization etc can be done in a position basis . now as to why the fourier basis is a convenient choice : ( 1 ) it simplifies derivative terms in the lagrangian - as usual the fourier basis turns derivative expressions into algebraic ones , which are much easier to manipulate . ( 2 ) it it more intuitive - written in terms of a fourier basis the feynman rules are in terms of momentum . so for example at the vertices momentum is conserved - its just a nice tidy way to think about whats happening at the vertex . ( 3 ) even if you start in position space , one method for doing the integrals you will encounter when writing for your loop expressions will be going to momentum space - so you sort of cut this step out from the outset . ( 4 ) ( following up on vibert 's comment ) plane waves are the basis we do the experiment in . that is , we send in wave packets highly localized in p space , i.e. this is the exact solution we perturb around .
your voice , like any sound , is a combination of many frequencies . physically , your voice consists of pressure waves . if we plot the pressure as a function of time , we see that it goes up and down in a way that looks somewhat random . you can measure these pressure waves with a microphone , then visualize them with an oscilloscope . here 's a youtube video where they do this , starting 4:50 into the video . you may be able to do this at home using the microphone on your computer and some software like audacity . the data collected by your microphone is a time series . the pressure is a function of time . if you sang a pure note ( or a reasonable approximation thereof ) , like you hear from an electronic tuner , the pressure would just be a sine wave . you could imagine a more complicated sound that was two sine waves on top of each other . this could produce beats . as you add more and more frequencies , more and more complicated sounds become possible . it is a remarkable result that in fact any sound can be represented as a sum of infinitely-many sines waves of different periods added up on top of each other . this is fourier 's theorem . a human voice thus consists of many sine waves combined simultaneously . presumably , each individual voice has some special patterns to the way these frequencies are combined , assisting us in recognizing voices . however , speaker recognition is probably based on other information as well . i do not know too much about it , but you can check out the wikipedia article . we frequently try to isolate the different frequencies in a sound . this is done electronically through electronic filters . a crude example is " turning up the bass " - amplifying the low-frequency components of a sound . of course , a professional music studio has far more sophisticated control of the various frequencies . this control can also be mimicked digitally through music sequencers . on a cruder level , you could simply talk directly into an open piano . the string in the piano will be excited by your voice . the strings each have a specific frequency , so the strings that are excited the most tell you that their particular frequency is present the most in your voice . your ear accomplishes a similar task . the cochlea has many small hairs , similar to piano strings , which are tuned to different frequencies . when they vibrate , they mechanically trigger an ion channel to open , beginning an action potential that is eventually interpreted as sound by your brain . so , in essence , you are distinguishing the various frequencies in people 's voices already .
up and anti-up . or down and anti-down . funny thing is , both of those have the exact same quantum numbers - parity , spin , baryon number and the rest . so a neutral pion can be a mixture of ( u + anti-u ) and ( d + anti-d ) . there actually result two types of neutral " pion " that decay differently . one is actually heavier , and we call it the eta meson . oops i did not mention yet the strange and anti-strange quark combination , which also gets tangled into the mixes . . . but it is not important to the neutral pion .
notice that $$\frac{l_{\bigodot}}{\pi r_{\bigodot}^2} = \sigma t_{\bigodot}^4$$ so that , dividing through the relation for an arbitrary star and that for the sun gives : $$\frac{l/l_{\bigodot}}{r^2/r_{\bigodot}^2} = t^4/t_{\bigodot}^4$$ using the other relations $$\frac{ ( m/m_{\bigodot} ) ^{3.5}}{m/m_{\bigodot}} = ( t/t_{\bigodot} ) ^4$$ or $$\left ( \frac{m}{m_{\bigodot}} \right ) ^{2.5} = \left ( \frac{t}{t_{\bigodot}}\right ) ^4$$
calculating the energy eigenvalue , will give you $\langle v^2 \rangle$ . this is how it is done : $$\langle v^2 \rangle = \frac{2}{m}\langle t \rangle=\frac{2}{m}\langle \psi|\hat t|\psi\rangle=\frac{2}{m}\int\psi^* ( x ) \hat t \psi ( x ) dx$$ where you should write $t$ ( the kinematic energy ) as an operator . this can be done by writing it as a function of $x$ and $p$ , and then replacing $p$ with its operator . however this is not what we call the expectation value of speed . to calculate the expectation value of speed , we calculate the expectation value of its momentum : $$\langle v \rangle = \frac{\langle p\rangle}{m}=\frac{1}{m}\int \psi^* ( x ) p\psi ( x ) dx$$ which can be calculated either by transforming $\psi$ to the momentum space , or replacing $p$ with its operator $-\mathfrak i \hbar\frac{\partial}{\partial x}$ . an important thing to note is : $\langle v^2 \rangle \ne \langle v\rangle^2$ in general . e.g. consider the symmetric harmonic potential where $\langle v \rangle=0$ but $\langle v^2 \rangle &gt ; 0$ . the difference is called variance $\sigma_v^2=\langle v^2 \rangle-\langle v \rangle^2$ .
you can use the center of mass formula . set the origin of your coordinate system at the center of the earth , then $\vec{r}_1 = \vec{0}$ and $\vec{r}_2 = d$ and $$r_{center} = \frac{m_1r_1+m_2r_2}{m_1+m_2} = \frac{m_2}{m_1+m_2} \cdot d$$ as you have as well .
hints to the question ( v2 ) : first note that the operator norm $||a||=||ua||=||au||$ of an operator $a$ is invariant if we compose with an unitary operator $u$ from either left or right . therefore $\dot{\rho} ( t ) $ is not the zero-operator : $|| \dot{\rho} ( t ) || = || [ h , \rho ( 0 ) || \neq 0 . $
molten solder has a low contact angle on ( clean ) copper . so if you looked at a cross section of the pipe joint as the solder was flowing in you had see something like : the solder is drawn into the joint in exactly the same way as water rises in a capillary tube . both are correctly described as capillary action .
a voltage or current given as a complex constant is a phasor . a voltage given as the complex constant $v_z$ represents the real voltage $$v ( t ) = \operatorname{re} \left ( v_z e^{i\omega t} \right ) \ \ , $$ where $\omega$ is the voltage 's angular frequency and $t$ is time . currents represented as phasors work the same way .
the square brackets mean antisymmetrization . that is : $$ x_{ [ a_1a_2\dots a_n ] } = \frac{1}{n ! }\sum_{p\in s ( n ) } \text{sign} ( p ) x_{a_{p ( 1 ) }a_{p ( 2 ) }\dots a_{p ( n}} $$ where $s ( n ) $ is the set of permutations of $n$ elements , and $\text{sign} ( p ) $ is the sign of the permutation $p$ , that is , $\text{sign} ( p ) =-1$ if you need an odd number of element exchanges , and $\text{sign} ( p ) =+1$ if you need an even number of element exchanges . in particular , $r_{ [ abc ] }{}^d = \frac{1}{6}\left ( r_{abc}{}^d+r_{bca}{}^d+r_{cab}{}^d-r_{bac}{}^d-r_{acb}{}^d-r_{cba}{}^d\right ) $ $\nabla_{ [ a}r_{bc ] d}{}^{e} = \frac{1}{6}\left ( \nabla_{a}r_{bcd}{}^{e}+\nabla_{b}r_{cad}{}^{e}+\nabla_{c}r_{abd}{}^{e}-\nabla_{b}r_{ac d}{}^{e}-\nabla_{a}r_{cbd}{}^{e}-\nabla_{c}r_{bad}{}^{e}\right ) $ you can " move " indices up and down using the metric tensor . that is , $$r_{abcd} = g_{de}r_{abc}{}^e , \quad r_{abc}{}^d = g^{de}r_{abce} . $$ the square brackets just affects the indices ; the $r$ is inside because the antisymmetrization affects indices both from $\nabla$ and from $r$ .
i find this sort of thing becomes much more intuitive if you can think of an analogy in terms of water . in this case , we can think of it like this : here we have water flowing through a hole in a bath tub , into another tub underneath . the stick figure has been given the task of keeping the water level constant , by lifting water back up into the top tub using a bucket . water plays the role of charge in this analogy , so a rate of water flow is analogous to electrical current . in this situation the water flows through a small hole , which represents the resistor . a small hole tends to resist the flow of water , so it is analogous to a high resistance . a large hole would represent a low resistance . if the stick figure lifts $m\:\mathrm{kg}$ of water in her bucket , she must do an amount of work equal to $mgh$ . if she lifts $i\:\mathrm{kg/s}$ then she must expend power at a rate $p=igh$ watts . if we compare this to $p=iv$ we can see that the height difference between the two tubs ( multiplied by $g$ ) plays the role of $v$ in this analogy , and by maintaining the height difference , the stick figure plays the role of the voltage source . now we have everything we need to see intuitively that the power must increase as $r$ decreases . first imagine that the hole in the top tub is very small , so that the water just drips gently through it . this represents a large resistor , and you can imagine that the stick figure does not have to do a lot of work to keep the water level constant , because the water is being drained very slowly . however , if you make the hole bigger then the flow rate of water will increase , and now the stick figure will have to work harder to keep lifting buckets of water to counter it . the bigger the hole ( i.e. . the lower the resistance ) , the faster the flow , so the more buckets of water she has to lift per second to maintain the water level , which means she has to spend more power to do it . hopefully in this analogy you can see that the low resistance does not cancel out the faster current . instead the faster flow is caused by the low resistance . the power itself does not directly depend on the resistance , but only on the voltage ( or height difference ) and current ( or rate of water flow ) . i have calculated the power that the voltage source ( stick figure ) puts into the system , rather than the power dissipated by the resistor . however , by the conservation of energy , these have to be equal . in the water system , the stick figure puts energy into the system in the form of gravitational potential . this is dissipated by viscous friction as the fluid flows through the hole and settles in the lower container . in the electrical case the battery puts energy into the system in the form of electric potential , which gets dissipated in the resistor . but the principle is the same : in both cases the power you put in has to equal the power that comes out as heat .
netwon 's law of cooling i think is exactly what you want to look at . roughly , heat exchange is porportional to the difference in temperature between the beer and the cooling apparatus ( be it water or something else ) and also to the contact area between the two . a secondary effect is the mixing of the beer . if the cold beer sits at the outer walls , the temperature difference is reduced and the middle will never cool . the reverse goes for the cooling liquid .
the whystringtheory page is written in a popularized style that makes it impossible to tell what they really have in mind . their statement does not make sense if interpreted according to the standard technical definitions of the terms . gr does not describe gravity as a force . in the system of units normally used in gr , with g=c=1 , force and power are unitless , so there is also no natural motivation for defining something like a planck force by analogy with the planck length , etc . possibly their " force " really means curvature , in which case this could be interpreted as a correct statement that gr breaks down when the riemann tensor corresponds to a radius of curvature comparable to the planck length .
to calculate fuel consumption , you can typically use the tsiolkovsky rocket equation shown here ( without taking relativity into account ) : $$ \delta v = v_e * ln ( \frac{m_0}{m_1} ) $$ $m_0$ is the initial total mass , including propellant , $m_1$ is the final total mass , $v_e$ is the effective exhaust velocity , and $\delta v$ is the maximum change in the speed of the vehicle ( with no external forces ) the $\delta v$ from earth 's surface to leo from kennedy space center is $9.3 - 10\ ; km/s$ , and leo ( kss ) to geo is $4.24\ ; km/s$ . source : delta-v budgets , earth-moon space , high thrust assuming an exhaust velocity of 4.5 km/s , single stage rocket , we get $$ \frac{m_0}{m_1} = e^{\delta v/v_e} = e^{9.3/4.5} = 7.90 $$ $$ m_{propellant} = ( 1 - \frac{m_1}{m_0} ) m_0 = ( 1 - \frac{1}{7.90} ) m_0 = ( 1 - 12.66\% ) m_0 = 87.3\%\ ; m_0 $$ just getting the rocket to leo takes 87.3% of your initial mass . now let 's try this again for leo -> geo : $$ \frac{m_1}{m_2} = e^{\delta v/v_e} = e^{4.24/4.5} = 2.57 $$ $$ m_{propellant} = ( 1 - \frac{m_2}{m_1} ) m_1 = ( 1 - \frac{1}{2.57} ) m_1 = ( 1 - 38.98\% ) m_1 = 61.02\%\ ; m_1 $$ however , this is a percentage of the initial mass at leo , which is 12.66% of the original . let $m_1$ be the new initial mass for the rocket equation achieved after reaching leo ( from above ) and $m_2$ be the final mass after reaching geo . the fraction of launch mass $m_0$ would be $$ \frac{m_2}{m_0} = \frac{m_2}{m_1}*\frac{m_1}{m_0} = \frac{1}{2.57}*\frac{1}{7.90} = . 3898 * . 1266 = 4.93\% $$ $$ m_{propellant} = ( \frac{m_1}{m_0} - \frac{m_2}{m_0} ) m_0 = ( 12.66\% - 4.93\% ) m_0 = 7.73\%\ ; m_0 $$ thus , getting from earth 's surface -> leo takes about 87.3% of the mass of whatever you launch ejected at 4.5 km/s , where leo -> geo takes only 7.73% of of that mass at 4.5 km/s , leaving you in geo with 4.93% of what you started with . note : because of the difficulty from earth-> leo , we typically use multi-staged rockets which can ease the fuel required . because you said ' a rocket ' , i took that literally and did the calculation with a single-stage-to-orbit configuration .
the dimension of the string is a special case of the concept of dimension for a much more general class of objects called manifolds . manifolds are a mathematical abstraction and generalization of the concept of a surface ( like the surface of a sphere ) . the dimension of a ( real ) manifold is , roughly speaking , the number of coordinates ( real numbers ) necessary to specify a point on the manifold . for example , the surface of a sphere is two-dimensional because any point on the surface can be uniquely identified by a particular lattitude and longitude , each of which is a real number . a string is a one-dimensional manifold ( or manifold with boundary in the case of open strings which have endpoints ) because it takes precisely one real coordinate to specify each point along the string . in the case of open strings , one can take the coordinate $t$ along the string to be some real number in the closed interval $ [ 0,1 ] $ where $0$ is the coordinate for one end of the string , $1$ is the coordinate for the other end , and any point in between has some coordinate between $0$ and $1$ . for open strings , one can take the coordinate $\phi$ to be an angle because closed strings are just loops . simply choose one of the points along the string to correspond to the angle $0$ , then going once around the string corresponds to going once around the unit circle in the complex plane , and one gets back to the original point after a total angle of $2\pi$ radians .
at low velocities like this you can ignore special relativity and simply add the two velocities . this is really easy to see if you imagine yourself standing still and the earth moving under you . relative to you the gun should fire just like you were standing still . this is called an inertial frame of reference . you see the bullet leave at $400\: \mathrm{m/s}$ ( relative to you ) and the earth sees the bullet leave at $800\: \mathrm{m/s}$ .
it fits remarkably well . one of the defining features of a cosmological constant is its equation of state . the equation of state , $w$ , is given by $p \over \rho$ , where $p$ is the pressure it contributes , and $\rho$ is the energy density . a cosmological constant has $w=-1$ . the wmap seven year report recorded the value as $w=-1.1 ± 0.14$ . within the error margins , the cosmological constant fits very well .
what you have missed is that the distance along the $x'$ axis is not the same as the distance along the $x$ axis . the locus of events that are 1 unit of proper distance from the origin is a hyperbola . this can be used to calibrate the $x'$ axis . see calibration hyperbola .
the " lift " produced by a sail is ( primarily ) directed horizontally . the term lift is used since the mechanism is the same as the one that produces lift on an aircraft wing . the key concept is that both wings and sails are airfoils ; the only difference is that wings are ( typically ) oriented horizontally , and sails are ( typically ) oriented vertically . due to the way it modifies the airflow around it , a horizontally oriented wing experiences a vertical lift force ; the sail modifies the airflow around it in exactly the same way ; however , because it is oriented vertially , the corresponding " lift " force is oriented horizontally . consider the usual type of picture of how a wing provides lift : imagine taking an airplane 's wing , and rotating it ninety degrees so that it is sticking straight up out of the ground . also imagine that there is wind blowing onto the wing . now , consider the above picture as though you were suspended above this vertical wing . due to the motion of the air across the wing , this upward pointing wing experiences a horizontal " lift " force . the only difference between the wing and the sail is that the base ( flatter ) side of the sail is not filled in ; what is going on is that there is a ( relatively ) stationary pocket of air sitting inside of the curve of sail , so that as the wind blows by it , it passes by the sail just like in the diagram above ( i.e. . imagine that the grey area is a stationary pocket of air ) . " the physics of sailing " page provides some useful diagrams spelling out the vectoral forces on sailboat . this link correctly describes the lift as being related to the change in momentum of the air in some places , but also refers to bernouli 's princple in others . its easy to conflate bernouli 's principle with the equal times fallacy , c.f. this question for more details on the mechanisms of lift .
your book is incorrect . since $p$ is a length , $pv$ and $l$ cannot be equal by dimensional analysis alone . the specific angular momentum , however , does equal $pv$ , though it is very misleading to use the letter $l$ for it . a body 's specific angular momentum is its angular momentum divided by its mass , i.e. its angular momentum per unit mass . it captures the interesting kinematics of angular momentum ( i.e. . how bodies move in space through time ) but it is quite useless when it comes to dynamics ( i.e. . how bodies interact ) .
this question involving " randomness " and quantum mechanics introduces some subtleties . firstly we have the definition of " randomness " to consider . it turns out that there are various ways to define this term : the two i shall consider here are ( the digit sequence of a ) normal number ( mentioned in the question ) and martin-lof randomness . as explained in the wikipedia article a " normal number " is " finite state machine random " . so it looks random to a fsm . however such numbers can be computable by a turing machine ( as the link shows that turing proved ) . a martin-lof random sequence is based on the familiar notion of incompressibility and cannot be computed by a turing machine . phrased alternatively because it is an infinite sequence it will not have a finite compression onto a turing machine - so there can be no program for it ( which has to be finite ) . the logical link between the two is that every martin-lof random sequence is normal ( but not conversely - as shown by turing ) . also note that every finite sequence can be generated by an algorithm ( and also it will be the solution of a polynomial ) . the reason why the random sequences work is that although the first n digits can be replicated via a program p , a different program ( in general ) is required for the first n+1 digits of the sequence ie p will fail to " predict " n+1 . to get the entire sequence requires an infinite series of programs so we are back where we started with the random sequence . now that we have some definitions available we can examine connection to physics . the problem in a experimental based theory is that we only ever have a finite amount of data . thus the claim that a given series is " random " is strictly not empirically provable . so this puts the copenhagen claim that " qm sequences are random " into an awkward semi-scientific status . such claims cannot be proved experimentally , yet copenhagen asserts this . so where is the proof ? indeed what constitutes a proof ? also we have seen several definitions of randomness ( there are more ) - so which type of randomness does qm have exactly ? to return to specific points in the op question : is there any way to differentiate if they came from a truly random or from a formula/algorithm ? how ? no , because one only ever has a finite amount of stream data to analyse , which can always be explained algorithmically ( as discussed above ) . if there is no way to decide this , then , i can not find any basis , to keep denying that behind the " truly random " of quantum mechanics can be a hidden algorithm . the " truly random " of quantum mechanics might be martin-lof randomness , for which there is no algorithm ; however if it is really normal number randomness then there might be an algorithm . i suspect that most physicists take the view that qm is as " random as it gets " , hence would prefer the martin-lof option . i know i am talking about the possibility of a " hidden variables " theory , but i can not find any other explanation . the link between algorithmic underlying structure and " hidden variables " would appear to be close . if its a non-algorithmic type of randomness then we have to decide whether it belongs to one of " oracle classes " associated with martin-lof randomness , and what that would mean in terms of " hidden variables " . some readers might recall that in his book " the emperor 's new mind " in 1989 roger penrose proposed that aspects of quantum mechanics were " non-computable " . although that argument was not formulated as i have here , it is consistent ( i believe ) with the idea of martin-lof randomness too .
the second law of thermodynamics - about the increasing entropy - which is apparently what you are talking about - holds for any system . permanent magnets are no exception . a ferromagnet may look " more ordered " than a non-magnetic material because the spins are oriented in the same direction , rather than random directions . but physical systems may only try to maximize their entropy among configurations that conserve energy ( much like the momentum , charge , and other conserved quantities ) . for ferromagnets , the configuration with spins oriented in random directions would have a much higher energy - because one reduces the energy by orienting the spins , elementary magnets , in the same direction . so the spontaneous disappearance of the uniform electrons ' spin would violate the energy conservation . among the configurations with the same energy , the magnet still tries to maximize its entropy . in particular , the heat is flowing from warmer pieces of the material to colder ones , and so on . more generally , the entropy never goes down , and that is the only general statement that follows from the second law of thermodynamics . ferromagnets are not special among physical objects that could have a higher entropy if you allowed the energy to increase . for example , any object would raise its entropy - the amount of disorder - if its temperature increased . but a higher temperature requires a higher energy , too . one can not violate the first law of thermodynamics ( energy conservation ) just because it would make it more straightforward to satisfy the second law . both of them hold in nature .
first thing , for a rotating ball , $i=\frac{2}{5}mr^2$ . you also need to be clear on what $\omega$ you are talking about . the kinetic energy of a rotating ball is $\frac12 i_{cm}\omega_{cm}^2 + \frac12 mv_{cm}^2$ . here , $v_{cm}=v$ . but , $\omega_{cm}=v_{cm}\times \frac{r}{r}$ . since $r&lt ; &lt ; r$ , we can take the net kinetic energy to be just $\frac12 mv^2$ ; the $\frac12 i_{cm}\omega_{cm}^2$ term becomes too small to matter . the main thing is is that you need to remember that the formula " kinetic energy=rotational energy + translational energy " works only when you consider all rotations about center of mass . you cannot just keep tacking on terms for each motion you see . even though the ball is revolving around the center of the loop , we still classify this as translational motion . if you do not do this , you can easily get confused while building the expression for ke . basically , for a ball of center of mass moment of inertia $i$ , mass $m$ , radius $r$ , rotating about itself with $\omega_cm$ , revolving in a circle of radius $r$ with $\omega'$ , the energy is not $\frac12 i\omega^2+ \frac12 ( i+mr^2 ) \omega'^2+\frac12 mv^2$ , it is $\frac12 i\omega^2+ \frac12 mv^2=\frac12 i\omega^2+ \frac m ( \omega'r ) ^2$ .
i will give it a shot . spoiler : i did this in the body frame so that the moment of inertia is time independent , before you get excited . . . starting with euler 's equations : $$ i_i\dot{\omega}_i+ ( i_j - i_k ) \omega_j \omega_k = 0 $$ and taking cyclic permutations of $i , j , k$ to get the three of them ; and in the absence of torques ( i ignore air friction ) . it is a symmetric top so $i=i_1=i_2 \neq i_3$ so write $$ \dot{\omega}_1 = -\frac{ ( i_3-i ) }{i}\omega_2 \omega_3 $$ $$ \dot{\omega}_2 = -\frac{ ( i - i_3 ) }{i}\omega_1\omega_3 $$ $$ \dot{\omega}_3=0 \implies \omega_3=k_1 $$ now for this problem the coin is spinning about one of the first two symmetric axies . i chose 1 . then consider small variations on the other two angular velocities from zero : $\omega_2 = \delta\omega_2$ , $\omega_3 = \delta\omega_3$ , and $\omega_1 \rightarrow \omega_1$ . so we make small changes in how the coin is rotating about a line through its center perpendicular to the coin , and about the other symmetric axis . in other words , it was spinning ideally like a coin would , then we changed the ideal to a little weird spinning . making the changes , and ignoring second order in perturbations : $$ \dot{\omega}_1=0 \implies \omega_1 = k_1 $$ $$ \frac{d}{dt} ( \delta\omega_2 ) =-\frac{ ( i-i_3 ) }{i}\omega_1 ( \delta\omega_3 ) $$ $$ \frac{d}{dt} ( \delta\omega_3 ) =0 \implies \delta\omega_3 = k_2 $$ then we can write $$ \frac{d}{dt} ( \delta\omega_2 ) =-\frac{ ( i-i_3 ) }{i}k_1 k_2 $$ everything on the r.h. s is a number so $$ \delta\omega_2 = -\left ( \frac{ ( i-i_3 ) }{i}k_1 k_2 \right ) t $$ so depending on how big $i$ is compared to $i_3$ will determine how $\delta\omega_2$ changes during the flip . if one uses a radius of $r=0.014$ m and $h=0.0015$ m for the hight of the coin , one gets a moment of inertia tensor like the following : $$ i=m ( 0.0000491875 ) \quad i_3 = m ( 0.000098 ) $$ which tells me that the variations are unstable . . . which i do not really believe since i have seen a coin in real life . so look this over . but i can not find anything wrong so i am going with it , and thinking that i can not really see a coin in real life up close while it is spinning . . . hope this helps .
in the frame of reference of the body , is the centripetal force felt or is only the centrifugal force felt ? it depends on what you mean exactly . consider , for example , the amusement park ride dumbo at disneyland : . on this ride , passengers sit in mini dumbo replicas and are swung around in a circle . what forces do they feel ? well , firstly , they feel a centrifugal force radially outward . but this is not all . if that were the only force they felt , then in the frame that is stationary with respect to dumbo , they would accelerate radially outward . instead , they also feel a normal force of dumbo pushing them inward that is precisely equal to the centrifugal force , and as a result , as measured in the dumbo frame , they remain stationary with respect to dumbo . now , we know that if we were to analyze the same situation from the frame of reference of a person watching the ride from the ground , then we would say that there is only one force on the passengers , namely the normal force of dumbo on them , and this force causes the passengers to accelerate , namely to move in a circle . as a result , the convention is to call the normal force the " centripetal " force . i personally think this is terrible terminology that confuses students because it leads them to believe that " centripetal force " is somehow an independent thing that does not need to be comprised of real physical interactions with objects . . . by anywho . now , going back to the accelerated frame , we had noticed that there were two forces acting on the passengers , the ( fictitious ) centrifugal force , and the normal force . would you now call the normal force a " centripetal " ? if we are doing the analysis in the accelerating frame , then that would be extremely non-standard because in that frame , no circular motion is occurring . does a body only feel the effect of pseudo forces in an accelerated reference frame ? no ! just look at the above example ! the passengers feel the centrifugal force , but they also feel a normal force due to their interaction with dumbo ! in general , there can be all sorts of forces that an object feels in an accelerated frame that are not pseudo forces like friction , gravitational forces , electromagnetic forces etc .
applying the law of cosines to the triangle $\triangle s_1s_2p$ will yield $$ r_2^2=r_1^2+a^2-2ar_1\cos ( \angle s_2s_1b ) . $$ the angle that appears is complementary to $\theta_m$ , so you can either use $\angle s_2s_1b=\frac\pi2-\theta_m$ and trigonometric identities , or simply see that $$ \cos ( \angle s_2s_1b ) =\frac{s_1b}{s_2b}=\sin ( \theta ) . $$
firstly , note that they postulate those commutation relations in the beginning of section 3.5 in order to show that they are wrong , which they demonstrate in the ensuing pages . the ultimate point is to show that one needs to impose anti-commutation relations on fermionic fields . in fact , the correct relations are postulated in equation 3.96 ; \begin{align} \{\psi_a ( \mathbf x ) , \psi_b^\dagger ( \mathbf y ) \} and = \delta^{ ( 3 ) } ( \mathbf x - \mathbf y ) \delta_{ab} \end{align} you could then ask , are these equivalent to the anti-commutation relations of the mode operators that they write in ( 3.97 ) ? namely , \begin{align} \{a^r_\mathbf p , {a^s_\mathbf q}^\dagger\} = \{b^r_\mathbf p , {b^s_\mathbf q}^\dagger\} = ( 2\pi ) ^3\delta^{ ( 3 ) } ( \mathbf p - \mathbf q ) \delta^{rs} \end{align} and the answer is yes . to show that the second set implies the first , write the fields in their integral mode expansions , compute the anti-commutator of these integral expressions , and apply the anti-commutators between modes . to show that the first set implies the second , invert the integral expressions for the fields in terms of the modes to obtain integral expressions for the modes in terms of the fields , and do the analogous thing . main point . the commutators/anti-commutators between fields are equivalent to the commutators/anti-commutators between modes .
due to the law of conservation of angular momentum they will only orbit if there is angular momentum in the initial conditions . if they start at rest , there is none and so they will collide . since we are supposed to provide links and whatnot , here 's the wikipedia entry . http://en.wikipedia.org/wiki/orbit part of the difficulty in answering this question , however , is that there are many different kinds of orbits and the details of the trajectories depends on the relevant masses and linear and angular momentums .
good question ! first you need to know that parity refers to the behavior of a physical system , or one of the mathematical functions that describe such a system , under reflection . there are two " kinds " of parity : if $f ( x ) = f ( -x ) $ , we say the function $f$ has even parity if $f ( x ) = -f ( -x ) $ , we say the function $f$ has odd parity of course , for most functions , neither of those conditions are true , and in that case we would say the function $f$ has indefinite parity . now , have a look at the time-independent schrödinger equation in 1d : $$-\frac{\hbar^2}{2m}\frac{\mathrm{d}^2}{\mathrm{d}x^2}\psi ( x ) + v ( x ) \psi ( x ) = e\psi ( x ) $$ and notice what happens when you reflect $x\to -x$: $$-\frac{\hbar^2}{2m}\frac{\mathrm{d}^2}{\mathrm{d}x^2}\psi ( -x ) + v ( -x ) \psi ( -x ) = e\psi ( -x ) $$ if you have a symmetric ( even ) potential , $v ( x ) = v ( -x ) $ , this is exactly the same as the original equation except that we have transformed $\psi ( x ) \to \psi ( -x ) $ . since the two functions $\psi ( x ) $ and $\psi ( -x ) $ satisfy the same equation , you should get the same solutions for them , except for an overall multiplicative constant ; in other words , $$\psi ( x ) = a\psi ( -x ) $$ normalizing $\psi$ requires that $|a| = 1$ , which leaves two possibilities : $a = +1$ ( even parity ) and $a = -1$ ( odd parity ) . as for what this means physically , it tells you that whenever you have a symmetric potential , you should be able to find a basis of eigenstates which have definite even or odd parity ( though i have not proved that here , * only made it seem reasonable ) . in practice , you get linear combinations of eigenstates with different parities , so the actual state may not actually be symmetric ( or antisymmetric ) around the origin , but it does at least tell you that if your potential is symmetric , you could construct a symmetric ( or antisymmetric ) state . that is not guaranteed otherwise . you had probably have to get input from someone else as to what exactly definite-parity states are used for , though , since that is out of my area of expertise ( unless you care about parity of elementary particles , which is rather weirder ) . *there is a parity operator $p$ that reverses the orientation of the space : $pf ( x ) = f ( -x ) $ . functions of definite parity are eigenfunctions of this operator . i believe you can demonstrate the existence of a definite-parity eigenbasis by showing that $ [ h , p ] = 0$ .
large inflatable balls such as soccerballs , footballs and basketballs have an internal rubber bladder which needs to be inserted by hand into the carcass of the ball and inflated to the desired pressure to suit the user ( eg : basketballs can be inflated to the produce the desired bounce height for the individual user ) . also , transporting deflated balls from the factory to distributors who package and pass onto retailers saves shipping volume and hence cost . small balls such as tennis balls can be pressurized in the factory , but will lose pressure within a few onto or so after being opened ( they come in a pressurized can ) . also , the higher curvature of small balls allows them to ' spring back ' more easily when they bounce , even when they are not pressurized , although unpressurized balls do not bounce as high as inflated balls .
consider the loop to the left to be loop a and that to the right to be loop b according to kirchhoff 's voltage law find out the current in loop a and loop b current in loop a comes out to be 0.4a in the anticlockwise direction current in loop b comes out to be 0.5a in the clockwise direction while applying kvl do not consider the 4v battery connecting the two loops since no current flows through it . now when it comes to finding the potential difference between the two points x and y assume you connect a voltmeter between the points x and y then the circuit becomes as shown in the diagram below ( you can also observe that i have marked different points on the loop with alphabets so that it is easier to explain in the later steps ) now when it comes to finding the potential difference you can either consider the potential drop recorded across the path xabpqy or the path xabpry ( you can later verify that both the paths give the same answer ) now i am going to choose the path xabpry since this is more simpler in this path the potential drop is recorded across the 2v battery , 4v battery in the middle and the 3ohm resistor in the second loop calculations : so total p . d recorded is going to be +2 volts ( by 2v battery ) , +4v ( by the 4v battery ) and +3*0.5 v ( 1.5v ) ( potential drop across the 3 ohm resistor - taken with positive sign because the assumed direction is opposite to the direction of current ) so the total potential drop between x and y comes out to be 7.5 v you will get the same answer if you go along the path xabpqy hope it helps .
to answer this question , we will first compute the values of $\lambda$ for which $\rho ( \lambda ) $ is ppt and separately compute the values for which it is entangled . let $t$ be the transpose map , such that the partial transpose map may be written as $ ( \mathbb{i}\otimes t ) $ , where $\mathbb{i}$ is the identity on $\mathbb{c}^d$ . one can show that the partial transpose maps the standard maximally entangled state into the swap operator $$ ( \mathbb{i}\otimes t ) |\psi\rangle\langle\psi|=\frac{1}{d}w , $$ where $w=\sum_{i , j}|i\rangle\langle j|\otimes|j\rangle\langle i|$ . for reference , you can take a look at john watrous ' excellent lecture notes . the swap operator has states with eigenvalue $-1$ , let 's call one of them $|w\rangle$ . we then have \begin{align} \langle w| ( \mathbb{i}\otimes t ) \rho ( \lambda ) |w\rangle and =\lambda\langle w|\frac{\mathbb{i}}{d^2}|w\rangle+ ( 1-\lambda ) \langle w|w|w\rangle\\ and =\frac{\lambda}{d^2}-\frac{ ( 1-\lambda ) }{d} . \end{align} we want this expression to be positive , which gives us the condition $$\lambda\geq\frac{1}{1+d} . $$ on the other hand , we can calculate the maximum overlap $\langle\psi|\rho_s|\psi\rangle$ that a separable state $\rho_s$ can have with $|\psi\rangle$ , such that if the overlap of $\rho ( \lambda ) $ is greater than this maximum , we know that $\rho ( \lambda ) $ is entangled . it can be shown ( see for example this review ) that in our case this maximum is precisely $\frac{1}{d}$ . therefore , $\rho ( \lambda ) $ is entangled whenever \begin{align} \langle\psi|\rho ( \lambda ) |\psi\rangle and \geq\frac{1}{d}\\ \rightarrow\frac{\lambda}{d^2}+ ( 1-\lambda ) and \geq\frac{1}{d} , \end{align} which gives the condition $$\lambda\geq\frac{d^2-d}{d^2-1} . $$ however , you can quickly check that both conditions cannot be met simultaneously , so there is no value of $\lambda$ for which $\rho ( \lambda ) $ is entangled and ppt .
mechanical efficiency is usually used as a metric to account for frictional losses in systems . for example , the transmission of a car transmits mechanical work from the engine to the wheels so the mechanical efficiency of the system will be $w_{transmitted}/w_{received/ideal}$ . even within an engine , there is friction between the piston and cylinder walls , bearings of crankshafts etc . mechanical efficiency is : $w_{output}/w_{obtained\ ; from\ ; gas}$ . the denominator is the work obtained from the work-fluid , the thermodynamic cycle . therefor it is a way to quantify the frictional loses in the system . what i have mentioned above is one of the common use of this term , but there could be other definitions too . you need to look into the context of usage
the angular speed of the minute hand is actually $1.75\times 10^{-3} rad/s$ . always do a quick consistency check on your calculations . the hour hand is moving at roughly 1/10 angular speed so the orders of magnitude should differ by ~1 .
add any instant in time , light of different wavelengths can be said to interfere . however , because of the extreme frequencies of optical light , any cross interference will get time-averaged away very quickly unless the two waves are very close in frequency .
your interpretation is not correct . the propagator $d_{\mu\nu} ( x-y ) $ describes the amplitude for a photonic field perturbation to go from $x$ to $y$ , with the implicit picture that you have a " source " $j ( x ) $ , and a " sink " $j ( y ) $ , which are perturbing the vaccuum . however , a field perturbation is not a real particle ( for instance , in the photon case , the fourier transform of the propagator is not proportionnal to a delta function $\delta ( k^2 ) $ , but rather proportionnal to $\frac{1}{k^2}$ ) . so there is no creation or annihilation operators , because there is no real particle . note that , at any vertex , in momentum space , even if we work with photonic field perturbations ( "virtual " photons ) , momenta , angular momenta , charges , etc . . are however conserved .
start with your $\hat{h} = \hbar \omega \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) $ . i will omit hat notation from this point . the commutator then reads as \begin{equation} \left [ h , a \right ] = \hbar \omega \left [ \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) a - a \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) \right ] = \hbar \omega \left ( a^\dagger a a - a a^\dagger a \right ) , \end{equation} which is nothing but \begin{equation} \left [ h , a \right ] = \hbar \omega ( a^\dagger a - a a^\dagger ) a = \hbar \omega \left [ a^\dagger , a \right ] a , \end{equation} but we know that \begin{equation} \left [ a^\dagger , a \right ] = -1 , \end{equation} therefore \begin{equation} \left [ h , a \right ] = -\hbar \omega a , \end{equation} qed . proof of the second relation is done in the same way .
show that if $|\nu\rangle$ is an eigenvector of $n$ with eigenvalue $\nu$ , and if $a|\nu\rangle\neq 0$ , then $a|\nu\rangle$ is an eigenvector of $n$ with eigenvalue $\nu-1$ . convince yourself that the spectrum of the number operator is non-negative . assume , by way of contradiction , that there is some non-integer eigenvalue $\nu^*&gt ; 0$ and let $m$ denote the smallest integer larger than $\nu^*$ . use property 1 repeatedly ( $m$-times ) to show that $a^m|\nu^*\rangle$ is an eigenvector of $n$ with eigenvalue $\nu^*-m &lt ; 0$ . this is a contradiction qed .
during the 1970s , the los alamos national laboratory carried out the pacer project , to explore the use of thermonuclear explosions as a way of generating electrical power and breeding nuclear materials . the general layout of the initially proposed fusion power plant can be seen in the following illustration : the system parameters were under exploration , but one of the ideas was to explode about 800 50 kt thermonuclear devices per year . as the conversion efficiency was expected to be about 30% , the generated electrical power would have been $\displaystyle 0.3 \cdot 800 \cdot 50\ , {\rm kt \cdot yr^{-1}}\frac{4.2 \cdot 10^{12}\ , {\rm j \cdot kt^{-1}}}{3.15 \cdot 10^7\ , {\rm s \cdot yr^{-1}}} \approx 1.6\ , {\rm gw}$ , about 80% of the nominal power , because that was the assumed capacity factor . heat loss was not much of a problem because of scaling properties . as the thermal conductivity of rock salt is about $10\ , {\rm w \cdot m^{-1} \cdot k^{-1}}$ , assuming a crudely simplified geometry consisting of a flat plate of about $1\ , {\rm km^2}$ with $ 100\ , {\rm m}$ of thickness and the whole $500\ , {\rm k}$ thermal gradient applied , the resulting thermal flux is about $\displaystyle 10\ , {\rm w \cdot m^{-1} \cdot k^{-1}} \cdot \frac{10^6\ , {\rm m^2}}{100\ , {\rm m^{-1}}}500\ , {\rm k} = 50\ , {\rm mw}$ , less than 1% of the thermal power . the technical limiting factors were the relatively low temperature achievable inside a rock salt cavity and the large cavity sizes required to avoid contact of the walls with the unmixed fireball . obviously , there were also safety and public perception problems . see page 8 of this magazine for an overview and la-5764-ms for the details ( warning : 22 mb pdf file ) .
one way to see the validity of the background field method ( bfm ) lies in the proof of the equivalence of the effective action calculated with the bfm to the standard effective action . let $\gamma [ v ] $ be the effective action ( legendre transform of the connected generating function $w [ j ] $ ) where $v=v ( j ) =\frac{\delta w [ j ] }{\delta j}$ is the " classical " field generated by the sources $j$ . if we modify the classical action by splitting the quantum fields into quantum + background , then the resultant modified effective action $\gamma [ v ( j ) , v ] $ now depends on both $v ( j ) $ and the background fields $v$ . you can show ( under reasonable assumptions ) that $\gamma [ 0 , v ] = \gamma [ v ] $ . one of the cleaner and more general discussions of this is in section 3.1 of the 2007 thesis by grasso : " higher order contributions to the effective action of n = 2 and 4 supersymmetric yang-mills theories from heat kernel techniques in superspace " . here he considers the more general ( non-linear ) quantum-background splitting that is needed for $n=1$ supersymmetric gauge theories . references to the original literature can also be found in this thesis . also worth reading are the introductory papers by abbott : " introduction to the background field method " and " the background field method beyond one loop " . abbott et al also show that the s-matrix is the same when using the bfm : " the background field method and the s matrix " . this holds irrespective of the gauge fixing , while the equivalence of the effective actions only holds if the same gauge is chosen . note that the quantum-background splitting is not really a doubling of the number of fields , but only just a background dependent change of variables ( in the simple case , just a shift ) in the path integral .
there are many sources of info about ads geometries on the net . i think the simplest introduction to these models is : http://www-thphys.physics.ox.ac.uk/people/maximegabella/rs.pdf this covers the randall-sundrum model , you can also check the original paper : http://arxiv.org/abs/hep-ph/9905221 the texts above address ads geometry in extra dimensional models . the paper below addresses extra dimensions in general and also talks about the ads extra dimensions : http://arxiv.org/abs/hep-ph/0404096 is this what you had in mind ? there is a lot more material to be found on the arxiv if you search it from spires ( www.inspirehep.net ) .
first , a small correction : it should be $m_\text{rel} = \gamma m$ , because $\gamma = ( 1-\frac{v^2}{c^2} ) ^{-\frac12}$ . your formula is close . the most general version , which is true all the time ( even for zero mass ) is $$e^2 = ( mc^2 ) ^2 + ( pc ) ^2 . $$ here $m$ is the rest mass and $p$ is the magnitude of the relativistic momentum , defined as $\mathbf{p} = \gamma m \mathbf{v}$ . note that the factor of $\gamma$ is already included in the momentum , and we use the rest mass , not the relativistic mass . you can get another formula for this one : if you replace the definition of $\mathbf{p}$ , you will find $$e = \gamma m c^2 = \frac{m c^2}{\sqrt{1-\frac{v^2}{c^2}}} = m_\text{rel}c^2 . $$ this formula is also valid at all times . in older texts it usually to refer to the relativistic mass $m_\text{rel}$ as simply the mass $m$ , and this is probably where the formula $e = mc^2$ comes from .
the line you wrote 105,000 $\frac{kg}{m*s^2}$ * 330 k * 287 $\frac{m^2}{s^2*k}$ has to read in fact $\frac{105,000 \frac{kg}{m*s^2} }{ 330 k * 287 \frac{m^2}{s^2*k}} = 1.11 \frac{kg}{m^3}$ . this comes from the gas law $p=\rho \ r \ t $ where $p$ is the air pressure and $\rho$ is the air density . solving for $\rho$ you get $\rho =\frac{p}{r t} $ from which the numerical solution follows .
i will try to give a short introduction into the ideas of scientific truth as i understand them . in mathematics , the world is beautifully simple . we have axioms that the set to be true , and from these we can deduce a plethora of statements to be undoubtedly true - given that the axioms are true . there may be undecidable statements about which we cannot say anything , but within axiomatic system everything is either true , false or undecidable . now , in reality , we are not in as comfortable a situation : we do not know the axioms the world is grounded upon , we try to guess them . the axioms we guess are what we call laws of physics . now , given two different hypotheses ( i.e. . guesses for the laws of physics ) $h_1 , h_2$ , we may look at some situation and , by logic alone , deduce that $h_1 \implies o_1$ and $h_2 \implies o_2$ . then we perform the experiment . if we observe $o_2$ not to be realized in reality , then , by the law of contraposition , we may conclude $\neg o_2 \implies \neg h_2 $ , so the second hypothesis is clearly and unambiguously false . but , suppose that , within the errors of our observation , $o_1$ holds . then we cannot say that $h_1$ holds because $ ( h_1 \implies o_1 ) \implies ( o_1 \implies h_1 ) $ is not a valid form of argumentation . all we can say is that our observation is compatible with $h_1$ being a law of nature , but it is not true in any rigorous sense . no statement about reality can ever be true in the axiomatic sense because of this reasoning . now , of course there could be a third hypothesis $h_3$ also predicting $o_1$ . then , until we perform experiments for which the hypotheses predict different things , $h_1$ and $h_3$ are equally true within our heuristic approach . and the more experiments we perform , the more often $h_1$ survives this process of constant enquiry , the more certain we grow that this is actually a good description of the world . this is what science is all about : taking the vast landscape of possible ideas about how the world works - take as a simple example aristoteles ' idea that everything wants to be at rest vs . newton 's idea that everything continues uniform motion unless acted upon - looking at the predictions these different ideas about the laws of nature make and then performing experiments eliminating those ideas that are false . thus , we make a gradual progress towards the underlying truth , which is empirically inaccessible . the longer we perform experiment , and think of new hypothesis , the more refined will our laws of nature be , the greater our confidence can be that these are really good approximations to the way the world really works . to say " we cannot prove it , we really do not know " is to vastly underestimate the power of falsificationism , and shows a callous disregard for the scientific method , whose success is reflected in every bit of technology around us . [ one may also find my answer to why should a ( physical ) principle be applicable to different systems in different positions in space and time ? to be relevant in this context . ]
the first bullet is correct , the outer shell does not contribute . this easily follows from gauss ' law . for this you use the fact that the electric field must be radial and any cylinder inside the cylindrical shell does not enclose the charge density $-\lambda$ . you might think that close to the negatively charged shell there is an additional electric field pointing in the same direction ( towards the shell ) , but this contribution is cancelled by the electric field created by the rest of the shell . the second bullet does not assign $r_b$ as $r_0$ and $r_a$ as $r$ in equation $ ( 1 ) $ . rather , it assigns $r_a$ as $r$ to calculate $v_a$ and $r_b$ as $r$ to calculate $v_b$ , which yields $v_a=\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_0}{r_a}$ and $v_b=\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_0}{r_b}$ . then $v_{ab}\equiv v_a-v_b=\frac{\lambda}{2\pi\epsilon_0}\left ( \ln\frac{r_0}{r_a}-\ln\frac{r_0}{r_b}\right ) =\frac{\lambda}{2\pi\epsilon_0}\left ( \ln ( \frac{r_0}{r_a}/\frac{r_0}{r_b} ) \right ) =\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_b}{r_a}$ .
when the electrostatic force was originally being studied , force , mass , distance and time were all fairly well understood , but the electrostatic force and electric charge were new and exotic . in the cgs system , the charge was defined in relation to the resulting electrostatic force ( it is called a franklin ( fr ) an " electrostatic unit " ( esu or ) sometimes a statcoulomb ( statc ) ) . in that system , we express the force on one charged particle by another as $f_e=\frac{q_1 q_2}{r^2}$ where the unit of charge is the esu , the unit of force is the dyne and the unit of distance is the centimeter . in the mks system ( now called si ) , we would write $f_e = k_e\frac{q_1 q_2}{r^2}$ where the unit of charge is the coulomb , the unit of force is the newton , and the unit of distance the meter . it would seem that if things are equivalent , then $k_e$ is indeed just a conversion factor , but things are definitely not equivalent . a little history is probably useful at this point . before 1873 , when the cgs system was first standardized , it finally made a clear distinction between mass and force . before that , it was common to express both in terms of the same unit , such as the pound . so if you think of it , people still say things like " i weigh 72 kg " rather than " i weigh 705 n here on the surface of earth " and they also say $1 \mathrm { kg} = 2.2\mathrm{ lb}$ confusing mass and weight ( the cgs imperial unit of mass is actually the slug ) . this is important , because there is a direct analogy to the issue of units of charge and to your question about the units of $k_e$ . the franklin is defined as " that charge which exerts on an equal charge at a distance of one centimeter in vacuo a force of one dyne . " the value of $k_e$ is assumed to be 1 and is dimensionless in the cgs system . in cgs , the unit of charge , therefore , already implictly has this value of $k_e$ built in . however in the si units , they started with amperes and derived coulombs from that and time ( $c=it$ ) . the resulting units of $k_e$ are a result of that choice . so although the physical phenomenon is the same , it is the choice of units that either gives $k_e$ dimension or not . see this paper for perhaps a little more detail on how this works in practice .
you define the density auto-correlation function as $$s_{\rho\rho} = \langle \delta \rho ( \mathbf{x}_1 ) \delta \rho ( \mathbf{x}_2 ) \rangle$$ where $\delta \rho ( \mathbf{x} ) = \rho ( \mathbf{x} ) - \langle \rho ( \mathbf{x} ) \rangle$ is deviation from the local mean value . the fourier transform of $s_{\rho\rho}$ is related to the structure-factor $$s ( \mathbf{q} ) = \langle \rho \rangle^2 ( 2\pi ) ^d \delta ( \mathbf{q} ) + \frac{1}{v}\int d^d x_1 d^d x_2 e^{-i \mathbf{q} \cdot ( \mathbf{x}_1-\mathbf{x}_2 ) } s_{\rho\rho}$$ where $\langle \rho \rangle$ is the average density of the whole system , i.e. , $v \langle \rho \rangle = \int d^d \mathbf{x} \ , \rho ( \mathbf{x} ) $ . the structure factor $s ( \mathbf{q} ) $ is related to the pair-correlation function $g ( \mathbf{x} ) $ via $$s ( \mathbf{q} ) = \langle \rho \rangle \big [ 1 + \langle \rho \rangle \int d^d \mathbf{x} \ , g ( \mathbf{x} ) e^{-i \mathbf{q} \cdot \mathbf{x}} \big ] $$ if the system is isotropic , then $g ( \mathbf{x} ) = g ( |\mathbf{x}| ) $ is called the radial-distribution function . most of these relations are already in the wikipedia page linked in the question .
is there any computer program to calculate the dose of the whole decay chain to get a picture of the artificial radiation and supports logging . i do not want to look up all the individual numbers and calculate it manually . yes , there is . mcnp will do dose calculations , among many many other things . it is a stochastic code , meaning it does random flights and interactions of radiation , which would be beta particles in your case . those are a good bit more tricky to model than gamma rays because they bounce around a lot more and also have continuous interactions as a charged particle . radioactive decay is also handled with the mcnp code , but not directly . the national labs use the monteburns code for this purpose and it is linked with origen2 , although i should specify these can change depending on the version number . you would likely get mcnp5 or mcnp6 if you ordered it now . in order to use these codes , you may submit a request to rsicc , although ideally you want to have a us university affiliation , you can also obtain it working in industry although probably at a higher price . you also would really need someone on your team with a few years on experience working with these codes . the cost of accurately obtaining the dose rate numbers will certainly affect that approach you take , as the acquisition of data , even from models , has a very real cost associated with it . it is likely that your lab will not have the budget and you will instead consult a textbook and try to combine rough estimates for the various sources you have . there are a variety of tools available for managing radiological dose to workers in a lab . in the us we would expect that a lab working with any significant sources would have an alara policy , which dictates minimizing exposure through the fundamentals of time , distance , and shielding . in a nutshell , alara reflects the precautionary assumption that there is no " safe " dose so any unnecessary or frivolous dose is unacceptable . modeling , direct radiation detection , and tld devices are common tools for radiation safety , but these are not necessary for all labs and any combination of these measures may be employed based on the needs of the specific lab . many sources are low enough hazard that none of these will be employed . however , an understanding of radiation and the types of sources you are working with should come before any of these options . if you are the primary person in your lab responsible for a source and you do not know the hazard level or the appropriate precautions that should be taken for it , then that would be a serious problem and you should consult the management in your organization or the nuclear regulating body in the area in which you reside .
consider two $h$ atoms $rotating^{ ( 1 ) }$ about their centre of mass , now both the atoms are electrically neutral and far apart so thay neither strong nor weak nuclear force comes into consideration . if they were to stay in this state only then they would never combine and form $h_2$ but what happens is that when the electrons of each atom are moving during various instances dipoles are formed and london dispersion forces come into play due to these interactions the 2 atoms move towards each other and at a certain distance acheive a stable equilibrium , and are therefore bonded to each other . on the other hand lets suppose you somehow got 2 balls of $h$ in elemental form and placed them near each other , the atoms of both ball will still get induced dipoles but since all the dipoles are randomly oriented , their would not be any significant overall dipole moment which may force the two balls to come close and join . this was highly simplified version of what happens with iron and other elements and compounds , on thd atomic scale considerable dipole moments are developed even in neutral atoms/compounds , these motivate the further bonding to form other objects such as lattices , crystals , sheets and so on . on the other hand at the macro scale , no considerable attraction forcd develops between two neutral objects which may motivate them to join/bond . but indeed if you have to oppoisitely charged objects they may join together . also to join objects on macroscales we have different proceses such as different types of welding etc $ ( 1 ) $ i said rotating about centre of mass to avoid considering attraction due to gravitational interaction between the two atoms .
a solid material is not made of electrons separated from the nuclei . the nuclei and electrons live together in a structure that can be ordered ( like crystals ) or not ( like glass ) . in all cases , the electron arrangement around the nuclei is very important as it allows for bonding between these atoms . in that process , some electrons are stuck close to the nuclei , they are called ' bound ' electrons . others are not totally stuck and can somewhat move from atom to atom , they are called ' free ' electrons . but the atoms are always stuck in a given position . that is why the rod stays as a rod and does not evaporate as a gas . imagine that the fixed atoms form a given stable structure . the free electrons can move inside this structure . these are the ones that are attracted to one side or the other . the nuclei do not move . that is why the rod just looks the same all the time : only a tiny number of electrons actually travel along .
topological order is a new kind of order in zero-temperature phase of quantum spins , bonsons , and/or electrons . the new order corresponds to pattern of long-range quantum entanglement . topological order is beyond the landau symmetry-breaking description . it cannot be described by local order parameters and long range correlations . however , topological orders can be described/defined by a new set of quantum numbers , such as ground state degeneracy , non-abelian geometric phases of degenerate ground states , quasiparticle fractional statistics , edge states , topological entanglement entropy , etc . fractional quantum hall states and quantum string liquids are examples of topologically ordered phases . the low energy effective theory of topological phases happen to be topological quantum field theory . in nature , topological quantum field theory always appears as the low energy effective theory of topological phase of quantum spins , bonsons , and/or electrons , etc . by definition topological phase is always a quantum phase of quantum spins , bonsons , electrons , etc . ie topological phase is always a quantum state of matter .
extreme hyperopia would correspond to your eye lens in relaxed conditions being close to an optical flat . in such a case you would need a contact lens with a focal length of about 25 mm ( typical human eyeball diameter ) . this corresponds to a lens of 1000/25 = 40 diopters . in other words : a farsighted eyeless requiring 20 diopter correction has a focal length at relaxed conditions of about twice the required ( 25 mm ) focal length . a simple ( approximate ) means to estimate the diopters of correction needed , is to use : $$\frac{1}{f_e} + \frac{1}{f_c} = \frac{1}{d}$$ here $1/f_c$ is the optical correction ( in diopters when the corresponding focal length $f_c$ is measured in meters ) , $f_e$ the focal length of the relaxed eye lens , and $d$ the inner diameter of the eyeball . note that $1/f_c$ is positive when correcting for farsightedness ( $d &lt ; f_e$ ) , and negative when correcting for nearsightedness ( $d &gt ; f_e$ ) .
let me focus on the context of rigorous equilibrium statistical physics ( see , e.g. , georgii 's book " gibbs measures and phase transitions" ) . there , one works with probability measures on infinite systems , often on a lattice ; let me assume it is $\mathbb{z}^d$ . in this context , a macroscopic observable is defined as a function $o:\omega\to\mathbb{r}$ ( where $\omega$ is the set of all configurations $ ( \omega_i ) _{i\in\mathbb{z}^d}$ ) which does not depend on the values of any finite sets of spins $\omega_i$ ( technically , one says that such an observable is measurable with respect to the tail $\sigma$-field ) . let me give you some examples of such observables , in the simple case of ising-type systems , i.e. , with $\omega=\{-1,1\}^{\mathbb{z}^d}$ . let $\sigma_i$ denote the spin at $i$ , $\sigma_i ( \omega ) =\omega_i$ . $\circ$ averages of local observables , e.g. , $$ \lim_{\lambda\uparrow\mathbb{z}^d} \frac1{|\lambda|}\sum_{i\in\lambda} \sigma_i\ ; . $$ $\circ$ events such as " there are no infinite connected components of $-1$-spins " . in both cases , changing a finite number of spins does not modify the value of the observable $o$ . one nice thing about this definition is that one can prove very generally that such observables take deterministic values ( i.e. . , are almost surely constant ) with respect to any pure phase ( extremal gibbs measure ) . in other words , they do not fluctuate ( remember one deals here with infinite systems ) .
when you say : " however , i am pretty sure that this is a dirac mass , and not a majorana mass . " that is where you are confused . ( why did you think you were sure of this ? ) a majorana mass has that form , and so does a dirac mass . they have the exact same feynman rule arrow structure when you use 2-component notation . it is just that for a majorana mass , the 2-component fields being connected are the same , and for a dirac mass they are different ( typically with opposite charge under some gauge or global symmetry ) . the answers about the majorana-weyl condition are not relevant . in 4 dimensions , a majorana fermion is simply a 2-component weyl fermion with a mass term by itself . a dirac fermion is a pair of 2-component weyl fermions with a mass term connecting them .
the answer is no , as lubos motl has already pointed out . here i would like to make a couple of general remarks . 1 ) on one hand , the notion of superalgebras is a huge topic , which includes , e.g. , associative superalgebras and lie superalgebras . important examples of lie superalgebras are super-poincaré algebras . 2 ) on the other hand , a clifford algebra $\mathrm{cl} ( v , g ) $ over a vector space $v$ satisfies $$vw+wv=2g ( v , w ) {\bf 1} , \qquad v , w\in v . $$ in physics applications , the vector space $v$ is often a vector space spanned by a basis of gamma matrices , $$\gamma^{\mu}\gamma^{\nu}+\gamma^{\nu}\gamma^{\mu}=2g^{\mu\nu}{\bf 1} . $$ in more mathematical applications , the vector space $v$ sometimes comes with an odd grading , so that the anticommutator is a supercommutator $$ [ v , w ] =2g ( v , w ) {\bf 1} , $$ which can be viewed as a super heisenberg algebra with odd grading , and which is hence an example of a superalgebra . more generally , the vector space $v$ could be a super vector space $v=v_0\oplus v_1$ with both an even and an odd sector , leading to a notion of super clifford algebras .
geant is a framework---which means that you use it to build applications that simulate the detector and physics you are interested in . the simulation can include all of physics and the complete detector including electronics and trigger ( i.e. . you can write your simulation so that it output a data file that looks just like the one you are going to get from the experiment 1 ) . 2 the various parts of geant are validated by being able to correctly predict the outcomes of experiments . particular models are tuned on well known physics early in the analysis of the data . this allows you to get simulated optical properties , detector gains and so on correctly matched to the actual instrument . geant is also heavily documented . read the introduction and the first two chapters of the user 's guide for application developers , which will give you the basics . after that you can delve into the hairy details in the physics and software references . there is much , much too much to cover in a stack exchange answer . ( i mean literally . . . . if i tried i would end up overrunning the 32k characters per post limit . ) it helps to know that geant4 derives from geant3 and earlier efforts . this thing has a history that goes back for decades and has been tested in thousands of experiments large and small . the use in the higgs search goes something like this we have a theory--the standard model--which tells us what coupling to expect for the particle we hope to detect we write ( and test ) a geant physics module implementing those physics . maybe more than one . we may need to write a new event generator or tweak an existing one in parallel to this effort . you construct a geant simulation of your detector . you include a simulation of the electronics , trigger and so on . 3 you simulate a lot of data from the desired channel and from possible interfering channels ( including detector noise and backgrounds ) . you are going to use a cluster or a grid for this , because it is a big problem you combine this simulated data . you run your analysis on the simulated data . 4 you extract from these results an " expected " signal . actually , you did all of the above at lower precision several times during the design and funding phase and used those result to determine how much data you would have to collect , what kinds of instrumentation densities you needed , what data rate you had to be able to support and so on ad nauseum . once you have got the data , you start by showing that : you can detect lots of well known physics in your detector ( to validate the detector and find unexpected problems ) 5 that your model correctly represents the detector response to that well known physics ( to let you debug and tune your model ) then you may need to re-run some of the " expected " processing . only then can you try to compare data to expectation . 6 1 indeed the data format is often thrashed out and debugged from the mc before the experiment is even built . 2 for big , complicated experiments like those at the lhc geant is usually paired with one or more external event generators . in the neutrino experiments i am currently working on that means genie and cry . not sure what the collider guys are using right now . 3 for speed reasons we often simulate the electronics and trigger outside of geant proper , but this decision is made on a case by case basis . 4 indeed the analyzer is often programmed and debugged from the mc output before there is real data . 5 this is also where most of the actual repetition of results in the particle physics world comes from . you will not get funding to repeat bigexper 's measurement of the wingding sum rule , but if your proposed nextgen spectrometer can do that as well as your spiffy new physics ( tm ) it helps your case with the funding agencies . 6 many of these steps will be done by more than one person/group in the collaboration to provide copious cross-checks and protection against embarrassing mistakes . ( see also , opera 's little issue last year . . . )
this choice is closest to the the correct one . i am tempted to shrug of the entire particle exchange as a mere numerical convenience ; a discretization of the maxwell equations perhaps . i am reluctant to say " virtual particle " because i suspect that term means something different to what i think it means . and virtual exchange is a correct description , because during the interaction the exchanged particle is not on mass shell . keep in mind that in the microcosm of particles nature is quantum mechanical . the particle scattering on another particle and the momentum and energy and quantum number exchanges between them are all described by one wave function , one mathematical formula that gives the probability for the interaction to take place in the way it has been ( will be ) observed . . thus it is not a matter for " knowing " but a matter of " being " . the feynman diagrams that give rise to the " particle exchange " framework are just a mathematical algorithm for the calculations and help in understanding how to proceed with them . to see how classical fields are built up by the substructure of quantum mechanics see the essay here .
the two phenomena feynman referred to are the $q\vec v\times \vec b$ part of the force acting on an electric charge carrier ; and the $\nabla\times \vec e =-\partial_t \vec b$ dynamical maxwell 's equation . in the most general situation when both the magnetic field and the shape of the wire is changing , we have to use and add both terms . that is true in each reference frame because for a complicated space-dependent , time-dependent geometry of the wires and fields , there will not be any inertial frame in which one of the phenomena would completely vanish . i believe that it is more likely ( but not certain ) feynman only meant this thing – that there is no way to eliminate or " explain " one of the terms in the general situation . on the other hand , the fact that both terms have the same origin is a consequence of special relativity and it was indeed one of the motivations that led einstein to his new picture of spacetime . it seems somewhat plausible to me that feynman was ignorant about this history . the full theory of electrodynamics is nicely lorentz-covariant and it implies both terms . however , these terms are not not really the same . the lorentz force comes from the integral $q a_\mu dx^\mu$ over the world lines of charged particles while maxwell 's equations arise from the $-f_{\mu\nu}f^{\mu\nu}$ maxwell lagrangian . so feynman would also be right if he said that the two terms can not be transformed to each other by any symmetry transformation . still , one can make physical arguments that do involve such transformations and imply that the two phenomena are inseparable . for example , if we assume the maxwell equation , it follows from the lorentz symmetry that $\vec e$ must transform as the remaining 3 components of the antisymmetric tensor whose purely spatial components give $\vec b$ . but then it follows that the force acting on a charged particle , $q\vec e$ , must also be extended by the remaining term $q\vec v\times \vec b$ for the theory to be lorentz-invariant . or one can run the argument backwards . still , we are dealing with transformations of two different terms in the action that just happen to have a " unified , simply describable " impact on the emf in wires with magnetic flux . the simplification and unity only occurs if we assume that the two different kinds of phenomena are in the action to start with but they deal with the same fields which respect the same lorentz symmetry ; and if we study situations that are " understood " or " simplified " on both sides in which some effects , e.g. the magnetic ones , are absent . let me say it differently : if the area enclosed by a wire goes to zero , it is an objective thing that is clearly independent of the reference frame . so one should not expect the shrinking of the area is just a matter of inertial systems ; it is a frame-independent fact . on the other hand , it is not shocking that the emf ultimately only depends on one thing , the change of the flux , which has a simple form although it may have different origin . i would conclude that feynman was more right than wrong . lorentz symmetry operates in both phenomena and it is the same one which is a constraint on the most general theory ; however , the fact that both possible sources of the changing flux influence the emf in the same way is a sort of " coincidence " , at least if we use the conventional variables to describe the electromagnetic phenomena .
my incomplete understanding is that the width of pulsars generally seems to depend on their periods and the angle between their magnetic axes and rotational axes . the general trend is for shorter period pulsars to have pulse widths that are a larger fraction of their periods . see on the pulse-width statistics in radio pulsars for some more detail . it looks like there are pulse half widths as narrow as a few degrees ( ~1/100 of a period ) , and some that are as wide as ninety degrees ( 1/4 of a period , meaning that if we were able to see both pulsing sides , the pulsar is on ~1/2 the time ) . given that level of variation , i am not sure if there is a very general sort of plot of intensity versus phase . some are nicely gaussian pulses , with a nice interpulse at 180 degree phase separation , whereas others have much more structure . take a look at some of the figure in multi-frequency integrated profiles of pulsars ( there are a total of 34 pulsar profiles plotted , and it should be available to all as it was posted to arxiv ) .
i will do the case where the material is homogeneous and isotropic , $\rho = \sigma^{-1}$ is a constant proportional to the identity matrix . we are interested in the steady state , where none of our variables depend on time . we have $\nabla \times e = 0$ from faradays law and , $\nabla\cdot j = 0$ from the equation of continuity , where $j$ is the current density . ohm 's law tell us that $j=\sigma e$ . taking the divergence of ohm 's law we get $\nabla \cdot e = 0$ . therefore in steady state $e$ must be a divergence-free , curl-free function . this means that the potential $\phi$ ( $\nabla\phi =e$ ) obeys laplace 's equation , $\nabla^2 \phi = 0$ . to solve this we need the appropriate boundary conditions which are as follows . at the boundary where your resistor connects to a lead the potential $\phi$ must be the same value as the potential on the lead . at the boundary of your resistor not connected to a lead you can not have current flowing out , so the appropriate condition is $\nabla\phi\cdot n = 0$ , where $n$ is the surface normal . this is sufficient to determine a single solution to laplace 's equation . laplace 's equation is a very nice and friendly equation and there is a lot of material on numeric and analytic solutions available , although the mixed boundary conditions will be annoying . once you have your solution to laplace 's equation you need the total current . to get that , pick any cross sectional surface $s$ of your resistor , and using $j = \sigma e = \sigma\nabla\phi$ , we get the total current i is equal to $i = \sigma\int ds\cdot\nabla\phi$ . then you can use $r = v/i$ to get an effective resistance . the case with a non homogeneous or non isotropic material is similar , you just end up with a different equation from laplace 's equation , which may be a little more annoying to solve . i can not imagine a dough based system will need this level of precision though : ) . for anything dough based probably just assuming its a cylinder and using $r = \rho l /a$ will get you close enough no ? i always thought of dough physics as really being an order of magnitude sort of game , like astronomy .
gravitational potential is a scalar quantity so can be added algebraically directly for both ( or more ) bodies . also gpe is just gravitational potential times mass . $$e=\underbrace{\big ( \sum p\big ) }_{\text{due to all bodies in vicinity}}\times m$$ now , rest of your aproach is allright ! continue using this .
great question lucas . the velocity of an object in orbit around a massive body can be expressed roughly as $v ( r ) \sim \sqrt{ \frac{gm}{r}}$ the closer you are to the mass ( e . g . the black-hole ) , the bigger v ( r ) becomes . it turns out , for a black-hole like the one at the galactic center , with stars about 100 au away . . . . they travel at about 500 km/s---fast ! now , the effects of general relativity are only significant when you are near the event horizon . in this case , even though the stars are relatively ' close ' ( about $10^{15}$ cm ) , they are still almost about 1000 times further away than the event-horizon ! and so the effects of general relativity ( e . g . time dilation ) are very very small ( in this case , currently unobservable at all ) .
i would recommend getting to know the telescope before adding a bunch of accessories . a few things i would recommend : • nightwatch by terence dickinson ( firefly ) . this is by far the best beginner 's book on astronomy , and contains an excellent set of star charts for finding objects in the night sky . • 10x50 binoculars . useful in themselves , but also excellent to scout a new part of the sky before exploring with the telescope . i use my 10x50s as much as i use my telescope . orion scenix 10x50s are excellent , and are currently on sale . • a good star atlas . my favourite is the sky and telescope pocket sky atlas ( sky ) . • a red flashlight . • a solar filter . the best ones are made by jim kendrick using baader astrosolar film , better optically than orion 's glass filters : http://www.kendrickastro.com/astro/solarfilters.html • the supplied eyepieces are a good match for this telescope . there is not much to look at that requires a wider field of view than the 17mm , and the 6mm is about as high as you can go with a 4.5" telescope . any more magnification than 75x would be " empty magnification . " for the same reason , i would not recommend a barlow . by the way , the " shorty plus " is twice as good as the " shorty " !
they talk about contact voltages because it affects “stop voltage” as measured by their instruments , but it doesn’t affect $\delta u_{\rm stop} / \delta\nu$ since aforementioned contact voltages are assumed independent of illumination conditions . because a photovoltaic cell made of a homogeneous piece of material won’t work . a piece of conductor will not produce any voltage , whereas you will be unable to extract the current from a dielectric . no , it doesn’t . you can also read an interesting discussion at ambiguity on the notion of potential in electrical circuits ? . btw , where does wikipedia tell you about the contact voltage ? https://en.wikipedia.org/wiki/volta_potential , or… ? a posting that vaguely refers to “books and wikipedia” without any specificity demonstrates a shortage in internet communication skills , especially given that the people refers to several different things as to contact voltages . i don’t known ( i’m not an english speaker , usually ) .
1 ) uncertainty principle is momentum and position or energy and lifetime , not energy and position . 2 ) if we confine the two particles in a infinite square well then they can only be in the well . their wavefunctions go to zero at the boundaries . 3 ) true 4 ) false . two particles can have the same energy . but thy have to be in two different states . for example , two electrons could have 1ev of energy , but one be in a spin up state and one be in a spin down state . 5 ) no . . . maybe somebody else can explain this one
from your comment it seems you know $x_0$ , $x_1$ , $x_2$ ( in your question you state you want to know $x_2$ ) , $t_1$ and $t_2$ . now as stated in my comment $$x_1=x_0+v_0t_1+\frac 1 2 a t_1^2$$ and $$x_2=x_1+\frac 1 2 a ( t_2-t_1 ) ^2 . $$ solving for $a$ gives $$a=\frac{2 ( x_2-x_1 ) }{ ( t_2-t_1 ) ^2} . $$ thus , $$x_1=x_0+v_0t_1+\frac{ ( x_2-x_1 ) }{ ( t_2-t_1 ) ^2} t_1^2 . $$ solving for $v_0$ is all that can be done now . all this can be adapted to whatever exactly is given in your problem .
in most cases , it does not really make sense to talk about a lowered effective mass caused by sitting in a gravitational potential well , since the equivalence principle says that locally the spacetime looks flat , and hence it looks like the gravitational field vanishes . however , in certain special cases , there is a sensible notion of energy that is different from your rest mass . this is when you have a timelike killing vector , which means there is a preferred time coordinate under whose flow the metric is invariant . the existence of time translation symmetry leads to a conserved energy . if $\xi^a$ is the killing vector representing the time flow , and $p^a$ is the 4-momentum of an object , the conserved energy is $$e = -g_{ab}\xi^a p^b$$ while the rest mass is $$m = ( g_{ab}p^a p^b ) ^{1/2} . $$ the schwarzschild spacetime is a classic example of this . the metric is $$ds^2 = -\left ( 1-\frac{2gm}{r}\right ) dt^2+\frac{dr^2}{\left ( 1-\frac{2gm}{r}\right ) }+r^2d\omega^2$$ since the metric is independent of time , it has a killing vector $\xi^\alpha = ( 1,0,0,0 ) $ . let 's first consider the 4-momentum of an observer initially at rest at some radius $r_0$ . initially at rest here means they start of with $p^a \propto \xi^a$ , and the normalization of $p^a$ tells us that it is $$p^\alpha = m\left ( \left ( 1-\frac{2gm}{r_0}\right ) ^{-1/2} , 0,0,0\right ) , $$ then the energy for this particle is $$e = -g_{0\alpha}p^\alpha=m\left ( 1-\frac{2gm}{r_0}\right ) ^{1/2}$$ as long as we are outside the event horizon $r=2gm$ ( which is the only place where the energy really makes sense ) , this shows that the killing energy $e$ is less than the rest mass . and if you are far away from $r=2gm$ , you can expand to first order in $gm/r$ to get $$e\approx m - \frac{gmm}{r_0}$$ which is the rest mass minus the newtonian gravitational potential energy . so in this sense the potential energy of the particle is negative , since it is killing energy is less than its rest mass energy . finally , for a particle falling in from at rest at infinity , we use the fact that killing energy is conserved along all points along the geodesic . at infinity , the metric is asymptotically minkowski , and being initially at rest means $p^a\propto\xi^a$ , hence \begin{align} p^\alpha and = m ( 1,0,0,0 ) \\ e and = m \end{align} since $e$ is conserved , we see that in this case it is always equal to the rest mass energy . you can sort of see this as a cancellation between kinetic energy and potential energy : to get the kinetic energy you need to specify who your observer is , so lets say it is the observers at constant radius . their 4-velocity is $$u^\alpha=\left ( \left ( 1-\frac{2gm}{r}\right ) ^{-1/2} , 0,0,0\right ) $$ and they would define the total energy ( rest mass plus kinetic , but not including potential energy ) as $$t = -g_{ab}u^a p^b = \frac{m}{1-\frac{2gm}{r}} \approx m + \frac{gmm}{r} . $$ to derive this , we used the fact that $e=m$ is conserved , which means that $p^t = \dfrac{m}{\left ( 1-\frac{2gm}{r}\right ) ^{1/2}}$ . if we continue to assign the potential energy $v = \frac{gmm}{r}$ , then we get $$e=t-v = m + k-v = m \implies k=v $$ so in some sense the relations you postulated hold when there is a well-defined " effective mass " i.e. killing energy , but in a general spacetime with no timelike killing vector , you will not be able to make a sensible definition of such a thing .
yes easily . fiber-fed 5-10kw nd-yag lasers are commonly used for cutting metal in machine shops . fibers are so transparent , especially when designed for a single wavelength laser , that the power loss and so heating in the fiber is very small . it is generally less than an optically fed laser where dirt accumulates on the lenses . many systems have a thin fiber wrapped around the main power delivery fiber with a small low power laser diode shining through it . if there is a fault in the main fiber , the high power laser beam will leak out and cut the thinner fiber . the loss of light in the wrapping fiber signals the controller to kill the power to the main laser .
no . the world we observe with our five senses is three dimensional . two independent measurements are enough to calculate the three dimensional position of everything , which is what our brain does with the input of two eyes . more eyes would only over constrain the solution , and might help in low lighting or long distance estimates when the errors are large .
breakdowns are electron cascades . there are different kinds : 1 ) intrinsic breakdown of the material occurs when the electric field is sufficiently strong to ionize an atom of the dielectric ( or accelerate a stray electron sufficiently to do the same ) , with the resultant new free electrons then being accelerated by the field to repeat the process with another atom . if more free electrons are produced than reattached , the process grows exponentially and breakdown results . that said , intrinsic breakdown is rare , because other types of breakdown occur at much lower field strengths : 2 ) if there is a void in the dielectric material ( a bubble ) the residual gas in the void will break down at some ( lower ) electric field strength ( again an electron cascade ) , and the freed electrons will strike the sides of the void , heating the dielectric and eroding it . this type of discharge is small and perhaps unnoticeable , but given enough time , the void will grow and eventually destroy the dielectric . 3 ) surfaces between different materials ( say the boundary between the dielectric and the material encasing the capacitor ) can delaminate , again creating an empty space in which gas breakdown can occur .
assume horizontal rod sum of forces equals acceleration of center of gravity $$ t - m g = m \ddot{y}_c $$ sum of moments about center of gravity equals mass moment of inertia times angular acceleration $$ t \frac{l}{2} = i \ddot{\theta} $$ as the bar rotates by a small angle $\theta$ the center of gravity height is found by $y = -\frac{l}{2} \theta$ or by differentiating twice , $\ddot{y} = -\frac{l}{2} \ddot{\theta}$ together it all comes as $$ \begin{aligned} t and = m g - m \frac{l}{2} \ddot\theta \\ t \frac{l}{2} and = \left ( \frac{m}{12} l^2 \right ) \ddot \theta \end{aligned} $$ solve the above for $t$ and $\ddot{\theta}$ . hint the value does not have to be more than $\frac{m g}{2}$ .
you can do the transformation to the relative coordinate $\mathbf{r} = \mathbf{r}_1-\mathbf{r}_2$ and center-of-mass coordinates $m\mathbf{r} = m_1\mathbf{r}_1+m_2\mathbf{r}_2$ and do one of the integrals trivially provided the two functions inside the integrals depend only on $\mathbf{r}$ ( or only on $\mathbf{r}$ ) . otherwise you will still be left with two integrals one over $\mathbf{r}$ and the other over $\mathbf{r}$ . in the part that you refer to in pathria 's book the two functions $$f ( \mathbf{r}_1 ) = r \frac{\partial u ( r ) }{\partial r} ; \qquad f ( \mathbf{r}_2 ) = g ( \mathbf{r}_2 - \mathbf{r}_1 ) $$ depend only on $\mathbf{r}$ . a transformation to $\mathbf{r}$ and $\mathbf{r}$ coordinates then decouples the $\mathbf{r}$ integral which has given a factor of the system volume ( equation ( 16 ) in the image ) . further if the functions inside the integrand depend only on $r=|\mathbf{r}|$ , then one can transform to spherical coordinates as is done in pathria . .
it goes out forever , but the total energy it imparts is finite . the reason is that when things fall off as the square of the distance , the sum is finite . for example : $$ \sum_n {1\over n^2} = {1\over 1} + {1\over 4} + {1\over 9} + {1\over 16} + {1\over 25} + . . . = {\pi^2\over 6} $$ this sum has a finite limit . likewise the total energy you gain from moving a positive charge away from another positive charge from position r to infinity is the finite quantity $$\int_r^{\infty} {qq\over r^2} dr = {qq\over r}$$ so there is no infinity . in two dimensions ( or in one ) , the electric field falls off only like ${1\over r}$ so the potential energy is infinite , and objects thrown apart get infinite speed in the analogous two-dimensional situation .
the basic setup is correct , conservation of energy might be the quickest way to go . $$ ( m_1 -m_2 ) g h = \frac 1 2 i \omega^2 + \frac 1 2 ( m_1+m_2 ) v^2 , i=\frac{mr^2}{2} , \omega = v/r$$ gives me one of your options as the result . the two $m$ in your formula seem to refer to different quantities .
the first way to consider the problem , as we have a simple rigid body is to consider two points on it , say one end ( the one that will touch the table first ) and the center of mass ( or the other end ) . the embedding configuration space is thus $r^2 \times r^2$ ( the three coordinates of the end of the rod and the three coordinates of the other ) if we consider that the rod can only move in the vertical plane including a line on the table ( i hope this is the actual question . . . ) . the action of the surface lead to the [ holonomic ] constraint equation that can be written $f ( x , y ) = 0$ which is the one of a surface ( here a curve ) in the configuration space $r^2$ . in addition to this constraint equation we have the newton 's equation which is $m \frac{d^2}{dt^2} \vec{r} = \vec{f}+\vec{n}$ ( where $n$ is the force of constraint . this is for one end . for the other ( or for the center of mass ) you have two similar equations , with the constraint being that the other end ( or the center of mass ) is at a given distance to the other end . in that view , we can consider the points as being free and we have 4 newtonian equations and 2 constraint equations thus reducing the degrees of freedom to 2 . we can also consider that the constraints do not work and that they are perpendicular to the plane of the table : $\vec{n} = \lambda \vec{\nabla}f$ where $f$ is the one of the constraint equation . newton 's equation can be rewritten $\langle m \frac{d^2}{dt^2} \vec{r} - \vec{f} , \vec{\xi} \rangle = 0$ where $\xi$ is a tangent vector defined as $\langle \vec{\xi} , \vec{\nabla} f \rangle= 0$ ** ( 1 ) ** . this is d'alembert principle . as the components of $\vec{xi}$ are constrained by ( 1 ) , this lead to 2n ( newton ) - 2 ( constraints ) independant relations . the next step is to choose a system of coordinates where ( 1 ) is automatically satisfied : these are the generalized coordinates . the rod is modeled when in addition to specifying the coordinate of the end that is on the plane you also specify the angle that the rod makes with the surface : we can choose to independent generalized coordinates : $x$ and $\theta$ . the motion of the system ( now the system is not free anymore ) will take place in the manifold $m$ where $x$ and $\theta$ are independent coordinates . these $ ( x , \theta ) $ are usually written $q = ( q_1 , q_2 ) $ . this manifold is the direct product of $r$ and $s^1$ ( the angles take values in a 1-torus , a segment whose ends are identified ) . it can then be derived that the motion in these coordinates follow lagrange 's equations . the lagrangian is this case as the natural $l ( x , \theta , \dot{x} , \dot{\theta} ) =t-v$ form . for your case ( b ) this problem is simplified : x is fixed and you should only care about $\theta$ , you have $l = \frac{m}{2} l^2 \dot{\theta}^2 - l m g sin\theta$ where $l$ is the distance between one end and the center of mass . from lagrange 's equation you obtain $m l \ddot{\theta} = -m g cos\theta$ from that you can obtain the complete motion of the rod . for the case ( a ) you have to consider a more complete lagrangian : $l = \frac{m}{2} ( l^2 \dot{\theta}^2 - 2 l \dot{x} sin\theta \dot{\theta} + \dot{x}^2 ) - l m g sin\theta$ this lagrangian lead to a more complicated motion ; if i did the calculation correctly we have $\ddot{\theta} = - \frac{g}{l} cos\theta$ $\ddot{x} = l sin \theta \ddot{\theta}$ . to obtain the constraint , you can solve this and then you go back to newton 's equations . in the case ( b ) you have in addition to force $\dot{x} = \ddot{x} = 0$ to obtain the tangential constraint .
if by " heating " you mean " adding heat " , then the answer is yes , except for the unusual situation where a material is at negative temperature . when you add heat to a system $$ ds = {dq\over t} $$ and this is always positive when t is positive . this is the definition of the thermodynamic temperature in the most fundamental way of looking at it , the partial derivative of s with respect to u at fixed volume and fixing all other conserved quantities is the reciprocal temperature $\beta$ . the only exception to this rule is for systems where there is a negative temperature . this occurs in spin-systems , where there is a maximum energy state and a minimum energy state . as you add heat energy to these systems , the entropy rises , then falls , which means that the inverse temperature smoothly goes down to zero , then turns negative . this corresponds to a temperature that goes to infinity and comes back out the negative side at negative infinity , going toward zero from the other diretion . negative temperature systems are rare , since they require an upper bound on the energy , which means you are restricted to nuclear spins , which are decoupled from electron spins for a long time . the question of whether entropy always increases with increasing temperature is a different question , and this has to do with the sign of the specific heat . for normal systems , the specific heat is always positive , so that the temperature increases with energy ( beta decreases ) , and this is true for negative temperature systems too , as long as you define the specific heat properly as the change in beta with u . even at negative inverse-temperature , the negative inverse-temperature becomes larger negative with increasing energy . for neutral large black holes , as carl brannen points out , the specific heat is negative .
you are right ; hamiltonian for center of mass of hydrogen atom should be : $h=\displaystyle\frac{p^2}{2 ( m+m_e ) }$ where $p$ is momentum of hydrogen atom ( please check what is $p$ in your book ) . now you can also write it as : $h=\displaystyle\frac{p^2}{2m} ( \frac {m}{m+m_e} ) $ $=\displaystyle\frac{p^2}{2m} ( \frac { ( m+m_e ) -m_e}{m+m_e} ) $ $=\displaystyle\frac{p^2}{2m} ( 1-\frac {m_e}{m+m_e} ) $ $=\displaystyle\frac{p^2}{2m}-\frac {m_e}{2m ( m+m_e ) }p^2$ so there must be some typos in your book .
intuitively , if the volume integral of a function is 0 over any arbitrary volume , the function itself must be 0 at all points in space . more concretely , consider a function for which $\int_v \ , f \ , \mathrm{d}x = 0$ for any volume $v$ . then , $\int_{v+dv} \ , f \ , \mathrm{d}x = 0$ for any infinitesimal addition to v . $$\int_{v+dv} \ , f \ , \mathrm{d}x - \int_v \ , f \ , \mathrm{d}x = \int_{dv} \ , f \ , \mathrm{d}x = f ( \text{at dv} ) = 0$$ in your case , $f = \nabla \cdot b$ , so $\nabla \cdot b = 0$ . ( note : i was a bit lazy with my notation above , so it is not a formal proof . however , it should still provide the intuitive answer to your question . )
in $s'$ , there is a flux of material across any surface of constant $x'$ as the material has the velocity of the $s$ frame in $s'$ . the material flux is the product of the material density in s ' and $v_{sx}$ . the invariance of charge means that the current density goes as the material flux$^1$ . $j'_x = \rho ' v_{sx} = \gamma \rho v_{sx}$ [ 1 ] extended answer to address ops comment . let $n$ be the number density , the number of particles per unit volume , in the frame of reference $s$ in which all of the particles are at rest . let $q$ be the electric charge , in $s$ , carried by each particle . in $s$ , the charge density is $\rho = qn$ . now , we know that $n ' = \gamma n$ due to length contraction . however , only if q is lorentz invariant will $\rho ' = qn ' = \gamma q n = \gamma \rho$ in $s'$ , there is a number flux across surfaces of constant $x'$: $n'_x = n'v_{sx} = \gamma n v_{sx}$ . but , since each particle carries charge , there is a charge flux , a current density $j_x = \rho ' v_{sx} = \gamma \rho v_{sx}$ . note that if $q$ were not lorentz invariant , e.g. , $q ' = \gamma q$ , then the charge and current density would not be components of a four-vector but rather a four-tensor since we would get a factor of $\gamma^2$ under a lorentz transformation .
let us call the two electric charges $q$ and $q$ with electric fields $|{\bf e}_q|=\frac{k_e |q|}{r^2_q}$ and $|{\bf e}_q|=\frac{k_e |q|}{r^2_q}$ , respectively . here $k_e=\frac{1}{4\pi\varepsilon_0}$ . the energy $$\tag{0} u~=~\frac{\varepsilon_0}{2}\iiint_{\mathbb{r}^3}\ ! d^3r ~|{\bf e}_q+{\bf e}_q|^2~=~u_q+u_q+u_{qq} $$ of the total electric field contains three contributions : the energy $u_q$ of the electric field of a charge $q$ $$\tag{1} u_q~=~\frac{\varepsilon_0}{2}\iiint_{r\geq\delta}\ ! d^3r ~|{\bf e}_q|^2~=~ \frac{k_eq^2}{2} \int_{\delta}^{\infty}\frac{dr}{r^2}~=~\frac{k_eq^2}{2\delta} . $$ the energy $u_q$ of the electric field of a charge $q$ $$\tag{2} u_q~=~\frac{\varepsilon_0}{2}\iiint_{r\geq\delta}\ ! d^3r ~|{\bf e}_q|^2~=~ \frac{k_eq^2}{2} \int_{\delta}^{\infty}\frac{dr}{r^2}~=~\frac{k_eq^2}{2\delta} . $$ in equation ( 1 ) and ( 2 ) , we have inserted a regulator $\delta$ . if the regulator $\delta\to 0$ is removed the energy becomes infinite . the energy from the exchange term $$ \tag{3} u_{qq}~=~\varepsilon_0\iiint_{\mathbb{r}^3}\ ! d^3r ~{\bf e}_q\cdot {\bf e}_q~=~\frac{k_eqq}{r} , $$ where $r$ is the distance between the two charges . the triple integral ( 3 ) can be analytically calculated by ( among other things ) using the azimuthal symmetry of the integrand . if we slowly vary the separation $r$ , we will only detect the variation of the exchange term ( 3 ) ( the coulomb energy ) as the two other contributions ( 1 ) and ( 2 ) remain fixed .
yes , you have taken the analogy too far : electrons do not actually move through the wire in the way that fluids flow through a pipe . hence , there is no reason why an analog of bernoulli 's principle should apply .
( revised somewhat to try and prevent misunderstandings . ) first : i would like to discourage you from trying to interpret superpositions in terms of schr&ouml ; dinger 's cat , when what you are trying to understand involves coherent unitary operations performed on the system . considering superpositions of states of a massive number of particles makes it difficult to meaningfully consider different bases of measurement , which ( among other things ) is necessary to describe phenomena such as entanglement . ( the fact that it prevents meaningful discussion of coherent evolution is basically the phenomenon of decoherence . ) if we were to try and describe teleportation in terms of cats , we had have to describe it this way : what is put into the boxes are not live cats and dead cats , nor even two independent indeterminate live/dead cats , but a pair of cats a and b whose states of living are entangled with one another in a single pure state $|\psi\rangle$ which is a superposition of one live cat and one dead , but where there is no definite fact of whether a is the live one and b the dead one , or vise versa . the measurement we perform is not just to open a box to see a cat is alive ; the pair of entangled cats are nothing but a correlated system which we will use to transmit more information . in this case , we are interested in transmitting yet another cat c , whose state $|\varphi\rangle$ of life/death is also indeterminate , but independent of a and b . we perform a quantum measurement corresponding to disentangling a and c ( although they were not entangled to begin with ) , and then observe whether they each of a and c is alive or dead ( which will cause a collapse of their wavefunctions ) . depending on the outcomes of these measurements &mdash ; a alive and c alive , or a alive and c dead , or etc . &mdash ; we then perform two operations : an operation which will toggle the state of b 's life or death ; either killing it or resurrecting it miraculously , or &mdash ; more generally , as b may not be definitely alive or dead &mdash ; rotating the state of b 's health through an angle of 180&deg ; about a cycle of life and death ; another operation which does , er , something which i can not describe easily in terms of the life and death status of cats . perhaps it changes the colour of its coat , or something ; after all , c 's coat may be differently coloured than b 's was to begin with . this is rather unweildly , and as you might imagine , does not actually shed any light on the matter . but this is not surprising &mdash ; remember , schr&ouml ; dinger 's cat was a thought experiment which was designed quite deliberately , by erwin schr&ouml ; dinger , to be absurd . if you hope to get any intuition from it , you are almost certainly going to fail , and if you seem to get a simple explanation for what is going on by pretending a quantum system is a cat , you have probably missed some essential details . quite seriously , teleportation does involve two elements which are very much quantum : the fact that an entangled resource state is used &mdash ; two particles a and b , neither of which are the system whose state is being ' teleported ' , in a joint state $$|\psi\rangle_{a , b} \ ; =\ ; \tfrac{1}{\sqrt 2}\bigl ( |0\rangle_a|1\rangle_b \ ; -\ ; |1\rangle_a|0\rangle_b \bigr ) \qquad\quad \bigr ( \text{or a similar state}\bigr ) $$ &mdash ; and that the state $|\varphi\rangle$ of a third system c can be transmitted to b , by performing a bell measurement on a and c , which can be simulated by performing the transformation $ ( h \otimes i \otimes i ) ( \text{cnot} \otimes i ) $ on the state $|\varphi\rangle_c\otimes|\psi\rangle_{a , b}$ and measuring $c$ and $a$ in the standard basis , and then performing simple single-qubit operations on b depending on the outcome . the information about the measurements of a and c have to be transmitted to b somehow ; these are sent as classical signals , and travel only at the speed of light . until they arrive , the system at b is essentially random : whoever had the particle a knows what state it is in , but it may not be the original state of c , and whoever owns b has no useful way of doing anything with it until they know what operations they have to perform to recover the state of c . however , despite the fact that the measurement outcomes on a and c are ' classical ' , the state which can be recovered with them is fully quantum . for more complete details about precisely what measurement is performed and how the entangled state is used , you could consult the wikipedia article on teleportation , this analysis of teleportation elsewhere on this site , or any reasonable text on quantum information ( such as nielsen and chaung ) . ultimately , there is little that you can do but crank through the mathematics , because that is what shows what is going on . in particular , if you want to see how teleportation does not make information instantly available at b without communication , you should look at it in terms of what the density operator at b is : up until it obtains the outcome of the measurements , it looks maximally mixed , i.e. &nbsp ; totally random . in any case , that is why it is called quantum teleportation . you could argue about the ' teleportation ' part of the name &mdash ; it is more like a radio transmission the than classic star trek style materialization via a beam of energy &mdash ; but there is no doubts about the quantumness of it .
after a really brief cursory review of the literature , i think that a dalitz decay is a meson decay that involves two leptons in the final state , plus a photon . a double dalitz decay has four leptons in the final state : see this paper and this paper for examples of the usage . the dalitz decay is when a virtual photon from 2 photon decay of $\pi_0$ internally converts to a real lepton pair before it gets too far , and analogous thing for other meson or higgs processes ( two electrons from an internal photon conversion , plus a neutral object ) . i guess that the usage comes from the kinematic decay product phase space is described by a dalitz plot , hence the name . i do not think it is anything deep .
i suspect you are thinking that pulling a string wrapped round a cylinder would compress it in the same way that pulling on a string wrapped round a coke can would crush it . if so , this will not happen because the tension in the string can not produce any force out of the 2+1d manifold that it lives in . however you are quite correct that the tension in the string will contribute to the stress-energy tensor , and therefore act as a source of gravity . i have to confess i do not know what the effect would be in 2+1d , however in 3+1d the effect would be similar to a cosmic string . assuming the mass of the string is negligable , so the tension is the only contribution to the stress-energy tensor , the spacetime around the string is flat but has an angular deficit . that means you do not feel any gravitational field from the string , but if you travel in a circle around the string you had find you had rotated by less than 360º . i suppose the equivalent to your 1d object in the 2+1d universe would be a 2d object in our 3+1d universe i.e. instead of a stretched string you had have a stretched membrane . an example of this would be a domain wall . this does produce a gravitational field , and in fact the field is repulsive i.e. you would be accelerated away from the wall in a direction normal to it . there is a description of the field produced in this book .
no , because not all the fundamental particles ( in the sense of the standard model ) are stable . in particular , electrons and photons are stable ; muons and tau leptons are not , and will decay into lighter leptons , e.g. $\mu^-\to e^- \bar\nu_e \nu_\mu$ . neutrinos are kind of funny because , while they are prevented from decaying in the traditional sense by conservation of energy , momentum , and lepton family number , they do oscillate - so if you start with an electron neutrino , for example , it will turn into something that can be observed as any flavor of neutrino , and in this way you can find yourself measuring something like $\nu_e\to\nu_\tau$ . but then you can just as well measure $\nu_\tau\to\nu_e$ , and if you have two ( actually three ) particles which can all decay into each other , does it even make sense to call it a decay ? quarks try to be funny but actually wind up just being annoying , because they are never found in isolation so nobody is exactly sure how an isolated quark would behave if you could put one in a universe by itself . that being said , heavy quarks ( charm , bottom , theoretically top ) are routinely observed to decay in high-energy collisions , where asymptotic freedom presumably applies , so it is not much of a stretch to imagine that an isolated heavy quark would decay into lighter quarks plus a pion or leptons . likewise , the weak gauge bosons decay all the time in collisions , so if you managed to create a universe that contained only a $w^\pm$ or $z^0$ and nothing else , it would presumably decay quickly into a lepton and antineutrino or into some combination of hadrons . same goes for the higgs , except with different possible decay products .
nature has no preferences , and therefore entropy tends to increase . sounds paradoxical ? the point is that each microscopic state ( describing the exact position and velocity of each atom in the system ) is equally likely . however , what we typically observe is not a micro state , but a course-grained description corresponding to incredibly many micro states . certain macro states correspond to far fewer micro states than other macro states . as nature has no preference for any of these micro states , the latter macro states are far more likely to occur . the evolution to ever more likely macro states ( until the most likely macro state , the equilibrium state , is reached ) is called the second law of thermodynamics . the decrease of potential energy is the consequence of the first ( energy conservation ) and second ( evolution to more likely macro states ) law of thermodynamics . as macro states with a lot of energy stored in heat ( random thermal motion ) contain many more micro states and are therefore much more likely , energy tends to get transferred from potential energy to thermal energy . this is observed as a decrease in potential energy .
the first assumption is that whatever vev the higgs picks up is constant in space , because this has less energy than one that increases the kinetic term in the lagrangian . so we can do one global transformation to make the vev be in the second component only . you can imagine doing this prior to symmetry breaking , if you know what it is going to be ahead of time , and since the other fields are invariant , bob 's yer uncle . stated differently , the pre-symmetry-breaking electrons and neutrinos are not the ones we observe , so we just label whatever remains as electrons and neutrinos . " without loss of generality " , we work in an electron-neutrino ( global ) basis in which the higgs starts out with only the second component of the vev being nonzero and real . if you buy that part , then it is just a matter of showing that you can perform a gauge transformation that gauges away all the other components of the higgs except the real part of the second component . this gauge transformation will of course mix $\nu$ and $e$ spatially , but you can say that when we perform the path integral we have a gauge redundancy , and so we only integrate along a slice that obeys some gauge fixing condition . the components of $l$ might as well be labeled $l_1$ and $l_2$ . it is only after we have chosen a gauge that we decide , hey , let 's name them $\nu$ and $e$ .
no . momentum is conserved . since momentum is mass times the velocity of the center of mass , if the momentum is zero , the center of mass can not move . alternately , if the center of mass is already moving , it will keep moving indefinitely in a straight line when there are no external forces . however , in curved spacetime the above may not hold . see http://dspace.mit.edu/handle/1721.1/6706
no , you are right . anywhere you can go , you are surrounded by some kind of matter at some nonzero temperature . though most matter is not quite a blackbody , it still does radiate when at finite temperature . however , at low temperatures the intensity of this radiation in the visible range vanishes proportional to $e^{-\frac1t}$ , which means you normally have only few visible photons around , typically much less than those reflected in some indirect way from the sun or artificial light sources . you definitely can not see these photons with your eyes , because they are at a higher temperature as the surrounding and therefore have more thermal noise intensity .
white dwarves used to be the interior of a star , which was the hottest part of the star . they shine white because they are still very hot from this past part of their history . as they age , they will cool , and as they cool , they will lose temperature , and their blackbody profile will shift to redder and redder colors , and eventually into the infared and radio ranges where they will not seem to shine at all to the naked eye . and yes , a white dwarf state is a stable final state of a star , so long as it does not interact further with any matter . if that happens , it is possible to have a white dwarf supernova
it seems to me that " symmetric fission " refers to any fission process where the end products are symmetric about some point . specifically , where the end products are symmetric in their atomic mass . this website explains it quite well , but for completeness i am quoting the relevant paragraph in the article below . it is thought to be helpful for a better understanding to take the atomic numbers into consideration . the atomic numbers of the above-mentioned elements are : ru = 44 , rh = 45 , pd = 46 , ag = 47 , cd = 48 , in = 49 and sn = 50 , respectively . by noticing that the atomic number 46 of palladium is just half that of 92 of uranium , it is supposed that one uranium atom splits into two palladium isotopes . when rhodium ( atomic number 45 ) is produced with some probability ( cross section ) , silver ( atomic number 47 ) is the counter fragment . in the same way , ruthenium ( atomic number 44 ) and cadmium ( atomic number 48 ) are the pairing fragments . thus , the nuclear fission observed by nishina and kimura is highly symmetric .
a group $g$ by itself is not a group of linear transformations , it is an abstract algebraic object . only its representations map its elements ( injectively if the representation is faithful ) to elements $\mathrm{aut} ( v ) $ of some vector space $v$ . now , physics seems to have no need of such abstract language at first . our " vector space " is pretty much our spacetime , and its pretty much $\mathbb{r}^4$ , so your symmetries are really just matrices on that spacetime . the lorentz symmetry is just $\mathrm{so} ( 1,3 ) $ in its fundamental representation on minkowski space $\mathbb{r}^{1,3}$ , right ? or non-relativistic , rotational symmetry is just $\mathrm{so} ( 3 ) $ on $\mathbb{r}^3$ , right ? . . . and then there is angular momentum and spin . if you solve the schrödinger equation for the energy levels of a hydrogen atom , you find that the energy levels are characterized by " quantum numbers " $ ( n , l , m , s ) $ . now $n$ is boring . but $l$ and $m$ are eigenvalues of the spherical laplacian , and lead to the beloved spherical harmonics $y^l_m$ as independent solutions . turns out , if you rotate the system in space , these harmonics behave differently depending on their $l$ ! formally , the space $$h_l := \{\sum_m c_m y^l_m | m \in \{-l , -l+1 , \dots , l\}\wedge c_m \in \mathbb{r}\}$$ is a vector space , and it carries a representation of the rotation group $\mathrm{so} ( 3 ) $ ! but not the fundamental one , if $l &gt ; 1$ . so there is your non-fundamental representation arising solely by solving the equations describing a physical system . it gets even weirder for these rotation groups , since it also turns out that there are objects , the fermions , which do not transform in a representation of $\mathrm{so} ( 1,3 ) $ or $\mathrm{so} ( 3 ) $ , but in a representation of their universal covers , $\mathrm{spin} ( 1,3 ) $ or $\mathrm{su} ( 2 ) $ , respectively . you have no chance to describe the kinds of phenomena you observe for fermions without accepting that they transform that way . and that is not the end of the story . if you build a gauge theory with gauge group $g$ , you will find that the associated field strength of the gauge field must transform as an element of the adjoint representation of $g$ . non-fundmental representations pervade many aspects of ( quantum ) field theory in that way .
first consider there is no friction . the point of contact between the ball and the table moves with the direction of the global motion . now introduce friction : you have kinematic friction slowing down this point thus make the ball roll due to the induced torque . you will have a motion in between the cases of pure sliding and pure rolling . in this case the direction of the friction force is obvious ( by definition of the friction ) . now if you do the things at the limit case , you will have a pure rolling . in that case the point of contact has zero instantaneous velocity and if the motion is horizontal , with constant and angular and linear motion , you do not need any friction , if you had friction , this would induce a torque and the angular momentum will change . if you introduce acceleration or a non horizontal surface : in that case you have static friction : the point cannot move forward , friction is directed opposite to the " accelerated " direction , you introduce a torque .
air has a specific heat capacity of slightly more than 1kj/kgk at room temperature so it takes a 1kw heater 1 second to heat 1kg ( roughly 1 m^3 ) of air 1 deg c fairly humid air ( say 60% rh at 20c ) will contain around 10g/m^3 of water vapour with a specific heat capacity of 1.8 kj/kgk - so it takes almost twice as much energy ( per unit mass ) to heat the water in the air than the dry air itself . but , only 10g in every 1kg of air is water vapour ( ie 1% ) so you only have to do twice as much work to that 1% . in other words - no you will not see any measurable difference .
for a hamiltonian of the form $$\hat h = -\frac{\hbar^2}{2m}\frac{d^2}{dx^2}+v ( x ) $$ in one single spatial dimension , all the energy eigenvalues are non-degenerate , under suitable regularity conditions for $v ( x ) $ . this means that if two eigenfunctions share the same eigenvalue , they must be equal or , at most , differ by a phase . for a proof of this fact , and what sort of horribleness you must introduce into the potential to break this behaviour , try " can degenerate bound states occur in one dimensional quantum mechanics ? " sayan kar and rajesh r . parwani . europhys . lett . 80 no . 3 ( 2007 ) , p . 30004 ; arxiv:0706.1135 . it is important to note that this is strictly a one-dimensional result , and fails to hold as soon as a second degree of freedom - be it spin or a second spatial dimension - is present ; examples for that are trivial to construct .
this approach most definitely does work , it just does not give the fundamental theory of the strong interactions , it gives string theory . string theory was originally defined by venziano 's bootstrap formula for the leading term in an s-matrix expansion , and the rest was worked out order by order to unitarize the s-matrix , not through a field theory expansion . this is good , because string theory is not field theory , and it can not be derived from a field theory lagrangian , at least not in the usual way . the result can nowadays be interpreted in terms of string field theory , or nonperturbative ads/cft constructions , but it is a new theory , which involves infinite towers of particles interacting consistently without a field theory underneath it all . the idea that s-matrix died is a political thing . the original 1960s s-matrix people found themselves out of a job when qcd took over , and the new generation that co-opted string theory in 1984 mostly wanted to pretend that they had come up with the theory , because they were all on the winning side of the s-matrix/field-theory battle . this is unfortunate , because , in my opinion , the most interesting physics of the 1960s and early 1970s was s-matrix physics ( and this is a period that saw the greatest field theory work in history , including quarks and the standard model ! ) chew bootstrap a bootstrap is a requirement that you compute the s-matrix directly without a quantum field theory . in order for the theory to be interesting , the s-matrix should obey certain properties abstracted away from field theory it should be unitary it should be lorentz invariant it should be crossing invariant : this means that the antiparticle scattering should be described by the analytic continuation of the particle scattering it should obey the landau property--- that all singularities of scattering are poles and cuts corresponding to exchange of collections of real particles on shell . it should obey ( mandelstam ) analyticity : the amplitude should be writable as an integral over the imaginary part of the cut discontinuity from production of physical particles . further , this cut discontinuity itself can be expanded in terms of another cut discontinuity ( these are the mysterious then and still mysterious now double dispersion relations of mandelstam ) . this is a sketchy summary , because each of these conditions is involved . the unitarity condition in particular , is very difficult , because it is so nonlinear . the only practical way to solve it is in a perturbation series which starts with weakly interacting nearly stable particles ( described by poles of the s-matrix ) which exchange each other ( the exchange picture is required by crossing , and the form of the scattering is fixed by the landau and mandelstam analyticity , once you know the spectrum ) . the " bootstrap property " is then the following heuristic idea , which is included in the above formal relations : the particles and interactions which emerge as the spectrum of the s-matrix from the scattering of states , including their binding together into bound states , should be the same spectrum of particles that come in as in-states . this is a heuristic idea , because it is only saying that the s-matrix is consistent , and the formal consistency relations are those above . but the bootstrap was a slogan that implied that all the consistency conditions were not yet discovered , and there might be more . this idea was very inspirational to many great people in the 1960s , because it was an approach to strong interactions that could accommodate non-field theories of infinitely many particle types of high spin , without postulating constituent particles ( like quarks and gluons ) . regge theory the theory above does not get you anywhere without the following additional stuff . if you do not do this , you end up starting with a finite number of particles and interactions , and then you end up in effective field theory land . the finite-number-of-particles version of s-matrix theory is a dead end , or at least , it is equivalent to effective field theory , and this was understood in the late 1960s by weinberg , and others , and this led s-matrix theory to die . this was the road the chew travelled on , and the end of this road must be very personally painful to him . but there is another road for s-matrix theory which is much more interesting , so that chew should not be disheartened . you need to know that the scattering amplitude is analytic in the angular momentum of the exchanged particles , so that the particles lie on regge trajectories , which give their angular momentum as a function of their mass squared , s . where the regge trajectories hit an integer angular momentum , you see a particle . the trajectory interpolates the particle mass-squared vs . angular momentum graph , and it gives the asymptotic scattering caused by exchanging all these particles together . this scattering can be softer than the exchange of any one of these particles , because exchanging a particle of high spin necessarily has very singular scattering amplitudes at high energy . the regge trajectory cancels out this growth with an infinite series of higher particles which soften the blowup , and lead to a power-law near-beam scattering at an angle which shrinks to zero as the energy goes to infinity in a way determined by the shape of the trajectory . so the regge bootstrap adds the following conditions all the particles in the theory lie on regge trajectories , and the scattering of these particles is by regge theory . this condition is the most stringent , because you can not deform a pure regge trajectory by adding a single particle--- you have to add new trajectories . the following restriction was suggested by experiment the regge trajectories are linear in s this was suggested by chew and frautschi from the resonances known in 1960 ! the straight lines mostly had two points . the next condition is also ad-hoc and experimental the regge slope is universal ( for mesons ) , it is the same for all the trajectories . there are also " pomerons " in this approach which are not mesons , which have a different regge slopem but ignore this for now . finally , there is the following condition , which was experimentally motivated , but has derivations by mandelstam and others from more theoretical foundations ( although this is s-matrix theory , it does not have axioms , so derivation is a loose word ) . the exchange of trajectories is via the s-channel or the t-channel , but not both . it is double counting to exchange the same trajectories in both channels . these conditions essentially uniquely determine veneziano 's amplitude and bosonic string theory . adding fermion trajectories requires ramond style supersymmetry , and then the road to string theory is to reinterpret all these conditions in the string picture which emerges . string theory incorporates and gives concrete form to all the boostrap ideas , so much so that anyone doing bootstrap today is doing string theory , especially since ads/cft showed why the bootstrap is relevant to gauge theories like qcd in the first place . the highlight of regge theory is the reggeon calculus , a full diagrammatic formalism , due to gribov , for calculating the exchange of pomerons in a perturbation framework . this approach inspired a 2d parton picture of qcd which is studied heavily by several people , notably , gribov , lipatov , feynman ( as part of his parton program ) , and more recently rajeev . nearly every problem here is open and interesting . for an example of a reasearch field which ( partly ) emerged from this , one of the major motivations for taking pt quantum mechanics seriously was the strange non-hermitian form of the reggeon field theory hamiltonian . pomerons and reggeon field theory the main success of this picture is describing near-beam scattering , or diffractive scattering , at high energies . the idea here is that there is a regge trajectory which is called the pomeron , which dominates high energy scattering , and which has no quantum numbers . this means that any particle will exchange the pomeron at high energies , so that p-pbar and p-p total cross sections will become equal . this idea is spectacularly confirmed by mid 90 's measurements of total p-p and p-pbar cross sections , and in a better political climate , this would have won some boostrap theorists a nobel prize . instead , it is never mentioned . the pomeron in string theory becomes the closed string , which includes the graviton , which couples universally to stress energy . the relation between the closed string and the qcd pomeron is the subject of active research , associated with the names of lipatov , polchinski , tan , and collaborators . regge scattering also predicts near beam scattering amplitudes from the sum of the appropriate trajectory function you can exchange . these predictions have been known to roughly work since the late 1960s . modern work the s-matrix bootstrap has had something of a revival in the last few years , due to the fact that feynman diagrams are more cumbersome for sugra than the s-matrix amplitudes , which obey remarkable relations . these partly come from the kawai-lewellan-tye open-string closed-string relations , which relate the gauge-sector of a string theory to the gravity sector . these relations are pure s-matrix theory , they are derived by a weird analytic continuation of the string scattering integral , and they are a highlight of the 1980s . people today are busy using unitarity and tree-level s-matrices to compute sugra amplitudes , with the goal of proving the all-but-certain finiteness of n=8 sugra . this work is reviving the interest in the bootstrap . there is also the top-down and bottom up ads/qcd approach , which attempts to fit the strong interactions by a string theory model , or a more heuristic semi-string approximation . but the hard bootstrap work , of deriving regge theory from qcd , is not even begun . the closest in the interpretation of a field theoretic bfkl pomeron in string theory by brouwer , polchinski , strassler , tan and the lipatov group , which links the perturbative pomeron to the nonperturbative 1960s pomeron for the first time . i apologize for the sketch , but this is a huge field which i have only read a fraction of the literature , and only done a handful of the more trivial calculations , and i believe it is a scandelously neglected world .
in principle , it is very simple and straightforward . the problem is to map out the region where the integer filling state is the ground state . suppose you have $l$ sites . take $n=l$ particles , find its ground state energy , which is denoted as $e_g ( l ) $ . note that here the hamiltonian does not contain the $\mu $ term . do it again for $n=l+1$ , the ground state energy is $e_g ( l+1 ) $ . then , you know below the line $\mu_+ = e_g ( l+1 ) - e_g ( l ) $ the $n=l$ state is the lower state with respect to the full hamiltonian containing $\mu$ . do it once again with $n=l-1$ , then you know above the line $\mu_- = e_g ( l ) - e_g ( l-1 ) $ the $n=l$ state is the lower state . therefore , between the two lines , the $n=l$ state is the lowest state . in this region , the unity filling state is the ground state . this is the first mott lobe . the idea is simple , but i really doubt you can get accurate results with ed . you had better do it with dmrg .
the square modulus of the transmission coefficient ( $|t|^2$ ) is the transmission probability , and you have the data to calculate that for your wave-packets ( i.e. . it is just $|\psi|^2$ as you surmised , if you normalized your wavefunction ) . you can compare that to the expected transmission coefficient for plane waves with the same average energy as your wave-packet . i do not know how well the comparison will be since you have uncertain energy , but you can use wider wave-packets and at least see if it seems to be converging to the plane wave solution . incidentally it is a good idea to check the norm of your wave function , it can help determine if anything is going wrong with your numerical simulation . ps : this http://arxiv.org/abs/quant-ph/0301114 might be of some value . it is an explicit solution for a wave-packet on a square barrier ( i.e. . like your problem , but only one ' edge ' . )
indeed you have two possibilities . however it does not matter here , in the sense that you can fix the direction you want in absence of further requirements , provided obviously , you are coherent with your choice at each step when you solve a particular problem . the " right " prescription has to be fixed as soon as you state faraday 's law of induction . in that case you have to fix a direction along the loop used to compute the integral of the electric field and the direction of $d\vec{a}$ . they are linked by the " law of the right hand": the preferred direction $d\vec{\ell}$ along the loop is that from the palm to fingertips of your right hand when it surrounds the loop . then , the associated preferred direction of $d\vec{a}$ is indicated by the thumb . with these choices , faraday 's law of induction is stated : $$\oint_{+\partial \sigma} \vec{e} \cdot d\vec{\ell} = - \frac{d}{dt} \int_{+\sigma} \vec{b}\cdot d\vec{a}$$
fluids are complicated systems described by non-linear differential equations that can not be reasonably treated in a full generality ( certainly not analytically ) . just consider the kinds of waves that propagate in the sea -- deep or shallow water , solitons , tsunami and many others ( this is not to say that these are sound waves ; but as an illustration of complicated wave behavior it should suffice ) . so , to proceed one often employs some approximation . by far the most popular one ( with many applications ) rests on the linearization of the problem around an equilibrium solution where one replaces complicated non-linear equations with second-order wave equation that describes propagation of the perturbation in the system . now , this places some consistency conditions on how big those perturbation can be so that higher-order effects can be ignored . depending on the precise form of the equations , this might require that the temperature or some other parameter stays constant ( otherwise the induced heat transport effects might destroy the linearization , for example ) roughly , isothermal processes are slow ( so that there is enough time for the transfer of the heat with the environment which keeps the temperature constant ) whereas in the adiabatic case the wave propagates so fast that the environment can not catch up with it and so no heat is exchanged ( but the temperature can change ) . i think for the most familiar types of materials where the speed of sound is quite big one uses the adiabatic approximation ( certainly for the propagation of sound in the air ) . i guess for less standard materials ( as encountered in astrophysics ) you might need isothermal approximation too but it is hard to say more than this without knowing what system you have in mind precisely .
the correct option is really option 3 . most of the time when a physicist says a theory is renormalizable , they mean that the theory is a relevant deformation of some conformal field theory . this is a non-perturbative definition . it contains the physically meaningful content that the other more technical definitions about counterterms in perturbation theory are attempting to capture . indeed , it implies them . ( however , the reverse implication is not always true . for example , perturbative qed is renormalizable in the sense of option 2 , but there is no underlying non-perturbative qed , so one can not even ask about option 3 . ) it is good practice to always try to think about the non-perturbative meaning of the physical formalism you are studying . perturbation theory is a sometimes useful tool for computations , but it can obscure the physics in a cloud of virtual technicalities . so what does it mean for a theory to be a relevant deformation of a cft ? it means that there is a cft whose observables are essentially the same as the observables in your qft , and that you can compute any correlation function in the qft as $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} = \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i \int\mathcal{o}_i} \rangle_{cft}$ where the $\mathcal{o}_i$ are relevant operators in the cft and $g_i$ are ( dimensionful ) coupling constants . knowing that your qft is near a cft in this sense is what allows you to study the behavior of the qft 's expectation values under changes of scale , which is the heart of the renormalization group analysis . edit : first , an easy example : the free scalar field theory is a conformal field theory . this theory is basically described by $\langle \mathcal{o} \rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int |d\phi|^2} \mathcal{d}\phi$ . in this theory , in dim > = 3 , the operator $\phi^2 ( x ) $ is relevant , so we can deform with this term and get a non-conformal field theory . the expectation value in this qft is then described by $ \langle \mathcal{o} \rangle_{qft} = \langle \mathcal{o} e^{i m^2 \int \phi^2 ( x ) dx}\rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int [ |d\phi ( x ) |^2 + m^2\phi^2 ( x ) ] dx} \mathcal{d}\phi . $ so , not surprisingly , the theory we get by deforming the free scalar cft with a mass term is the massive free scalar . second , a subtlety that i should point out . the first equality above is not exact in most situations . the problem is the deformations we want may not be integrable with respect to the cft 's path integral measure , thanks to uv singularities . this is dealt with by regularizing . so , in most qfts , what we get is a family of approximations $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} \simeq \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i ( \lambda ) \int\mathcal{o}_i ( \lambda ) } \rangle_{cft}$ where the relevant operators and the coupling constants depend on a cutoff scale $\lambda$ and the errors vanish as $\lambda \to \infty$ .
the translational accelleration will be the force divided by the mass . the cross product of the force vector with the vector from the touch point to the center of mass is the torque applied to the object .
the capacitance is the ratio of charge on the plates over the voltage applied . $$c = \frac{q}{v} \leftrightarrow q = c \cdot v$$ the calculation you show determines the capacitance from measured voltage and charge on the plates . you basically know the result you want and determine the size of the capacitor you need . a larger capacitor , with a larger capacity , will hold a bigger charge at the same voltage . doubling the area will double the capacitance ( in case of a plate capacitor ) , so for 4 farads of capacity you get $$q = c \cdot v = 4 f \cdot 5 v = 20 c$$ the pysics works as follows : the voltage is a driving force , pushing electrons through the wires an onto the plates of the capacitor ( or sucking them off on the positive pole ) , until the mutual repulsion of the electrons leads to a balance of foces . if you have a larger plate , the charge can distribute over a larger area , there is less " pileup " and therefore a smaller " pushback force " . this is why , with larger plates , you get a bigger charge into your capacitor with the same voltage .
i do not understand question 1: where does he equate a speed to a position ? as far as question 2 is concerned , it is basically what davephd said , but maybe i can extend it a bit more saying something about the conservation of linear momentum : along the x-direction , there is no external force ( because gravity points downwards only , assuming a flat surface ) so the linear momentum of the projectile is conserved . since $p_x = mv_x$ , $v_x$ is constant .
let us deal with this on two parts : ( 1 ) how do you take the curl ? the e-field you describe has the form ${\bf e} = e_0 \sin ( kz-\omega t ) \ , {\bf i}$ , where the latter unit vector assumes the wave is polarised along the x-axis , while the wave travels along the z-axis . ( you can equally polarise it along the y-axis if you choose ) . if i define it in this way , then the e-field only has an x-component . i.e. $e_{y}=e_{z}=0$ the curl of the wave can be evaluated as described in the answer by jamals , so in this case , as $e_{y}=e_{z}=0$ , then the partial derivatives of these components are also zero and there are only two possible non-zero terms in the curl . $$\nabla \times {\bf e} = {\bf j} \frac{\partial e_{x}}{\partial z} - {\bf k} \frac{\partial e_{x}}{\partial y}\ , . $$ because we chose to have the wave travelling along the z-axis then $e_{x}$ is not a function of $y$ , so the second term is zero and $$\nabla \times {\bf e} = {\bf j} \frac{\partial e_{x}}{\partial z} = ke_{0} \cos ( kz -\omega t ) \ , {\bf j}\ , , $$ perpendicular to the e-field and the direction of motion of the wave , but changing direction with time . ok , that is the maths , but ( 2 ) how to " visualise " or deduce without doing the maths ? i have to admit to struggling with this one . the paddle wheel analogy is always the one i use . a field with a non-zero curl will make the paddle wheel turn and the axis of rotation points in the direction of the curl . if i assume that the matlab plot you show has z along the horizontal axis , then you can imagine the e-field as vertical arrows of size proportional to the e-field strength at that instant in time . imagine placing a paddle wheel at some point along the axis . in general , the e-field on one side of the wheel will be of a different strength to the e-field on the other side of the wheel and hence it will rotate around an axis perpendicular to z . if we then roll the clock forward to some later instant of time , the situation could reverse with the e-field now stronger on the other side and the wheel rotates in the opposite direction - i.e. a cosinusoidally varying curl in a direction perpendicular to the wave motion and perpendicular to the e-field . here 's an attempt to show this . the two plots show the instantaneous e-field strengths and directions at two instants of time . i add a paddle wheel which sits in the field at the horizontal positions shown . in the top plot the wheele rotates clockwise . some time later the field has changed as shown in the bottom plot and the wheel would rotate anti-clockwise .
what you calculated there is the electric flux generated by a proton , measured at a distance that is greater the size of the proton : you have used e as the charge , which means that your surface d s encloses the whole of the proton ( at least within classical electrodynamics , where we can assume that the proton has some kind of spherical/definite shape ) . the number of field lines is not something that has much of a physical meaning . basically because there could be an infinite number of field lines through a given patch of surface d s . you would need to specify the distance between these field lines , so that you can divide a length ( say 10 m ) through the separation between each field line ( say 1 m ) therefore resulting in 10 field lines . this , however , assumed that the field lines are equally spaced . check this
presumably , the analytical solution is using \begin{equation} p ( x , y , z ) = \lim_{t\to \infty} \frac{1}{t} \int_{-t/2}^{t/2} p ( x , y , z , t ) \ , dt \end{equation} note the limit that takes $t$ to infinity . if the solution is periodic with period $t$ , then this is precisely equivalent to writing \begin{equation} p ( x , y , z ) = \frac{1}{t} \int_{-t/2}^{t/2} p ( x , y , z , t ) \ , dt \end{equation} which is the same thing over just one cycle . the average over infinite time averages over infinitely many cycles , but in the truly periodic case , these cycles are all identical , so they do not change the average at all . now , your numerical simulation will naturally not be precisely periodic , in the sense that it lasts for only a limited amount of time . however , that finite amount of time is just a model for the infinite time of the analytical solution . so if some portion of the numerical solution is periodic , you can just use this last version of the equation over one cycle -- or several cycles to get better numerical behavior . if it is not periodic over even a single cycle , it sounds like the numerical and analytical models just do not agree very well . [ note , of course , that you may have to shift your time axis , since you probably just start your simulation with $t=0$ like most of us . just shift it so that the integration starts at -- let 's say -- the peak of the pressure , and stops at another peak . then divide by the time between those peaks to get the average . of course , using troughs or any other identifiable feature would work just as well . ]
some instruments would require modifications in order to facilitate playing them . large instruments would need to be strapped down , and something like a double bass or drums would probably require its player to be strapped into a harness in order to prevent them from pushing themselves off as they played and floating away from the instrument . i can imagine that it would take some time to develop a technique for playing the drums without the assistance of gravity , though i suspect it would be possible . other instruments would require modification in order to be played at all . pianos , for example , rely on gravity to return the keys and hammers to their original position after releasing a key , so a piano would not work without modification . ( you would probably have to introduce springs to replace the action of gravity , and it would probably be hard to replicate the traditional feel of the keys ' movement . ) it is also possible that a grand piano would need to be re-tuned , because its bass strings are long and heavy enough that the lack of gravity might make a ( barely ) audible difference to their frequency . ( but i would expect a piano to need tuning after being launched into space anyway . ) it is surprisingly hard to think of instruments that fundamentally rely on gravity , and which therefore would not work at all . there are a few though - they include the glass harp and rainstick that you mentioned , as well as the waterphone . speaking of water-based instruments , a hydraulophone would probably work but would make a terrible mess . however , most small , hand-held string , brass and woodwind instruments would work just the same as on earth , just like the guitar does . the air pressure on a spacecraft might be different from on earth 's atmospheric pressure . you might expect brass and woodwind instruments to have tuning issues in such a situation , but in fact they would not . this is because the frequency is determined by the speed of sound . the air is a good approximation to an ideal gas , and ( perhaps surprisingly ) for ideal gases the speed of sound does not depend on pressure but only on temperature . however , if the air 's composition is different ( e . g . pure oxygen instead of an $\mathrm{o_2}$-$\mathrm{n_2}$ mixture ) then the change in density might cause tuning issues for these instruments , for the same reason that helium makes your voice go squeaky . this issue will not affect the tuning of stringed instruments , though it might affect their tone slightly by changing the resonant frequencies of their body . of course , the really interesting question is whether there are any instruments that could only be played in microgravity . i can not think of any ideas off the top of my head , but it is an interesting thing to think about .
i sincerely doubt that a computer could use 400+w under normal circumstances . that is the typical power rating of the transformer powering the computer . so , at peak the computer could use that much power . this peak consumption in practice means something like : cpu fully loaded gpu fully loaded hard disk at maximum throughput all the usb and firewire ports supporting powered devices wi-fi and bluetooth turned on and trasmitting also , ibm compatible motherboards and power transformers are made to support different hardware configurations . examples : some motherboards support multiple cpus all support many hard drives etc . . . you can think of it this way as well . servers normally have two psus ( for redundancy ) . both are plugged in at the same time and both could theoretically support the server by themselves . so if the maximum peak wattage of the server is 500w , you would have 2 500w psus plugged in at the same time . however , the consumption of the server will not exceed 500w .
suppose that two molecules are at distance $b$ and have zero kinetic energy . there is a lower potential energy position in $c$ and therefore the molecules will attract . they will convert $\epsilon$ potential energy into kinetic energy and reach $c$ . now , the law of inertia states , and the fact that they have positive kinetic energy indicates , that they will maintain their state of motion at $c$ towards $a$ . going towards $a$ they will gain potential energy by converting kinetic energy into it ( in other words , slowing down ) . once at the distance $a$ , they will have gained exactly $\epsilon$ potential energy and will have therefore zero kinetic energy . at this point the cycle will repeat inverted , they will move towards a distance of $c$ because it is lower energy , surpass it and reach $b$ with zero kinetic energy , which will make the cycle repeat from the start .
when first coming into contact with the water , it is conduction . the skin feels the water colder than air because water is a better conductor of heat than air . so the skin cools faster in water than in air . for longer intervals convection will enhance the effect bringing cooler water next to the skin and removing the water already heated by the skin . the difference will persist plotting water temperatures ( equal with air temperature ) up to the temperature the skin raises the water when in contact with it . after that , the water is felt as warm .
a lot of what the lhc hopes to achieve in this area is described in a paper by carlos salgado , so i would suggest you take a look at that . in summary : collisions of nuclei yield much better data about parton saturation and the color glass condensate than collisions of individual nucleons . lhc p-pb runs will allows us to study the high-energy behavior of the cgc . the kinematic range of high-energy nucleon-nucleus collisions extends down to values of björken $x$ as small as $10^{-6}$ ( compared to $10^{-3}$ for rhic ) , which expands our knowledge of the nuclear parton distributions ( e . g . whether and how well they factorize into individual nucleon pdfs ) asymmetric collisions in particular allow measurements of certain characteristics of the nuclear pdfs independent of uncertainties in the proton/neutron pdfs jet quenching and " quarkonia suppression " ( essentially jet quenching where the jet progenitors are bound states of heavy quarks ) can be used as a probe of the qgp at higher energies and densities than were accessible at rhic the lhc presents the first opportunity for collisions of multi-hadron systems ( i.e. . nuclei ) with energies exceeding 1 tev . so in general , if there is anything new and cool to be discovered at those energies , this is where we will find it .
yes the free body moves outward , but there are two critical things you have to know to interpret this statement correctly . first , this is the effective potential , taking into account gravity and centrifugal force . it has this form because we went into the non-inertial frame co-rotating with the two masses . mathematically , the potential is $$ \phi_\mathrm{eff} ( \vec{r} ) = -g \left ( \underbrace{\frac{m_1}{\lvert \vec{r}-\vec{r}_1 \rvert}}_\text{potential from mass 1} + \underbrace{\frac{m_2}{\lvert \vec{r}-\vec{r}_2 \rvert}}_\text{potential from mass 2} + \underbrace{\frac{m_1+m_2}{2\lvert \vec{r}_1-\vec{r}_2 \rvert^3} \lvert \vec{r} \rvert^2}_\text{centrifugal component}\right ) , $$ and it only decreases far away because of that last term . physically , this is because placing an object " at rest " in this frame corresponds to having it move with the same angular frequency as $m_1$ and $m_2$ about the center of mass . if you initialize an object $5\ \mathrm{au}$ on a tangential path having the same angular velocity as the earth , it will be moving too fast for a circular orbit at that distance , and so it will move away from the sun . this does not mean the object will go away forever , and that brings us to the second point , explained in chay 's response : not all effective forces have been accounted for ; in particular , the coriolis force does not arise from $\phi_\mathrm{eff}$ . the coriolis force depends on velocity , so it has no scalar potential depending solely on position , and so it is not included in the analysis so far . once your test object starts moving in your rotating frame , it will experience a perpendicular deflection that will eventually force it to turn around .
dear onkar , they are the same thing . the ( massless or at least light ) scalar fields are what parameterizes the moduli spaces - in any theory - and the metric on the moduli space ( which is a mathematical concept that does not " a priori " exist in physics ) is defined from the ( ultimate low energy ) kinetic terms of these scalar fields . in a supersymmetric theory , these kinetic terms $$\frac{1}{2}g_{ij} ( \phi_a ) \partial_\mu \phi^i \partial^\mu \phi^j$$ are determined from the kähler potential , $g_{i\bar j}\sim\partial_i \partial_{\bar j} k$ because of the basic supersymmetric calculus . there is a lot of nontrivial maths here but the particular statement you are quoting is a tautology .
the reason you encountered higher and higher pressure at the center of the rod as you cut it into more pieces is that you were essentially approximating an integral , but the integral diverges ( "is infinity " colloquially ) . when the rod has zero thickness , but still has mass , the density of the matter is infinite , and this leads to infinitely strong gravitational forces . to answer this question , we will imagine the cylinder has some small , finite radius $r$ . we want to find the force between the two halves of the cylinder . we will let one half just sit stationary in space . it will create a gravitational potential . then we will grab the other half and pull it away to some distance $d$ . the gravitational potential energy is a function of $d$ . the force between the two halves of the cylinder is the derivative of the gravitational potential energy with respect to $d$ when $d=0$ . the problem described above is too hard . it is quite difficult to calculate the gravitational potential of a cylinder at an arbitrary point . the gravitational potential of a point mass is just $-gm/r$ , but for a cylinder that extends out in three dimensions , we need to replace $m$ with the density $\rho$ and then integrate over the mass of the entire cylinder . the expression for $r$ , the distance from an arbitrary point outside the cylinder to a point inside it , is not very tractable . however , at a point on the axis of the cylinder , the gravitational potential is more accessible due to the extra symmetry . if we set up cylindrical coordinates with the axis of the cylinder along the z-axis , and then integrate over the bottom half of the cylinder , we get $v ( z ) = \int_{z&#39 ; =0}^{-l/2}\int_{r=0}^{r}\int_{\theta=0}^{2\pi} \frac{g\rho}{\sqrt{ ( z-z&#39 ; ) ^2+r^2}} r\textrm{d}\theta\textrm{d}r\textrm{d}z&#39 ; $ and doing the integral of $\theta$ it is $v ( z ) = 2\pi g\rho\int_{z&#39 ; =0}^{-l/2}\int_{r=0}^{r} \frac{1}{\sqrt{ ( z-z&#39 ; ) ^2+r^2}} r\textrm{d}r\textrm{d}z&#39 ; $ . this allows us to make an approximation . although the half of the cylinder we use to calculate the potential must have finite width , we can calculate the potential energy by assuming that the other half of the cylinder is located perfectly along the axis . as long as the radius of the cylinder is very small compared to the length , this is a valid approximation . so the potential energy comes from integrating the previous expression for $v$ along the $z$-axis for the length of the cylinder . we do not actually want the potential energy , but the derivative of the potential energy . so we imagine moving the top half up the cylinder up a little bit $dz$ , and ask how the potential energy changes . moving the entire top half of the cylinder up by $dz$ is equivalent to taking a piece of thickness $dz$ and slicing it off the bottom and moving it to the top . so we really just need to find the difference in the potential between the top and bottom of the top half of the cylinder and multiply by the mass-per-unit-length of the cylinder . the force between the two halves of the cylinder is $\frac{m}{l} [ v ( l/2 ) - v ( 0 ) ] $ that still leaves two integrals to evaluate . $v ( l/2 ) $ is easy , because it is far away from the half of the cylinder providing the gravitational potential ( compared to $r$ ) . that lets us approximate $v ( l/2 ) = \frac{-gm}{l} \int_{-l/2}^0 \frac{1}{l/2-x}dx = -\frac{gm}{l}\ln 2$ . the integral for $v ( 0 ) $ is trickier , so i put it in mathematica and got $v ( 0 ) = -\frac{gm}{l}\textrm{arcsinh}\left ( \frac{l}{2r}\right ) $ . in the regime we are interested in ( $r$ small compared to $l$ ) the $\sinh ( x ) $ is just $e^x/2$ , so this simplifies to $v ( 0 ) = -\frac{gm}{l} \ln\left ( \frac{l}{r}\right ) $ this gives a final answer for the force $f = \frac{m^2g}{l^2}\ln\left ( \frac{l}{2r}\right ) $
the einstein field equations , describing the gravitational field are given by $$r_{\mu\nu}-\frac12 g_{\mu\nu}r = -\frac{8\pi g}{c^4}t_{\mu\nu}\ . $$ they relate in a complicated manner the gravitational field that can be seen as the metric $g_{\mu\nu}$ itself to the stress-energy tensor $t_{\mu\nu}$ that might also depend on $g$ . newtonian gravitation if now velocities are small compared to the speed of light $c$ , the stress-energy tensor approximately only consists of its time-component , $$t_{tt} \approx \rho c^2$$ and the metric is flat with the exception of $$g_{tt} \approx 1 + \frac{2 u}{c^2}$$ where we can find , directly from the einstein equations , that the newtonian $$\delta u = 4\pi g \rho$$ holds . this is a static approach and we see that there is no dependence on any flux term $j_i \propto t_{ti}$ . velocity matters : rotating disc of dust considering the whole theory , we find that of course the metric will depend on contributions of all components of $t$ but only have a meaningful effect if associated characteristic velocities are approaching the speed of light . a prominent analytic solution is that of a rigidly rotating disc of dust . taking this solution , you can get an idea of how relativistic effects are important for the theory calculating the multipole moments $q_n$ with respect to some relativistic parameter $\mu$ ( corresponding also to the angular frequency $\omega$ of the disc ) . in the following picture you can see that the kerr-spacetime is approached ( from above ! ) for all moments $q_n ( \mu ) $ for $\mu \rightarrow \infty$ . this means that there is some $\mu$ where the effects of rotation dominate those of the mass itself . ( picture taken from here . ) so , to conclude , there will only be some measurable time change for a person living on a rotating planet if it is extremely fastly rotating . it is hence needless to say that this person would have some other difficulties than to measure this deviation from an almost flat metric . sincerely
yes you can say that they energies are the same . and with energy we usually mean that the potential plus kinetic energy always remains constant . so kinetic energy can change and so does potential but the sum of those 2 is always constant unless you add friction .
if you connect a battery with a voltage $v$ between two points that already have a potential difference of $v$ , then no current will flow out of the battery . of course if you remove the other source of the potential difference , then the battery will start to push out the current needed to keep that voltage drop across the load . when you say that the the cell is in parallel with the potentiometer you make a mistake : the cell is in parallel with the part of potentiometer and with the dc source ( which is in series with the remaining part of the potentiometer ) .
i ) firstly , we are talking about the direct or cartesian product $su ( 2 ) \times su ( 2 ) $ of groups , not the tensor product$^1$ $su ( 2 ) \otimes su ( 2 ) $ of groups . ii ) secondly , $su ( 2 ) \times su ( 2 ) $ is not isomorphic to the lorentz group $so ( 3,1 ) $ but rather to a compact cousin $$ [ su ( 2 ) \times su ( 2 ) ] /\mathbb{z}_2~\cong~ so ( 4 ) . $$ in particular , a $ ( \frac{1}{2} , \frac{1}{2} ) $ irrep under $su ( 2 ) \oplus su ( 2 ) $ corresponds to a 4-dimensional fundamental vector representation under $o ( 4 ) $ . iii ) thirdly , op might be thinking of the complexified lorentz group $so ( 3,1 ; \mathbb{c} ) $ , which has double cover $sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) $ , $$ [ sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) ] /\mathbb{z}_2~\cong~ so ( 3,1 ; \mathbb{c} ) . $$ cf . this phys . se post . in particular , a $ ( \frac{1}{2} , \frac{1}{2} ) $ irrep under $sl ( 2 , \mathbb{c} ) \oplus sl ( 2 , \mathbb{c} ) $ corresponds to a 4-dimensional fundamental vector representation under $o ( 3,1 ; \mathbb{c} ) $ . -- $^1$ note that there exist various abelian and non-abelian tensor product constructions for groups . e.g. for the abelian group $ ( \mathbb{r}^n , + ) $ , the tensor product is $\mathbb{r}^n\otimes\mathbb{r}^m\cong \mathbb{r}^{nm}$ , while the cartesian product is $\mathbb{r}^n\times\mathbb{r}^m\cong \mathbb{r}^{n+m}$ .
i think the thing you have to realize is that in relativity , the time that an observer sees something happen - that is , the time at which the light signal of the event reaches the observer - is not the time at which it actually happened . if you are going to talk about these relativistic effects like time dilation , you have to correct for the time it takes light signals to travel , or only compare events ( e . g . timer readings ) which happen at the same location in space and at the same time . regarding your issues : as observed by b , the ship does take 101.01 days to get from a to b . now , suppose a light signal is emitted from a when the ship passes it . that light signal takes 100 days to get to b , and the ship takes 1.01 days more than that . so b sees a delay of 1.01 days between the image of the ship arriving at a and the image of the ship arriving at b . but that does not mean the ship actually made it from a to b in 1.01 days . the ship passes a when a 's timer reads 1000 days , and b when b 's timer reads 1101.01 days . i do not know where you are getting 1201.01 days from - that value does not show up anywhere in the analysis .
the duel gets tenser and tenser . $\epsilon^{xyab}\partial_a\partial_b= ( -\epsilon^{xyba} ) \partial_a\partial_b= ( -\epsilon^{xyba} ) \partial_b\partial_a= ( -\epsilon^{xycd} ) \partial_c\partial_d=-\epsilon^{xyab}\partial_a\partial_b$ $\longrightarrow\ \ \epsilon^{xyab}\partial_a\partial_b=0$ more abstractly , if $a^{ab}=-a^{ba}$ and $s_{ab}=s_{ba}$ , then $a^{ab}s_{ab}=0$ .
the aharonov-bohm effect arrises in a hypothetical situation when the magnetic field $b=\nabla\times a=0$ whereas the vector potential $a$ may not be null . this situation in particular arrise when we choose the gauge in such a way that $a'=0\leftrightarrow a=-\nabla\chi$ , which indeed verifies $b=-\nabla\times\nabla\chi=0$ . aharonov and bohm gave the particular example of a two-slit interference setup , when a magnetic flux is enclosed inside the interfering paths . this can be done using a infinitely long solenoid , which do produce a magnetic flux , but without magnetic field outside the solenoid . now , one can integrate the gauge choice along a given path such that $$ a=-\nabla\chi\rightarrow\int_{a}^{b}a\cdot dl=-\int_{a}^{b}\nabla\chi\cdot dl=\chi\left ( a\right ) -\chi\left ( b\right ) $$ for a path from $a$ to $b$ . then , the phase shift along a path is given by the circulation of the vector potential along the same path in the chosen gauge . now , as conventional for a two-slit experiment , one can separate the total wave function $\psi$ as a superposition of the wave function $\psi_{\uparrow}$ passing on the upper slit and the wave function $\psi_{\downarrow}$ passing throw the lower slit . suppose we choose to separate the $\psi$ wave function at the point $a$ and recollect it at the point $b$ . then , one has reads $$ \left\vert \psi\left ( b\right ) \right\vert ^{2}=\left\vert \psi_{\uparrow}\left ( b\right ) \right\vert ^{2}+\left\vert \psi_{\downarrow}\left ( b\right ) \right\vert ^{2}+2\text{re}\left [ \psi_{\uparrow}\left ( b\right ) \psi_{\downarrow}\left ( b\right ) ^{*}e^{\mathbf{i}q\left ( \chi_{\uparrow}\left ( b\right ) -\chi_{\downarrow}\left ( b\right ) \right ) /\hslash}\right ] $$ and , using the above definition for the phase drop along a path $$ \chi_{\uparrow}\left ( b\right ) -\chi_{\downarrow}\left ( b\right ) =\int_{a\rightarrow b ; \downarrow}a\cdot dl-\int_{a\rightarrow b ; \uparrow}a\cdot dl $$ where the two integral paths are from $a$ to $b$ . the first integral follows this path using the bottom slit , whereas the second integral follows the upper slit . thus , one has , for the probability to find the particle after its passage though the system $$ \int_{a\rightarrow b ; \downarrow}a\cdot dl-\int_{a\rightarrow b ; \uparrow}a\cdot dl=\left ( \int_{a\rightarrow b ; \downarrow}+\int_{b\rightarrow a ; \uparrow}\right ) a\cdot dl=\int_{a\circlearrowleft a}a\cdot dl $$ and thus corresponds to the integral along the closed contour made by the two interfering paths . using that $$ \oint a\cdot dl=\iint b\cdot ds=\phi $$ with $\phi$ the flux enclosed in between the two interfering paths , one finally obtain that the total probability amplitude $\left\vert \psi\left ( b\right ) \right\vert ^{2}$ is $$ \left\vert \psi\left ( b\right ) \right\vert ^{2}=\left\vert \psi_{\uparrow}\left ( b\right ) \right\vert ^{2}+\left\vert \psi_{\downarrow}\left ( b\right ) \right\vert ^{2}+2 {\re}\left [ \psi_{\uparrow}\psi_{\downarrow}^{*}\right ] \cos\frac{2\pi\phi}{\phi_{0}}+2 {\im} \left [ \psi_{\uparrow}\psi_{\downarrow}^{*}\right ] \sin\frac{2\pi\phi}{\phi_{0}} $$ with $\phi_{0}=2\pi\hslash/q$ is called the flux quantum . in a superconductor , the basic charge is $2e$ , thus the flux quantum becomes $\phi_{0}=\pi\hslash/e$ which is a fundamental constant of quantum circuitry .
free energy perturbation is a free energy method , i.e. it allows one to calculate the difference in free energy between two states a and b , during a molecular simulation . other , and possibly better-known free energy methods , include thermodynamic integration and umbrella sampling . the idea behind the free energy perturbation method is that , if your states a and b are close enough , i.e. if they represent well-overlapping regions of phase space , you can get information about their free energy difference from one single molecular simulation in one of the states . the big gain of using a small perturbation is that you do not need to actually sample both a and b , because they are close enough .
nothing can travel faster than light yet the expansion of the universe itself can . there are already parts of the universe receding from us " faster than light " . this is because every small part of the universe is expanding only a little - but if you sum it up over cosmic distances , the expansion speed , total over a large distance , becomes huge . so , in a sense , nothing actually moves faster than light . it is just that the universe is so big that this tiny space expansion actually adds up a lot over large distances . space itself is growing , and it is not subject to relativistic limitations - only things moving through space are limited by c . back to your original question : there was a time , early on , when the universe expanded much faster than today . it went from essentially nothing to a huge size in almost no time . things that were previously close together , all of a sudden found themselves separate by large distances . so , those photons emitted 12.9 billion years ago , might have arrived here a very long time ago , if it were not for this sudden " additional " distance in between . btw , this is not entirely rigorous , but it is good enough as a pop-sci explanation , and at least will get you started .
magnetic fields do no work since $$ \left ( \frac{d{\bf{r}}}{dt} \times {\bf{b}} \right ) \bullet d{\bf{r}} = 0 , $$ so they cannot change a charged particle 's speed ( they can , however , change the direction of its velocity ) .
the article that describes this research does discuss their favored model for explaining these features . the article starts off by talking about earth analogues , which are always helpful . in this case , it talks about brines rising in antarctic ice shelves ( brines being just what they are in cooking -- salty water , and salt lowers the freezing point of water by up to and over 10°c ) . the model the authors suggest starts with a thermal plume that heads from the liquid interior up through the ice and melts some of the overlying ice . the melting causes the surface to collapse a bit and fracture ( volume of water is &lt ; volume of ice ) , and this confines the water to that region . the thermal plume was transient/temporary and sinks back down , and the water at the base of the " lake " re-freezes . the article then talks a lot about the morphology ( what it looks like ) in images and theory , showing that the two intersect . as the lens of water lies below the surface , the surface fractures and calves , like glaciers calving on earth . material fills in with an impurity-rich matrix and freezes . when the lake underneath refreezes , since the volume of the ice is greater than water , it creates a positive convex topographic relief ( jumbled because of the previously calved blocks ) as opposed to the concave one when there was liquid below . from what i can tell , the article is not actually suggesting that all these areas presently contain liquid water lakes underneath them . they do suggest one particular area , thera macula , to be presently active , however . their argument for this is based on its morphology : " the large concentric fracture system encircling thera macula resembles those of collapsing ice cauldrons , and , given the absence of a continuous moat , suggests that subsurface melt and ice disaggregation is forming thera macula , rather than the collapse of a dome . " they suggest , " today , a melt lens of 20,000–60,000 km 3 of liquid water probably lies below thera macula ; this equates to at least the estimated combined volume of the great lakes . " the timescale for this amount of liquid to re-freeze is 100,000-1,000,000 years . the type of terrain that this creates ( "chaos terrain" ) is spread throughout the moon , so if their model is correct , then it has had many intra-ice lakes throughout its history , and likely throughout its recent history . the authors conclude with , " our analyses suggest that ice–water dynamics are active today on europa , sustaining large liquid lakes perched in the shallow subsurface . "
if i understand correctly , then i believe you’re right . we are considering a situation where the drill pipe in the ground gets stuck , but where all the rest of the equipment is functioning properly , right ? in particular , we are assuming that the blocks are not jammed or anything . the proper way to define mechanical advantage $m$ is as the ratio between the distance $d$ traveled by the driving force and the distance $l$ traveled by the load . then $d$ is just the length of cable that passes through the driving winch at the base during a certain amount time , and $l$ is the height that the drill pipe moves during the same time . this ratio is equal to the inverse of the ratio of the tension $t_d$ on the cable at the winch and the tension $t_l$ on the cable attached to the drill pipe : $m = \frac{d}{l} = \frac{t_l}{t_d}$ this ensures that the amount of work done by the winch , $t_d d$ , is equal to the work done on the pipe , $t_l l$ . if the drill pipe gets stuck , then it will require a much larger tension to pull it from the ground , right ? this means $t_l$ will shoot up . but the mechanical advantage $m$ is the same , so the tension $t_d$ at the winch will likewise shoot up . this doesn’t mean the mechanical advantage is lost . to summarize : the ratio between $d$ and $l$ is not affected by the pipe jamming ; it’s just that it becomes more difficult to get them to move at all . therefore , the mechanical advantage $m=d/l$ doesn’t change i’m not sure that this is feasible , but you could confirm this by measuring the tensions at the winch and at the pipe individually .
yes , and it would be very cold . the paper " finite temperature in a desitter universe " explains that the cosmological constant ( if it is really a constant ) creates a " horizon " that acts somewhat like an inside-out event horizon : objects that get too far away from you are unreachable . this horizon will radiate hawking radiation at an extremely low temperature of 10^ ( -30 ) k ; the wavelength range of light this corresponds too is the same order of magnitude as the horizon 's radius ( also the case for black holes ) . it is conceivable that some relatively compact system could have an excited quantum state so close to it is ground state that it is thermally accessible even at this extremely cold temperature . thus , there is still some form of heat swimming around . however , there is no reservoir colder than this temperature to dump heat into so this heat can not be converted into useful energy .
as explained by iwo bialynicki-birula in the paper quoted , the maxwell equations are relativistic equations for a single photon , fully analogous to the dirac equations for a single electron . by restricting to the positive energy solutions , one gets in both cases an irreducible unitary representation of the full poincare group , and hence the space of modes of a photon or electron in quantum electrodynamics . classical fields are expectation values of quantum fields ; but the classically relevant states are the coherent states . indeed , for a photon , one can associate to each mode a coherent state , and in this state , the expectation value of the e/m field results in the value of the field given by the mode . for more details , see my lectures http://www.mat.univie.ac.at/~neum/ms/lightslides.pdf http://www.mat.univie.ac.at/~neum/ms/optslides.pdf and chapter b2: photons and electrons of my theoretical physics faq .
fermionic generators of course do not act just geometrically on a bosonic space ; all differential operators acting on bosonic coordinates are bosonic . at most , you could consider a superspace extension of $ads_5 \times s^5$ but superspaces are not too useful if there are too many supercharges ( they have too many components ) . so it is a kind of misguided approach to ask about the action of the supercharges on the spacetime only ; one should learn what is the action of the supergroup at the hilbert space – the whole actual theory – and it is pretty straightforward if you define the $n=4$ gauge theory . witten – when he mentions that the group acting on the ads-space times the sphere is the supergroup – really wants to say that $psu ( 2,2|4 ) $ is the ( or " a" ) maximal supergroup of symmetries that a theory defined on $ads_5\times s^5$ may have . but he surely does not mean that all the generators - the fermionic ones in particular - may be defined as differential operators acting on the 10 bosonic coordinates of this spacetime only . for the terminology of superalgebras ( and the same supergroups ) , look at page 58 of this kac 's review ( pdf ) : http://projecteuclid.org/dpubs?service=uiversion=1.0verb=displayhandle=euclid.cmp/1103900590 finally , the extra $p$ in $psu$ means that one eliminates a block-diagonal " hypercharge-like " generator from $su$ . it is similar to the embedding of $su ( 3 ) \times su ( 2 ) \times u ( 1 ) $ in $su ( 5 ) $ in grand unification ; in the superalgebra case , one can consistently eliminate the $u ( 1 ) $ here , at least if the number of bosonic entries and fermionic entries ( dimensions of the fundamental representation ) are equal . and it is equal , both are 4 for $psu ( 2,2|4 ) $ . if you check a se question about an $su ( 5 ) $ decomposition here introduction to physical content from adjoint representations the equal dimensions allow us to set the " hypercharges " of the off-block-diagonal entries ( all the fermionic generators ) which were $\pm 5/6$ above to zero and eliminate the " hypercharge " $u ( 1 ) $ which was just shown to become a center ( generator commuting with all others ) completely . without the $p$ which stands for " projective " , the bosonic subgroup of $su ( 2,2|4 ) $ would really be $su ( 2,2 ) \times su ( 4 ) \times u ( 1 ) $ with the extra last factor that actually gets eliminated in $psu$ .
without doing the analysis , i would think that a cooling system is more effective at extracting heat from a warm container than from a cold one . for a fridge , the effectiveness ( or coefficient of performance ) is $eff=q_c/w$ is the ratio of the heat removed from the cold source ( the fridge ) to the energy used for the purpose . it increases with the temperature of the cold source . this is actually the prime factor to be considered in the analysis . if we assume that , for a given current temperature of its contents , and thus for a given coefficient of performance , the cooling capacity of the fridge ( heat removed per second ) is limited only by the power of its cooling engine ( i do not know whether that is the case ) , then the heat pump will pump more heat per second when the fridge is warm . hence it is better to put all bottles at once and get the fridge warmer to have a maximum heat pumpimg capacity from the heat pump . heat sharing rate within the fridge may also be an important issue , but there are no data available to measure how important . if it is really low , thus leaving an important temperature gradient in the fridge , it may be useful to exchange the position of bottles , so as to have the warmer part of the load near the heat pump and have work with highest possible coefficient of performance . precise figures about the load do not matter very much . however , a load with large heat capacity will take longer to cool and will thus allow more time for heat sharing . in the second part below , we prove formally that all bottles should be cooled at once , and we use the understanding to discuss the heat sharing issue in some more depth . the variability of the coefficient of performance with temperature is central to this analysis . formal statement and proof a refrigerator is a carnot machine functionning as a heat exchanger , where we are interested in removing heat from the low-temperature reservoir , using work from an engine that provides compression . the effectiveness , or coefficient of performance , noted here $z$ , is defined as $z=q_c/w$ where $w$ is the work provided and $q_c$ is the amount of heat extracted from the cold source ( the refrigerator ) with that work . if we note $q_h$ the amount of heat delivered to the hot source ( outside the refrigerator ) , we have the equality $q_h=w+q_c$ . for an ideal carnot cycle , we have $z_{ideal}=q_c/w=q_c/ ( q_h-q_c ) =t_c/ ( t_h-t_c ) $ where $t_h$ and $t_c$ are the temperatures of the hot and cold source . ( see http://en.wikipedia.org/wiki/coefficient_of_performance ) . of course , the actual coefficient of performance $z$ is less that the carnot ideal . short of knowing its specific , we will only assume that , like the ideal coefficient , it depends monotonically on the temperature $t_c$ of the cold source , the hot source ( outside the refrigerator ) being considered at constant temperature . hence we only assume that the coefficient of performance $z$ is a strictly increasing function of ( cold source ) temperature , i.e. , such that $t_1&lt ; t_2\ \rightarrow\ z ( t_1 ) &lt ; z ( t_2 ) $ we also assume that the mechanical power available for compression is invariant , i.e. does not depend on the temperature of the sources , at least within the range of temperatures considered . finally , we also assume that the heat capacity of the refrigerator itself is negligible , and that heat sharing within the refrigerator takes negligible time compared to cooling time so that the content may be considered to have uniform temperature . these later two assumptions will be discussed afterwards . with the above assumptions , given two masses $m_1$ and $m_2$ to be cooled in the cold source , it is faster to cool booth simultaneously than to try to cool one first and later add the second one . it also consumes less energy . proof cooling a mass $m$ actually , the formulae above are about heat and work increments . this is necessary since $z$ is temperature dependant , and temperature may vary . also , since we intend to analyze the system from the point of view of the cold source , the heat increments are actually removed from that source and must be countd as negative . so we can write $z= -dq_c/dw$ , or $dq_c/dw=-z$ . the power provided by the compressor is a constant $p=dw/dt$ . hence $dq_c/dt= ( dq_c/dw ) \times ( dw/dt ) =-z\times p$ . on the other hand we know that removing heat reduces the temperature according to the formula $\delta q=-cm \delta t$ , where $m$ is the mass being colled and $c$ is the specific heat for the substance constituting that mass . hence we have $dq_c/dt= cm ( dt_c/dt ) $ . combining the two formulae , we get $dt_c/dt=-zp/cm$ . but we cannot resolve this equation since $z$ is an unknown function of $t_c$ . what we know is that $z ( t_c ) $ , $p$ , $c$ and $m$ are strictly positive values . so $dt_c/dt$ is strictly negative . hence $t_c$ will decrease with time . since the function $z ( t_c ) $ is a strictly increasing function , its value will also decrease with time , and hence the absolute value of the derivative $dt_c/dt$ will also decrease with time . hence the graphical representation of the evolution of the temperature will look like the red curve in the figure , where $t_0$ is the initial temperature at time $t_0$ . cooling the same mass $m$ in two steps if we consider cooling independently ( in an identical refrigerator ) another mass $m_1$ , smaller than $m$ , with initial temperature $t_0$ we get another curve , like the curve in blue in the left part of the figure up to point c , corresponding to the equation $dt_c/dt=-zp/cm_1$ . it is below the red curve because the smaller mass $m_1$ cools faster than $m$ . formally , if we draw a horizontal line like the line cutting both curves in a and b , this corresponds to the same temperature for both curves , hence to a common value of the coefficient of performance $z$ . then $m_1&lt ; m\ \rightarrow\ ( dt_c/dt ) _a&lt ; ( dt_c/dt ) _b$ . since this is true for any value of the temperature $t_c$ , it confirms that the blue curve for $m_1$ decreases faster than the red curve for $m$ . suppose now that at time $t_2$ the mass $m_1$ has been cooled to temperature $t_2$ corresponding to point c below the red curve . we add to $m_1$ another mass $m_2$ such that $m=m_1+m_2$ , the mass $m_2$ being at the initial temperature $t_0$ . the mass $m_2$ being warmer than $m_1$ will share its heat with $m_1$ ( in negligible time according to our hypothesis ) so that both reach the temperature $t_1$ corresponding to point d , and pursue cooling . at any time between $t_0$ and $t_2$ , the temperature of $m_1$ ( blue ) is less than the temperature of $m$ ( red ) . hence the refrigerator works with a lower coefficient of performance $z$ for $m_1$ than for $m$ , and less heat has been removed from the refrigerator containing $m_1$ than from the refrigerator containing $m$ at time $t_2$ . when we introduce the mass $m_2$ with $m_1$ , the total heat introduced in the refrigerator is that of $m_1+m_2$ at temperature $t_0$ . this is exactly the same as the heat introduced in the refrigerator containing $m$ . since less heat was removed from the $m_1+m_2$ refrigerator at time $t_2$ , it is at a higher temperature that the $m$ refrigerator . hence the point d is above the red curve . the $m_1+m_2$ refrigerator now contains the same mass as the $m$ refrigerator . hence it will follow an identical curve . but it is at temperature $t_1$ that was attained earlier , at time $t_1$ by the $m$ refrigerator . so the right part of the blue cooling curve for $m_1+m_2$ , starting at point d is the same as the right part of the red coling curve for $m$ starting at point b , translated by a duration $\delta t=t_2-t_1$ . conclusion the masses $m_1+m_2$ will always reach any temperature with a delay $\delta t$ after the mass $m$ has reached it . actual figures would be required to be more precise . given the problem , the cooling engine will be working at maximum power to get the fastest possible cooling in both cases . then it is obvious that the faster solution is also the most economical energetically . this assumes either that the cooling is started just at the right time , or that the cooling power is reduced once the right temperature has been attained . these results are based exclusively on our assumptions , independently of any actual figures . we will now discuss some of these assumptions . discussion heat capacity of the refrigerator we have assumed that the heat capacity of the refrigerator itself is negligible . we should however analyse its effect . we first note that the objective is to extract heat from a given number of bootles to bring them from $t_0=30^{\circ}c$ to $t_f=6^{\circ}c$ . that corresponds to a precise amount $q$ of heat to be removed , independently of the process used for that purpose . if the refrigerator itself is initially at temperature $t_f$ , it will at the start share heat with the mass to be cooled , thus warming up and cooling the bootles . but then it will have to be cooled down to $t_f$ again , so that it net contribution to the cooling process will be null , and the same amount $q$ of heat has to be removed by the heat pump . however , by sharing heat at the beginning , it induces an early cooling , thus making the whole heat removal process operate at a lower temperature , hence with a lower coefficient of performance $z$ . the net effect of the heat capacity of the refrigerator is thus to provide some early cooling , which can sometimes be considered an advantage , but at the expense of a lower effective coefficient of performance $z$ . these effects increase with the heat capacity of the refrigerator . note that if the initial temperature of the refrigerator is below the targeted final temperature $ t_f$ , the difference multiplied by the heat capacity is a net contribution to the refrigeration process , though the loss on the coefficient of performance remains . hence it is better not to have anything else in the refrigerator , even already cooled , unless it is cooled to a much lower temperature than the final temperature $ t_f$ intended for the bottles . heat sharing rate as we have seen from the previous discussion , the main objective is to remove a given amount $q$ of heat , and the effectiveness of the removal decreases as temperature is lowered . if the rate of heat sharing within the refrigerator is small , the volume near the cooling system will cool faster , thus reducing the coefficient of performance , i.e. the rate of heat removal . hence , ensuring the best possible heat sharing can help in all circumstances . it should be noted that direct sharing between cylindrical bottles will be reduced to a minimum : just one line of contact . so , if space allows , it is probably preferable to help air circulate between the bottles . keeping the bottles in vertical position will help if the refrigerator has grid shelves that let air through , rather than glass shelves . and , of course , the bottles must be unpacked . opening the door to quickly exchange bottles so that the warmer ones are placed near the cooling system will improve the coefficient of performance and reduce cooling time . it may have some cost in warming the refrigerator , but that is less important if the heat capacity of the load to be cooled is large ( actual measurements would be useful ) . exchanging bootles will also avoid having to cool those close to the cooling system below the required temperature $ t_f$ in order to have all the bottles temperature at least as low as $ t_f$ .
as mark eichenlaub said in his comment , start by writing down $f=ma$ , and including all the forces ( damping force , spring , driving force ) on the left . the result is a differential equation for the unknown function $x ( t ) $ . you are looking for a " steady state " solution , which just means a solution in which $x ( t ) $ is a sinusoidal oscillation : $x ( t ) =a\cos ( \omega t+\phi ) $ . there are such steady solutions for any $\omega$ you care to name . find the value of $\omega$ that leads to a solution with the biggest amplitude . and , sorry if this sounds rude , but you should probably also look up " resonance " and " driven harmonic oscillator " in your textbook . there is bound to be a bunch of useful stuff there .
the cooling over large distances is a challenge but the engineers and physicists are working on it . the current record is several km but not enough to connect cities , etc . this basically only works if the cables are in a kind of vacuum tube to reduce the cooling power . it is not practical to transmit electric energy if you need liquid helium temperatures . the cooling costs are prohibitive . the current state of the art are cables using thin films of bscco ( phys . org ) . they can operate at 77 k without problems . the current world record for such a cable in a vacuum tube is several kilometers but after some distance you need a small building along the cable to cool the liquid nitrogen inside the cable again . there is a tremendous research effort to find superconductors with higher critical temperatures and currents but that is not so easy . the usage for practical applications is increasing but the progress is rather slow . in more exotic applications such a cern or iter you absolutely need superconducting cables , if it is only for space reasons :
i am going to offer a completely different way of doing this . sometimes it is nice to work in symbols before getting into the very specific numbers . i take my inspiration from the bohr atomic model here . the total energy of the electron in orbit around the nucleus at any given radius is calculated as follows ( wikipedia 's equation here , not mine ) . that is , the sum of the kinetic and potential energy is just half the potential energy ! neat , is not it ? so , how would we apply this to the earth ? well , $z k_e e^2$ is going to have to be replaced with $g m m$ . but let 's not forget , the entire point was to introduce a radius $r&#39 ; =1.05 r$ , and i am seeking a value of $\delta e = e&#39 ; -e$ . also , the kinetic energy is 1/2 the magnitude of this total energy metric , i will use $e_k$ for that . $$\delta e = gmm/2 \left ( -1/r&#39 ; + 1/r \right ) = e \left ( -1/1.05+1\right ) =e\frac{0.05}{1.05} = 2 e_k \frac{0.05}{1.05}$$ so the energy would change by about 9.5% times the original kinetic energy . given your original energy , i believe this would be $8.8 \times 10^9 j$ . this all said , your question says : if the initial magnitude of the satellite’s mechanical energy was $e_{m , i} = 9.26 \cdot 10^{10}$ j and it continues at the same speed , how much work was done by the rockets in moving the satellite to the higher orbit ? our work assumed that it would attain a new speed . maybe the question is written wrong . i do not know .
let be $$\frac{2a}{q}v ( \theta , \varphi ) =f ( \theta , \varphi ) =2\sin\theta\cos\varphi+\cos^2\theta . \tag 1$$ the laplace spherical harmonics form a complete set of orthonormal functions and thus form an orthonormal basis of the hilbert space of square-integrable functions . on the unit sphere , any square-integrable function can thus be expanded as a linear combination of these : $$ f ( \theta , \varphi ) =\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell f_\ell^m \ , y_\ell^m ( \theta , \varphi ) \tag 2 $$ where $y_\ell^m ( \theta , \varphi ) $ are the laplace spherical harmonics defined as $$ y_\ell^m ( \theta , \varphi ) = \sqrt{{ ( 2\ell+1 ) \over 4\pi}{ ( \ell-m ) ! \over ( \ell+m ) ! }} \ , p_\ell^m ( \cos{\theta} ) \operatorname{e}^{i m \varphi } =n_{\ell}^m p_\ell^m ( \cos{\theta} ) \operatorname{e}^{i m \varphi }\tag 3 $$ and where $n_{\ell}^m$ denotes the normalization constant $ n_{\ell}^m \equiv \sqrt{{ ( 2\ell+1 ) \over 4\pi}{ ( \ell-m ) ! \over ( \ell+m ) ! }} , $ and $p_\ell^n ( \cos\theta ) $ are the associated legendre polynomials . the laplace spherical harmonics are orthonormal $$ \int_{\theta=0}^\pi\int_{\varphi=0}^{2\pi}y_\ell^m \ , y_{\ell'}^{m'*} \ , d\omega=\delta_{\ell\ell'}\ , \delta_{mm'} , $$ where $δ_{ij}$ is the kronecker delta and $\operatorname{d}\omega = \sin\theta \operatorname{d}\varphi\operatorname{d}\theta$ . the expansion coefficients are the analogs of fourier coefficients , and can be obtained by multiplying the above equation by the complex conjugate of a spherical harmonic , integrating over the solid angle $ω$ , and utilizing the orthogonality relationships . this is justified rigorously by basic hilbert space theory . for the case of orthonormalized harmonics , this gives : $$ f_\ell^m=\int_{\omega} f ( \theta , \varphi ) \ , y_\ell^{m*} ( \theta , \varphi ) \operatorname{d}\omega = \int_0^{2\pi}\operatorname{d}\varphi\int_0^\pi \operatorname{d}\theta\ , \sin\theta f ( \theta , \varphi ) y_\ell^{m*} ( \theta , \varphi ) . \tag 4 $$ where $ y_\ell^{m*} ( \theta , \varphi ) = ( -1 ) ^m y_\ell^{-m} ( \theta , \varphi ) $ . the evaluation of the expansion $f_\ell^m$ may be very long in this way . . . we can use some tricks in your case for $f ( \theta , \varphi ) =2\sin\theta\cos\varphi+\cos^2\theta$ observing that $$ \sin\theta=-p_1^1 ( \cos\theta ) $$ and $$ y_{1}^{-1} ( \theta , \varphi ) - y_{1}^{1} ( \theta , \varphi ) = {1\over 2}\sqrt{3\over 2\pi}\cdot e^{-i\varphi}\cdot\sin\theta-{-1\over 2}\sqrt{3\over 2\pi}\cdot e^{i\varphi}\cdot\sin\theta =\sqrt{3\over 2\pi} \sin\theta\cos\phi $$ so that $$ 2\sin\theta\cos\varphi=2\sqrt{\frac{2\pi}{3}}\left ( y_1^{-1} ( \theta , \varphi ) -y_1^{1} ( \theta , \varphi ) \right ) =-2p_1^1 ( \cos\theta ) \cos\varphi\tag 5 $$ and observing that $$ \cos^2\theta=\frac{1}{3}p_0^0+\frac{2}{3}p_2^0 $$ and using the relation $y_\ell^0 ( \theta , \varphi ) =\sqrt{\frac{2\ell+1}{4\pi}}p_\ell^0 ( \cos\theta ) $ where $p_\ell^0 ( \cos\theta ) $ are the ordinary legendre 's polynomials $p_\ell ( \cos\theta ) $ , we obtain $$ \cos^2\theta=\frac{1}{3}p_0^0 ( \cos\theta ) +\frac{2}{3}p_2^0 ( \cos\theta ) =2\sqrt{\pi}y_0^0 ( \theta , \varphi ) +\frac{4}{3}\sqrt{\frac{\pi}{5}}y_2^0 ( \theta , \varphi ) . \tag 6 $$ finally , putting together ( 5 ) and ( 6 ) in ( 1 ) we obtain $$ f ( \theta , \varphi ) =2\sqrt{\frac{2\pi}{3}}y_1^{-1} ( \theta , \varphi ) -2\sqrt{\frac{2\pi}{3}}y_1^{1} ( \theta , \varphi ) +2\sqrt{\pi}y_0^0 ( \theta , \varphi ) +\frac{4}{3}\sqrt{\frac{\pi}{5}}y_2^0 ( \theta , \varphi ) \tag 7 $$ so that , comparing ( 7 ) and ( 2 ) , the coefficients $f_\ell^m$ are $$ f_1^{-1}=2\sqrt{\frac{2\pi}{3}}\qquad f_1^{1}=-2\sqrt{\frac{2\pi}{3}}\qquad f_0^{0}=2\sqrt{\pi}\qquad f_2^{0}=\frac{4}{3}\sqrt{\frac{\pi}{5}}\tag 8 $$ so you have $$\small v ( \theta , \varphi ) =\frac{q}{a}\left [ \sqrt{\frac{2\pi}{3}}y_1^{-1} ( \theta , \varphi ) -\sqrt{\frac{2\pi}{3}}y_1^{1} ( \theta , \varphi ) +\sqrt{\pi}y_0^0 ( \theta , \varphi ) +\frac{2}{3}\sqrt{\frac{\pi}{5}}y_2^0 ( \theta , \varphi ) \right ] $$ the general solution to the laplace equation outside the sphere is $$ \phi ( r , \theta , \varphi ) =\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell \frac{b_\ell^m}{r^{\ell+1}} \ , y_\ell^m ( \theta , \varphi ) \tag 9 $$ with $b_\ell^m=\frac{q}{a}f_\ell^m$ , that is $$\small \phi ( r , \theta , \varphi ) =\frac{q}{a}\left [ \frac{1}{r^2}\left ( \sqrt{\frac{2\pi}{3}}y_1^{-1} ( \theta , \varphi ) -\sqrt{\frac{2\pi}{3}}y_1^{1} ( \theta , \varphi ) \right ) +\frac{\sqrt{\pi}}{r}y_0^0 ( \theta , \varphi ) +\frac{1}{r^3}\left ( \frac{2}{3}\sqrt{\frac{\pi}{5}}\right ) y_2^0 ( \theta , \varphi ) \right ] $$
building an action : if you know the field content ( which i assume means you know the gauge group and reps of all the fields ) then : write down every term that is lorentz scalar ( so combinations like $\partial_\mu a^\mu$ , $\bar{\psi}\gamma^\mu \partial_\mu\psi$ allowed but not things like $\vec{n}\cdot\nabla \phi$ where $\vec{n}$ is some random 3-vector ) . stop at terms with dimension greater than the spacetime dimension ( 4 in 4d , so include terms like $\phi^4$ and $\phi\bar{\psi}\psi$ but not higher dimension terms like $\phi \partial_\mu \phi \bar{\psi} \gamma^\mu \psi$ ) . cross out terms that are not gauge invariant . this means that gauge fields can only appear through covariant derivatives and field strength tensors , and matter fields must appear in singlet combinations . cross out terms that violate any global symmetries you want to impose ( though be warned - if these symmetries are anomalous you can not drop these terms consistently ) . in susy theories you need to impose relations between coupling constants as well . after you have done this you can probably use field redefinitions ( orthogonal/unitary rotations in flavour space ) to simplify some of the coupling constants . an example would be the standard model where the lepton yukawas can be diagonalised and the quark yukawas can be diagonalised leaving just the physical ckm matrix . it is really a lot like lego - the fields are the building blocks and symmetries and gauge invariance tell you what you can put together .
let 's say your target is a film $10^{-2}$ mm thick . nuclei are about $10^{-14}$ m in diamater at most . this means that the alignment of the beam with the target would have to be $10^{-9}$ radians , which is not possible with realistic beam optics . even if your beam optics were that perfect , the perfect parallelism would be destroyed by scattering once the beam entered the target .
since you say you are a programmer , i see where criterion #1 comes from . but telescopes are not computers , you can not upgrade the cpu today , the ram tomorrow , and so on . a scope is defined largely by its aperture ( the diameter of the objective lens or mirror ) . that puts a major cap on pretty much everything else , performance-wise . aperture is like an old boarding school taskmaster who says " you are allowed up to here , no more " , and anything else you may do can only place you lower than that ideal level of performance . scopes optimized for visual and scopes optimized for photo are different animals . they are interchangeable to some extent , but after some point their respective traits start acting up . usually , people start with a small price-efficient visual scope ( like a small dobsonian ) , then migrate to astrophoto after some learning is done . but if you are intent on doing ap directly , fine . the instrument that is typically used for ap is some kind of catadioptric , like a schmidt-cassegrain ( sct ) , on a tracking mount . it does not have to be an sct , it could be a refractor , a ritchey-chretien , it could be a newtonian , or what have you . but an sct is typically short , stubby , and rather not unwieldy for its aperture , which are good attributes if you put it on a tracking mount . the mount does not necessarily have to be a go-to mount . i would argue that go-to is a waste of money if you are smart enough that you can use a star map ( either paper , or software ) . but it absolutely needs to track the motion of the sky on one axis , because you are going to take a lot of long-exposure photos . some examples : http://www.telescope.com/telescopes/astrophotography-telescopes/pc/1/19.uts?currentindex=0pagesize=34defaultpagesize=20mode=viewallcategoryid=1subcategoryid=19type=thumbnail2level as you can see , ap scopes and mounts are expensive . you could get away with a mediocre scope , and still take good pictures , but a bad mount is a deal breaker . a lot of beginners are like " okay , that stuff is way too expensive " and simply purchase a small cassegrain on a cheap go-to . it is definitely less expensive , but the performance changes accordingly . some examples : http://www.telescope.com/telescopes/cassegrain-telescopes/pc/1/14.uts?currentindex=0pagesize=51defaultpagesize=20mode=viewallcategoryid=1subcategoryid=14type=thumbnail2level finally , if you decide to take a while and school yourself in purely visual astronomy before purchasing a killer ap rig , you could start with a bang-for-the-buck visual scope such as a classic or intelliscope dobsonian - as much aperture per coin spent as possible : http://www.telescope.com/telescopes/dobsonian-telescopes/pc/1/12.uts ( i am in the us , but the general principles should apply ; maybe less so the particular examples above . ) edit : here 's a bare bones rig for ap : http://www.garyseronik.com/?q=node/52 if you have a digital camera already , it will cost you almost nothing , and you could assemble it in one week-end .
brief answer : read only the bold part ( and ignore grammar then ) . the answer you already mentioned lies in quantum field theory ( qft ) . but to fully understand it , you must give up a particle as a point-like thing that is well-localized . there is one quantum field per sort of particle , e.g. the electron field for all electrons , and the photon field for all photons . ( the fact that there is a single field for all electrons also results in the pauli exclusion principle . ) what you consider a particle is basically just a local peak in the respective particle field , but one cannot even say " this peak corresponds to electron a , this one to b " . now qft , more specifically quantum electrodynamcis ( qed ) , describes the local interaction between the electron field and the photon field . but since the fields have a dynamic , a local change induced in the photon field by the electron field will propagate with the speed of light ( flat space assumed ) and interact with the electron field in another place , thus creating the impression " electron a emitted a photon that told electron b to interact electromagnetically " . it is similar for the other interactions , there is a gluon field for the strong interaction ( quantum chromodynamics ) , and for the electroweak interaction there is kind of a combination of the photon field and the weak-interaction-bosons .
faraday 's cage is known to block static and non-static electric fields . the mechanism of blocking depends on whether the electric field is static or non-static ( em field ) . i suppose you question is about how the cage works in non-electrostatic case . in em case ( time changing field ) , two scenarios could happen . the first is electric discharge where the current flows from a distant electrode to the cage . the second is an em wave with high power propagating toward the cage generating its current locally within the conductor . i will explain how the cage works for both cases . with respect to the first case , it can be mathematically described by charge continuity equation ( equation 3 in this link ) . this equation basically relates the current flowing through a conductor to the charge accumulating in it . what happens in the first scenario is that the external current ( being moving charges ) coming from the electrode accumulates at the point where it ( the spark or the streamer ) hit the cage . because the cage is a conductor the charge continuity equation tells us that the local accumulation of charge where the spark hit the cage will cause current to flow within the conductor to remove that accumulation . the characteristic time required to remove the accumulation is called the relaxation time . it can be derived from charge continuity equation . for the derivation have a look at pages 57-59 of this document . i think that is taken from a book called elements of electromagnetics chapter 5 . if the conductor is made from a material with infinite conductivity , the relaxation time is zero . that means the current will keep flowing though the cage without any problem and that the electric field in the conductor is always zero . in other words , the electrostatic point of view holds even for non-electrostatic case if the conductivity is infinite . that is a direct consequence from charge continuity equation . for non infinite conductivity cases , the electric field within the conductor will survive within the conductor with a time scale related directly to relaxation time of that conductor . i hope it is clear now with respect to first case . the second case is related to em waves where they generate their currents locally within the conductor , that is where the skin effect comes into play . an em wave penetrates into a conductor the skin effect occurs . in general , em waves when they penetrate a conductor they are attenuated until their fields become almost zero . a characteristic depth of penetration is called skin depth . the skin depth is the distance it takes an em wave to be attenuated to certain value . this skin depth depends on many factors such as conductivity and frequency , the following figure taken from wikipedia shows the skin depth of different materials for different frequencies : for the cage to protect from em waves , it is thickness has to be larger than multiples of skin depth at the particular frequency of interest . so briefly with respect to the second scenario , the skin depth becomes relevant when we speak about shielding from electromagnetic waves rather than discharge current . the first and the second scenarios can be put together in frequency spectrum , the first scenario describes why the cage protects current in low frequencies while the second scenario describes why it protects from both current and radiation at high frequencies . i think the cage in the picture shows scenario 1 . you can clearly see the distant electrodes and the point at which the spark hits the cage i hope that answered your question
work done by tension on both the blocks can be regarded as 0 . this can be said by the virtual work method . the virtual work method : consider that block 1 ( mass $2kg$ ) displaces by a certain $d\vec{s_1}$ . infinitesimal work done on the block 1 by tension will be given by $$dw_1 = \vec t . d\vec s_1=tds_1\cos\theta_1$$ similarly , for block 2 we can say that $$dw_2=\vec t . d\vec s_2=tds_2\cos\theta_2$$ using string constraint , we can say that displacement of each block along the string is zero ( because the string is inextensible ) . so we get $$ds_1\cos\theta_1+ds_2\cos\theta_2=0$$ notice that i have used the same $\theta$ for each block as in tension because the direction along the string is the direction along tension vector .network done by tension thus becomes $$dw_t=t ( ds_1\cos\theta_1+ds_2\cos\theta_2 ) $$ $$\therefore dw_t=0$$ $$w_t=0$$ the solution to the actual problem : if we apply $w=\delta k$ on the system of the two blocks from initial position to the final position where block 1 is at the bottom of the cicrular arc , we get $$m_1g\delta h_1+m_2g\delta h_2=\frac 12 m_1 v_1^2+\frac 12 m_2 v_2^2$$ i do not include work done by tension on the system because i proved it to be 0 . we now need to find a relation between $v_1$ and $v_2$ . we can do this by applying string constraint . $$v_1\cos\theta=v_2$$ where $\cos\theta=\frac 35$ ( by geometry ) . $$\therefore \frac 35 v_1 = v_2$$ substituting $v_2$ in terms of $v_1$ in the above equation , we can find $v_1$ . then applying $w=\delta k$ on block 1 only we get $$w_t + w_{gravity} = \frac 12 m_1v_1^2$$ substitute $w_{gravity}$ and $v_1$ in this equation and find $w_t$ .
the temperature of the gas that is sprayed goes down because it adiabatically expands . this is simply because there is no heat transferred to or from the gas as it is sprayed , for the process is too fast . ( see this wikipedia article for more details on adiabatic processes . ) the mathematical explanation goes as follows : let the volume of the gas in the container be $v_i$ , and its temperature $t_i$ . after the gas is sprayed it occupies volume $v_f$ and has temperature $t_f$ . in an adiabatic process $tv^{\ , \gamma-1}=\text{constant}$ ( $\gamma$ is a number bigger than one ) , and so $$ t_iv_i^{\ , \gamma-1}=t_fv_f^{\ , \gamma-1} , $$ or $$ t_f=t_i\left ( \frac{v_i}{v_f}\right ) ^{\gamma-1} . $$ since $\gamma&gt ; 1$ and , clearly , $v_f&gt ; v_i$ ( the volume available to the gas after it is sprayed is much bigger than the one in the container ) , we get that $t_f&lt ; t_i$ , i.e. the gas cools down when it is sprayed . by the way , adiabatic expansion is the reason why you are able to blow both hot and cold air from your mouth . when you want to blow hot air you open your mouth wide , but when you want to blow cold air you tighten your lips and force the air through a small hole . that way the air goes from a small volume to the big volume around you , and cools down according to the equations above .
the index of refraction is found to be a function of the frequency in an analysis of radiation scattering in a medium : for example in the book of panofsky and philips " classical electricity and magnetism " chapter 22 , radiation scattering and dispersion , on paragraph 7 . the book seems to be available for a free download here . here is a link with feynman lectures on light to to get the extended framework on light .
i worked a lot with this kind of formulas : ) it would be useful to recover the imaginary unit in $\exp$ and think of integration variables as taking real values . then , upon appropriate normalization $$ \int\int \exp ( i u_\alpha v^\alpha ) d^2u d^2v=\int \delta^2 ( u ) d^2u=1$$ once you agree with this formula , which is a standard representation for the delta function ( that the variables have some spinorial meaning is irrelevant , they are usual commuting variables ) the rest is easy . for example , let us compute $y_\alpha \star g ( y ) $ where $g ( y ) $ is any function $$\int ( y+u ) _\alpha\ , g ( y+v ) \exp ( iu_\alpha v^\alpha ) =\\\int y_\alpha\ , g ( y+v ) \exp ( iu_\alpha v^\alpha ) +\int u_\alpha\ , g ( y+v ) \exp ( iu_\alpha v^\alpha ) =a+b$$ a is easy , one can integrate over $u$ to get delta function $$a=\int \delta^2 ( v ) y_\alpha g ( y+v ) d^2 v=y_\alpha g ( y ) $$ to compute b we integrate by parts $$ b=\int g ( y+v ) ( -i\frac{\partial}{\partial v^\alpha}\exp ( iu_\alpha v^\alpha ) ) =\\ =\int ( i\frac{\partial}{\partial v^\alpha}g ( y+v ) ) \exp ( iu_\alpha v^\alpha ) =\\ =i\frac{\partial}{\partial y^\alpha}\int g ( y+v ) \exp ( iu_\alpha v^\alpha ) =\\ =i\frac{\partial}{\partial y^\alpha} g ( y ) $$ alltogether we derived , $$y_\alpha \star g ( y ) = ( y_\alpha+i \partial_\alpha ) g ( y ) $$ which for example gives $$y_\alpha \star y_\beta=y_\alpha y_\beta+i \partial_\alpha y_\beta=y_\alpha y_\beta+i\epsilon_{\alpha\beta}$$ the integral formula for star product is a very useful one if you are to compute star products of something more complicated than polynomials . this is what happens with vasiliev theory since the functions encode higher-spin fields together with all derivatives thereof , so only the anti-de sitter solution ( vacuum ) is of type $yy$ while perturbations are analytic functions of $y$ . for example , the bulk-to-boundary propagator is somethins like $\exp ( yy+y ) $ . that the integral formula is more useful can be seen in trying to compute the star product of two gaussians $\exp ( yy ) \star \exp ( yy ) $ , which is just a gaussian integral with the help of the integral formula and i have no idea how to compute this using $\exp ( \partial\partial ) $ formula
for the reasons given in the comment above , i think the argument from the $m\rightarrow 0$ limit is valid . but if one does not like that , then here is an alternative . suppose that a massless particle had $v&lt ; c$ in the frame of some observer a . then some other observer b could be at rest relative to the particle . in that observer 's frame of reference , the particle 's three-momentum $\mathbf{p}$ is zero by symmetry , since there is no preferred direction for it to point . then $e^2=p^2+m^2$ is zero as well , so the particle 's entire energy-momentum four-vector is zero . but a four-vector that vanishes in one frame also vanishes in every other frame . that means we are talking about a particle that can not undergo scattering , emission , or absorption , and is therefore undetectable by any experiment .
one possible explanation is delayed phase transformation in a nickel sulphide inclusion in toughened glass ( http://www.glassonweb.com/articles/article/330/ ) , which glass is sometimes used for cooking ware . as noted in a comment to the referenced article , this process can be accelerated by temperature variations . judging by the description , the pattern of failure seems typical for toughened ( tempered ) glass . again , i cannot be sure that this mechanism was indeed present in the case described in the question .
the general issue is that you cannot plug your equations of motion into the lagrangian and naively expect to get the same equations of motion back out again . why not ? let us look at your specific example . for the usual story we start with $$ l = \frac12 m ( \dot r^2 + r^2\dot\theta^2 ) - v ( r ) . $$ we find that the angular momentum , defined by $\ell=m r^2\dot\theta$ , is conserved so the equation of motion for the radial coordinate is $$ m \ddot r - \frac{\ell^2}{m r^3} + \frac{\partial v}{\partial r} = 0 . $$ now , you want to plug $\ell$ back into the lagrangian . if we do that we have $$ l = \frac12 m \left ( \dot r^2 + \frac{\ell^2}{m^2 r^2} \right ) - v ( r ) . $$ naively , if we calculate the equation of motion from this lagrangian that we will get the opposite sign for the $\ell^2/m r^3$ term . this is not correct ! recall that when we call $\ell$ a conserved quantity we mean it is a constant in time , that is $\dot\ell=0$ . explicitly writing out the euler-lagrange equations we have $$ \frac{\mathrm{d}}{\mathrm{d}t}\left [ \left ( \frac{\partial l}{\partial\dot r} \right ) _{r , \theta , \dot\theta} \right ] - \left ( \frac{\partial l}{\partial r} \right ) _{\dot r , \theta , \dot\theta} = 0 . $$ here i have included the reminder that when we take partial derivatives we mean that " everything else " is held constant and what that " everything else " is . for the problem at hand note that $$ \frac{\partial\ell}{\partial r} = \frac{2\ell}{r} \ne 0 $$ so it is not a general constant . keeping this in mind , we do get the correct equation of motion ( as we must ) .
main point : you should allow the possibility of sign factors appearing into the definition of the hilbert space representation of fermionic operators , cf . fermionic fock space . in more detail , consider the car algebra $$\tag{1} \{c_{\sigma} , c_{\tau}\}~=~0 , \qquad \{c_{\sigma} , c^{\dagger}_{\tau}\}~=~\hbar {\bf 1} , \qquad\{c^{\dagger}_{\sigma} , c^{\dagger}_{\tau}\}~=~0 , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ next define $$\tag{2} c_{\sigma}\left|0\right&gt ; ~:=~0 , \qquad \left|\sigma\right&gt ; ~:=~ c^{\dagger}_{\sigma}\left|0\right&gt ; , \qquad \left|\sigma\tau\right&gt ; ~:=~ c^{\dagger}_{\sigma}\left|\tau\right&gt ; , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ note that these definitions imply that $$\tag{3} \left|\sigma\tau\right&gt ; ~=~ -\left|\tau\sigma\right&gt ; , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ in particular $$\tag{4} \left|\sigma\sigma\right&gt ; ~=~ 0 , \qquad \sigma\in \{\uparrow , \downarrow\} . $$
chemistry and physics have a lot of overlaps . this could be atomic physics too . . . i do not have a short explanation , but look up orbital hybridization and molecular orbital theory for a quantum-mechanical view of this . the above two should point you in a promising direction . good luck with understanding the physics of bonding .
first of all , according to earnshaw 's theorem one can not create a potential well in the ball . second , the function should fit laplace 's equation : $$ \delta \varphi = 0 . $$ in fact the second point is enough to prove the first one . so the answer is : no , this can not be made for every continuous function . yes , this follows from gauss 's law .
the idea is correct ( it is called the hubbard-stratonovich transformation ) , but i can not say more without the details of the action . it is discussed in any good textbook on quantum field theory for condensed matter . concerning you questions ( if you are okay with the physicist 's approach to grassman numbers ) : the product of two grassmann numbers commutes with any c-number ( `bosonic ' number ) or grassmann number , so it can be considered for most purposes as a c-number . by the way , that is why the action of fermions is also a c-number , as you would expect . therefore , 1- you can safely shift $\phi$ by $\bar\psi\psi$ , that is just a change of variables . 2- when you shift $\phi$ , you do that at $\bar\psi\psi$ constant ( because you do the integral over $\phi$ first ) , so there is no problem with the integration either . i am sure mathematicians would find plenty of subtleties ( well , they do not even agree that functional integrals exist . . . ) , but as a physicist , you are ready to go !
no . here is a counter-example . i will consider a $h_4 \otimes h_4$ space , where $h_4$ is four dimensional . i will express the counter-example basis in terms of the standard separable basis states $|0\rangle , \ldots , |3\rangle$ . to form the basis , first take all states $|kj\rangle$ , where $k$ and $j$ are such that $k \neq j$ or $k \in \{0,1\}$ or $j \in \{0,1\}$ . so basically all the usual basis states except $|22\rangle$ and $|33\rangle$ . now we have 14 of the 16 states in our basis . the final two are $2^{-1/2} ( |22\rangle + |33\rangle ) $ and $2^{-1/2} ( |22\rangle - |33\rangle ) $ . the last two are entangled , the rest are not .
for a high degree of accuracy you would have to probe the particle with a high energy ( short wavelength ) photon so there is plenty of energy that can go into vibrational excitation . after such hard hit the particle will be smeared across a wide range of states $$\psi=a_0*\psi_0+a_1*\psi_1+ . . . $$this is not an entanglement but a simple superposition of eigenstates . the expectation value of the particle 's energy $\bar{e}=\sum_{i}a_i^2e_i$ should not be equal to the energy of any particular state and the second measurement will yield $e_i$ with $a_i^2$ probability . so , the extra energy comes from an interaction with a probe particle and it does not have to be precisely equal to the energy of a certain vibrational state .
the energy conservation law is compatible with every single observation we have made inside the milky way in science , or outside science , so the empirical evidence in favor of it is overwhelming , diverse , and universal . theoretically , the case is also clear . emmy noether demonstrated that conservation laws are linked to symmetries . the validity of the energy conservation law is equivalent to the time-translational symmetry of the laws of physics : the same phenomena occur if one starts with the same initial conditions but just a bit later . this is true for the laws of mechanics , field theory , electromagnetism , nuclear interactions , classical physics , quantum mechanics , thermodynamics and statistical physics , special relativity . the energy conservation law is valid in all these situations and respected by all the major theories describing these subfields of physics . motors , those produced by faraday , tesla , or anyone else , as well as all other engines and objects in the universe preserve the energy , too . and there exists no equivalence or analogy between the energy conservation law and the existence of tesla 's or faraday 's motor . in particular , there has never existed any solid evidence – empirical or theoretical – that electricity or magnetism could not do work . so any analogy between these totally different questions is an example of something technically referred to as demagogy . to create legitimate doubts about the validity of such an important and well-established law , one would need an observation or an arguments that actually discusses the technical properties of energy ( and one would probably have to construct a viable theory disagreeing with the energy conservation law that is compatible with the observations ) – rather than demagogic comparisons to completely different questions that the speaker desires to be answered by the same answer no although there does not exist a glimpse of a rational reason why the answers should be the same . to see violations of the energy conservation law , one has to go to cosmology . however , due to the slow evolution of the universe today , one needs to wait approximately for 10 billion years for the total energy of a system to change by an amount comparable to 100% . in the early stages of the cosmological evolution of our universe , the total energy was not conserved – this is particularly important for cosmic inflation that created the energy of the whole cosmos out of " almost nothing " . but this non-conservation depended on the background spacetime 's heavy violation of the time-translational symmetry .
peltier elements available in trade are commonly rated for operating down to to vicinity of -50 °c this is still much higher than ln2 . while the electric properties should not change much ( though the efficiency drops ) , such thermal stress may simply damage them mechanically . their primary profit in cooling applications is to raise the temperature threshold versus the ambient level : it is much easier to get a heatsink down from 120°c to 100°c in room temperature , than from 60°c to 40°c so while the heatsink runs really hot , the cpu enjoys nice , low temperature despite the additional heat produced by the element . now , applying ln2 to the heatsink completely negates these benefits . your peltier element is just a fancy heater vulnerable to low temperatures . i suggest you get a normal resistive heater instead for regulation . they are vastly more cold-resistant and you have such a surplus of " cold " that you really do not need the option to go " even cooler " . edit : the thermal coefficient of unpowered ( and shorted , as it will power itself up with seebeck voltage even unconnected ) peltier module is around 2w/ ( m*k ) ( 2 ) with a typical 36x36x4mm module that would give heat transfer of about 130 watt at temperature difference 200k ( assuming -196°c of ln2 , +4°c of cpu ) this is also about the limit of heat the module of this size is able to transfer when powered to vmax , which would mean this temperature difference ( 200k ) would be about what you had be able to maintain between the heatsink and the cpu while powering up the module for blocking heat transfer , and so the whole contraption would work just right under the dubious assumption that the module will perform correctly . why dubious ? the modules are rated for operating completely under temperatures of above -50°c and with δт not exceeding 100°c they will be operating with one side in temperature -196°c and δт in vicinity of 200°c you exceed both the minimum temperature rating and the temperature difference between the two sides by 100% . while the semiconductors should not be affected , the plastic that binds them is not nearly so frost-resistant . the most likely outcome is that the module will come apart at its seams , the binding plastic will shatter and crumble , massive condensation in dehermetized inside will short the circuit and the whole device will fail completely .
the general two-particle state will look like $\displaystyle \int dp_1 dp_2 \psi ( p_1 , p_2 ) a^\dagger_{p_1} a^\dagger_{p_2}| 0\rangle $ here $\psi ( p_1 , p_2 ) $ is the momentum-space wavefunction . since the creation operators commute , only the symmetric part matters , so we may as well take $\psi ( p_1 , p_2 ) =\psi ( p_2 , p_1 ) $ ( there would be a minus sign if they were fermions ) . if you would like the state to be normalizable , it should be square integrable . the position-space wavefunction $\psi ( x_1 , x_2 ) $ , is the fourier transform . you can then choose this to be supported when $x_1$ and $x_2$ are close to the positions at which you would like to localize the particles ( or vice-versa , because of the symmetry : the particles are indistinguishable ) , for example by gaussians . the phase can then carry information on the momenta of the particles ( as is hopefully familiar from 1-particle gaussians ) , as well as on how the two are entangled . this really does not depend much on the details of the sort of particle you are talking about , except the wavefunction will be symmetric or antisymmetric depending on whether you have bosons and fermions , and particles other than scalars will carry spin degrees of freedom so the wavefunction becomes a matrix .
this follows immediately from schrodinger 's demonstration of the equivalence of wave mechanics to heisenberg 's matrix mechanics , although i did not read the original paper , and i could not find it in a quick search online . there are many ways to argue this , and i will just give a few of the more obvious ones . theoretical justifications the property of entanglement is the statement that a general wavefunction for 2 particles is a function of their 6 coordinates , not two functions of their 3 coordinates . schrodinger toyed with the idea that the wavefunction is a physical wave , but abandoned it once he came in contact with the bohr/heisenberg matrix mechanics . in matrix mechanics , you treat many particles using independent x operators and p operators which have the property that $ [ x_j , p_k ] = i\delta_{jk}$ . in wave mechanics , you interpret $p$ as a differential operator for functions of the $x$ coordinates , and $x_j$ as multiplying by the appropriate coordinate , and this gives a representation of the algebraic canonical commutation relations . the number of dimensions is determined by the number of commuting x coordinates , and is the same as the classical configuration space , half of phase space , ignoring the momenta . adding more coordinates is adding more commuting x operators , and more independent p operators , so that the wavefunction grows in dimension without bound , and there is no natural restriction you can choose . a measurement of the $x_i+x_j$ operator will prepare a state which is not entangled in the $x_i+x_j$ $x_i-x_j$ basis , but this state is entangled in the $x_i$ $x_j$ basis . the sum of product wavefunctions of the form $\psi ( x , y ) =f ( x ) g ( y ) $ does not respect the product form , so if you restrict yourself to unentangled wavefunctions , the evolution operator cannot be a general linear map . the constraint which produces entangled wavefunctions is that $$ \partial_x \partial_y \log ( \psi ( x , y ) ) = 0 $$ which is the differential constraint that asserts that the logarithm is a function of x plus a function of y only . these product wavefunctions are only eigenfunctions of hamiltonians of the form $h_1 ( x ) +h_2 ( y ) $ , where one h is a function of x alone , and the other is a function of $y , p_y$ alone , so that it is impossible to couple such systems to each other . adding more particles adds more canonical coordinates , but so does adding more dimensions to space , so that the particle has more independent directions to move in . the two operations are indistinguishable in matrix mechanics , because you do not specify what the dimensions mean until you specify the hamiltonian and the observable interpretation . helium atom this is a theoretical argument with a startling conclusion , that particles have entangled states , so it is important to verify it experimentally . the ground state of helium provides a particularly clean test . two independent electrons will both have to sit in an s-wave cloud around the nucleus , and repel each other heavily by their electrostatic interaction . by admitting non-product wavefunctions , you can make the amplitude for the two electrons to be both on the same side of the nucleus be smaller , and the amplitude for the two electrons to be on opposite sides bigger , without spoiling the overall spherical symmetry of the two electron system altogether . the result is that the helium atom is 30% lower in energy in the entangled true ground state than it would be in the artificial unentangled ground state . these calculations are best done using the variational method , and the entangled ground state will predict the ionization energy for helium as precisely as you like ( i believe you can get to 1% accuracy with paper and pencil calculations--- the once-ionized comparison state is easy because it is the exactly solvable he+ ion , essentially the same as the h atom ) .
i have turned my comment into an answer so the question can be closed . a very similar question was asked and answered here .
from what you describe there are two different length scales associated with this problem . the one ascociated with flow through the porous medium ( the ' bed packed with some objects' ) , and the second ascociated with the flow around the ' bed ' . i will assume you are talking about the flow through the porous ' bed ' . normally the characteristic dimension or length scale for internal flows is taken to be the hydraulic diameter . this is defined to be four times the cross-sectional area ( of the fluid ) , divided by the wetted perimeter . however , for such things as ' pebble beds ' etc . the reynolds number is defined differently . for flow of fluid through a bed of approximately spherical particles of diameter d in contact , if the voidage ( fraction of the bed not filled with particles ) is ε and the superficial velocity v ( that is , the fluid velocity through the bed as if the spheres/objects were not present ) then a reynolds number can be defined as : $$re = \frac{\rho v d}{\mu ( 1 - \epsilon ) }$$ laminar conditions apply up to re = 10 , fully turbulent from 2000 ( wikipedia ) . there are more advanced formulas for this , and they work in a variety of regimes ; from not-so-packed beds , to very packed-beds , also with a variaty of pebble/object shapes . many experiments have been done on convective and radiative heat transfer in pebble bed nuclear reactors and other such heat exchangers . i am sure you should be able to find some journal papers on this stuff along with the standard correlations you need for your particular flow . for the convective heat transfer coefficient for this flow however , you should be using the nusselt number which is a measure of the ratio of convective to conductive heat transfer a solid-fluid boundary . i hope this helps .
i do not know what a " pile " of fuel is . i assume you mean a container full of it . gasoline needs oxygen to burn , and it needs the correct mixture . too little oxygen and burning is impossible . too much oxygen causes the same problem . to achieve ignition with gasoline , you need between 1.4 and 7.6% petrol vapour ( by volume ) in the air . ouside this range burning will not start . if these conditions are met , as the flash point of gasoline ( the minimum temperature at which it will burn ) is $-40c$ , your red-hot poker will do the job . in fact , gasoline and air will self-ignite at around $250c$ . for more details , start with the wikipedia article on the flammability limit .
the use of the chemical potential $\mu$ as state variable is useful in situations where composition $n$ is variable and/or cannot be easily controlled . from an experimental point of view the chemical potential is fixed when the system is in contact with energy and particle reservoirs . at equilibrium , the chemical potential of the system equals that of the reservoir $\mu = \mu_\mathrm{res}$ . thus by modifying the parameters of the reservoir you can control the chemical potential of the system . a typical example is when the system is a layer of molecules adsorbed in a surface . this is an open system and composition is variable --and generally unknown-- . by using a gas of the same molecules as reservoir you can fix the value of the chemical potential of the open system . the chemical potential $\mu_\mathrm{res}$ of the gas can be obtained from evaluating the fundamental equation of the gas or , if this is not available , from integrating the gibbs duhem relation if the equation of state of the gas is known .
charging and discharging a capacitor periodically surely creates electromagnetic waves , much like any oscillating electromagnetic system . the frequency of these electromagnetic waves is equal to the frequency at which the capacitors get charged and discharged . that means that if you have just dc , the frequency is de facto zero and the resulting electromagnetic waves will be pretty invisible . for the frequency to be that of the visible light , the circuit would have to be as small as an atom . ideally , it would have to be an atom because atoms are the " circuits " that naturally emit visible light .
that really depends on how you are going to deflect the motion . this is the best case : you apply a radial force . for instance , you could attach a fixed string to your mass , effectively making it a pendulum ( if that helps your imagination ) . what is important is that the mass will be moved in a circular path , because the taut string provides a fixed radius . the radial force will never do any work ( since force and movement are always perpendicular to each other ) . if you release the string after the 90° turn has been completed , you will not have put in any work into the system . it could be much worse though ( and will in reality be somewhat worse ) . for instance , you could also slow the mass to rest and then accelerate it to the terminal velocity in the new direction . this would require you to put in an amount of work equal two twice $e_{kin}=mv^2/2$ . given your quantities , that would be $w=1\ , \text{j}$ . but in theory , your initial energy is the same as the final energy , because kinetic energy is a scalar quantity and does not depend on the direction of travel . therefore , if you do it cleverly , you do not need any energy ( or work ) at all .
the period of the pendulum is roughly $t\approx2\pi\sqrt{\frac{l}{f}}$ where $l$ is its length and $f$ is the downward-pulling force ( usually gravity ) . let 's call our reference period $t_{rest}$ . now let 's examine your cases . 1 . when the train is in circular motion in a curve of radius r with constant speed . if the train is in circular motion the pendulum experiences a fictitious centrifugal force that points radially outward . firstly , that means that the total force on the pendulum is not straight down any more , but somewhat diagonally downward/outward . since you have to add this force ( vectorially ) to the gravitational force , $f$ will also be larger than before . looking at our equation above , this means that indeed $t_1 &lt ; t_{rest}$ . 2 . the train is going up a constant slope with constant speed . if the train travels along a straight line ( and a constant slope is a straight line ) at constant speed , then the pendulum is in an inertial frame . therefore , there are no fictitious or other forces adding to gravity . hence , $t_2 = t_{rest}$ . 3 . the train moves over a hill of radius r with constant speed . this one is tougher . the centrifugal force is still pointing radially outwards - but now it is not perpendicular to the gravitational force any more . for instance , at the top of the hill , it is pointing upward , and thus cancelling gravity partly . at other positions it will point diagonally upward . this case needs its own somewhat more involved treatment , because the total $f$ is not constant . therefore , $t_3$ will also change gradually . in particular , it will be maximal at the top of the hill , and will decrease symmetrically down both sides of the hill . sorry that i do not have any pictures to clarify this . feel free to ask if anything is unclear .
for the reasons that you already mentioned in the question , it would not be possible to modify stereoscopic depth without adding artifacts . so instead i would argue if the depth is actually increased . it probably is not . we have gotten used to watching monoscopic ( regular ) photographs with both our eyes . theoretically the perceived scale of the scenes on those photographs should be huge . but we have gotten used to it . so , the depth effect of an stereoscopic image that is captured in the range of monoscopic ( 0 camera distance ) up to a regular human eye-distance will be perceived natural . a small amount of depth is already obvious to the brain . if you would increase the camera distance beyond human eye distance , then your brain would get an unusual stimulus hence it might conclude that the scene is a miniature . also note that the viewing angle ( field of view ) of a photograph or mobile screen is also significantly smaller than the captured angle . not that it would compensate for reduced depth , but just another example of our tolerance in accepting the illusion of a photograph . if you are shooting images for a more immersive viewing experience ( eg . 3d cinema ) , then the tolerances are probably much smaller .
when you mix colors using watercolors , then they mix as " subtractive colors " . however , light itself mixes as " additive colors " . even though it might seem strange why the inherently same thing works so differently , it makes sense if you think about watercolors , etc . as absorbing everything but that specific color .
i am definitely not a physicist , however i can think of 2 engineering problems off the top of my head . if i am incorrect about any of these , please let me know . 1 ) structural engineering . if forces acting on structure are stronger than structure will support , the structure collapses . 2 ) any kind of oscillator/wave propagation , including a/c electrical phase alignment/cancellation sound/vibration propagation rf propagation joe
just compare the resolution of the two : prism depending on n , there is no good material n> 1.7 ( besides diamond ) depending on base length if you use a equilateral triangle have to use more than one to overcome this prism absorb light , you have got scattering ( stray light ) too now a grating : optimize it for your wavelength choose lines per milimeter resolution depending on the number of lines that are illuminated compact device just transmission gratings have got absorption , you can do your measurement in reflection with a blazed grating design your blazed grating to get the most light in e.g. 2nd order quantitatively prism : $\frac{\lambda}{\delta \lambda} = t \frac{dn}{d\lambda}$ grating : $\frac{\lambda}{\delta \lambda} = \frac{zd}{g}=zn$ where t is your base length , z . . . order of spectrum , g . . . grating constant , d . . . entrance beam diameter , n . . . number of illuminated lines so just use a grating , nowadays they can be fabricated in excellent quality . on my university learning the pros of a diffraction grating is part of the 1st year laboratory exercises .
the first law is not violated if stated properly : $$ dm = \frac{\kappa}{2 \pi}da + \omega dj + \phi dq $$ ( reference : wikipedia ) where $\kappa , \omega , j , \phi , q$ are the surface gravity , angular velocity , angular momentum , electric potential and charge of the black hole , respectively . compare this to the usual expression for the first law : $$ de = tds + pdv + \mu dn $$ one can ( heuristically ) make the identifications $ t = \frac{\kappa}{2\pi}$ , $ s = a/4 $ , $ \mu = \phi $ and $ n = q $ . the first two of these are well-established . hawking showed that the temperature of the black hole is proportional to its surface gravity ( $t \propto \kappa$ ) and bekenstein showed that its entropy should be proportional to its area ( $s=a/4$ ) . the third and fourth equalities ( $\mu = \phi$ and $ n = q $ ) can be understood if we think of the black hole as an aggregate of n particle with unit charge . adding another charged particle to this ensemble of $n$ particles , with total charge $q$ , will cost an amount of work given by $\phi dq$ . for the case of a single black hole , one can use the framework of dynamical horizons developed by ashtekar , badri krishnan , sean hayward and others [ refs 1 , 2 ] . turns out the laws of black hole entropy can be extended to completely dynamical black holes with a well-defined expressions for the first and second laws in terms of fluxes through the dynamical horizon . the definition of a dynamical horizon is in terms of the expansion of the inward pointing , null normal vector field on the 2+1 d boundary of a 3+1 d region . i can not think of the detailed expressions of the top of my head , but you can find them in the above reference . i can not give a concrete answer for the case of multiple black holes , but i would think you could extend the dynamical horizon framework to that case - probably not without some serious effort though . in any case , there will be no violation of energy conservation . the sum of the energy and momentum carried off by gravitational waves ( and detected by an observer at infinity ) and the change in the black hole 's energy and momentum ( again w.r. t such an asymptotic observer ) will remain a constant . hope that helps ! edit : corrected proportionality constants for $t$ and $s$ . thanks @jeff !
general approach first recall that euler-lagrange equations are conditions for the vanishing of the variation of action $s$ . for a scalar field $\phi$ with lagrangian density $\mathcal l$ on some open subset u we have $$s [ \phi ] = \int_u {\mathcal l} ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) {\rm d}^4 x$$ consider a variation of the field in direction $\chi$ and compute $$s [ \phi + \varepsilon \chi ] = \int_m {\mathcal l} ( \phi ( x ) + \varepsilon \chi ( x ) , \partial^{\mu} ( \phi ( x ) + \varepsilon \chi ( x ) ) ) {\rm d}^4 x$$ then using taylor expansion $$s [ \phi + \varepsilon \chi ] - s [ \phi ] = \int_u \left [ \varepsilon \chi ( x ) {\partial{\mathcal l} \over \partial \phi} ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) + \varepsilon ( \partial^{\mu} \chi ( x ) ) {\partial{\mathcal l} \over \partial ( \partial^{\mu} \phi ) } ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) + o ( \varepsilon^2 ) \right ] {\rm d}^4 x$$ using integration by parts on the second term ( assuming $\chi$ vanishes on $\partial u$ ) , diving by $\varepsilon$ on both sides and letting $\varepsilon \to 0$ this becomes a variation in direction $\chi$ $$\delta s [ \phi ] [ \chi ] = \int_u \chi ( x ) \left [ {\partial{\mathcal l} \over \partial \phi} ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) - \partial^{\mu}\left ( {\partial{\mathcal l} \over \partial ( \partial^{\mu} \phi ) } ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) \right ) \right ] {\rm d}^4 x$$ by requiring variations in all directions equal zero we obtain $$ {\partial{\mathcal l} \over \partial \phi} - \partial^{\mu}\left ( {\partial{\mathcal l} \over \partial ( \partial^{\mu} \phi ) }\right ) = 0 $$ ( arguments the same as always , so omitted ) . massive scalar field example consider lagrangian density $${\mathcal l} = {1 \over 2}\eta_{\mu \nu} \partial^{\mu} \phi \partial^{\nu} \phi - {1 \over 2} m^2 \phi^2 $$ by using the e-l equations we have just derived we obtain klein-gordon equation . $$ \eta_{\mu \nu} \partial^{\mu} \partial^{\nu} \phi + m^2 \phi = \square \phi + m^2 \phi = 0$$
yes . you simply use the wave equation on either side of the interface between the two media , and then you impose appropriate smoothness conditions at the interface . in one dimension , the wave equation is \begin{align} \frac{\partial^2y}{\partial x^2} = \frac{1}{v^2}\frac{\partial^2y}{\partial t^2} \end{align} if , for example , the wave is a transverse wave along a string , then $y = y ( t , x ) $ can be thought of as the transverse displacement of the string as a function of position $x$ along the horizontal axis and time $t$ . the parameter $v$ in this equation is the wave speed which is determined by the properties of the medium ( like its density ) and is , in general , different for different media . when you have an interface between two media , like in the animation , one solves the wave equation on either side of the interface and then imposes appropriate smoothness conditions at the interface such as continuity of $y$ and its first derivative . if the interface is at $x=0$ , then these conditions would read \begin{align} y ( t , 0^- ) = y ( t , 0^+ ) , \qquad \frac{\partial y}{\partial x} ( t , 0^- ) = \frac{\partial y}{\partial x} ( t , 0^+ ) . \end{align} you will probably find the following physics . se post illuminating : boundary conditions on wave equation
i think there are two questions here . how do you focus high energy photons and how do you detect them . generally detecting high energy photons is not a problem . they have lots of energy ( ! ) , even measuring their energy directly as you collect them is pretty straightforward . so you do not need spectrographs in the same way as for visible light . focussing x-rays is a lot trickier . you can scatter them by either a glancing incidence reflection from a metal surface ( wolter optics ) as used in most gamma-ray telescopes or by scattering in an optical material ( raleigh scattering ) . but the amount of scattering is low and the absorption in an optical material is high so you can not use a lot of material and so can not get significant angles . the new research suggests that there are different scattering mechanisms at very high energies which would allow you to divert an x-ray photon through a significant angle in a real material .
comment to the question ( v1 ) : it seems op is conflating , on one hand , a gauge transformation $$ \tilde{a}_{\mu} ~=~ a_{\mu} +d_{\mu}\lambda $$ with , on the other hand , a gauge-fixing condition , i.e. choosing a gauge , such e.g. , lorenz gauge , coulomb gauge , axial gauge , temporal gauge , etc . a gauge transformation can e.g. go between two gauge-fixing conditions . more generally , gauge transformations run along gauge orbits . ideally a gauge-fixing condition intersects all gauge orbits exactly once . mathematically , depending on the topology of spacetime , it is often a non-trivial issue whether such a gauge-fixing condition is globally well-defined and uniquely specifies the gauge-field , cf . e.g. the gribov problem . existence and uniqueness of solutions to gauge-fixing conditions is the topic of several phys . se posts , see e.g. this and this phys . se posts .
a slight variant on your fine answer . . . a reference is ramo et al , fields and waves in communication electronics , chapter 12 . first , reciprocity : $z_{21}=z_{12}$ tells you that ( assuming a conjugate-matched load ) : $$ g_{dt} a_{er} = g_{dr} a_{et}$$ for both transmitting ( subscript t ) and receiving ( r ) antennas , $g_d$ is the antenna directional gain . $a_{er}$ is the effective area of the receiving antenna , defined as the ratio of useful power removed from the receiving antenna $w_r$ to average power density $p_{av}$ in the incoming radiation . thus the ratio $g_d/a_e$ is the same for both transmitting and receiving antennas . for large aperture antennas , it can be shown that the maximum possible gain satisfies : $$ \frac{ ( g_d ) _{max}}{a_e} = \frac{4 \pi}{\lambda^2} $$ for other geometries , $a_e$ is defined to give the same result . for example , for a hertzian dipole , with a maximum directivity of 1.5: $$ ( a_e ) _{max} = \frac{\lambda^2}{4 \pi} ( g_d ) _{max} = \frac{3}{8 \pi} \lambda^2 $$ anyway , for the problem at hand , as you deduced , the useful power removed from the receiving antenna is : $$ w_r = p_{av} a_{er} \text{ , with the power density } p_{av} = \frac{e_b^2}{2 z_o} , z_o=377 \text{ ohms} $$ ( here , electric field and voltage are sinusoids measured as peak values . ) with a conjugate-matched load with real part $r_l$ , equating load power dissipated with power delivered gives for the receiving antenna 's thevenin equivalent source voltage $v_a$: $$\frac{ ( v_a/2 ) ^2}{2 r_l} = \frac{e_b^2}{2 z_o} a_{er} $$ $$ v_a = 2 \sqrt{a_{er}} \sqrt{\frac{r_l}{z_o}} \ , e_b $$ substituting for $a_{er}$ from the reciprocity relation , the maximum voltage $v_{a , max}$ is : $$ v_{a , max} = \sqrt{\frac{ ( g_{dr} ) _{max}}{\pi }} \sqrt{\frac{r_l}{z_o}} \ , \ , \lambda e_b $$ i am cautious about the $\cos \psi$ factor because beam patterns differ for different antennas .
after stating the solution , i will try to give some physical insights to the best of my knowledge and some more references . the dimension of the required state space is given by the verlinde formula , having the following form for a general compact semisimple lie group $g$ on a riemann surface with genus $g$ corresponding to the level $k$: $$ \mathrm{dim} v_{g , k} = ( c ( k+h ) ^r ) ^{g-1} \sum_{\lambda \in \lambda_k}\prod_{\alpha \in \delta} ( 1-e^{i\frac{\alpha . ( \lambda+\rho ) }{k+h}} ) ^{ ( 1-g ) }$$ ( please see blau and thompson equation 1.2 . ) . here , $c$ is the order of the center , $h$ is dual coxeter invariant , $\rho$ is half the sum of the positive roots , and $r$ is the rank of $g$ . $g$ is the genus , $\delta$ is the set of roots and $\lambda_k$ is the set of integrable highest weights of the kac-moody algebra $g_k$ . for the torus ( $g=1$ ) , this formula simplifies to : $$ \mathrm{dim} v_{\mathrm{torus} , k} = \# \lambda_k $$ i.e. , the dimension is equal to the number of integrable highest weights of the kac-moody algebra $g_k$ . the integrable highest weights of a level-$k$ kac-moody algebra are given by the following constraints : $$ \lambda - \mathrm{dominant} , 0 \leq \sum_{i=1}^r \frac{2 \lambda . \alpha^{ ( i ) }}{ \alpha^{ ( i ) } . \alpha^{ ( i ) }}\leq k$$ where $ \alpha^{ ( i ) }$ are the simple roots , please see , for example , the following review by fuchs on kac-moody algebras . ( my favorite reference for the representation theory of kac-moody algebras is the goddard and olive review which seems not available on line ) for example for $su ( 3 ) _k$ whose dominant weights are $2$-tuples of nonnegative numbers $ ( n_1 , n_2 ) $ , the above condition reduces to : $$\mathrm{dim} v^{su ( 3 ) }_{\mathrm{torus} , k} = \# ( n_1\geq 0 , n_2\geq 0 , 0\leq n_1 + n_2 \leq k ) = \frac{ ( k+1 ) ( k+2 ) }{2}$$ to perform the computations for the more general cases , one can use the seminal review by slansky . the verlinde formula was discovered before the chern-simons theory came into the world . originally it is the dimension of the space of conformal blocks for the wzw model . this formula has been derived in a large variety of ways , please , see footnote 26 in the fuchs review . it is still an active research topic , please see for example a new derivation in this recent article by gukov . the chern-simons theory may be the most sophisticated example in which the dirac quantization postulates can be carried out in spirit . ( more precisely their generalization in geometric quantization ) . i mean starting from a phase space and utilizing a specified set of rules to associate a hilbert space to it . in the case of the chern-simons theory , the phase space is the set of solutions of the classical equations of motion . the classical equations of motion require the field strength to vanish , in other words the connection to be flat . this phase space ( the moduli space of flat connections ) is finite dimensional , it has a kähler structure and it can geometrically quantized as a kähler manifold , just like the case of the harmonic oscillator . thus the problem can be reduced in principle to a problem in quantum mechanics . the case of the torus is the easiest because everything can be carried out explicitly in the abelian and the non-abelian case , please see the following explicit construction by bos and nair , ( a more concise treatment appears in dunne 's review ) . in the case of the torus , the moduli space of flat connections in the abelian case is also a torus and in the non-abelian case it is : $$\mathcal{m} = \frac{t \times t}{w}$$ where $t$ is the maximal torus of $g$ . basically , a fock quantization can be carried away , but there is a further restriction on the admissible wave functions coming from the invariance requirement under the large gauge transformations ( please see for example , the dunne 's review ) . the invariant wave functions are called non-abelian theta functions and they are just in a one to one correspondence with the kac-moody algebra integrable highest weights . ( in the abelian case , the wave functions are the jacobi theta functions ) . in the higher genus case , although the quantization program leading to the verlinde formula can be carried out in principle , few explicit results are known , please see the following article by lisa jeffrey ( and also the following lecture notes ) . the dimension of these moduli spaces is known . in addition . witten in an ingenious work computed their symplectic volumes and their cohomology ring in some cases . witten 's idea is that as in the case of a simple spin , the dimension of the hilbert space in the semiclassical limit ( $k \rightarrow \infty$ ) becomes proportional to the volume and the leading exponent of $k$ is the complex dimension of the moduli space ( please observe for example , that in the case of $su ( 3 ) $ on the torus , the leading exponent is $2$ which is the rank of $su ( 3 ) $ which is the dimension of the maximal torus $t$ ) .
no . when you hit the wall , the bicycle rotates around the front axis . the angular momentum l that you create for an arbitrary number of mass particles is $$l=\sigma_i ( r_i \times m_iv_i ) . $$ if you split location r=r+r_i and v=v+v_i with r and v being center of mass location and velocity , respectively , and r_i and v_i deviation from it , then it can be shown that l does not change when the center of mass does not change . so , the wood block on wheels should work ( in theory ) .
as with anything that has to do with supersymmetry the details will be dependent on your exact conventions , but we can obtain the result as follows : assume we have two grassman variables $\theta_1$ and $\theta_2$ . by applying your first formula twice we find $$\int d\theta_1 d\theta_2 \ , \theta_2 \theta_1 = 1$$ now combine these into $$\theta = \theta_1 + i\theta_2 \qquad \text{and} \qquad \bar{\theta}=\theta_1-i\theta_2 . $$ we then have $$\bar{\theta} \theta = - 2i\theta_2\theta_1$$ and hence $$\int d\theta_1 d\theta_2 \bar{\theta} \theta = - 2i$$ which is exactly your second integral , if we identify the measure $$d^2\theta = d\theta_1 d\theta_2 . $$
firstly , you can not just assert that $v=ex$ . absolute potential is not defined here , since there is an infinite increase of potential energy when going from $x=-\infty$ to $x=+\infty$ . we can only use absolute potential when the assertion that absolute potential is $0$ at infinity holds . however , this was not really your issue here , it would have worked regardless . you basically forgot a negative sign while calculating v : $$\delta v=\color{red}{-}\int\vec e\cdot d\vec l$$ so , we use : $$\delta ke+\delta pe=0$$ and , $$\delta pe= ( -e ) \delta v= ( -e ) ( -\int\vec e\cdot d\vec l ) =ee\delta x$$ $$\frac12m0^2-\frac12mv_0^2+ee\delta x=0$$ $$\implies eex=\frac12mv_0^2\implies x=\frac{mv_0^2}{2e}$$ which solves the problem .
first make sure all your screws are tight , and that there is not any shaking because of slack in any areas where things connect to each other . another thing you can do is buy vibration dampening pads to put your tripod on . finally , you can add counter weights and pendulum weights to the tripod to give it more mass to withstand the wind and touches .
i would like to expand a bit on the answer to the second question , but for completeness i will do both . as said in brightblades ' answer , the expansion of space is not limit by the speed of light . so objects can be moving at moderate speeds , but because the space between them and us expands faster than light , it is emissions will never reach us . for this reason , there are already regions of space that are beyond our horizon . whether or not this will always be depends on what " big thing " your cosmology ends with . if it is a " crunch " , then everything comes back together in a reverse " bang " at the end of time . if it is just a " freeze " , then the acceleration expands and possibly even accelerates forever . abraham loeb wrote a curious paper ( available on the arxiv ) about how to reach cosmological conclusions in the absence of nearby galaxies . about 100 billion years from now , all the galaxies in our local group will be beyond the horizon of the milky way ( or rather milkomeda , after the milky way collides with andromeda ) , and the cmb will be at a wavelength longer than the observable universe . but you will still be able to reach conclusions about cosmology by using hypervelocity stars being ejected from the galaxy . the point is that you can still get accurate results about the global structure of the universe using local results . as for our current model , we have a great deal of evidence that the cosmos is structured according to the concordance model . you can always say " it might turn out to be wrong " , but it can not turn out to be that wrong because of said evidence . it is like gr as a generalization of newtonian gravity : yes , newton was " incorrect " but his theory was also quite accurate , to the extent that we still use it for , say , n-body simulations of star clusters .
light++ it is not open source but you can try to contact the author , werner benger . a few years ago we have access to the source code of ' light++' . not anymore : ( light++ raytracer ! ( general relativistic raytracing ) simulation of a black hole by raytracing the black earth about the simulation of galactic close encounters , or a n-body general simulation , under the constraints of gr i found nothing . edit add " i found nothing " can be read like this " there is not a single software package " because , afaik , no one knows how to apply gr in the computation of planetary and galaxy dynamics ( small scale with matter ) . the zeldovich approximation is used in the linearization of gr ( with caveats ) : and has been successfully applied to describe the large scale clustering in the distribution of galaxy clusters . . . however , within the zeldovich prescription , after a pancake forms in correspondence of crossing of particle orbits , such particles continue travelling along straight lines , . . i think that your aim is hopeless because gr is around since 1917 and no one succeeded . interesting questions , imo : how close to the reality are the simulations that are performed with newtonian codes . what kind of problems we may expect if we are gonna try to do a simulation code . edit add end
the system has constant pressure by definition . even in a system with changing pressure , though , for some small time , dt , there would be constant pressure . in that moment , and with those conditions , the relationship holds . adjust the pressure slightly , and a new , similar relationship is set up . the second question is related to this being a thought experiment ( an ideal situation ) . if gravity is not a factor , the pressure remains constant throughout the container . the international space station may be the best environment for testing this out , and i suspect someone may get around to trying it , if they have not already .
$\phi$ in this context is typically known as the " inflaton " ( a somewhat silly name , i know , but we already have quarks ) , it is the scalar field that drives inflation . any field may have a potential component , typically written as $v$ . then the lagrangian can be written as the sum of kinetic term ( s ) with ( the relevant covariant ) derivatives and the potential term that describes how the field behaves ( plus interaction terms ) . for the case of the inflaton there are many models for what $v ( \phi ) $ should be , such as $v ( \phi ) =\frac14\lambda\phi^4$ as mentioned in the paper ( a rather popular model right now too ) . that means that there is such a term in the lagrangian . of course , $v ( \phi ) $ could be any arbitrary function of $\phi$ ( of course most such functions would not be applicable to inflation ) .
the definition is true everywhere , but only as long as you and the caesium atom are in the same place and moving at the same rate . suppose two scentists calibrate their clocks on earth to make sure they measure time at the same rate , then one scientist stays on earth while the other scientist flies off in a rocket travelling near the speed of light . if the scientists count the number of oscillations of their caesium atom in one second they will both count 9,192,631,770 ( give or take experimental error ) . however if they count the oscillations of the other caesium atom they will both count less than 9,192,631,770 because they will see time running slowly for the other scientist . re your q2 , i would not say : the fundamental properties of sub-atomic particles change firstly the scientist on the rocket would deny there was any change ( though they would claim the earth 's time had changed ) . secondly though the earth scientist would claim time has changed on the rocket , this change affects everything on the rocket not just the fundamental properties of sub-atomic particles .
this can be resolved by being clear about what surface you are integrating over . in the first equation , $$ \oint {\overrightarrow{b} . \overrightarrow{da}} = 0 , $$ you are integrating over any closed surface , i.e. a surface without a hole in it , such as a sphere . the equation says that the magnetic flux coming in must equal the magnetic flux going out . but in the second equation , $$ e = \frac{d}{dt} \int_\sigma {\overrightarrow{b} . \overrightarrow{da}} , $$ you are integrating over a surface with a hole in it , where the hole is a loop of wire , as shown in this diagram from wikipedia : this equation says that the rate of change of flux passing through the surface must equal the emf in the wire loop . you can imagine it as coming in through the hole , and out through the surface . ( or the other way round or a bit of both . ) you can see a connection between the two equations if you imagine making the wire loop smaller and smaller until the hole closes completely . then you are back at a closed surface again , where the first equation applies - and this infinitely small loop of wire can never experience an emf .
first , why is the the static killing vector $\frac{\partial}{\partial t}$ equal to $-f^{\frac{1}{2}} dt$ ? the vectors $\partial/\partial t$ and $-f^{\frac{1}{2}} dt$ are not equal to each other . they are parallel to each other , and the factor of $-f^{1/2}$ is just so that the four-velocity is properly normalized . if you plug $u$ into the metric , you have to get 1 . second , why is the velocity , along the killing time vector ? what would happen if there is a component perpendicular to it ? does this mean , the fluid does not move through space ? the killing vector supplies a preferred frame of reference , one in which the observables ( e . g . , curvature scalars ) stay constant . a perfect fluid is one for which there exists a frame such that the stress-energy tensor is diagonal ; in this frame , there is no spatial flux of energy-momentum . so basically this means that in this frame , the fluid does not move through space ( in the sense that there is no flux ) .
i recommend using octave ( or matlab , which is much more user-friendly but you will need a license ) . for every quantity that you mentioned there is a command in octave and it is as simple as a=mean ( y ) or v=cov ( x , y ) . importing and exporting data is also very easy .
here is a rough explanation of what they do . you start with an integral over a complex variable $k=k+iz$ along a path $\mathcal c_0$ ( in the complex plan ) that follow the real axis ( that is a path characterized by $z=0$ and $k$ between $-\infty$ and $+\infty$ ) . calling $i$ the integral without the prefactors , we thus start with $$ i = \int_{\mathcal c_0} dk \frac{ke^{ik\delta x}}{\sqrt{k^2+m^2}} . $$ now the integrand is analytic everywhere but on the cuts , so you can change the contour the way you want as long as it does not go through the cut , and the integral still converges . the usual way to do this kind of integrals is to change the contour such that you just have to integrate around poles or along the cuts . but if you want to do that , you need that the contour has a contribution from $z$ at $\pm \infty$ , depending if you go along the cut in the upper/lower half plan . now you see the role of $\delta x$: if $\delta x&gt ; 0$ , $e^{ik\delta x}$ converges/diverges if $z\to\pm\infty$ , so you can not integrate along the lower cut ( the integral is divergent ) , but you can for the upper cut . that is how you get the contour in your graph , let 's call it $\mathcal c_1$ . to do the integral , you now have to parametrize $\mathcal c_1$ by two integrals : first the part along the left of the cut : $k=-\epsilon$ and $z$ going from $\infty$ to $m$ ; and second the part along the right of the cut : $k=\epsilon$ and $z$ going from $m$ to $\infty$ . $\epsilon$ is an infinitesimal number that you have to send to zero at some point . with all that , you should be able to recover the result you are looking for .
the velocity after a time t 1 of accelerating is the starting velocity of the deceleration phase . thus $$a t_1 = b t_2$$ ( not worrying about the sign here . i suppose you could ) further you have $$\frac12at_1^2+\frac12bt_2^2=s$$ now you have two equations with two unknowns . solving : rearrange first equation $$ \frac{a}{b} = \frac{t_2}{t_1}\\ t_2=\frac ab t_1\\ t_2^2=\left ( \frac ab\right ) ^2 t_1^2 $$ now substitute into second equation : $$ \frac12at_1^2+\frac12\frac{a^2}{b}t_1^2=s\\ $$ substitute values : $$9t_1^2 + 27t_1^2=14400\\ t_1^2=\frac{14400}{36}\\ t_1=20\\ t_2=3t_1\\ t_1+t_2=80$$
1 ) why does length contraction only occur in the direction of travel , ( not in all directions ) when approaching the speed of light ? there are a number of ways to answer that vary in the level of sophistication . but first , do understand that the lorentz transformation gives length contraction only in the direction of motion . so , if you accept the lorentz transformation as the correct coordinate transformation between relatively moving reference frames , you accept that there is no transverse contraction . also remember , the lorentz transformation was originally derived so that the measured speed of light would always be c in any inertial reference frame . but , assume for the sake of argument that transverse length contraction does occur . we almost immediately arrive at a contradiction . consider a wall upon which two parallel horizontal stripes have been painted . the vertical distance between the stripes is exactly 1 meter which can be confirmed by placing an ideal meter stick vertically on the wall and finding that the ends of the meter stick touch the top and bottom stripes . now , imagine that the same vertical meter stick moves horizontally with respect to the wall . if there is vertical contraction then , according to someone at rest with respect to the wall , the meter stick no longer reaches between both lines . since the stick is vertically contracted , the spacing between the lines is greater than the length of the meter stick . however , to someone at rest with respect to the meter stick , it is the wall that is moving and it is the wall that is vertically contracted . to this person , the meter stick overlaps the two lines ; the spacing between the lines is less than the length of the meter stick . but this is a contradiction ! it cannot be the case that the top end of the meter stick is both below and above the top stripe and similarly for the bottom end . we conclude that transverse contraction leads to a paradox . therefore an outside observer would then see the car expanded by a factor of 2 not contracted . no , that is not the correct conclusion . first , the phrase " outside observer " is ambiguous . what we should specify here is whether the observer is at rest with respect to the road or with respect to the car . for an observer at rest with respect to the road , your car is contracted and fits in between the markers . for an observer at rest with respect to your car , the distance between the markers is contracted and your car is longer than the markers . this is not a contradiction because , and this is crucial , of the relativity of simultaneity ; relatively moving observers do not agree on whether spatially separated events , along the axis of motion , are simultaneous . in order to tell if the car fits between the markers , we must determine the location of the rear and front of the car at the same time according to spatially separated clocks synchronized in our reference frame . so , imagine that there is a clock at the front of the car and at the rear of the car and that they are synchronized by einstein synchronization according to the observer at rest with respect to the car . then , at some time , the location of the clocks are recorded . according to the observer at rest with respect to the car , the recording of the location of the two clocks occurred at the same time . thus , the difference in the location of the clocks is the length of the car . however , according to the observer at rest with respect to the road , the locations were not recorded at the same time , i.e. , the clocks on the car are not synchronized . thus , the two observers do not agree on the length of the car .
it is a steady state . if there were a pressure gradient , there would be net force on the gas ( ignoring gravity ) . there is no net force here because the air is not accelerating . thus the pressure is constant . the number density varies across the box inversely to the temperature so the ideal gas law holds .
sure , it is no problem to do this . the thing that has to change is that $i$ should index over all possible configurations of the $n$ elements , and the energy in the boltzmann distribution has to be the total energy of the system . so if $m=10$ , $n=3$ and $d=2$ then , for example , $$ p ( [ 1,0,0,1,0,0,1,0,0,0 ] ) = \frac{1}{z}e^{-\frac{\epsilon_1 + \epsilon_4 + \epsilon_7}{kt}} , $$ but $$ p ( [ 1,0,1,0,0,0,1,0,0,0 ] ) = 0 $$ because it is not allowed by the constraint . to calculate the normalising factor ( or " partition function" ) $z$ , you have to sum over all allowed configurations of the system . it is not immediately obvious ( to me ) how to do that analytically in this case , but you are the mathematician so i am sure you can find an elegant way . incidentally , you should be able to see that if there are no interactions between the $m$ positions then this reduces to the formula you originally quoted .
note that you begin with a superpotential $w$ which gives you a generalization of the creation/anihilation operator as $a^{+} = -\frac{d}{dx} + w ( x ) $ and $a^{-}= +\frac{d}{dx} + w ( x ) $ . you will get 2 hamiltonians $h_- = a^+ a^-$ and $h_+ = a^- a^+$ . so you obtain 2 different potentials $v_{+}$ and $v_{-}$ for superpartners $v_{-} ( x ) = w ( x ) ^2 - \frac{dw}{dx}$ and $v_{+} ( x ) = w ( x ) ^2 + \frac{dw}{dx}$ . then , suppose that $\psi^-$ is a solution of $a^-\psi^- ( x ) = 0$ with $\psi^-$ normalizable , you will find that $h_- \psi_- = a^+a^-\psi^- = 0$ , so $\psi^-$ is an eigenstate of $h_- $ , with eigenvalue 0 . you have $\psi^- ( x ) \sim e^{-\int^x_{x_0} w ( x ) dx}$ ( from the definition of $a^-$ ) with the same reasoning , you will find that a $\psi^+$ solution of $a^+\psi^+ ( x ) = 0$ will give you $\psi^+ ( x ) \sim e^{+\int^x_{x_0} w ( x ) dx}$ which would be an eigenstate of $h_+ $ , with eigenvalue 0 . but you have a problem , you can see that you cannot have , at the same time $\psi^+$ and $\psi^-$ normalizable , because $\psi^+ ( x ) \sim \frac{1}{\large \psi^- ( x ) }$ . so , one of the functions $\psi^+$ or $\psi^-$ has to vanish . so , you have , at most , one ground state with zero energy . [ edit ] the practical link with supersymmetry is as follows . we define a 2 -dimensional space , with $\psi = ( \psi_- , \psi_+ ) $ . one of the $\psi$ states is a bosonic state , the other is a fermionic state . we define the supersymmetric generators : $q^- =\left [ \begin{array}{cccc} 0 and 0 \\ a^- and 0 \end{array} \right ] $ $q^+ =\left [ \begin{array}{cccc} 0 and a^+ \\ 0 and 0 \end{array} \right ] $ the hamiltonian is $h = q^-q^+ + q^+q^-$ $h =\left [ \begin{array}{cccc} a^+a^- and 0 \\ 0 and a^-a^+ \end{array} \right ] = \left [ \begin{array}{cccc} h_- and 0 \\ 0 and h_+ \end{array} \right ] $ it can be easily seen that $ ( q^- ) ^2= ( q^+ ) ^2 = 0$ and $ [ h , q^- ] = [ h , q^+ ] = 0$ unbroken supersymmetry corresponds to a ground state $|0&gt ; $ such as $&lt ; 0|h|0&gt ; = 0$ , and this imply $q^+|0&gt ; = q^-|0&gt ; = 0$ in this case , one of the functions $\psi^+$ or $\psi^-$ is normalizable , and the other vanishes . for instance , suppose $\psi^-$ is normalizable , then it means that $a^- \psi^- =0$ the case of ( spontaneously ) broken supersymmetry , is when $&lt ; 0|h|0&gt ; \neq 0$ , which implies $q^+|0&gt ; \neq 0$ or $q^-|0&gt ; \neq 0$ . in this case , neither of the ground states $\psi^+$ nor $\psi^-$ are normalizable .
i think you might try approaching this in the heisenberg picture . the time derivative of the position operator is : $$\dfrac{d \hat x}{dt} = \dfrac{i}{\hbar} [ \hat h , \hat x ] $$ which is a reasonable velocity operator . the time derivative of the velocity operator is then : $$\dfrac{d^2 \hat x}{dt^2} = \dfrac{i}{\hbar} [ \hat h , \dfrac{d \hat x}{dt} ] $$ for example , consider a free particle so that $\hat h = \frac{\hat p^2}{2m}$ . the velocity operator would then be $\frac{\hat p}{m}$ . this certainly looks reasonable as it is of the form of the classical $\vec v = \frac{\vec p}{m}$ relationship . but , note that the velocity operator commutes with this hamiltonian so the commutator in the definition of the acceleration operator is 0 . but that is what it must be since we are assuming the hamiltonian of a free particle which means there is no force acting on it . now , consider a particle in a potential so that $\hat h = \frac{\hat p^2}{2m} + \hat u$ . the velocity operator , for this system , is then $\frac{\hat p}{m} + \frac{i}{\hbar} [ \hat u , \hat x ] $ . assuming the potential is not a function of momentum , the commutator is zero and the velocity operator is the same as for the free particle . the acceleration operator is then $\dfrac{i}{\hbar} [ \hat u , \frac{\hat p}{m} ] $ . in the position basis , this operator is just $\frac{-\nabla u ( \vec x ) }{m} $ which looks like the acceleration of a classical particle of mass m in a potential given by $u ( \vec x ) $ .
displacement :$$\vec r_f-\vec r_i = \vec d =|d| \hat d $$ $\vec r_i$= position vector initial . ie . $3\hat i -7 \hat j$ ; $|d|=150m$ as found , and $\hat d$ is the direction .
in terms of your ordinary matrix multiplication , you have , for the case of a 4x4 matrix $m = g_{ab}$: $m\cdot m^{-1} = i$ , which is the same thing as $g_{ab} g^{bc} = \delta_{a}{}^{c}$ and $tr\left ( m\cdot m^{-1}\right ) = 4$ , which is the same thing as $g_{ab}g^{ab} = \delta_{a}{}^{a} = 4$
it could be any vector from bc to oa . let 's assume that $r$ is the vector perpendicular to bc to the point oa . any vector from bc to oa ( whether or not it is perpendicular to bc ) has the form $r + s$ , where $s$ is some vector parallel to bc . of course , you have told us that $\omega$ is also parallel to bc , so we could also write that as $r+\alpha\ , \omega$ , for some number $\alpha$ . so let 's take the cross product : \begin{equation} \omega \times ( r+\alpha\ , \omega ) = \omega \times r + \omega \times ( \alpha\ , \omega ) = \omega \times r + \alpha\ , \omega \times \omega = \omega \times r~ , \end{equation} since $\omega \times \omega = 0$ . so $\omega$ cross any vector from bc to oa will equal $\omega$ cross the perpendicular vector . so you do not need to specifically find the perpendicular .
simple answer : nothing is guaranteed 100% . ( in life or physics ) now to the physics part of the question . soft-answer : physics uses positivism and observational proof through the scientific process . no observation is 100% accurate there is uncertainty in all measurement but repetition gives less chance for arbitrary results . every theory and for that matter laws in physics are observational representations that best allow prediction of future experiments . positivism can overcome theological and philosophical discrepancies such as what is the human perception of reality . is real actually real type questions . the scientific process is an ever evolving representation of acquired knowledge based on rigorous experimental data . no theory is set in stone so to speak as new results allow for modification and fine tuning of scientific theory .
there is no " vectorlike " gauge theory in the standard model , and this is a consequence of naturalness . this means that all particles in the standard model are naturally massless , and the mass only comes from higgs mechanism . this is one of the great features of the standard model that is easy to break in any modification or extension . the teminology " vectorlike " comes from the 1950s , when people did not like 2-component spinors and thought that the world is fundamentally parity invariant . a " vectorlike " gauge field couples to a 4-spinor according to $\gamma^\mu a_\mu$ , while a " pseudovectorlike gauge field " couples to a 4-spinor according to $\gamma^5\gamma^\mu a_\mu$ . both are parity invariant , but in the first case , a is a vector ( meaning it changes sign under reflection ) , and in the second case it is a pseudovector . but the gauge fields in nature are neither vectors or pseudovectors , they are parity-violating . they couple as " v-a " meaning $ ( 1-\gamma_5 ) \gamma^\mu a_\mu$ , which is a projection operator to one two component part of the 4 component dirac spinor . this means that 4-component language is a little obfuscatory for this ( although 4 component spinor notation is still useful , becuase feynman trace identities are easier than fierz identities , and the 4-component notation most easily generalizes to higher dimensions ) . the point is that there is no parity , and the gauge fields are neither " vectors " or " pseudovectors " , they are parity violating vector fields which do not have a definite transformation under parity , because nature is chiral . so i would drop the " vectorlike " terminology , and use the term " naturally mass allowing " . a vectorlike gauge theory is " naturally mass allowing " because you can make the fermion massive . this means that the left and right partners have the same charges , and this can be considered an accident . the correct question is " why are all gauge theories in nature mass forbidding ? " this is true of all the fields on the standard model--- none of the right handed and left handed fields in the standard model can pair up to form a mass , because they are different su ( 2 ) multiplets and have different u ( 1 ) charge . why are they all unpartnered and charged ? there is a simple reason for this--- any field which can get partnered will have an arbitrary mass term in the lagrangian , and this term , without fine tuning , will end up generically being of order the planck mass . so the only fermions we see at low energies are those which are forbidden to have a mass , and therefore are chiral fermions without a partner to make a mass term with . further , all the fermions we see at low energies need to have a gauge charge , because without a charge of some sort , the fermion can get a majorana mass even without a partner , just by mixing with it is antiparticle . this is only forbidden if the particle is gauge charged in some way , so that the antiparticle has the opposite charge and the majorana mixing is forbidden . so all the fermions are chiral fermions with no partner to make a mass , so none of the low energy theories are vector-like . the simplest right way to formulate gauge theories in a parity violating universe is in terms of 2-component spinors , each with an independent coupling to a collection of gauge fields . this procedure can lead to an inconsistency , if there is an anomaly in one of the gauged symmetries , so there are global constraints on the type of chiral fermions and the representations they can be in . if none of the fermions have a partner , then the theory is natural , meaning " naturally massless " and the fermions can only get a mass from a higgs mechanism . the naturalness arguments say that the higgs mechnism must be the source of mass of all fermions in nature . but if the higgs is a fundamental scalar , the higgs itself can have a mass , and the naturalness argument fails for the higgs itself . so there is the question of why the higgs has an unnaturally light mass . this is the hierarchy problem .
most higher derivative theories —and in particular lee-wick 's model— do not have ghost excitations but they are unstable ( hamiltonian unbounded from bellow ) . yes , almost everyone says the opposite but all them are unfortunately wrong . they do not quantize the theory properly . whenever a degree of freedom has negative energy at the classical level , it must have negative energy at the quantum level as well . people try to fix the problem of negative energy exchanging the frequencies between creation and annihilation operators at the price of introducing ghosts . but this is not the right way of quantizing the theory because the classical limit is totally wrong . one may canonically quantize the harmonic oscillator with an additional higher derivative term to convince himself of what i am emphatically claiming . anyway , either one quantizes the theory correctly getting an unstable theory or one quantizes the theory incorrectly getting a theory with negative norm states , the quantum theory does not make any sense . it is not a quantum theory . is there any general mathematical theorem by which we can show that a nth order derivative theory can be quantized into n different kind of particles ? the classical and the quantum theory must have the same number of degrees of freedom ( dof ) . the classical theory has half dof of the number of initial conditions must be given to determine a solution . in a normal field theory —let 's say klein-gordon— one must specify initial value of the field and momentum ( or velocity ) for every field . thus one has one dof for a real field , two for a complex field , four for two complex fields . when one adds higher time derivatives , one requires more initial conditions to know a solution ( initial accelerations , initial fourth derivatives , etc ) . when there are constrains the counting of degrees of freedoms is a little bit more subtle . for instance , electrodynamics is a second-order theory and the electromagnetic four-potential $a_{\mu}$ has four components so one could naively think that the theory has four degrees of freedom , but this is not true because there are gauge redundancies ( there are first class constraints ) that make different ( gauge related ) $a_{\mu}$ correspond to the physical situation . so that the theory has only two dof corresponding to the two polarizations of electromagnetic waves . regarding the quantum theory , the number of degrees of freedom corresponds to the number of particles ( each physical polarization counts as a particle ) .
you have exact equations for the solution in the related question time it takes for temperature change . here i would add a few comments . it is actually easier if container is thick ! then suppose that all water is at same temperature $t$ and all the air in the frizer is at the same temperature $t_e$ . $t_e$ is constant . if that is so , you can use only fourier 's law to describe how heat $q$ leaves the container $$\frac{\text{d}q}{\text{d}t} = \frac{\lambda a}{d} ( t-t_e ) . $$ $d$ is thickness , $a$ area and $\lambda$ thermal conductivity of the container . knowing that water cools as heat is leaving the container $$\text{d}q = m c \text{d}t , $$ where $m$ is mass and $c$ is specific heat capacity of the liquid , you get rather simple differential equation $$m c \frac{\text{d}t}{\text{d}t} = \frac{\lambda a}{d} ( t-t_e ) , $$ $$\frac{\text{d}t}{ ( t-t_e ) } = \frac{\lambda a}{d m c } \text{d}t = k \text{d}t , $$ which has exponential solution : $$k t = \ln \left ( \frac{t-t_e}{t_0-t_e}\right ) , $$ $$t = t_e + ( t_0-t_e ) e^{-kt} . $$
in special relativity momentum is part of a 4-vector with energy as the time component as you correctly said . angular momentum is not part of a 4-vector . the cross product you used in the question does not give a vector when generalised from 3 to 4 dimensions . instead it gives an antisymmetric matrix . so angular momnetum is part of an antisymmetric rank-2 tensor ( a matrix ) which has six independent components . the angular momentum vector from 3d space forms three of those components and the other three components form another 3d vector , so this works differently from a 4-vector . in 3d an antisymmetric tensor is a hodge dual to a vector which is why we think of angular momentum as a vector like momentum , but in 4d spacetime an antisymmetric tensor is hodge dual to another antisymmetric tensor and cannot be thought of as equivalent to a 4-vector . this is obvious because the number of components is different . to understand what the other 3-components of this antisymmetric matrix are you have to look at the relationship between conserved quantities and symmetry as understood via noether 's theorem . energy and momentum correspond to symmetry under translations in time and space , but angular momentum is symmetry under rotations . in special relativity rotations and combined with lorentz boosts to form the six parameter group of lorentz transformations . so your question is equivalent to asking what conserved quantity corresponds to lorentz boosts . this has been answered here before see there for the details , but in short it is the centre of mass at a fixed time .
in classical electrostatics , gauss ' law can be used to derive the relationship between the electric potential , $\varphi$ for a homogeneous medium ( constant permittivity , $\epsilon$ ) and volume charge density , $\rho_{v}$ in the form of the poisson equation , which , in cartestian co-ordinates , is given by : $$\nabla^{2}\varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}=-\frac{\rho_{v}}{{\epsilon}_{r}{\epsilon}_{0}} , $$ where ${\epsilon}_{r}$ is the relative permittivity ( dielectric constant ) for the homogenous medium . now , for an ionic solution at thermal equilibrium and at temperature $t$ , the charges are uniformly distributed . under the effect of an electrostatic field , the positive ions are attracted towards the negative electrode and negative ions attracted towards the positive electrode . furthermore , positive ions become repelled by other positive ions and similarly negative ions become repelled by other negative ions , until a new equilibrium is reached . at equilibrium , the ions are distributed with various energies , $e$ given by the maxwell-boltzmann distribution law , in which the probability of a particle having energy $e$ is proportional to $\exp ( -\frac{e}{kt} ) $ where $k$ is boltzmann 's constant and $t$ is the ( absolute ) temperature [ in kelvins ] . if $z_{i}$ is the number of charges of the $i^{th}$ ionic species , then its electric potential energy is $z_{i}e\phi$ where $e$ is the elementary electric charge ( $e = 1.602\times10^{-10}$ coulombs ) . the concentration ( number density ) of the $i^{th}$ ionic species at position $\textbf{r}$ is then given by : $$n_{i} ( \textbf{r} ) =n_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) , $$ where $n_{i}^{\infty}$ is the number concentration of the $i^{th}$ ionic species in the bulk solution . the volume charge density is therefore : $$\rho_{v} ( \textbf{r} ) =\sum\limits_{i=1}^nz_{i}en_{i} ( \textbf{r} ) =\sum\limits_{i=1}^nz_{i}en_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) . $$ which , when substituted into the poisson equation , gives the poisson-boltzmann equation for potential of an ionic solution : $$\nabla^{2}\varphi=-\frac{1}{{\epsilon}_{r}{\epsilon}_{0}}\sum\limits_{i=1}^nz_{i}en_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) . $$ this is a non-linear partial differential equation , the solution of which is dependent upon the specific geometry and properties of the electrolyte . for the case of an infinite sheet ( plate ) electrode located in the y-z plane at the origin , with the potential of the plate $\phi=\phi_{0}$ at $x=0$ and the potential in the solution $\phi\rightarrow0$ , $\frac{d \phi}{dx}\rightarrow0$ as $x\rightarrow\infty$ , the solution , for low potential , $\phi$ , that is , $\lvert\frac{z_{i}e\phi}{kt}\rvert\ll 1$ , the poisson-boltzmann equation becomes linearized to $\nabla^{2}\varphi=\kappa^{2}\phi$ ( the debye-hueckel equation ) which has the solution : $$\varphi ( x ) =\varphi_{0}\exp ( -\kappa x ) $$ where $\kappa=\sqrt{\frac{2z^{2}e^{2}n}{\epsilon_{r}\epsilon_{0}kt}}$ . for the non-electrostatic case ( ie : current flow and/or ion flow ) , the ions in the solution will undergo transport under the effect of the applied electric field . these ions will initially accelerate up to a speed , $s$ , limited by the hydrodynamic properties of the solute ( stokes law ) . we can calculate the hydrodynamic force , $f_{h}$ exerted on an ion of radius $r$ , travelling at speed , $s$ through the solute of density , $\rho$ and viscosity , $\eta$ to get $f_{h}=6\pi\eta rs$ from which the stokes-einstein equation for the diffusion coefficient is derived : $d=\frac{kt}{6\pi \eta r}$ from the nernst equation we can calculate the electrochemical potential of an ionic species from ionic concentration ( activity ) gradients present in solution . when coupled with the conservation of mass , we get the nernst-planck equation : $$\frac{\partial n_{i}}{\partial t}= \nabla \cdotp \lgroup d_{i} ( \nabla n_{i}+\frac{q_{i}n_{i}}{kt}\nabla \phi ) \rgroup . $$ of course , these models include assumptions which may not always be valid , such as the use of stokes law or the assumption of non-interaction of ions . more accurate numerical models may require the use of empirical data to account for these and other effects , including chemical reaction kinetics .
presumably you are thinking that when c and d annihilate they turn into massless photons so their gravitational field must disappear , hence the problem of what happens to the gravitational energy that a and b acquired in moving away from c and d . the answer is that the gravitational field of c and d is not generated by their mass , but by their energy density . energy gravitates in exactly the same way that mass does . the equivalence between the two is just the formula we have all see a hundred times : $e = mc^2$ . life is actually a bit more complicated than this , because the spacetime curvature that creates gravity is actually proportional to an object called the stress-energy tensor . however the mass/energy density is usually the most important part of this object so at the instant that c and d annihilate , the other two particles feel no change in the gravitational field and their gravitational potntial energy does not change . what happens next is more complicated because the photons created by the annihilation fly off at the speed of light so of course the gravitational field felt by a and b changes . calculating the gravitational field created by a photon is not a trivial business , though googling should find you papers on the subject .
the x-ray diffraction pattern is the fourier transform of whatever is doing the diffracting . if you had an infinite plane of atoms then the spots ( rings in a powder pattern ) would be infinitely sharp because the fourier transform of an infinite wave is a delta function . however a real crystal is the product of an infinite plane with an envelope function , where the envelope is the size of the crystal , so the spot is the convolution of a delta function with the fourier transform of the envelope function . in a powder pattern we have many crystals of differing sizes , so the average fourier transform of the crystal size ends up looking something like a gaussian and the spots have a roughly gaussian profile . re your formula , suppose the average crystal ends up looking like a sphere , i.e. a disk in profile , then it is fourier transform is going to be an airy disk ( but blurred out by the variation in particle size ) . the half angle subtended by the airy disk , $\beta$ , is ( for small angles ) : $$ \beta \propto \frac{\lambda}{d} $$ which is the scherrer equation for small $\theta$ ( i would have to go away and look at the derivation of the scherrer equation to remember why there is a factor of $\cos\theta$ , but for small $\theta$ this factor is approximately $1$ anyway ) . response to comment : the incoming x-rays are scattered by the atoms in the crystal , so each atom acts as an x-ray source . we get the diffraction pattern by summing up the x-rays emitted ( i.e. . scattered ) by all the atoms in the crystal . if you start with one atom then the scattered x-rays will be just be a spherical wave . add a second atom and now the pattern will be the same as the young 's slits experiment . as you add more and more atoms the pattern will tend towards the pattern we expect from a large crystal , however to get an infinitely sharp spot would require an infinite number of atoms . when we have a finite number of atoms the spot will have a finite width . it is a bit like a fourier synthesis ( which is where we came in ) . each atom adds a term to the fourier sum , but to get a perfect transform of the lattice requires contributions from an infinite number of atoms . with a finite number of scatterers the diffraction pattern will only be an approximation to the ft of the lattice .
this document ( nb it is a pdf ) contains details of the beam operation . here 's a key graph nabbed from the presentation : at the end of an experimental run the beam is dumped , and it takes about an hour and a half to get the beam back up to full energy and intensity . once the beam is at full strength the lhc generates data continuously for somewhere between 10 and 20 hours before the beam intensity is too low and the beam needs to be dumped again . note that the lhc is not an experiment that runs once and generates one result , then repeated to generate a second result and so on . once the beam is live it generates data continuously and this data builds up for days and months . because signals like the higgs are so weak you need months and months worth of data , i.e. months and months of beam time , to get enough data to see these small signals . cern have made an animation showing how the higgs signal built up over time , which you can see here .
hints to the question ( v1 ) : let $\hat{z}^i$ be operators that satisfies a heisenberg algebra $$\tag{1} [ \hat{z}^i , \hat{z}^j ] ~=~i\hbar ~\omega^{ij} ~{\bf 1} , $$ where $\omega^{ij}=-\omega^{ji}$ is an antisymmetric real matrix . the important property will be that the commutators $ [ \hat{z}^i , \hat{z}^j ] $ belong to the center of the heisenberg algebra , i.e. commute with everything . let $f ( \hat{z} ) $ and $ ( \partial_{j}f ) ( \hat{z} ) $ denote the weyl-ordered operators corresponding to the functions/symbols $f ( z ) $ and $$\tag{2} ( \partial_{j}f ) ( z ) ~:=~\frac{\partial f ( z ) }{\partial z^j} , $$ respectively . then using e.g. this formula for the weyl-ordered operators , it is possible to show$^1$ $$\tag{3} [ \hat{z}^i , f ( \hat{z} ) ] ~=~\sum_j [ \hat{z}^i , \hat{z}^j ] ~ ( \partial_{j}f ) ( \hat{z} ) . $$ the corresponding classical formula $$\tag{4} \{z^i , f ( z ) \}_{pb}~=~\sum_j \{z^i , z^j\}_{pb}~ ( \partial_{j}f ) ( z ) $$ is a consequence of the poisson property/leibniz rule for a poisson bracket ( pb ) . -- $^1$ equation ( 3 ) actually also holds for many other operator-orderings .
a dispersion relation tells you the conditions under which a certain solution holds , which means that in order to get a dispersion relation you need to assume a solution of some general form . this is a system of harmonic oscillators coupled over a long range , so it is natural to assume a plane wave solution . using your notation , try $x_n= e^{i ( kna - \omega t ) }$ where $\omega$ is the frequency of phonon oscillation . you want to specify which $\omega$ satisfy the equations of motion . it should only take a couple of steps to arrive at the dispersion relation from this . i hope this helps .
generally speaking , localized electrons are confined to one particular orbital , while conduction electrons are " free " to float from orbital to orbital due to the nearly degenerate nature of the states in the conduction band . so , when one discusses a localized electron it is not that the electron is pinned to a precise location in space , but isolated to a particular orbital or small region of space when compared to the finite extent of the material . the passage you cite above , the introduction of additional materials into the structure varies the potential felt by the electron , creating a local potential from which the electron may not have sufficient energy to escape , which is a way of introducing localization . the wiki article on anderson localization is accessible , and may help with some additional insight .
i think $2k_2 + 1$ is correct . if you write down the pairs of $k_{1z}$ and $k_{2z}$ they are : ( $k_1 - 1$ , $-k_2$ ) ( $k_1 - 2$ , $-k_2 + 1$ ) etc ( $k_1 - 2k_2$ , $k_2 - 1$ ) ( $k_1 - ( 2k_2+1 ) $ , $k_2$ ) so that is $2k_2+1$ of them .
start with $$ m{dv\over dt}=b_1 ( v_1-v ) -b_2v . $$ move everything involving $v$ to one side of the equation , and everything involving $t$ ( in this case , just $dt$ ) to the other side . integrate both sides . one side will be just $\int dt$ , or $t+c$ . the other side will be some function of $v$ . algebraically solve the result to get $v$ in terms of $t$ .
the schwarzschild solution for a neutral black hole is $$ c^2 {d \tau}^{2} = \left ( 1 - \frac{r_s}{r} \right ) c^2 dt^2 - \left ( 1-\frac{r_s}{r}\right ) ^{-1} dr^2 - r^2 \left ( d\theta^2 + \sin^2\theta \ , d\varphi^2\right ) $$ you see that the terms $dt^2$ and $dr^2$ are multiplied by $ ( 1-r_s/r ) $ or its inverse where $r$ is the radial coordinate and $r_s$ is a constant , the schwarzschild radius . an important subtlety of this $ ( 1-r_s/r ) $ is that it becomes negative for $r_s\lt r$ ; this is true for any black hole beneath its event horizon . that is why in the black hole interior , the changes of the coordinate $r$ are actually timelike and the changes of the coordinate $t$ are spacelike ; the role of the space and time are interchanged in the interior relatively to the exterior ! when we say that the outgoing/infalling particles from the pair have positive/negative energy , we are talking about the energy that generates translations of the $t$ coordinate . however , the $t$ coordinate is really spacelike in the black hole interior so the $t$-component of the energy-momentum vector is interpreted as a spatial component of the energy-momentum vector by observers inside . it means that from the internal observers ' viewpoint who are capable of observing the infalling particle ( and not all of them can ! ) , it is just an ordinary particle with some value of the momentum $p_t$ , which is a spatial component and may unsurprisingly be both positive and negative . there is certainly no " new kind of matter " occuring in general relativity right beneath the horizon . all the local physics obeys the same laws as it does outside the black hole . this general assertion is a special example of the fact that the event horizon is a coordinate singularity , not an actual singularity . when one choose more appropriate , minkowski-like coordinates for the region of the spacetime near the horizon , it looks almost flat – as seen from the fact that the riemann curvature tensor has very small values , at least if the black hole is large enough . so an observer crossing the event horizon of a large enough black hole does not feel anything special at all . his life continues for some time before he approaches the singularity and this is where the curvature becomes intense and where he is inevitably killed by extreme phenomena . but life may continue fine near the horizon and even beneath it . note that the energy is the only conserved component of the energy-momentum vector on the schwarzschild background – because this solution is time-translationally invariant but surely not space-translationally invariant . due to the intense curvature caused by black hole , one must be careful and not interpret individual components of vectors " directly physically " . we saw an example that what looks like a temporal component of a vector , namely energy , from the viewpoint of the observer at infinity , can really be a competely different , spatial , component from the viewpoint of coordinates appropriate for a different observer , one who is inside .
yes , gravitational waves travel along light-like geodesics , and short-wavelength gravitational radiation will be bent around gravitating bodies just like light . for details , see chapter 35 , " propagation of gravitational waves " in misner , thorne and wheeler , especially section 35.14 , " effect of background curvature on wave propagation " , exercise 35.15 , " geometric optics " , and exercise 35.17 , " gravitational deflection of gravitational waves " .
first of all , an observed , which is a material body , cannot reach the speed of light in vacuum since it requires an infinite amount of energy . second , you can not observe electrons , protons or quarks . there is something called the uncertainty principle , it states that you can not be sure about both the location of a particle and it is momentum at the same time . if you were sure where an electron , for instance , is located then you can not be sure about it is momentum that is where it will go and so you can not observe electrons or other particles . further information about the uncertainty principle : [ 1 ] third , let 's suppose the observer goes at $c$ , and that he can observe electrons , then we know from special relativity that length is contracted relative to the observer , and we can calculate that contraction using this formula : $$l'=l_0 \times \sqrt{1-\frac{v^2}{c^2}}$$ where $v$ is the speed of the observer . in your example the observer is moving at $c$ , so : $$\begin{align} l ' and =l_0 \times \sqrt{1-\frac{c^2}{c^2}}\\ and =l_0 \times 0\\ and =0 \end{align}$$ so the observer will see everything being contracted to a length of $0$ , this means that he will see nothing . can you see something that has $0$ length , no volume ? no . so he will not be able to observe what is inside the atom .
i think a good , and classic , reference for your case is the following , introductory functional analysis with applications the very last chapter of kreyszig deals with quantum mechanics . and , once you have learned how to " translate " the language of functional analysis into that of quantum mechanics , you can go to more advanced texts in specific topics .
a capacitor has a capacitance of 100 microfarads , does that mean that when it is charged with 100 volts will the charge of the plate be 0.01 coulomb ? well , it has two plates , with an equal and opposite charge of $0.01 \:\mathrm{c}$ the top cap is connected to one of them . i will be shocked with a charge of 0.01 coulomb a fraction of the $0.01 \:\mathrm{c}$ will be discharged into your body . the thing is , the charge is " held " there by the other plate , you can not discharge one plate unless you simultaneously discharge the other . if the other plate is grounded ( and you are as well ) , then yes , you will get a full discharge of $0.01 \:\mathrm{c}$ . 100 volts ? depends on the configuration . if the other plate of the capacitor is grounded and you touch the cap , yes , you will have a $100\:\mathrm{v}$ potential difference across your body . if some part of the circuit is grounded , you will have a significant p.d. set up across your body . if it is not grounded at all , then there is a chance that the p.d. set up will be negligible . it all really depends on the rest of the circuit , specifically where it is grounded . to discharge a capacitor you need a closed circuit , so there must be some path that connects the two plates through your body and ground for something significant to happen .
suppose you have some collection of matter that is so dense it has an event horizon where the escape velocity is greater than the speed of light . the escape velocity is obviously due to the strong gravitational field of the matter inside the event horizon , and equally obviously that matter is also pulled by it is own gravity towards it is centre of mass . also obvious is that because the surface of your collection of matter is nearer to the centre of mass than the horizon is , the gravitational pull on it must be even stronger than the gravity at the event horizon i.e. the ( hypothetical ) escape velocity would be even faster than the speed of light . the reason why this situation is not stable is that the matter making up your object cannot resist the force of it is own gravity and is irrestably pulled inwards until it forms a singularity . at that point we have a standard black hole with an event horizon and a singularity at the centre . to understand why the matter within the event horizon cannot avoid being pulled down into a singularity you have to do some maths . if you are interested my answer to why is a black hole black ? gives a hopefully not too scary explanation of the maths . i think there is a semi-plausible way to explain why the matter can not avoid collapsing into a singularity , but do not take this too literally . i have mentioned above that if the escape velocity at the event horizon is the speed of light , the escape velocity inside the event horizon must be faster than light . but all forces , e.g. the electrostatic forces that hold you in shape , propagate at the speed of light . that means inside the event horizon the electrostatic force can hold matter in shape because it can not propagate outwards fast enough . this also applies to the weak and strong forces , and the end result is that no force is able to resist the inwards fall of the matter into a singularity .
i am guessing this is related to the archaic paris inch , which is $27.069$mm , i.e. $10^8 \times$ the conversion factor . reference : scientific papers vol 2 1881-1887 , john william strutt .
the name " hidden valley " models was coined in 2006 by strassler and zurek http://arxiv.org/abs/hep-ph/0604261 and the paper above , references 1-6 , show you the origin of these papers ; 6 are stringy constructions . the models may be supersymmetric but they do not have to be . the " valley " refers to some place in the hidden dimensions of spacetime where an additional gauge group $g$ added as a factor to the standard model is localized . the new states are uncharged under the standard model and only charged under the new gauge group . the new gauge group and the new particles/fields charged under the new group are ad hoc from the field theory viewpoint and there is no minimal choice . on the other hand , such new gauge groups and states are natural in string theory so it makes to study and realize what consequences such hypothetical states would have .
you are right that the question is making a very crude model of a tennis ball . however , it does capture some important qualitative features of the classical limit that are worth understanding . the intuition that you are supposed to get is that the classical limit corresponds to large $n$ . the key thing to look at is $\delta e_n/e_n$ , the fractional difference in energy between two adjacent energy levels : \begin{equation} \frac{\delta e_n}{e_n} = \frac{ ( n+1 ) ^2-n^2}{n^2} = \frac{2n +1}{n^2} \end{equation} when $n$ is small , the jump to the next energy level is relatively large , and you notice that the tennis ball is quantum mechanical . the tennis ball is not free to move freely because the energy levels of this bound system are discrete , and it can not bounce around however it likes . when $n$ is huge , the jump to the next energy level is very small ( it scales like $2/n$ ) . in that case you can approximate the energy levels as continuous , and you do not notice quantum behavior at all . based on all of this , you should notice that $n=2$ is probably not a good guess for the energy level of a tennis ball . indeed , taking $a=10 {\rm cm}$ and $m=10 {\rm g}$ , i find \begin{equation} e_1 = \frac{\hbar^2 \pi^2}{2 m a^2} \approx 10^{-64} {\rm j} \end{equation} which is muuuuuch less than the average kinetic energy of a tennis ball ! and $e_2$ is only a factor of 4 bigger than this , which is really no better some parting thoughts : ( 1 ) if it bothers you that a tennis ball is a system with many internal degrees of freedom , but here we are modeling it as a single quantum particle with no structure , well congrats because that is definitely a very crude model . however we can say we are only looking at the center of mass of the tennis ball : quantum mechanics allows us to separate out the center of mass for special treatment in a similar way as occurs in classical mechanics . ( 2 ) i said the limit $n\rightarrow \infty$ is the classical limit . we can also phrase it as $\hbar\rightarrow 0$ . to see this , note that $e_1\rightarrow 0$ as $\hbar\rightarrow 0$ , so any particle with finite energy must have $n\rightarrow \infty$ to compensate .
there are two possibilities . water is not leaking up between the log and the dam and flowing over the dam . in this case , the pressure on the bottom of the log depends only on the depth , and the amount of force needed to balance the water is the same as the weight of a log with the density of water , with an extra corner added ; i.e. , the red area in the crude picture below . this would be the weight of water in a volume $l ( \frac{3}{4} \ ! \pi r^2 + r^2 ) $ , where $l$ is the length of the log and $r$ is the radius of the log . ( i expect this was not the way that your teacher intended you to solve the problem . also note that this log is denser than water , which means that it is very difficult to see how it could have reached this position . ) water is leaking , and flowing up between the log and the dam . in this case , you have a very complicated problem in hydrodynamics which you cannot solve without more information . ,
zeta-function regularization can be thought of as analytic regularization with a special choice of the subtraction scheme . like any other regularization , there are going to be possible ambiguities that unless treated consistently across a calculation will make the results of a naive/minimal subtraction result incorrect . however , these ambiguities should always be able to be accounted for by a finite counter-terms . 1 so the art of regularization is in setting up a consistent subtraction scheme -- either by using a consistent minimal subtraction , using renormalization conditions , an r-operation , or by consistently using some implicit subtraction like zeta-function regularization . however , as you noted in your question , the latter option is not always so easy . the zeta-function regularization approach to one-loop qft calculations comes from the observation that $\partial_t h^{-t}|_{t\to0} = -\log ( h ) $ . then 2 $$\begin{align} \log\det ( h ) and = \mathrm{tr}\log h := -\zeta&#39 ; _h ( 0 ) \ , , \\ \zeta_h ( s ) and = \mathrm{tr} h^{-s} = \frac1{\gamma ( s ) }\int_0^\infty\mathrm{d}t\ , t^{s-1}\mathrm{tr} ( \exp ( -h t ) ) \end{align}$$ the heat kernel is $k ( x , x&#39 ; |t ) = \exp ( -h t ) \delta ( x , x&#39 ; ) $ and taking its trace involves setting $x&#39 ; \to x$ and integrating over all spacetime $x$ ( and taking the trace over any group or flavour indices ) . often the spacetime integral is not performed as you want the answer as an effective action . $\zeta_h$ is called the zeta function of $h$ since $$ \zeta_h ( s ) = \mathrm{tr} h^{-s} = \sum_{n} \lambda_n^{-s} \ , , $$ where the $\lambda_n$ are the eigenvalues of $h$ . $\zeta_h ( s ) $ is basically just the analytically regularized one-loop integral and you could just as easily expand it in powers of $s$ to extract the divergent part ( $s^{-1}$ ) , the finite part ( $s^0$ ) and the terms that vanish as $s\to0$ . zeta-function regularization just returns the $s^0$ part and throws away the rest - as you noted in your question , this is not always guaranteed to work in all cases and all order of operations . there is no reason to think that the implicit subtraction scheme of zeta-function regularization is better than any other subtraction scheme for analytic regularization . normally the coincidence limit $x&#39 ; \to x$ is done before the propertime integral , since it makes things simpler . also , the heat kernel is often calculated via momentum space and then it is possible to leave the momentum integral until after the propertime integral - this means you never have a position space expression for the heat kernel , but it can also make calculations simpler . if different results are obtained by different operation orders , then there is some sort of conditional convergence that is not fixed by your regularization scheme . the result of a zeta-function regularized calculation or any other renormalized qft calculation should not be taken as correct unless it satisfies a sensible choice of physically motivated renormalization conditions ( and ward identities etc . . . ) . any other subtraction scheme is merely a convenient inbetween result . finally , as noted in stackexchange-url the trick of writing $\log h$ as the derivative of some $h^{-n}$ is not unique . this non-uniqueness can be used to parameterize the ambiguity of zeta-regularization so that different methods can be compared and renormalization conditions more easily enforced . the inadequateness of the naive " zeta-function " regularization of heat kernels becomes clear in higher-loop calculations . 1 . that said , i have done calculations where the ambiguity arises in a finite ( higher-mass dimension ) term that is not present in the classical action nor amenable to correction by any renormalizable counter-term . this ambiguity , being in a finite term , comes from conditional convergence in the one-loop integrals in either their unregularized or regularized forms . in such cases , i am not sure how to deal with the ambiguities . . . 2 . all equality signs are to be taken with a pinch of salt . they depend on the regularization and renormalization scheme etc . . .
your first derivation , using energy , uses two different meanings for the same symbol $\omega$ . in one place , you interpret it as $$\omega = \dot{\theta}$$ the time derivative of the angle of the line from the center of the ball to the center of the bowl with the vertical . in another place , you interpret $\omega$ as the time derivative of the unnamed angle through which the ball itself has rotated . these two angles are related to each other by the $r/ ( r-r ) $ factor by which you are off .
your question can be translated into " if right now we would send a powerful omnidirectional light pulse from earth into space , would there be galaxies that never see this light pulse ? " the answer is " yes " . due to the accelerated expansion of the universe , as described by the lambda-cdm model , only galaxies currently less than about 16 billion light years ( the difference between the cosmological event horizon and the current distance to the particle horizon ) away from us will at some time observe the light pulse . a nice visual representation of this can be found in figure 1 of this publication .
inertia does not suddenly " break " in the sense that the axis will remain fixed until some force threshold is reached , and move thereafter ( for that matter , an ice skater cannot change direction by any clever combination of heel-toe maneuvering ) . in reality , any change in the mass distribution of the earth will move the orientation of the axis . small changes in the mass distribution will result in small changes in the orientation of the axis , but a small change is still a change . so , in a technical sense , moving some gravel from a quary to your driveway will change the axis . even walking around will change the axis . however , for a measurable change the mass redistribution would need to be immense . practically this is impossible , but only in the engineering sense , not the theoretical .
there are , essentially , three types of ensembles used in thermodynamics : microcanonical ensemble : this is used to describe closed systems . the number of particles and the total energy are constant ( since no interaction with the environment takes place ) . in such a system , the entropy will eventually be maximized . canonical ensemble : now , your system is in contact with a big reservoir and allowed to exchange energy with the system . this means that your system will be in thermal equilibrium with the bath . grand canonical ensemble now , your system can exchange both energy and particles with the system . hence , it will be in thermal equilibrium with the bath and in chemical equilibrium , i.e. the chemical potential for adding a particle to your system equals the chemical potential for adding a particle to the bath . if exchanging particles was not allowed in the grand canonical ensemble , it would become the canonical ensemble .
you need to be careful what you mean by a force in general relativity . the usual definition of a force is that you get a non-zero reading on an accelerometer you are holding , but this can lead to some surprising conclusions . to illustrate this suppose you are falling towards some massive body ( with no atmosphere to complicate the issue ) . does the gravity of the massive body create a force ? the answer has to be no , because you are in free fall so you are weightless and any accelerometer you were carrying would read zero . suppose know we give you a harness and tie you to some support fixed wrt the massive body . now you feel a force , and your accelerometer reads non-zero . but this force is due to the fact you have been restricted ( by the harness ) from following the geodesic you would otherwise follow . it is the harness that exerts the force on you ( and you on it ) because it is pulling you away from geodesic motion and therefore imparting a non-zero four acceleration . the force is not due to gravity , it is due to the harness and without the harness there will be no force . incidentally , although it is peripheral to this issue there is a nice calculation of the four acceleration and force in twistor59 's aswer to what is the weight equation through general relativity ? . let 's go back to expanding spacetime . hopefully you will now see why we say that the expansion of spacetime does not create a force . suppose we place you and me at some distance apart in an frw universe and constant comoving position and we wait to see what happens . we will each have an accelerometer so we can tell if we are accelerating . if we now wait the expansion of spacetime will increase the proper distance between us - that is , we will move apart . however because both of us are at constant comoving position we are moving along a geodesic and experience no acceleration . our accelerometers will read zero , which means we feel no force . in this sense the expansion of spacetime does not produce a force . this is exactly analogous to the claim i started out with , that the gravity of a massive body does not create a force either . now suppose we tie ourselves together with a rope . once we have done this we cannot remain at constant comoving position and this means we must be accelerating . our accelerometers would now register a non-zero acceleration towards each other and we had feel a force . any objects we drop will fall away from us . this is exactly analogous to using a harness to support yourself against the gravity of a massive body . it is the rope between us that generates a force not the expansion of spacetime . by now you are probably thinking that this is all a bit of a swindle and i have just redefined what is meant by force to make it zero . well , yes , but this is key to understanding general relativity . we do not often talk about force , but four acceleration is precisely defined in gr and can be calculated as described in the question i linked . when a general relativist talks about force they implicitly mean four acceleration . this does mean they are using the term in a different way to the general public - hence the confusion .
in the restricted three body problem , where you consider two objects orbiting each other , such as the sun and earth , and the motion of a third object that does not affect the movement of the first two , but is affected by their gravity , you can sort of figure out how far/fast from one object you have to be to not be orbiting it anymore . the picture above is taken from shane d . ross ' ph . d . thesis . depending on the total energy of the third mass , it will never be able to go into the shaded areas . so if you are orbiting earth , which would be $m_2$ in the sun-earth example , or $m_1$ in an earth-moon one , there is a minimum energy at which can break out of the first and start orbiting the other body . the transition point is the lagrangian point $l_1$ . at a higher energy , it is possible to break away to infinity from both objects , the transition point corresponding to the lagrangian point $l_2$ . so depending on a more precise definition of you question , a possible answer is that a satellite beyond the sun-earth l1 point is more orbiting the sun than the earth . the sun-earth l1 point is , according to this , about 1% of the way to the sun . so that is about 1,500,000 km . you could of course calculate the corresponding $e_1$ enery and translate that to kinetic energy and velocity .
what force particle mediates electric fields and magnetic fields ? photons , as you have suggested . 1 ) would not that mean that a charged particle ( e . g . an electron or even a polarized h2o molecule ) would constantly be losing energy from sending out photons ? you must describe this process in a quantum field theory . virtual photons emitted by charged particles are reabsorbed in a time consistent with uncertainty principle . hence over some finite amount of time , energy is conserved . 2 ) would not that mean that an electric field is inseparable from a magnetic field , as photons have both - and that one can not have one without the other ? you can already show this in classical electromagnetism - see maxwell 's equations . 3 ) would it be possible , then , to determine the wavelength of magnetic-field-mediating photons ? if so , what is the wavelength - is it random or constant ? the wavelength of a photon is related to it is energy , which is again related to the uncertainty principle . the longer the time borrowed from the vacuum , the lower the energy of the photon , so it has a longer wavelength . hence the wavelength of virtual photons at large distances from the em source is much longer than at short distances . 4 ) how can a photon ( which has momentum ) from one electrically charged particle to an oppositely charged particle cause these particles to be pulled toward each other - or how can a magnetic field cause an electrically charged moving particle to experience a force perpendicular to the source of the magnetic field if a particle with a non-zero mass moving between the two is the mediator of that force ? this become less intuitive depending on your background . richard feynman introduced a trick which offers a way to imagine the process . imagine the photon is emitted between opposite-charge particles in the future and travels ' backward in time'- therefore its momentum minus minus what it really is . this is explained in good detail here . if " virtual photons " are involved , please explain why they work differently from regular photons unlike ' real ' photons ( which have transverse polarisation ) , virtual photons have both transverse and longitudinal polarisations . the energy momentum four vector of the virtual photons , and generally all virtual particles , is not necessarily 0: virtual particles are off mass shell . this means that virtual photons may have non-zero mass - which means that they also have a longitudinal polarisation state . it is important to consider the extra polarisation in your calculations .
before thinking about circuits , let 's think about two conducting spheres of charge that i connect by a wire . before i connect the wire , sphere 1 is at voltage $v_1$ and sphere 2 at voltage $v_2$ , let 's say $v_1&gt ; v_2$ . i find it useful , in terms of thinking about what is going on , to notice that if the spheres are the same size then saying the spheres have different potentials is equivalent to saying that the 2 spheres have different charges residing on their surfaces ( you can justify this by noting that the capacitance of a sphere is determined by its radius ) . now let 's connect the spheres . what will happen ? well , a current will flow in the wire . this will take positive charge off of sphere 1 and deposit it on sphere 2 [ strictly speaking if you want electrons to be charge carriers , then negative charge is flowing from 2 to 1 ; but in terms of thinking about what is going on it is easier to imagine , and mathematically equivalent to say , that positive charges are going from 1 to 2 ] . this in turn changes the voltages on the two spheres ; $v_1$ decreases and $v_2$ increases . the process stops when $v_1=v_2$ . again , if the spheres are the same size this condition is equivalent to the charges on both spheres being equal . ok , now imagine a battery hooked to a resistor and a switch , the simplest circuit imaginable . before we close the switch , terminal 1 is at $v_1$ and terminal 2 is at $v_2$ . at this point , it makes perfect sense to think of each terminal of the battery as being a sphere of charge . then we close the switch , this is like connecting our spheres with a wire . based on our silly model of a battery , you would the voltage between the two terminals of the battery ( ie , $v_1-v_2$ ) to decrease until eventually it reached equilibrium with $v_1=v_2$ . clearly , a battery does not behave like two spheres of charge after the circuit is closed . the whole point of a battery is that it maintains the potential difference between its two terminals . after we close the switch , a little bit of positive charge flows from terminal 1 to terminal 2 by going through the circuit . naively this means that terminal 1 has less positive charge and terminal 2 has more positive charge , so terminal 1 's voltage decreases while terminal 2 's voltage increases . inside the battery , some process takes place to to take the excess positive charge on terminal 2 and put it back onto terminal 1 . whatever this process is , it cannot be electrostatic , because positive charges following the electric field can only ever move from terminal 1 to terminal 2 [ positive charges move from high voltage to low voltage , if the only force is electrostatic ] . the details of what the battery does to maintain the potential difference varies depending of the kind of battery . a conceptually simply example of a battery is a van de graaff generator . in a van de graaff generator , you have a conveyer belt that literally carries the excess positive charge on terminal 2 and deposits it back on terminal 1 , undoing the naive ' equilization process . ' most useful batteries rely on some chemical process to maintain the potential difference . for example , one can use oxidation reactions to do this . the details involve some chemistry ( there is a wikipedia summary at http://en.wikipedia.org/wiki/electrochemical_cell ) , but essentially you put each terminal in a bath of ions , and the chemical energy of the reactions at each terminal [ balancing oxidation and reduction ] forces ionized atoms to carry electrons from terminal 2 to terminal 1 .
just sum over each permutation of [ 1,2,3,4 ] , for each permutation $ [ i_1 , i_2 , i_3 , i_4 ] $you would have a factor of $\theta ( t_{i_1}-t_{i_2} ) \theta ( t_{i_2}-t_{i_3} ) \theta ( t_{i_3}-t_{i_4} ) $ times the corresponding operator product , etc .
maybe you are getting confused with the notation . notice that $$ \partial_\mu n^\mu \equiv \partial_0 n^0 +\nabla\cdot\vec{n} , $$ where $\nabla\cdot\vec{n} \equiv \partial_in^i$ . now , another way o writting this expression is : $$\partial_\mu n^\mu = \eta^{\mu\nu}\partial_\mu n_\nu=\partial_0 n_0 - \partial_i n_i . $$ vectors are tensor with only one contravariant indice , i.e. upstairs . vectors and covectors are related through the metric by $$v^\mu =\eta^{\mu\nu}v_\nu \leftrightarrow \{v^0 , v^i\}=\{v_0 , -v_i\} , $$where i assumed $\eta_{\mu\nu}= ( 1 , -1 , -1 , -1 ) $ .
if a tachyon starts from where you are and goes away at faster than the speed of light , you will see the photons it emits earlier than it actually departs . so you will see all these photons coming as if the tachyon were coming toward you at a speed slower than light , and then bang , the tachyon leaves . in fact , the faster it is going away , the slower it appears to be arriving . edit : you can just tell this from a space-time diagram : T s C | / / |/ / f | s / / |/ / f / | / / / |/__/_/___X | / / | / / |/ / | / |/  here , the time t axis is vertical and the space x axis is horizontal . line c represents the speed of light . photons move parallel to that line . if something is moving away from you slower than light , it is a diagonal line falling in the slow ( s ) region . when it emits photons , they travel parallel to c , so each one arrives back at you at a later time . that is the normal behavior that you are used to . if something is moving away from you faster than light , it is a diagonal line in the fast ( f ) region . when it emits photons , they travel parallel to c , and thus arrive back to you at a negative time , relative to when the object left you . in fact the faster it is moving ( closer to horizontal ) the earlier its photons will arrive ( negative t ) . the slower it is moving ( closer to c ) the more its photons will appear to come all at once , just before it " departs " .
it may actually work , as evaporating liquids need heat to evaporate , and water will somewhat evaporate even in the fridge . i am not sure it works in practice , because the paper also causes an adverse effect , it provides insulation , hard to tell which effect is dominant . i am pretty sure that the balance of both effects depends in a very large part on the physical structure of the paper used . there are multiple independent physical effects in relation to the paper : on the picture , we see there is air under or inside of the paper in some locations . i would think of the contribution to heat flow to be similar to the cooling from evaporation , except for the sign . this is related to the paper structure by depending on how " soft " the paper is - when it is wet , but was not wet for a long time . the total effect of cooling is related to the time the surface of the paper stays wet , and after that , to the time the paper stays wet at all . the mention of a paper towel supports this idea , because paper towels are specifically optimized to keep water . the inner surface area of the paper gets very relevant at the time the surface is not longer " fully wet " . for the relevance of the size of the paper 's pores , the same holds like for the inner surface above . if the water on and in paper freezes , the result is a layer of - depending on how wet it was - porous material , insolating relatively well . dry paper is insolating . there are also relevant effects not related to the paper : the air temperature influences the rate of evaporation . when it is colder , the evaporation is slower , so that the cooling effect is weaker when it is cold . evaporation depends on humidity of air . and humidity of air in a fridge largely depends on condensation on the surface of the cooling elements . that depends on the surface temperature , which in turn depends both on the user set temperature , but , to a large part on the geometry of the cooling element ! [ 1 ] but , quite counter-intuitively , not directly on the fridges overall temperature as set by the user . if it is very cold , the upper layer ( or more ) of the wet paper could freeze , leaving just a little sublimation instead of evaporation for cooling . see the paper section for paper related effects of freezing . some fridges use an internal fan to make move more cold air to goods hard to reach from the cooling element . as for the bottle moving more air around also means moving more moisture away , allowing for more evaporation , it could help the cooling effect a lot . additionally , condensation of the moisture elsewhere in the fridge is made quicker in a similar/inverse way . [ 1 ] simplified , the geometry of the cooling element is about the area available to exchange heat . if it is large , a small temparature difference is enough for cooling . that reduces condensation on the element - keeping more moisture in the air . if it is small , it needs to be freezing cold , collecting moisture as ice . a medium size collects moisture by condensing to water - which is often removed through a small drain pipe . ( simplifying by ignoring the influence on convection near the cooling surface . )
$\hat{p}$ is hermitian and hermitian operators $o$ satisfy , by definition , $$\hat{o} = \hat{o}^\dagger$$ adjoint is not synonym for complex . $\hat{p} = -i\hbar \nabla \rightarrow +i\hbar \nabla^\dagger \rightarrow -i\hbar \nabla =\hat{p}^\dagger$ , but $\hat{p} \neq \hat{p}^*$ .
if the energy difference between two sites separated by $\mathbf{r}$ , then the effective electric field $\mathbf{e}$ between those two sites is given by \begin{equation} \mathbf{e}\cdot\mathbf{r}=\frac{de}{e} , \end{equation} where $e$ is the electron charge .
the step from 1 to 2 is : multiply by $\gamma^0$ and trace over the result . 2-> 3: $tr ( ( \gamma^0 ) ^2 ) =tr ( 1 ) =1$ on the left hand side and $2tr ( \gamma^0\gamma^\mu ) p_\mu=8\eta^{0\mu}p_\mu=8p_0=8p^0$ on the right hand side . moreover the anticommutator gives $2q_r^2$ so you can just strip off a factor of 2 on both sides . 3-> 4: $p^0$ is the total energy by definition , as the right hand side is in fact mulitplied by an unit operator ( as $q$ are operators ) one has $p^0 1 = h$ giving the final result . for some magic involving the gamma matrices see the wikipedia article http://en.wikipedia.org/wiki/gamma_matrices
this has actually been tested and presented on television : the projectile will indeed fall straight to the ground to the stationary observer . however , you the shooter will view the projectile traveling with a speed of 40 m/s while the projectile sees you traveling with a speed of 40 m/s , thus the airspeed remains .
the non-compact $u ( 1 ) \cong \mathbb{r}$ symmetry , which the book conformal field theory by philippe di francesco et . al . is referring to , is the translation symmetry $$\varphi\to\varphi+a , \qquad a\in \mathbb{r} , $$ of the main boson field $\varphi$ of the coulomb-gas formalism , reflecting the zero-mode of the $\varphi$-field .
you have a problem with the requirements described in your question . you say you a want to prepare the state of ( a1 , a2 , a3 ) to have 3-way entanglement . in that case , the state of ( a1 , a3 ) will be a mixed state , since the system of ( a1 , a3 ) is entangled with a2 . but you want to use the pair ( a1 , a3 ) for an entanglement swapping protocol . this protocol requires ( a1 , a3 ) to be in a state which is maximally entangled ( such as a bell state ) . but a mixed state is never maximally entangled , and therefore you cannot use the usual entanglement swapping protocol in this case . of course the same reasoning applies to ( b1 , b2 , b3 ) . the deeper reason for the problem here is that entanglement tends to be " monogamous " , in the sense that if a1 is entangled with a2 , it puts limits on the entanglement it can have with a3 . if you still want to try and perform a bell state measurement on a3 and b3 , even without satisfying the requirements of the entanglement swapping protocol , in general you will get some sort of 4-way entanglement in the state of ( a1 , a2 , b1 , b2 ) . the specifics of the state you will get depend on the states you started with for ( a1 , a2 , a3 ) and ( b1 , b2 , b3 ) . as to whether the entanglement in the systems ( a1 , a2 ) and ( b1 , b2 ) will remain intact , they could not have been very strongly entangled in the first place ( again because of entanglement monogamy ) , but they would tend to be even less entangled after the above measurement since now they are a part of a 4-way entangled state instead of a 3-way entangled state .
update 2: it might also be useful add the relation to usual galilean physics . this will probably only make sense after reading update 1 . recall that one can parametrize boosts in ( 1+1 ) by $ [ \cosh ( \eta ) , \sinh ( \eta ) ] $ . as it so happens this equals to $ [ \gamma , \gamma {v \over c} ] $ . the classical physics correspond to the asymptotic case $c \to \infty$ ( i.e. . no limit on the speed of light ) . so this reduces to $\gamma \to 1$ , ${v \over c} \to 0$ and that means $\eta \to 0$ . so in galilean case all boosts degenerate to trivial transformation and this is why time and space separate and addition of velocities starts to work . update 1: having seen kennytm 's strange answer i decided to add some notes to this answer . first , there exists a concept of rapidity . this is a natural variable for parametrization of boosts . first consider a circle . why circle ? because it has to do with rotations and rotations are very similar to boosts . circle is an object given by an equation $x^2 + y^2 = 1$ . you can decide that you will parametrize ( part of ) it by coordinates $ [ x , \sqrt{1 - x^2} ] $ . now what if you want to rotate the circle by some angle ? can you figure out how will the parametrization change ? maybe you can but i assure you this is not pretty . but there is a nicer way . let 's try parametrizing the circle by an angle $\phi$ so that it would become set of points $ [ \cos ( \phi ) , \sin ( \phi ) ] $ and now the rotation by angle $\psi$ corresponds to parametrization $ [ \cos ( \phi + \psi ) , \sin ( \phi + \psi ) ] $ . so our parameter is additive ! you will not find any better parametrization of the circle than this . okay , so we understand circles a little better now . but as already said , rotation in two space dimensions is almost the same thing as boost in ( 1+1 ) space-time dimensions . in the same way that rotations ( around origin ) preserve circles , boosts preserve hyperbolas . so instead of working with $\sin$ and $\cos$ you will work with hyperbolic functions $\sinh$ and $\cosh$ and instead of $\phi$ you will obtain rapidity $\eta$ . now , this only works this nicely in ( 1+1 ) -dimensions . in ( 3+1 ) you will have many more interesting effects ( similarly to like rotations are strange beasts in 3 dimensions as opposed in 2 ) . but it is still true that like the general 3-dimensional rotations are nicely parametrized by an axis and rotation angle , the boosts are nicely parametrized by the direction of boost and rapidity . so if you perform two rotations about the same axis it is the same as rotation by a sum of the angles and if you perform two boosts in one direction it is the same as doing a boost with added rapidities . i will assume you are talking about galilean principle of relativity whereby the velocities transform by pure addition . this concept breaks down when the speeds one is dealing with are too large . speed of light is an extreme case of such speed . then one has to use special relativity and instead consider four-vectors transforming by lorentz transformation . now , this transformations preserve the minkowski length of four-vectors ( in the same way that rotations preserve length of usual vectors ) . the point is that velocity of light corresponds to zero minkowski length and so light moves at the speed of light in every inertial frame . this is the famous einstein 's postulate .
there are several points of evidence that the oort cloud exists , though it is indeed still a hypothesis and lacks direct observation . the first is indirectly observational , as proposed by ernst öpik back in 1932 as the source of long-period comets . this was revised by jan oort in 1950 . all you need to determine an orbit is three observations of the object , separated in time . the greater the separation in time and the more observations , the more certainty we have in its orbit . comets with periods longer than pluto 's must , by definition , have come from beyond pluto . pluto 's orbit basically loosely defines the extent of the kuiper belt ( 30-50 au ) . so there needs to be a source for these bound objects , and interstellar ones do not cut it because if they are interstellar , then they should not be on bound orbits . the second is theoretical : solar system formation models predict that the formation of the giant planets would have scattered small icy objects into the outer solar system . while some would be given enough energy to completely escape the solar system , others would be scattered out to the hypothetical oort cloud . third , we have seen kuiper belts around other star systems , and it is likely that the oort cloud is a continuation of the kuiper belt , so this may be evidence for oort clouds as well . so if we need a source for long-period comets and the orbits work out to this cloud beyond the kuiper belt , dynamical models predict that the bodies would exist there , and we see similar dynamical structures around other stars , then that is fairly compelling evidence it exists . but , you are correct that , at present , it is not technologically possible to view comets that are members of the oort cloud that are still in the oort cloud . viewing a chunk of ice 1/4 of the way to the nearest star is simply not possible . . . yet .
you are correct , but probably not for the reason you think . at low frequencies the inertia of the electrons is small enough to be ignored , that is we assume that when we apply a voltage the electrons instantaneously accelerate to their steady state velocity . as you increase the frequency of the applied voltage the inertia of the electrons starts to become significant , and above a frequency called the plasma frequency the electrons can no longer move fast enough to respond . for most metals the plasma frequency is around or slightly below that of visible light .
let 's try the hard way without feynman 's argument . just snell 's law . we can choose a frame $ ( x , z ) $ where $z$ is along $oo'$ and $o$ is at $ ( 0,0 ) $ . we suppose the curve equation is written $z=f ( x ) $ where $ ( x , z ) $ is the location of point $p$ . the vector normal to the curve at $p$ can be written $\vec n = ( 1 , -1/f' ( x ) ) $ . now , knowing snell 's law , you can write $\sin\theta =n\sin\theta'$ , and rewrite it with vector products as $$\frac{\vec{op}\times\vec n}{op\ ; \ ; n}=-n\frac{\vec{o'p}\times\vec n}{o'p\ ; \ ; n}$$ where $n$ can be eliminated on the denominator . expressing all terms as a function of $x$ and $z=f ( x ) $ , you can obtain a differential equation on $f ( x ) $ . $$ [ x+f ( x ) f' ( x ) ] \sqrt{ ( x'-x ) ^2+ ( z'-f ( x ) ) ^2}+n [ ( x'-x ) + ( z'-f ( x ) ) f' ( x ) ] \sqrt{x^2+f ( x ) ^2}=0$$ that seems really difficult to solve unless there is some clever calculus to do . in fact , feynman argument saying that the light should take the same time to travel that distance , whatever the trajetory , is very clever . it should not be absolutely necessary to use that , but it simplifies the problem : $op+no'p=\mathrm{cste}=c$ . this translates into $$\sqrt{x^2+z^2}+n\sqrt{ ( x'-x ) ^2+ ( z'-z ) ^2}=c$$ which you can rewrite as a fourth order equation .
your definitions are in fact those for proper , orthochronous lorentz transformation , not for general lorentz transformations , that is why you are having trouble telling the difference ! ( if it makes you feel any better , yesterday a collegue and i were trying to debug his test setup and two hours of complex testing passed before we two geniusses realised we had not switched the power to a key bit of kit on ! ) a general lorentz transformation is defined by criterion 1 ) alone - it is simply any linear transformation that preserves the quadratic form $t^2 - x^2 - y^2 - z^2$ . the proper , orthochronous transformations are those that belong to the identity connected component $so^+ ( 1 , \ , 3 ) $ of the full lorentz group $o ( 1 , \ , 3 ) $ . that is , the proper , orthochronous transformations are those that can be reached from the $4\times4$ identity matrix by following a continuous path through the lorentz group . equivalently , they are the matrices that are on paths through the lorentz group defined by the differential equation : $$\begin{array}{lcl}\mathrm{d}_s l and = and ( a_x ( s ) \ , j_x + a_2 ( s ) \ , j_y+a_z ( s ) \ , j_z + b_x ( s ) \ , k_x + b_y ( s ) \ , k_y+b_z ( s ) \ , k_z ) \ , l\\l ( 0 ) and = and \mathrm{id}\end{array}\tag{1}$$ where $\mathrm{id}$ is the $4\times 4$ identity , $a_j ( s ) , \ , b ( s ) $ are continuous functions of the parameter $s$ and the $j_j , \ , k_j$ are six matrices $4\times 4$ that span the lie algebra of the lorentz group , i.e. the real vector space of all possible " tangents to the identity " , i.e. all possible values of $\mathrm{d}_s l|_{s=0}$ . one possible set is : $$\begin{array}{lcllcllcl}j_x and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and -1\\0 and 0 and 1 and 0\end{array}\right ) and j_y and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and 0 and 1\\0 and 0 and 0 and 0\\0 and -1 and 0 and 0\end{array}\right ) and j_z and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and -1 and 0\\0 and 1 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) \\k_x and = and \left ( \begin{array}{cccc}0 and 10 and 0 and 0\\1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) and k_y and = and \left ( \begin{array}{cccc}0 and 0 and 1 and 0\\0 and 0 and 0 and 0\\1 and 0 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) and k_z and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 1\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\1 and 0 and 0 and 0\end{array}\right ) \end{array}\tag{2}$$ ( see how the $j_j$ are skew-hermitian , thus have pure imaginary eigenvalues , so that $\exp ( a_j\ , j_j ) $ has stuff like $\sin , \ , \cos$ of an angle and is a rotation matrix , whereas the $k_j$ are hermitian , with purely real eigenvalues , so that $\exp ( b_j\ , k_j ) $ has stuff like $\sinh , \ , \cosh$ of a rapidity and is a pure boost matrix ) . an intuitive description : imagine you are sitting at the console of your spaceship 's " hyperdrive": it has two track balls each with their own levers marked " spin " and " boost " and a set of accelerometers - linear and rotational . your spaceship is initially moving inertially . you roll the trackballs around to set the axis of rotation and direction of boost respectively . when you pull on the levers , the spin lever accelerates the angular speed about the rotation axis , the boost lever accelerates the linear velocity in the boost direction . otherwise put , the " rotate " trackball and its lever set the superposition weights $a_j ( s ) $ of the $j_j$ in ( 1 ) when we use the definitions in ( 2 ) and the " boost " trackball and its lever set the weights $b_j$ of the $k_k$ . you go through a control sequence , ending so that your accelerometers read nought , so that now a set of $x , \ , y , \ , z$ axes attached to your spaceship is moving inertially relative to the beginning frame . the proper , orthochronous transformations are precisely every transformation between the beginning frame and an inertial frame that you can reach with your controls . however , there are other transformations possible that preserve the quadratic form $t^2 - x^2 - y^2 - z^2$ that do not fulfill your criteria 2 . and 3 . but they follow only a " simple " pattern that makes them " not much different " from the identity connected component . a discrete subgroup of the full lorentz group is $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ with $$p=\text{"parity flipper"} = \mathrm{diag} [ 1 , \ , -1 , \ , -1 , \ , -1 ] ; \\t=\text{''time flipper''} = \mathrm{diag} [ -1 , \ , 1 , \ , 1 , \ , 1 ] $$ with the exception of $\mathrm{id}$ , none of these can be reached from the identity by paths fulfilling ( 1 ) . they belong to different connected components from the identity component $so^+ ( 1 , \ , 3 ) $ . indeed , the identity connected component is a normal subgroup of the full lorentz group $so ( 1 , \ , 3 ) $ and the quotient $o ( 1 , \ , 3 ) / so^+ ( 1 , \ , 3 ) $ is the little group $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ . so any full lorentz transformation can be represented as a proper orthochronous transformation followed by one of $p , \ , t$ or $p\ , t$ . there are four separate connected components to the full lorentz group . ( an aside : $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ is the klein " fourgroup": the only possible group of four elements aside from $\mathbb{z}_4$ ) . to sniff out a non-proper or non-orthochronous transformation , you do one of two things : compute the matrix 's determinant . if it is -1 , then you know it has to include one of $p$ or $t$ , so it is not proper or not orthochronous . you can further differentiate the $p$ and $t$ cosets by looking at the $l_0^0$ component of the transformation : the $t$ coset has $l_0^0&lt ; 0$ , since such a transformation swaps the roles of the " future " and " past " ( actually reflects minkowsky vector space in the $t=0$ plane ) . if the determinant is $+1$ , then it may belong to the $p\ , t$ coset of $o ( 1 , \ , 3 ) $ . as in point 1 , the $t$ coset and the $p\ , t$ coset can be recognised as transformations with $l_0^0&lt ; 0$
equation ( 7a ) describes a system of differential equations : you have one such equation for each value of $j$ , i.e. for each nuclear state $\chi_j$ . and i do not like the way author ( s ? ) write the first term on the left side , i would better write it as $ [ t_{n} + v_{nn} ] \ , \chi_j$ . anyway , the point is that equations in the system ( 7a ) are as we say coupled in a sense that solution of $j$-th equation ( $\chi_j$ ) enters all other equations through this $\lambda_{ji} \ , \chi_i$ terms . that is why we call equations ( 7a ) coupled channel equations and terms $\lambda_{ji} \ , \chi_i$ coupling terms . later to simplify the system we decouple the equations by introducing what is called the adiabatic approximation , in which we neglect all off-diagonal coupling terms . and thus , we call these terms non-adiabatic . why do we call the approximation in which we neglect $\lambda_{ji} \ , \chi_i$ for $j \neq i$ adiabatic ? the term " adiabatic " from ancient greek αδιαβατος ( α - " not " , δια - " through " , βατος - " passable" ) literally means the situation when something " is not passing through " something else . in thermodynamics adiabatic process is a process occurring without exchange of heat of a system with its environment , i.e. a process in which heat is not passing through system enclosure . in quantum mechanics adiabatic refers to a process in which no abrupt transition from one state to another with respect to continuous changes of some parameters happens and thus energy of a system changes continuously with respect to continuous changes of these parameters . and that exactly the picture we have here because if you take a look at ( 7b ) then $\lambda_{ji} \ , \chi_i = 0$ for $j \neq i$ can be interpreted as follows : for some particular $i$-th nuclear state when varying nuclear coordinates no transition from corresponding $i$-th electronic state to any other $j$-th electronic states can happen . p . s . you are reading quite advanced book on the subject . since you do not know the meaning of the word " non-adiabatic " , you would better start from something simpler .
backspin ! those shots in which the cue ball " draws " backwards after hitting the target ball involve backspin . without backspin , the cue ball cannot reverse direction . consider what happens when the cue ball is not spinning at all when it hits the target ball . the cue ball will come to a dead stop if it hits the target ball straight on . think of newton 's cradle . the cue ball will continue moving forward ( but at an angle ) if a non-spinning cue ball hits the target ball obliquely . the cue ball always moves forward after striking the target ball if the cue ball is rolling without slipping whilst hitting the target ball . a rolling cue ball will initially stop if it hits the target ball straight on . the cue ball will still be spinning , however , and this spin will soon make the cue ball start moving forward again . when a rolling cue ball hits the target ball obliquely , the collision will change the cue ball 's direction and the spin will accentuate the forward motion . the only way to combat these effects is to have the cue ball spinning backwards when it strikes the target ball . a backspinning cue ball that hits a target ball straight on will initially stop , but now the backspin will make the cue ball reverse direction . so how can one make the cue ball have backspin ? the answer is simple : strike the ball below center . how much below depends on the distance to the target ball . this is easy if the target ball is close to the cue ball : strike the cue ball a bit below center . you will need to strike the cue ball a bit further below center if the target ball is further away . when the target ball is very far away ( across the length of the table ) , it is very hard to have the cue ball spinning backwards at the point of collision . you need to take care in your shot and how far from off-center you hit the cue ball . hit the cue ball too far off-center and you will hear a nasty " clink " sound . you have just miscued ; the cue ball will not move anything like you planned . and maybe you have even ripped the table , bad move !
@dmckee guessed correctly . from an excerpt from an address delivered before section a of the american association for the advancement of science , on august 23 , 1882 , by prof . win . harkness , chairman of the section , and vice president of the association : ( ref ) he was destitute of what would now be regarded as the commonest instruments . the invention of telescopes was only twenty years old , and a reasonably good clock had never been constructed . his observatory was situated in paris , and its appliances were of the most primitive kind . by admitting the solar rays into a darkened room through a small round hole , an image of the sun nine or ten inches in diameter was obtained upon a white screen . for the measurement of position angles a carefully divided circle was traced upon this screen , and the whole was so arranged that the circle could be made to coincide accurately with the image of the sun . to determine the times of ingress and egress , an assistant was stationed outside with a large quadrant , and he was instructed to observe the altitude of the sun whenever gassendi stamped upon the floor .
the total initial internal energy is $u=u_1 + u_2=\frac{\nu_1}{2}c_1rt_1 + \frac{\nu_2}{2}c_2 rt_2 $ where the last equality comes from joules ' first law for ideal gases and where $c_i$ is the number of moles of species $i$ and $\nu_i$ is the number of degrees of freedom of the molecule ( 3 for atoms , 5 for diatomic molecules etc . . ) . now , once equilibrium is reached everybody should have the same temperature $t$ . since you are dealing with an ideal gas it implies that : $u= \frac{ ( \nu_1c_1+\nu_2c_2 ) rt}{2}$ and hence since the whole system is isolated $t = \frac{\nu_1c_1t_1 + \nu_2c_2 t_2}{\nu_1c_1+\nu_2c_2}$ once the temperature is known , the rest follows easily . the pressure can be gotten straightforwardly as $p=\frac{ ( c_1+c_2 ) rt}{v_1+v_2}$ because the ideal gas law is independent of the number of degrees of freedom of the different species .
the book is referring to this paper . as far as i can tell ( which is not very far ! :- ) the maths is valid , but whether the maths describes anything that happens in the real world is another question . if you are reading books in this area you have probably heard of inflation , i.e. the idea that the universe expanded exponentially soon after the big bang . when the universe stops inflating we get the slowly expanding universe that we see around us today . there is a theory called eternal inflation in which different bits of the inflating universe stop inflating at different times . this gives many areas that look like our universe today , but those areas are separated in the sense that you had need to be able to travel faster than the speed of light to get from one of these areas to another . because of this separation these areas are generally known as bubble universes . arguably this is a misnomer , since the bubble universes are really just different areas of a single inflating universe , but it is only terminology so i can not get too wound up about it . anyhow , it is possible that two bubble universes might be close enough that they could touch each other . the idea behind the paper is that the interaction between the two bubble universes would change the cosmic microwave background ( cmb ) and this change would be detectable . various searches of the cmb have been done looking for evidence of such a collision , but so far nothing has been found . if you want to find out more about this google for " universe collision cmb " or something like that . one of the hits will be this blog , which i recommend as a good description of the idea .
this is quite far from a silly thought although this is not apparent at first sight . apart from a couple of details which are well understood and have firm physics behind them - such as the fact that deuterium and tritium exist in some proportion and the hyperfine-structure distinction between ortho- and parahydrogen , as far as we can tell all hydrogen atoms are exactly the same . this is in fact the case for all atoms and molecules : all iron atoms are exactly replaceable ( so long as you take the right isotope ) and nitrogen molecules are all the same ( so long as you take them in the correct electronic , nuclear and spin states ) , and so on . this is one of the most profound symmetries in nature and it holds irrespective of geographical / astronomical position , chemical history , temperature , and so on . how can we tell ? well , the very fact that we can do chemistry with atoms is why - the basic tenet is that the world is made of a finite set of " blocks " and that combinations of them make the interesting materials around us . the success of chemistry as a discipline means that there is something to that basic tenet . how can we tell that atoms in places we have not been are the same as here ? of course , our evidence for that is not as strong , but it is built on the fact that astrophysics works just using physics of different kinds we can see experimentally here on earth . we can do spectral analysis of the solar corona , for example , and if we see energy levels slightly displaced then we can explain that as doppler shifts or magnetic fields that let us explore a richer and ( as far as we can tell ) fully consistent physical picture . we can do chemistry on the atmospheres of other planets and , though it is rather hard , come up with consistent chemical explanations for all our observations . we can link the nuclear physics we observe in accelerators and reactors to explain our observations of our sun and other stars and see that they match what we do here . this represents another of nature 's deepest symmetries that is ( again , as far as we can tell ) completely exact : physics is all the same wherever and whenever you do it . emission and absorption of light works exactly the same as here , and so on and so forth . so what happens when something comes up that is not quite right ? well , so far we have always been able to explain that as a result of new physics . some of these observations are in play right now . for example , the physics of em emission and absorption is ( possibly ) slightly different in other galaxies ; to explain this a " drift " of the relevant constant ( the fine-structure constant $\alpha$ ) has been proposed , and there are currently earth-bound experiments to measure this drift going on . ( also this paper . ) so far , however , and despite the number of open problems in physics , no definite evidence for physics being different elsewhere has come up .
sure , feynman was concerned with a physical system doing the actual computation , and all physical evolution is unitary , and hence the hamiltonians must be unitary . a slight but trivial generalization would be to allow for evolution with completely positive maps , but such evolution can always be mimicked by unitary evolution with some auxiliary degrees of freedom followed by tracing out of those extra degrees of freedom . allowing for non-unitary evolutions would allow for very unphysical situations ( signalling , . . . ) and would also allow quantum computers to solve np-complete problems ( which is certainly not expected to be possible )
the maximum electric field develops near the depletion layer around the p-n interface . the maximum value , which occurs at $x=o$ ( the interface ) is given by the equation $e_{max}=\frac{2 ( v_{bi}-v-kt/e ) }{w}$ where : $w$: is the width of the depletion layer , $v_{bi}$: is the difference between the highest and lowest values of the bottom of c-b in the n-type , or the between the highest and lowest values of the top of the v_b in the p-type . $t$: is the temperature of the semiconductor material $v$: is the applied voltage $k$: is boltzmann’s constant $e$: the electric charge on the electron
yes , the factor $2\pi$ in eqn 1 should be $4\pi$ – if the area integral is normalized conventionally . for example , for a sphere , the scalar curvature is $r=2/a^2$ . when multiplied over the $4\pi a^2$ surface , we get $8\pi$ which is $4\pi$ times the euler character of the sphere , $\chi=2$ . well , at least i hope that these considerations are not affected by using the metric $\hat h$ instead of $g$ .
the eigenvalue of the generator $t_a$ are integer multiples of $g_{min}$ because $t_a$ is a generator of a ( cyclic ) $u ( 1 ) $ group and $$ \exp ( 2\pi i t_a/ g_{min} ) = 1 $$ holds as an operator equation . this equation says that the exponentiation of the generator with some imaginary coefficient that i parameterized as $2\pi i / g_{min}$ is equal to the identity . the rotation of a sphere by $2\pi$ is an example . when the equation above acts on an eigenstate , $t_a$ is replaced by its eigenvalue , but because $\exp ( 2\pi i n ) =1$ only for $n\in{\mathbb z}$ , it follows that the eigenvalue of $t_a/g_{min}$ is integer . the representation of a lie algebra and the representation of the corresponding lie group is the same thing . one may be used for the other .
there is no " better " or worse here . it is just that " work " in physics is defined differently than in chemistry . in chemistry , all quantities follow this sign convention : they are positive if their effect is on the system . so , basically , $du$ is ( infinitesimal ) energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done on the system by the surroundings in physics , the sign convention of $w$ is the opposite $du$ is energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done by the system on the surroundings which means that $du_c = \delta q_c + \delta w_c$ ( $c$ means chemistry ) becomes $d u_p = \delta q_p - \delta w_p$ try to keep these conventions separate in your mind . do not use the physics flt for a chemistry problem and vice versa , many times problems specify values of $w , q , u$ and expect you to know the sign convention . note that these are the iupac/iupap conventions . some books ( as @dmckee mentions , feynman 's lectures is one of them ) use different conventions . in such cases , just make note of the convention and remember that the flt is just a statement of conservation of energy . here 's the menemonic i used to remember it . it is not a great one , but it works : chemists are interested in supplying hear/energy/pressure to a reaction to make it occur . thus , action done by the surroundings on the system is " good " or " positive . physicists are more interested in supplying heat/energy to a system and making it do work . so , supplying heat/energy is " good " , and getting work out is " good " .
your charge distribution is correct , though i usually do not focus on using a " correct " $\pm$ distribution in capacitors--if your sign is incorrect , you get a negative value of charge on the + plate . no biggie . in complex situations , it is sometimes even impossible to predict cjarge distribution without solving the circuit . and your book is incorrect . $c_1-c_2$ are in series , though the entire branch is in parallel with $c_3$ and its opposite branch . parallel is when the current is split , while series is when current is constant . series is a single wire with an in and an out , with components along the wire . parallel is when there are many wires with their ends twisted together . current goes in/out through the twisted ends . you seem to have grasped that , though : ) out of curiosity , which book is this ? ( the capacitors look like they are from resnick )
there is a prescription by deutsch for the quantum mechanics of closed timelike curves . it works on the level of density states , instead of hilbert space states . given his prescription , he showed that a fixed point solution always exists no matter what the initial conditions are . however , this solution is not unique in general . also , pure states can be converted into mixed states . not only that , the solution is not even linear in the initial density state . it has been shown by aaronson and watrous that this prescription allows time travelling computers to solve pspace-complete problems . there is another prescription based upon post-selection , as expounded by seth lloyd where you would observe the same decay time . this prescription has the drawback that not every initial condition admits a solution . it has been shown that time-travelling computers can only solve pp-complete problems . both prescriptions violate unitarity . in fact , the density matrix evolves nonlinearly in both prescriptions . this nonlinearity has the potential to wreak havoc upon quantum mechanics . the difference between both prescriptions can be seen most clearly with the grandfather paradox . to simplify matters , consider a qubit which can only take on two states , $| 0 \rangle$ and $| 1 \rangle$ . let 's also suppose this qubit is sent around on a closed timelike curve and loops back onto itself . during a cycle around the loop , the qubit is flipped . now classically , there clearly is not any consistent solution . however , according to deutsch 's prescription , the mixed density state $\begin{pmatrix} \frac{1}{2} and a\\ a and \frac{1}{2} \end{pmatrix}$ where $a$ is a real number between $-\frac{1}{2}$ and $\frac{1}{2}$ is a fixed-point solution . he chooses to interpret this using the many worlds interpretation as follows ; say the qubit starts off with a value of $0$ in one world . after a loop , it ends up with a value of $1$ in a different parallel universe . according to lloyd 's prescription , on the other hand , there is no solution at all ! however , for the example you presented , both prescriptions will give the same answer , namely the time traveller will observe the same decay time both times around . this is because the nuclear decay is not an integral part of the closed timelike loop . to see this , suppose we have the original prepared unstable particle , plus some experimental apparatus with a clock and a clock pointer which will be set to the time of the nuclear decay . after waiting for some time much longer than the half-life , the clock pointer will end up in the state $\sqrt{k}\int^\infty_0 dt\ , e^{-kt/2} | t \rangle$ . the whole point is , it does not make any difference if the time traveller does not have direct access to the unstable particle , but only to the clock pointer , and it does not make any difference either if the clock pointer is prepared outside the time machine , i.e. is part of the initial conditions . of course , it might turn out the correct prescription is none of the above . or it might also turn out that closed timelike curves are absolutely forbidden in a complete theory of quantum gravity . yet another possibility might be closed timelike curves are allowed , but a complete theory of quantum gravity somehow manages to preserve unitarity , just as it presumably preserves unitarity in evaporating black holes .
energy is conserved so it can not be created or destroyed . all we can do is change energy from one form to another . in your example we are changing the potential energy of the mass $m$ into kinetic energy . the increase in kinetic energy must be equal to the decrease otherwise energy would not have been conserved . by an external force i assume you mean some third party outside the system . to give a slightly ridiculous example this could be me standing well away from the earth and the mass and poking the mass with a long pole to accelerate it . in this case the energy of the earth + mass would not be conserved , but also my energy would not be conserved . however the energy of the earth , the mass and me would be conserved . the distinction between internal and external forces is a bit artificial because all systems are closed and all forces are internal if you look on a big enough scale .
general remarks . in general , you cannot " derive " a representation of a given group $g$ on the objects you are considering , but there are some really standard definitions of certain group representations which are given special names like " scalar , " " vector , " and so on . however , given the representation of a lie group $g$ , this induces a representation of its lie algebra $\mathfrak g$ , and determining an explicit formula for this lie algebra representation is precisely what we do when we find the so-called " infinitesimal generators " of the corresponding group representation . an example . $\mathrm{so} ( 2 ) $ let $c^\infty ( \mathbb r^2 ) $ denote the vector space of smooth functions on the plane $\mathbb r^2$ . the scalar representation $\rho$ of $\mathrm{so} ( 2 ) $ acting on $c^\infty ( \mathbb r^2 ) $ is defined as \begin{align} ( \rho_0 ( r ) \phi ) ( \mathbf x ) = \phi ( r^{-1}\mathbf x ) . \end{align} for each $\phi\in c^\infty ( \mathbb r^2 ) $ and for each $r\in\mathrm{so} ( 2 ) $ . what the heck is going on here ? well , notice that this can also be written as follows : \begin{align} ( \rho_0 ( r ) \phi ) ( r\mathbf x ) = \phi ( \mathbf x ) \end{align} so this definition encapsulates the intuitive idea that the transformed field $\rho ( r ) \phi$ evaluated at the transformed point $r\mathbf x$ agrees with the untransformed field $\phi$ evaluated at the untransformed point $\mathbf x$ . in physics , it is common to see " primed " notations for the transformed field and transformed point ; \begin{align} \rho_0 ( r ) \phi = \phi ' , \qquad r\mathbf x = \mathbf x ' \end{align} in which case the definition of the scalar representation can be written as \begin{align} \phi' ( \mathbf x' ) = \phi ( \mathbf x ) \end{align} this probably looks familiar . so basically the " invariance " that is happening is that the value of the field does not change provided the transformed field is evaluated at the transformed point . infinitesimal generators . to find the infinitesimal generators of a given representation , we are really just trying to find a certain representation of the lie algebra of the group . this lie group representation $\rho$ naturally induces a lie algebra representation $\bar \rho$ as follows : \begin{align} \bar \rho ( x ) = \frac{d}{dt}\rho ( e^{tx} ) \big|_{t=0} \end{align} so , for the $\mathrm{so} ( 2 ) $ example , we know that the lie algebra $\mathfrak{so} ( 2 ) $ is generated by the single element \begin{align} j = \begin{pmatrix} 0 and -1 \\ 1 and 0 \\ \end{pmatrix} , \end{align} and we can determine how this element is represented in representation induced by the scalar representation defined above as follows : \begin{align} ( \bar\rho_0 ( j ) \phi ) ( \mathbf x ) and = \frac{d}{dt}\phi ( e^{-tj}\mathbf x ) \big|_{t=0} \\ and = \frac{d}{dt}\phi ( x-ty , y+tx ) \big|_{t=0} \\ and = -y\partial_x\phi ( x , y ) + x\partial_y\phi ( x , y ) \\ and = ( -y\partial_x + x\partial_y ) \phi ( \mathbf x ) \end{align} in other words , in the scalar representation , the generator of rotations on the plane is represented by a differential operator ; \begin{align} \bar\phi_0 ( j ) = -y\partial_x + x\partial_y . \end{align} this same procedure can be extended to find infinitesimal generators of other representations as well , like the vector representation $\rho_1$ of $\mathrm{so} ( 2 ) $ which is defined to act on vector fields $\mathbf v$ on the plane as follows : \begin{align} ( \rho_1 ( r ) \mathbf v ) ( \mathbf x ) = r\mathbf v ( r^{-1}\mathbf x ) \end{align} by the way , you might find the following links interesting and/or helpful as well : tensor operators representations of lie algebras in physics differential realizations of certain algebras generators of poincare groups idea of covering group unitary spacetime translation operator rigorous underpinnings of infinitesimals in physics
the answer is : anti-clockwise ( for a right-handed this is contrary to what seems ( to me anyway ) to be intuitively obvious . the mechanism at work is the same as two adjacent rotating gears - they rotate in opposite directions . indeed , this phenomenon is known as the ' gear effect ' . the club face rotates clockwise ( opens ) which imparts anti-clockwise spin to the ball . the question should have been put better because this effect is only noticeable if the centre of mass of the club is well behind the face ( like on a driver or wood ) . this is because the rotation of the club around the com causes the face to move right , pushing the back of the ball right , spinning it anti-clockwise .
re your edited question , this is just simple spherical geometry . if the initial separation is $d$ then the separation at time $t$ is $d \cos ( vt/r ) $ , where $r$ is the radius of the sphere , $v$ is the vehicle speed and $t$ is time . the diagram shows a cross section through the poles . the vehicle is driving north at a velocity v , so the distance it drives in a time $t$ is just $s = vt$ , so the angle $\theta$ is : $$ \theta = \frac{vt}{2\pi r} 2\pi = \frac{vt}{r} $$ suppose the vehicles start out at a separation $d$ . the angular separation along the equator $\delta\phi$ is : $$ \delta\phi = \frac{d}{2\pi r} 2\pi = \frac{d}{r} $$ as the two vehicles drive north the angular separation $\phi$ does not change , so we just need to calculate the circumference of the line of latitude at the angle $\theta$ , $c_\theta$ , and the separation will be $c_\theta\tfrac{\delta\phi}{2\pi}$ . $$c_\theta = 2\pi r \cos\theta $$ so the separation $s$ is : $$\begin{align} s and = 2\pi r \cos\theta \frac{\delta\phi}{2\pi} \\ and = 2\pi r \cos\left ( \frac{vt}{r}\right ) \frac{d/r}{2\pi} \\ and = d \cos\left ( \frac{vt}{r} \right ) \end{align}$$
your understanding that a change in rotational velocity requires a net torque ( calculated around any convenient axis ) is correct . the only forces acting are gravity , the normal force from the ramp , and the force of friction between the ramp and the block . choosing an axis across the ramp through the center of mass of the block makes life simpler ; the force of gravity ( all of it ) acts through this point , and thus exerts no torque . next , consider the friction force , that is given by $mg \sin \theta$ . this acts up the ramp , at the point of contact of the ramp and block , and thus exerts a torque around the center of mass depending on the dimensions of the block . ( tall blocks are more tippy than squat blocks ) in this case , the torque is a clockwise one . in order for there to be no rotation , there must be a counter-clockwise torque about the cofm . this means that the diagram is wrong ; the normal force no longer is distributed uniformly along the base , and cannot be ignored as acting through the cofm . in fact , the normal force shifts towards the front of the block as the ramp slope increases , and when it reaches the front corner , the rear corner lifts and the block tumbles . . .
this problem originated with passengers using electronics ( they call them ped 's - portable electronic devices ) during flight . while all consumer electronics have to be qualified by a regulatory body ( fcc , etc . ) to prove they do not emit harmful interference , this does not mean they emit no interference especially to high gain sensitive navigation equipment . " the first national committee that investigated interference by passenger-carried peds was created in the early 1960s . its activities were initiated by a report that a passenger-operated portable fm broadcast receiver caused an airplane navigation system to indicate that the airplane was off course by more than 10 deg . the airplane was actually on course and , when the portable receiver was turned off , the malfunction ceased . a final report from this committee , rtca do-119 , was issued in 1963 and resulted in the revision of the faa federal aviation regulations ( far ) by establishing a new rule ( far 91.19 , now 91.21 ) , which states that the responsibility for ensuring that peds will not cause interference with airplane navigation or communication systems remained with the operator of the airplane . " -- from boeing this reference also has some incidents which i hightlight here : 1995 , 737 a passenger laptop computer was reported to cause autopilot disconnects during cruise . 1996/1997 , 767 over a period of eight months , boeing received five reports on interference with various navigation equipment ( uncommanded rolls , displays blanking , flight management computer [ fmc ] / autopilot/standby altimeter inoperative , and autopilot disconnects ) caused by passenger operation of a popular handheld electronic game device . 1998 , 747 a passenger’s palmtop computer was reported to cause the airplane to initiate a shallow bank turn . one minute after turning the ped off , the airplane returned to " on course . " now this " all electronics off " rule has become legacy and broadly applied . there are a mix of opinions about how relevant it is today . but the fact remains that while manufacturers do a decent job of limiting rfi ( radio frequency interference ) there is still a good chance that the very sensitive radios on board aircrafts could have a problem with one of the rfi emissions . and due to the nature of the aircraft radios it is almost impossible to test every possible interference scenario . these radios do a lot of frequency mixing , amplification and filtering . if you have ever done a detailed spurious analysis on a mixer , you will know that each time you mix a single you create a tremendous number of possible interference problems . there has been recent studies showing rfi issues but with a very low probability ( 1:1,000,000 ) of harmful interference . ( sorry , you have to pay $63 to read the report ) . so for everyone 's safety the " all electronics off " rule during critical takeoff/landing has remained in effect . for a good overview , i suggest you read the first link in my answer from boeing .
dust . the light in this band originates from un-resolved stars and other material that lie within the galactic plane . dark regions within the band , such as the great rift and the coalsack , correspond to areas where light from distant stars is blocked by interstellar dust . cosmic dust consists of small particles of matter from the periodic table which have the usual properties of matter and can be detected in various ways , as the article linked extensively discusses .
there is no difference in physics between a propeller and fan . in english the distinction is probably that a fan moves air while the propeller moves the vehicle through the stationary air ( or water ) . edit : although aerospace engineers call the front stage of a turbine engine a " fan " . there is a difference between an aerodynamic regime at low speeds and higher pressures where the air behaves as a fluid , and low pressure regimes where it is purely mechanical ' billard balls ' hitting flat blades - as in a high vacuum turbo molecular pump
if you mean what takes place in each transformation between different states of matter then you must first understand the components of each state . a solid : •the particles cannot move , only vibrate • the bonds that hold particles close together will be very tight • the solid will not be able to be deformed due to the fact , that the particles cannot get any closer , and the object will denser than a gas or liquid . a liquid : • the particles can move past one another but are still held so that they have a fixed volume • the particles are held together by loose bonds and they can move . • the liquid can be moulded to a shape and be deformed , it is not as dense as a solid but denser than a gas . a gas : • the particles can go anywhere and are not held by any bonds and can take on any form . • they can be moulded into any shape . •the gas is not as dense as liquid or solid . plasma : •the particles are separate and the protons are apart from the electrons ( ionization ) . a picture is below of the states of matter : there is a graph to show changing states of matter below : the graphs shows that when there is a mixture of states of matter the particles cannot heat up anymore due to the fact , certain states of matter cannot be above or below certain temperatures . when freezing water specifically , ( at a particle level ) , the bonds strengthen and the particles become locked into place , this results in ice which is solid and cannot be deformed . at an atomic level though , water is different to all other liquids , as it expands . this happens when hydrogen particles join together they must have a certain formation . this is because hydrogen is made up of 2 positive atoms and one negative . the positive must pair with a negative , resulting in a gap between particles , this is shown in the picture below : hope that helps .
meteoroids come in a very large range of sizes , from specks of dust to many-kilometer-wide boulders . explosions like that of the chelyabinsk meteor are only found meteors that are larger than a few meters in size but smaller than a kilometer . though the details are argued endlessly by those who study such phenomena ( it is very hard to get good data when you do not know when/where the next meteor will occur ) , the following qualitative description gets much of the important ideas across . the basic idea is that the enormous entry velocity into the atmosphere ( on the order of $15\ \mathrm{km/s}$ ) places the object under quite a lot of stress . the headwind places a very large pressure in front of it , with comparatively little pressure behind or to the sides . if the pressure builds up too much , the meteor will fragment , with pieces distributing themselves laterally . this is known as the " pancake effect . " as a result , the collection of smaller pieces has a larger front-facing surface area , causing even more stresses to build up . in very short order , a runaway fragmentation cascade disintegrates the meteor , depositing much of its kinetic energy into the air all at once . this is discussed in [ 1 ] in relation to the tunguska event . that paper also gives some important equations governing this process . in particular , the drag force has magnitude $$ f_\mathrm{drag} = \frac{1}{2} c_\mathrm{d} \rho_\mathrm{air} a v^2 , $$ where $c_\mathrm{d} \sim 1$ is the geometric drag coefficient , $\rho_\mathrm{air}$ is the density of air , $a$ is the meteor 's cross-sectional area , and $v$ is its velocity . also , the change in mass due to ablation is $$ \dot{m}_\text{ablation} = -\frac{1}{2q} c_\mathrm{h} \rho_\mathrm{air} a v^3 , $$ where $q$ is the heat of ablation ( similar to the heat of vaporization ) of the material and $c_\mathrm{h}$ is the heat transfer coefficient . since the mass-loss rate scales as $a \sim m^{2/3}$ , sublinearly with mass , smaller objects will entirely ablate faster , setting a lower limit on the size of a meteor that can undergo catastrophic fragmentation before being calmly ablated . meteors that are too big , on the other hand , will cross the depth of the atmosphere and crash into the ground before a pressure wave ( traveling at the speed of sound in the solid ) can even get from the front to the back of the object . there simply is not time for pressure-induced fragmentation of the entire object to occur , meaning the kinetic energy is not dissipated until the entire body slams into earth . [ 1 ] chyba et al . 1993 . " the 1908 tunguska explosion : atmospheric disruption of a stony asteroid . " ( link , pdf )
it seems that it is an electron microscope image . it does not use photons , but electrons . so the image has been coloured digitaly .
i ) op essentially asked ( v1 ) : if two lagrangian densities ${\cal l}$ and $\tilde{\cal l}$ have the same eqs . of motions , must they necessarily differ by a total divergence ? answer : no , one e.g. can always multiply a lagrangian density ${\cal l}$ with a constant factor $\tilde{\cal l}=\lambda {\cal l}$ different from one $\lambda\neq 1$ without altering the el equations , but the difference $$\tilde{\cal l}-{\cal l}= ( \lambda-1 ) {\cal l}$$ is not a total divergence if ${\cal l}$ is not . ii ) op essentially asked ( v4 ) : if el equations are trivially satisfied for all field configurations , is the lagrangian density ${\cal l}$ necessarily a total divergence ? answer : yes , modulo topological obstructions in field configuration space . this follows from an algebraic poincare lemma of the so-called bi-variational complex , see e.g. ref . 1 . we should mention that an elementary follow-your-nose-type proof exists for lagrangians of the form $l ( q^i , \dot{q}^j , t ) $ without higher-order derivatives , see e.g. ref . 2 . we stress that the proof-technique of ref . 2 does not work in the presence of higher-order derivatives or in the case of field theory . references : g . barnich , f . brandt and m . henneaux , local brst cohomology in gauge theories , phys . rep . 338 ( 2000 ) 439 , arxiv:hep-th/0002245 . j.v. jose and e.j. saletan , classical dynamics : a contemporary approach , 1998 ; section 2.2.2 , p . 67 .
as djbunk mentions the delta function is symmetric $$\delta ( x ) =\delta ( -x ) $$ so you certainly have $$\delta ( x-x' ) =\delta ( x'-x ) . $$ but you should also know that in general we have $$\langle a | b \rangle = \langle b | a \rangle^* $$ and since in this case the inner product is real , you will also have $$\langle x | x ' \rangle = \langle x ' | x \rangle . $$ so it does not matter which way you write the delta function or the inner product .
as was stated in the comments , your question can not be answered precisely . here 's my reactions , i hope it could help you : from a biological ( evolution ) point of view , a creature can be considered as " good " or " bad " only in a given environment . it means that there can not be a " best creature " , because you will always find environments in which the " best " becomes " weak " . from a physical point of view , you also need to precise the task that you want to evaluate . strength and agility is very vague . in my opinion , the human body can be outclassed by other species in any precise physical task . so maybe your question was more about the impression that the human body is the best design when you consider all the activities of human beings ? or a selection of these activities , like in olympic games ? in this case , it is not a surprise because : that is how evolution works ( the design was selected based on the activities necessary to survive , so we are not bad for these activities in our environment ) and because human beings prefer doing things that their body enables them to do ( that seems stupid i know , but would you add ' flying ' or ' digging ' without tools in the olympic games ? )
as you described , we substitute $y=0$ and $x=r$ into the trajectory equation : $$0=h+r\tan{\theta}-r^2\frac{g}{2u^2}\sec^2\theta . \tag{1}$$ then , differentiating with respect to $\theta$ and setting $\frac{dr}{d\theta}=0$: $$0=r_{max}\sec^2\theta-r_{max}^2\frac{g}{2u^2}2\sec^2\theta\tan\theta , $$ which simplifies to $$r_{max}=\frac{u^2}{g}\cot\theta . \tag{2}$$ solving $ ( 1 ) $ and $ ( 2 ) $ will yield the desired expressions for $\theta$ and $r_{max}$ .
briefly : because the moon 's orbit " wobbles " up and down , so it is not always in the plane of the earth 's orbit around the sun . there is a 2d plane you can form from the ellipse of the earth 's orbit and the sun . this plane is known as the ecliptic . the moon 's orbit is not exactly in the ecliptic at all times ; see this ( slightly overcomplicated ) picture from wikipedia : so the moon has got its own orbital plane , separate from the ecliptic . this orbital plane " wobbles " around - there are two points of the lunar orbital plane which intercept the ecliptic , known as the " nodes , " and these nodes rotate around the earth periodically . the moon will only pass right in front of the sun and cause an eclipse when one of the two nodes is along the line of sight to the sun and right in the ecliptic plane ( hence the name " ecliptic" ) .
i finally found some prior art . this object has been introduced as the " husimi matrix " by harriman " some properties of the husimi function " harriman , john e . , the journal of chemical physics , 88 , 6399-6408 ( 1988 ) , http://dx.doi.org/10.1063/1.454477 and briefly referred to by morrison and parr " approximate density matrices and husimi functions using the maximum entropy formulation with constraints " morrison , robert c . and parr , robert g . , international journal of quantum chemistry , 39: 823–837 http://dx.doi.org/10.1002/qua.560390607 the treatment was fairly basic . from what i can tell , harriman primarily introduced the husimi matrix to highlight an analogy with density matrices ( since you can use it as the kernel of an integral operator ) . morrison and parr use it for something related to calculating a density matrix as a maximum entropy husimi function , but i do not really understand . i do not believe anyone has explored the relationship to decoherence .
there are two phenomena present diffusion , which happens due to inhomogeneity in concentration . particles " want to " go from areas of higher concentration to the lower ones . one can write this in the form of diffusion current $$j_{diff} ( x ) = - d \nabla \rho ( x ) $$ where $\rho ( x ) $ is the concentration . this expression is known as fick 's law but it is actually just the standard linear response to inhomogeneities . drift , which is the terminal velocity particles attain due to presence of some force . e.g. the drift one can observe for balls falling in viscous liquid . one can write $$j_{drift} = \rho ( x ) v ( x ) = \rho ( x ) b f ( x ) = -b \rho ( x ) \nabla u ( x ) $$ from the requirement of equilibrium we have that $j_{diff} + j_{drift} = 0$ and from boltzmann statistics we can obtain the concentration $\rho ( x ) \sim \exp ( -{u ( x ) \over k_b t} ) $ . putting it all together we get $$0 = - d \nabla \rho ( x ) - b \rho ( x ) \nabla u ( x ) = - \nabla u ( x ) \rho ( x ) ( -{d \over k_b t} + b ) $$ and we can see the required relation in the last term .
the clock placed on the rotating ring would tick slower by a factor of $\sqrt{1-\omega^2 r^2/c^2}$ , where $\omega$ is the angular velocity of the ring , $r$ is the radius of the ring and $c$ is the speed of light .
i do not know a good answer to your first question ( i would be interested in a good text for that myself ) , but i can answer the second . it is easier to explain if we temporarily imagine $\phi$ represents the concentration of some dye made up of little particles suspended in the fluid . the convective term ( aka advective term ) is transport of $\phi$ due to the fact that the fluid is moving : a single " particle " of $\phi$ will tend move around according to the velocity of the fluid around it . the diffusive term , on the other hand , represents the fact that the dye tends to spread out , regardless of the motion of the fluid , because each particle is undergoing brownian motion . so if you were moving along at the same velocity as the fluid you would see a small spot of dye tend to become more and more blurred over time . for quantities like energy and momentum the diffusion happens for a slightly different reason ( transfer of the quantity between fluid molecules when they collide ) but the principle is the same . the property is transported along with the fluid 's bulk velocity ( convective term ) but also tends to spread out and become blurred of its own accord ( diffusive term ) .
the cartesian integral you have set up is correct . now do the integrals : $$\begin{align*} v and =\frac{\sigma}{\pi\epsilon_0}\int_0^{a/2}dy\ ; \left . \sinh^{-1}\frac{x}{y}\right|_0^{a/2}\\ and =\frac{\sigma}{\pi\epsilon_0}\int_0^{a/2}dy\ ; \sinh^{-1}\frac{a}{2y}\\ and =\frac{\sigma}{\pi\epsilon_0}\int_{\infty}^{1}-\frac{a}{2u^2}du\ ; \sinh^{-1}u\\ and =\frac{\sigma a}{2\pi\epsilon_0}\int_{1}^{\infty}du\ ; \frac{\sinh^{-1}u}{u^2}\\ and =\frac{\sigma a}{\pi\epsilon_0}\sinh^{-1} ( 1 ) \end{align*}$$
ok , so i realized that $\mu$ is the inverse of the compton 's wavelength for a particle . $$ \lambda_c := \frac{\hbar}{mc} $$ from f . schwabl 's " quantum mechanics": " compton wavelength ( . . . ) can be interpreted as the de broglie wavelength of highly relativistic electrons " ( p . 135 ) . that is the sort of thing i was looking for , but i am not sure if my question was well understood .
compressive strength to density ratio is not the most critical factor for vacuum balloons , as the most dangerous failure mode for vacuum balloons is buckling . the elasticity modulus to density squared ratio is more important . this issue is considered in my us patent application 20070001053 ( 11/517915 ) ( written together with my coauthor ) - http://akhmeteli.org/wp-content/uploads/2011/08/vacuum_balloons_cip.pdf . it is shown that no homogeneous shell can be both light enough to float in air and strong enough to withstand atmospheric pressure . however , finite element analysis shows that spherical sandwich structures made of commercially available materials can meet these requirements . the face sheets can be made of beryllium , boron carbide , or some other materials ; the core can be made of aluminum honeycomb or some other materials . however , manufacturing of such vacuum balloons is not easy and has not been done yet .
if two particles are close to each other , there is more space for the rest of the particles to move . this gives rise to an effective entropic attraction between the particles because when looking at two particles for different separations while " tracing out " over the degrees of freedom of the rest of the system , the entropy of the rest is higher when the two tagged particles you are looking at are close to each other . in fact at high density , you should also observe oscillations in the g ( r ) and not a single peack . the width of the bumps in these oscillations is related to the particle size .
since the cable is not moving horizontally you know the horizontal component of tension is the same at both ends . the total tension is the horizontal component divided by the cosine of the angle . so the ratio is the tensions is the ratio of the cosines . since you know the shape of the curve you should be able to take it from here . update the general equation for a catenary ( with lowest point at x=0 ) is $$y = a \cosh \frac{x}{a}$$ where $$a = \frac{h}{w}\\ h = \text{horizontal tension}\\ w = \text{weight per unit length}$$ for a given horizontal distance and vertical displacement , we have to figure out the location of the lowest point and the tension - two equations , two unknowns . from wikipedia . org/wiki/catenary#determining_parameters : given $s$ , $v$ , and $h$ , then $a$ can be solved for numerically : $$\sqrt{s^2 - v^2} = 2a \sinh \frac{h}{2a} , a &gt ; 0$$ where $h$ is the horizontal distance between ends , $v$ is the vertical distance between ends , $s$ is the length of the cable , and $a$ is the y coordinate of the lowest point . next , we just need to find the position of the lowest point relative to the ends . to get the actual locations of $x_1$ and $x_2$ ( the horizontal distances from the lowest point to the the left and right ends , respectively ) you now have to solve $$\begin{align}\\ v and = a ( \cosh \frac{x_2}{a} - \cosh \frac{x_1}{a} ) \\ h and = x_2 + x_1 \tag1 \\ v and = a\left ( \cosh \frac{x_2}{a} - \cosh \frac{x_2-h}{a}\right ) \tag2 \end{align}$$ solve ( 2 ) for $x_2$ then substitute into ( 1 ) to get $x_1$ finally , the ratio of tensions comes from the ratio of cosines of the angle at the point of suspension : $$\frac{t_2}{t_1} = \frac{\cos\theta_1}{\cos\theta_2}$$ we know the tangent at $x$ is given by $$tan\ , \theta = \frac{dy}{dx} = \sinh \frac{x}{a}$$ combine with the trig identity $$cos\ , \theta = \frac{1}{\sqrt{1+\tan^2\theta}}$$ you finally obtain $$\frac{t_1}{t_2} = \sqrt{\frac{1+\sinh^2\frac{x_2}{a}}{1+\sinh^2\frac{x_1}{a}}}$$
imho , the current use of the word helicities happens only when one is looking at some representation of $su ( 2 ) $ . 1 ) now , a first point of view is to try to go back to representations of $ \otimes^n su ( 2 ) $ , when working with representations of $so ( d-2 ) $ . in the best case , you will have different kind of " helicities " . suppose we work with $d=6$ , so spin-$1$ massless particles are in the fundamental representation of $so ( 4 ) $ , which i write $4$ . in term of $su ( 2 ) \otimes su ( 2 ) $ representations , this gives : $4 \to ( 2,2 ) $ [ here i write the number of states in the representations ] so , multiplying photon representations gives $4 \times 4 \to ( 2,2 ) \times ( 2,2 ) = ( 3,3 ) + ( 1,3 ) + ( 3,1 ) + ( 1,1 ) $ $ ( 3,3 ) $ is the graviton traceless symmetric representation that we are looking for , with $9 = \dfrac{6 ( 6-3 ) }{2}$ so here photons have " helicities " $ ( \pm 1 , \pm 1 ) $ , while gravitons have " helicities " $ ( 0 \pm 1 , 0 \pm 1 ) $ gravitons states could be written from photons states , for instance : $ ( +1 , +1 ) = ( +1 , +1 ) ( +1 , +1 ) $ $ ( -1 , -1 ) = ( -1 , -1 ) ( -1 , -1 ) $ $ ( +1 , -1 ) = ( +1 , -1 ) ( +1 , -1 ) $ $ ( -1 , +1 ) = ( -1 , +1 ) ( -1 , +1 ) $ $ ( +1,0 ) = \frac{1}{\sqrt 2} [ ( +1 , +1 ) ( +1 , -1 ) + ( +1 , -1 ) ( +1 , +1 ) ] $ $ ( 0,0 ) = \frac{1}{ 2} [ ( +1 , -1 ) ( +1 , -1 ) + ( +1 , -1 ) ( -1 , +1 ) + ( -1 , +1 ) ( +1 , -1 ) + ( -1 , +1 ) ( -1 , +1 ) ] $ and so on . 2 ) a second point of view is to work directly with the representations of $so ( d-2 ) $ let us use this ( french ) lie group on-line tool ( université de poitiers ) . choose $d3 ( so ( 6 ) ) $ , " tensor product decomposition " ( then " proceed" ) . let 's type $ ( 1,0,0 ) \times ( 1,0,0 ) $ , ( then " start" ) , and you get $ ( 2,0,0 ) + ( 0,1,1 ) + ( 0,0,0 ) $ . here we are working with dynkin indices . so $ ( 2,0,0 ) $ is the graviton symmetric traceless representation , and it is also the highest weight state of the representation . you may get the other states of the representation by substracting with the simple roots you may directly from the cartan matrix of $d3= so ( 6 ) = su ( 4 ) $ ( they are the lines of the cartan matrix ) until you get no positive number . here the simple roots are $ ( 2 , -1,0 ) , ( -1,2 , -1 ) , ( 0 , -1,2 ) $ . so , for instance , substracting the first root , you get the state $ ( 2,0,0 ) - ( 2 , -1,0 ) = ( 0,1,0 ) $ , and so on . so each state for the gravitons ( or the photons ) could be represented by $3$ integers , so it is an alternative way to classify the states into a given representation .
this is the way in which all physical theories get formulated--- you first acquire certainty regarding the behavior of many special cases you have some experimental data or theoretical insight about , then you try to formulate a precise theory which extends these heuristic laws to a precise understanding , and when you succeed in matching the heuristic laws ( when they apply ) and you can predict everything consistently and correctly , you are done . in fundamental physics , one has a pretty solid understanding of phenomena which are not quantum gravitational , because we have a precise fundamental theory of relativistic quantum fields . so the most significant things that are not fully understood at the precise level are the class of insights deriving from hawking radiation and black hole classical behavior . these are semi heuristic , because there are puzzles that are not yet fully resolved within string theory . this class includes : the near horizon behavior of semiclassical black holes : an observer falling into a black hole sees nothing special when crossing the horizon , and this has not been rigorously demonstrated in string theory , it is only rigorously true classically . now some people claim that it can not work in quantum gravity , that black holes are " firewalls " . i read the arguments , and i find them uncompelling , because they mix assumptions about the semiclassical behavior at late times with full quantum observations on the hawking radiation which are restricted when you measure the semiclassical state at late times , so the argument smells fishy and does not stop smelling fishy even with later clarifications , although understanding what is going wrong is important , and this is at the heuristic level , because we can not reconstruct black hole interiors completely from quantum gravity scattering data . the related holographic principle and black hole complementarity : this is also heuristic for semiclassical thermal black holes , although it is precise in ads/cft , for certain extremal black holes . cosmological horizon entropy : the entropy of the cosmological horizon is not even in-principle understood in string theory , and it is a major clue to understanding how to do quantum gravity in desitter space , because it is understood at the heuristic level--- it is the same as black hole entropy . rindler horizons : the behavior of strings on a rindler background is nontrivial , even though this is just minkowski space . quantum fields are completely understood on rindler , but string theory on rindler is harder , because you do not have an s-matrix ( everything falls into the rindler horizon ) . for string theory , we only have heuristic guidelines regarding compactification and susy breaking . these heuristics are summarized in guidelines about what topologies and matter configurations give rise to which gauge fields and representations , and where one should expect to find the standard model . these are relatively well understood , since there are many explored vacua , but there are always surprises . in cases where we know the fundamental laws without serious doubt , there are still cases where we understand things only at the heuristic level . the following are in qcd : confinement and regge theory : we know the qcd confines to regge trajectories for mesons and topological soliton baryons , but this formulation is not mathematically linked to qcd by any rigorous path . pomerons : this is related--- we know that high energy scattering is dominated by pomeron exchange , and this is heuristic only , because we can not relate this to qcd high energy diffractive scattering except in certain regimes which are not completely diffractive . chiral perturbation theory : again related , this is the low-energy approximation to qcd , and the parameters are from assumptions on low energy condensates . i suppose i should not include this , because you can extract chiral data from lattice data in principle exactly , but i had something else in mind for what constitutes a full understanding--- it would mean that any chiral configuration can be mapped to a qcd configuration , and you could do the path integral for qcd in two steps--- as a path integral over long wavelength chiral configurations plus an additional path integeral over qcd in the given chiral background . nobody did this , although it should not be hard . instanton fluid : there is a class of semiheuristic models that give the qcd vacuum as a dense instanton fluid , and this is not 100% understood . it is essentially the same problem as before--- condensates and confinement . quark condensates : there are condensate models where we can semi-heuristically calculate the effects on hadrons using the svz sum-rules ( qcd sum rules ) , but the condensate values again are not derived from qcd , although they can be measured from experiment . in principle , lattice gives them , but this is not enough--- you want to know the values and effects with more insight . there are no doubts qcd resolves these questions , but the exact best way to make these heuristic things precise is unclear . there are condensed matter systems where the understanding is heuristic in the same way--- the most famous and the one i like best is probably : hightc : the condensate state of the hightc superconductor can be described phenomenologically using a d-wave superconducting condensate . getting this d-wave from the fundamental interactions has not been done in a universally convincing way , although there is no doubt that the fundamental theory will do it . there are many other such things , this is usually all the open problems people work on . historically , if you look at any phenomenon that was understood , it was understood heuristically before it was understood precisely , and learning the heuristic stuff is an important prerequisite for getting certainty about the final explanation , because physical theories are evolved by common sense , they do not emerge fully formed from nothing .
you have 2 kinds of transformation to help you to find the ope $ ( 2.4.14 ) $ , dilatations $ ( 2.4.13 ) $ and translations . for each of these transformations , we have to identify infinitesimal transformations quantities $v ( z ) $ defined by : $z ' = z+\epsilon v ( z ) $ and the infinitesimal modification of the fields $\delta a ( z , \bar z ) $ . the current being given by $j ( z ) = i v ( z ) t ( z ) $ $ ( 2.4.5 ) $ , we are going to use ward identities $ ( 2.3.11 ) $ : $$res_{z \rightarrow z_0} ( j ( z ) a ( z_0 , \bar z_0 ) ) + \bar res_{\bar z \rightarrow \bar z_0} ( \bar j ( z ) a ( z_0 , \bar z_0 ) ) = \frac{1}{i \epsilon} \delta a ( z_0 , \bar z_0 ) $$ we suppose an ope of the form : $t ( z ) a ( 0,0 ) \sim \cdots + \frac{a}{z^2}a ( 0,0 ) + \frac{b}{z} \partial a ( 0,0 ) +\cdots$ , where $a$ and $b$ are to be determined . dilatations the infinitesimal transformation corresponding to $z'= \zeta z$ , is , using $\zeta = 1 + \epsilon$ , $z ' = z +\epsilon z$ , so here $v ( z ) = z$ ; and $\bar v ( z ) = \bar z$ the transformation of fields is $a ( z ' , \bar z' ) = \zeta^{-h}\bar \zeta^{- \tilde h} a ( z , \bar z ) $ , this corresponds to a infinitesimal transformation $\delta a ( z , \bar z ) = - \epsilon h~ a ( z , \bar z ) - \bar \epsilon \tilde h~ a ( z , \bar z ) $ so , appying ward identity , and only keeping the holomorphic part , we see that : $$res_{z \rightarrow 0} ( i ~z ~t ( z ) ~a ( 0,0 ) ) = \frac{1}{i ~\epsilon} ( - \epsilon h a ( 0,0 ) ) $$ this means that $t ( z ) ~a ( 0,0 ) $ has a component $\frac {h}{z^2}a ( 0,0 ) $ , in order to have a pole with the correct residue . translations here $v ( z ) = v$ = constant ; and $\delta a = - \epsilon ( v\partial a + \bar v \bar \partial a ) $ . so , applying the ward identity , keeping the holomorphic part , we get : $$res_{z \rightarrow 0} ( i ~v ~t ( z ) ~a ( 0,0 ) ) = \frac{1}{i ~\epsilon} ( - \epsilon v \partial a ( 0,0 ) ) $$ this means that $t ( z ) ~a ( 0,0 ) $ has a component $\frac {1}{z}a ( 0,0 ) $ , in order to have a pole with the correct residue . so , finally : $$t ( z ) a ( 0,0 ) \sim \cdots + \frac{h}{z^2}a ( 0,0 ) + \frac{1}{z} \partial a ( 0,0 ) +\cdots$$ of course , an equivalent demonstration is valid for the anti-homorphic part .
obviously , the coil will move . when we are moving the bar magnet toward the coil a current will flow through the coil in a direction to reduce the rate of change of flux through it . now you can think the coil as a tiny magnetic dipole of dipole moment m =i a ( or a small bar magnet for your convenience ) . and this will move in the presence of other magnet if it is freely hanging . you may find this wikipedia article useful .
what exactly is a boson ? a boson is a particle whose spin ( = intrinsic angular momentum ) is an integer number . for example , the photon ( the particle that is responsible for the electromagnetic force ) is a boson . contrast this with a fermion , such as the electron , whose spin is a half integer . in everyday terms , the bosons are the microscopic particles that make up the forces : electromagnetism and gravity , as well as the weak and strong forces . fermions are the particles that make up matter : protons , neutrons , and electrons . the fact that forces come from bosons while matter comes from fermions is a very deep observation , and is related ( at least in part ) to something called the ' spin-statistics ' theorem , but let me not go into that . now let me note that all the bosons that are responsible for producing forces have spin 1 or 2 . the higgs boson is another kind of boson , which does not behave like a force . technically , this is because its spin is 0 . in fact , it is the only fundamental particle with spin 0 that we know exists : all the others that are known have spin 1/2 ( fermions ) , 1 , or 2 . is the higgs boson the cause of gravity or a result of it ? does the collision of particles at the lhc create a gravity field or waves or somehow interact with the gravity field of the earth ? the higgs boson has nothing to do with gravity . it does not cause gravity and it is not the result of gravity . the particle that ' causes ' gravity is called the graviton , it is a boson like all the other force-particles , and it has spin 2 . the higgs boson does give mass some to the other fundamental particles , and this mass then interacts with gravity just like all other mass . but this is an indirect relation . the particles at the lhc interact with the gravitational field of the earth just like all other particles . as part of the collisions , certainly some gravitons ( = ' gravity particles' ) are also created . but these are completely negligible effects that can be ignored , because gravity is a much weaker force than all the others . the higgs boson is supposed to be quite massive and equivalent to a large number of protons . were many particles needed to create it or only a few travelling at high speeds ? was the high energy converted into the large mass ? i am not sure what you mean by ' equivalent to a large number of protons ' . it is true that if you weigh one higgs particle it will weigh as much as about 120 protons , but they are not related beyond this simple fact . the higgs particle , like all other particles produced at the lhc , is created by a collision between two highly energetic ( = very fast ) protons . indeed , in the collisions in which a higgs was created , the energy of the protons was converted into the mass of the higgs particle . why is the particle so short lived and what does it decay into ? the higgs is short-lived essentially because it is very massive . roughly speaking , the more massive a particle is , the faster it decays into other particles . this is not a precise statement because it really depends on what the particle can decay to . for example , the proton is massive relative to the electron , but it seems not to decay to anything due to various reasons . the higgs particle has different possibilities to decay to ( these possibilities are called ' decay channels' ) . for example , it can decay to two photons , or to two quarks like bottom and anti-bottom .
because of the scale involved , the object would have to be much more massive than the bullet , so the answer will be independent of the mass of the bullet , just as objects of different mass undergo the same acceleration due to gravity . ( phrased differently , we can ask what the reduced mass needs to be ) . to answer the question , you would have to specify how big the object is also . if all the mass is concentrated into a point , then black holes aside , the bullet can not hit it and conserve angular momentum at the same time . a good treatment of the problem involves solving kepler 's law , but more details are need . you can get an order-of-magnitude answer by finding out what transverse acceleration is needed to deflect the bullet by $0.5\ , km$ in $t$ seconds , i.e. , $g \approx \frac{1000m}{t^2}$ and $t$ solves $1000m = ( 10 \frac{m}{s} ) t + \frac{1}{2} g t^2$ . then $500m = ( 10 \frac{m}{s} ) t$ , so $t =50s$ so $g \approx 0.4 \frac{m}{s^2}$ . here i am treating the $x$ and $y$ coordinates as both subject to the same acceleration of constant magnitude $g$ , which is not at all orbital motion and only meant to give a ballpark figure . hopefully nobody decides to crucify me . ok so the bullet is undergoing $g \approx 0.4 \frac{m}{s^2}$ at a distance $r \approx 1118m$ . into $g = \frac{g m}{r^2}$ we get $m \approx \frac{g r^2}{g}$ works out to be roughly $m \approx 7\times 10^{15} kg$ . the largest asteroids have masses well in excess of $10^{18} kg$ .
best description of the problem of those i found is given by nobel winner vitaly ginzburg . see , e.g. http://ufn.ru/en/articles/1973/3/k/ or ( even better ) vl ginzburg , theoretical physics and astrophysics ( pergamon , oxford , 1979 ) ; other editions with somewhat different titles also available .
i have found this article in wikipedia on relative energy costs . it contains a lot of data and some discussion . to summarise parts of it we note that the official un bodies have developed a formula called " levelised energy cost " ( lec ) which is a lifetime accounting formula which includes the following cost factors : investment ( per year ) - the totals are over the n years of the system 's life . maintenance and operations ( per year ) fuel use ( per year ) [ minimal for some sources like nuclear , obviously high for anything coal/oil based . ] with the benefit factor being the electricity generation itself ( per year ) . as the other answers have reminded us , with nuclear in particular we need to be careful about including the " hidden subsidy " and dual use ( civilian/military ) of these technologies , so maybe one should use the nuclear columns in the data as a reference point , and investigate it in more detail if required . for the less military fuels the data may be more accurate ( at least from that perspective ) . so the data tables in these lists give the lec values , for the different energy types as estimated by various national bodies : us , uk , australia . here is the us doe table here is a recent ( commercial ) study for the uk:- technology -------------------------------------- cost range ( £/mwh ) new nuclear -------------------------------- 80-105 onshore wind ------------------------------ 80-110 biomass ------------------------------------- 60-120 natural gas turbines with co2 capture--- 60-130 coal with co2 capture -------------------- 100-155 solar farms ------------------------------- 125-180 offshore wind ----------------------------- 150-210 natural gas turbine , no co2 capture------ 55-110 tidal power ------------------------------- 155-390 in all these data the " research costs " have not been separated out , but seem to be included in the " investment costs " associated with the given estimates . as the articles also point out there are other costs associated with each type in addition to those considered in this question including any transmission and connection costs which any " geographically remote " technologies might incur . a few extra points about reading the data from commercial organisations . if company a has expertise in technology a then obviously their assessment ( no matter how genuine ) of the costs of technologies a , b , c will price the r and d and other costs for b and c much higher than a . ( an oil company estimating solar versus oil costs , might be an example . ) whereas a specialist company b in technology b ( which might be much smaller than a if dealing with newer technologies ) will have a lower cost figure for b ( versus anything else for them , and lower than a 's estimate of b too if their product is good . ) of such are commercial and political arguments made .
one that jumps to mind is hooke 's law ( extension of a spring ) . hang a spring or thick elastic band and load it with increasing weights . see that extension is proportional to load at least initially . a natural extension of that is to also measure oscillation time/frequency . another one would be archimedes principle , and play with floating/sinking different size/density objects to a tub of water and try and arrive at the correct law . this that go bang are always good , so you could do bottle rockets or baking soda and vinegar and look at the optimal quantities to get the highest/biggest reactions . if you are looking for experiments , this thread has many ( nominally ) home experiments . some of which might be appropriate for you ( some people have a different idea of home experiment ! ) . edit : as you mention you are a chemist there are loads of good chemistry experiments with household products . vinegar and baking soda is one . you can do homemade indicators , red cabbage is one iirc . you can put small coins in coke or vinegar and they get cleaned . you can make a simple electrolysis cell to get hydrogen and oxygen ( do not use table salt or you make chlorine gas . . . ) . i am sure you could adapt some of these to look at some parameter and prove a theory .
this is exactly the approach taken in bernard shutz 's note " gravitational waves on the back of an envelope " ( am . j . phys . 50 vol 5 pp 412 ) . the abstract reads : using only newtonian gravity and a little special relativity we calculate most of the important effects of gravitational radiation , with results very close to the predictions of full general relativity theory . used with care , this approach gives helpful back‐of‐the‐envelope derivations of important equations and estimates , and it can help to teach gravitational wave phenomena to undergraduates and others not expert in general relativity . we use it to derive the following : the quadrupole approximation for the amplitude h of gravitational waves ; a simple upper bound on h in terms of the newtonian gravitational field of the source ; the energy flux in the waves , the luminosity of the source ( called the ‘‘quadrupole formula’’ ) , and the radiation reaction in the source ; order‐of‐magnitude estimates for radiation from supernovae and binary star systems ; and the rate of change of the orbital period of the binary pulsar system . where our simple results differ from those of general relativity we quote the relativistic ones as well . we finish with a derivation of the principles of detecting gravitational waves , and we discuss the principal types of detectors under construction and the major limitations on their sensitivity . ( if you do not have access to am j phys , this talk seems to recapitulate the details . ) a major difference in this newtonian scalar theory from the real gr theory of gravitational waves is in the effect of the waves on an inertial test particle . this newtonian theory predicts that the waves would appear as an oscillating force along the direction between the source and the test particle . by contrast , gr predicts an oscillating differential tidal effect in the plane perpendicular to the line connecting the source and the test mass . as a result , while i think ligo would still detect the " newtonian " form of gravitational waves , the antenna pattern of the detector would be different . the l-shaped ligo detector has optimal sensitivity to a source located directly overhead in the gr case ( allowing the gravitational wave to stretch one arm while it is compressing the other ) . there would be no sensitivity to a " newtonian " source directly overhead . however , you could detect it if the " newtonian " source were aligned with either arm . by the way , " newtonian noise " ( the near-field action of newtonian gravity arising from density waves in the material near the detector ) is a real concern for terrestrial gravitational wave detectors ! p.s. to be pedantic , it is best to avoid the term " gravity wave " ( as opposed to " gravitational wave" ) , since a " gravity wave " ( "newtonian gravity wave " even ! ) is something completely different .
when one says that an elementary particle is point-like , one is referring to the fact that theoretically , there is no limit to how small a region a detector can localize a particle to . for the sake of argument , let 's imagine two wrong things ( a ) that such an ideal detector is possible and ( b ) complications arising from planck scale physics do not change anything conceptually . even if you allow for that , your worry that information is being stored in a zero volume region is still unfounded . it would be a legitimate worry in pre-relativistic-qft physics . but we know particles are not pellets that move around carrying information . they can disappear and be spontaneously created out of the vacuum . what this is hinting at ( though some might prefer a different picture ) is that particles are not fundamental - fields are . the quantum fields for various particles are defined everywhere in space . once you specify what kind of structure the quantum field is ( a scalar , vector , spinor , etc . ) and what its other properties are ( say , the symmetry group under which it has local gauge invariance ) , you have specified what spin , charge , mass etc . its particle excitations will carry . since the field is defined everywhere in space , there is plenty of room for all that information . so in a certain sense , the information that a detector detects is encoded everywhere in space ( because the field is everywhere ) - and the specific structure of the detector just picks out the right information you asked for . finally , two point-particles ( say an electron and a muon ) have different properties because they are excitations of two different fields defined everywhere in space - and the detector you build specifically for the electron will pick out the " signal " from the electron .
the answers given so far are fine , but to my surprise nobody 's mentioned the most important point : in modern terminology , we generally do not say that the mass of an object increases with speed . " relativistic mass increase " is outdated terminology , not used by most physicists anymore . in general , nowadays , " mass " means " rest mass " and is independent of velocity . igor ivanov 's answer to this question says it all . i have not read the article by lev okun that he refers to , but i like the term " pedagogical virus " for this notion .
size is a nebulous concept when you get down to the quantum domain . all you can do is measure the interaction potential of your particle with whatever it is hitting . if there is a hard core to the potential then you can take this as a measure of the size , but unless the potential goes to infinity at the hard core the particle will still be able to get through slits that are too small for it by tunneling . as the comments have said , electrons do not have a hard core , or at least none that we have been able to measure ( the current limit is about $10^{-18}$ m or about a thousand times smaller than the size of a proton ) . so when diffracting electrons reducing the slit size just reduces the probablility of the electron passing through the slit and makes the final diffraction image fainter . protons do have a hard core , which gives them an effective diameter of about $1.6 \times 10^{-15}$ m . if you make the slit size smaller than this then you will drastically decrease the probability of the proton passing through the slit , but there is still a non-zero probability that it will tunnel through so you will still get an interference pattern but it will be extremely faint . we will gloss over the practical problems of making the slit size smaller than a proton !
when an object resonates , it will have a tendency to vibrate in a characteristic way ( it is normal modes ) to produce sounds at its characteristic frequencies , of which there may be more than one . theses frequencies are basically a function of the geometry of the object , as well as the mass density and ' stiffness ' of the material used . for example , an empty bottle will produce a characteristic pitch when air is blown across the top ( similar to a flute ) . the bottle 's length sets up the characteristic wavelength of the pitch produced . half-filling it with water will produce the same pitch an octave higher ( ie : half the wavelength ) . the pitch produced is also a function of the density and velocity of air . blowing harder ( higher velocity ) can produce a different ( alternative ) mode of oscillation with different pitch . in the case of a tuning fork , there is usually just one characteristic pitch ( normal mode ) , which is a function of the density of the fork material , its characteristic length , cross-sectional area , as well as its ' stiffness ' . a heavier ( higher density ) metal will oscillate slower and have lower pitch than a tuning fork made of a lighter metal . a longer tuning fork will also produce a deeper pitch ( longer wavelength ) , all else being equal . using a ' stiffer ' material ( a material with higher young 's modulus which does not ' stretch ' or ' bend ' as easily ) will tend to produce a higher frequency pitch .
in general , decoherence and renormalization are two different things . decoherence is loss of quantum correlations due to lack of information ( either on the interaction , or - on states of some particles ) . the most typical example is when you loss a particle - then you need to trace out with respect to their degrees of freedom , effectively changing all entanglement in ( classical ) randomness . renormalization is a procedure , when you construct ( or deal with ) an effective theory , disregarding some degrees of freedom only to threat them as a single object ( e . g . instead of four particles only one " effective particle" ) . typically you do not " loss " particles , but treat collections of then as a new one . however , the procedure is lossy - i.e. you lose some properties ; in particular , from a pure state you can go to mixed ones . however , typically you rather still hold an approximate pure state , rather than the exact one which ( after neglecting some degrees of freedom ) is mixed ; see e.g. : pietro silvi , tensor networks : a quantum-information perspective on numerical renormalization groups ( 2011 ) , http://www.sissa.it/cm/thesis/2011/silvi.pdf
the big problem with controlled fusion is that the equations governing the plasma are highly non-linear . so each time the physicist increase the size of the tokamak , new effects are discovered . so i guess that the answer is no-one really knows the correct scaling laws ! this contrasts a lot with fission reactors , where the relevant equations are essentially linear ( neutron diffusion ) . it was then possible to ' easily ' scale up enrico fermi 's first nuclear reactor chicago_pile-1 which had a power of just 0.5w in 1942 to the design of the b reactor in 1944 , which had a power of 250 megawatt . that is essentially a factor 500 millions between the first and the second nuclear reactor ! edited to add i have just found this wikipedia page about dimensionless parameters in tokamaks which is quantitative . it essentially says that constructing a 1:3 model of a power producing tokamak having the same turbulence transport processes is essentially infeasible , because it would need a too high magnetic field . then , there is a discussion i do not fully understand in order to try to guess the properties of the large machine . . . in short : the turbulence in the plasma make the use of scaling laws difficult .
let me give it a shot : if i interpret this correctly , $\mathbf{f}$ will be the operator for the full spin of the coupled system , $\mathbf{s}$ will be the operator of the electron spin ( usually , one would consider $\mathbf{j}$ , the spin containing also spin-orbit coupling , but we are on the s-shell , hence no angular momentum ) and $\mathbf{i}$ will be the nuclear spin . then it should hold that $\mathbf{f}=\mathbf{s}+\mathbf{i}$ , right ? first , let 's have a look at the hyperfine structure hamiltonian $\mathbf{h}_{hf}$ . by construction of $\mathbf{f}$ , the eigenstates of $\mathbf{h}_{hf}$ will be eigenstates of $f^2$ and $f_z$ . this is just the same as for angular momentum and electron spin ( and we construct $\mathbf{f}$ to have this property - this lets us label the eigenstates by the quantum number corresponding to $\mathbf{f}$ ) . hence the hamiltonian must be diagonal in the $|f^2 , m_f\rangle$-basis . one can also see that $f^2$ commutes with $i^2$ and $s^2$ ( and so does $f_z$ ) , since $\mathbf{f}=\mathbf{i}+\mathbf{s}$ . now we have a look at $\mathbf{h}_b$ , the interaction hamiltonian with a constant magnetic field . we can see that ( up to some prefactor ) $\mathbf{h}_b=s_z$ . hence the eigenstates of $\mathbf{h}_b$ must be eigenstates of $s_z$ and thus also of $s^2$ and , since the two operators are independent ( they relate to two different types of spins , hence the operators should better commute ) also to $i^2$ and $i_z$ , if you want . the crucial problem is that $s_z$ and $f^2$ do not commute . why ? well : $\mathbf{f}=\mathbf{i}+\mathbf{s}$ hence $f^2=s^2+i^2+2\mathbf{s}\cdot \mathbf{i}$ . now $s_z$ and $\mathbf{s}$ do not commute , because $s_z$ does not commute with e.g. $s_x$ , which is part of $\mathbf{s}$ . since $f^2$ commutes with $\mathbf{h}_{hf}$ and $s_z$ commutes with $\mathbf{h}_b$ , but not with $f^2$ , we have that $\mathbf{h}_{hf}$ does not commute with $\mathbf{h}_b$ . this means that $\mathbf{h}_b$ and $\mathbf{h}_{hf}$ cannot be diagonal in the same basis , hence you need to have off-diagonal elements . in order to see how the matrix representing $\mathbf{h}_b$ looks like in the $|f^2 , m_f\rangle$-basis , you can express the $|m_i , m_s\rangle$-basis ( in which $\mathbf{h}_b$ is diagonal ) in terms of the other basis . this is exactly what equations ( 4.21 ) do . these are obtained by ordinary addition of angular momenta . from there , you can construct the unitary transforming the basis $|m_i , m_s\rangle$ into $|f^2 , m_f\rangle$ and $\mathbf{h}_b$ will be the diagonal matrix in the basis $|m_i , m_s\rangle$ conjugated with this unitary . edit : i am not quite sure whether i understand correctly what your problem is , but let me elaborate : we want to find the hamiltonian $\mathbf{h}_b$ in the $|m_im_s\rangle$ basis . in this basis , it is diagonal , because $\mathbf{h}_b$ is essentially $s_z$ ( hence commutes with $s_z$ ) and it must also commute with $i_z$ since $s_z$ and $i_z$ are independent . if we order the basis according to $|\frac{1}{2} , \frac{1}{2}\rangle , |-\frac{1}{2} , -\frac{1}{2}\rangle , |\frac{1}{2} , -\frac{1}{2}\rangle , |-\frac{1}{2} , \frac{1}{2}\rangle$ , then , we can just read off the hamiltonian : the first and fourth vector are eigenvectors to eigenvalue $\mu b$ , the others of $-\mu b$ ( by definition of $s_z$ , since the second component in $|m_im_s\rangle=| ( si ) i_z , s_z\rangle$ tells us the eigenvalue of $s_z$ that the basis vector corresponds to ) , i.e. $$ \mathbf{h}_b=\begin{pmatrix}{} \mu b and 0 and 0 and 0 \\ 0 and -\mu b and 0 and 0 \\ 0 and 0 and -\mu b and 0 \\ 0 and 0 and 0 and \mu b\end{pmatrix}$$ now , as i said , you just have to change the basis . the matrix transforming the above basis into the new basis is given by eqn . ( 4.21a-d ) : $$u:=\begin{pmatrix}{} 1 and 0 and 0 and 0 \\ 0 and 1 and 0 and 0 \\ 0 and 0 and \frac{1}{2} and \frac{1}{2} \\ 0 and 0 and -\frac{1}{2} and \frac{1}{2} \end{pmatrix}$$ where the ordering of the $|fm_f\rangle$-basis is as for $\mathbf{h}$ in your text . now calculate $u\mathbf{h}_b u^{\dagger}$ and that should give you the part of $\mathbf{h}$ coming from $\mathbf{h}_b$ in the $|f , m_f\rangle$-basis and this will be exactly what is written in your book . edit 2: i sort of suspected this , so here is some more linear algebra for the problem . i will use dirac notion since i suspect you are more familiar with this : now suppose you have given two bases $|e_i\rangle$ and $|f_i\rangle$ and suppose they are orthonormal bases . what we want is a matrix $u$ that transforms one basis into the other ( i will call it $u$ , since it'll be a unitary - if the bases are not orthonormal , it'll only be an invertible matrix ) . so we want a matrix such that $$ |f_i\rangle:=u|e_i\rangle \qquad \forall i$$ how to construct this matrix ? well , given an equation for $|f_i\rangle$ in terms of the $|e_i\rangle$ will give you the i-th row of the matrix . you can also see the matrix elements in dirac notation : $$ \langle e_j|u|e_i\rangle=\langle e_j|f_i\rangle $$ in your case , $|e_i\rangle=|m_im_s\rangle$ and $|f_i\rangle=|f^2 , m_f\rangle$ . hence equation ( 4.21a ) will give you the first row of the matrix ( the ordering of the basis vectors $|m_im_s\rangle$ as i proposed above ) , ( 4.21c ) the second ( notice the basis ordering in the matrix $\mathbf{h}$ ! ) ( 4.21b ) the second and ( 4.21d ) the last row of the matrix . using the equation for the matrix elements above , you should be able to check that with not too much trouble . you can also easily check that $u$ is indeed a unitary ( i.e. . $uu^{\dagger}=u^{\dagger}u=\mathbb{1}$ . then we can calculate the matrix elements : $$ \langle e_i |\mathbf{h}|e_j\rangle=\langle e_i|u^{\dagger}u\mathbf{h}u^{\dagger}u|e_j\rangle=\langle f_i| u\mathbf{h}u^{\dagger}|f_j\rangle $$ , which tells you how the matrix looks like in the other basis .
it actually gets a bit complicated , since several effects are involved : evaporating water does require heat , which comes primarily from the hot stones . so throwing water on the stones does cool them down . ( this is where the claim one occasionally hears , that " throwing water on the stones makes the sauna colder " , comes from . technically it is true , if one considers the total heat content of the sauna as a whole . but since most of that heat is in the stones , and since you do not sit on the stones , that is pretty much completely irrelevant to how hot the part of the sauna that you do sit in gets , or feels . ) on the other hand , throwing water on the stones also significantly increases the heat transfer rate from the stones to the air : the evaporation produces a lot of hot steam , which will rise and mix with the ambient air in the sauna . so it is possible for the air temperature in the sauna to increase , even as the stones are cooled down . also , the introduction of steam obviously increases the humidity of the air , which will increase the rate of water precipitation on skin , and/or decrease the rate of sweat evaporation . ( the relative importance of these two effects will depend on the baseline humidity of the air , which can vary quite a lot . my gut feeling , based on experience , is that in all but the driest of saunas condensation probably dominates , simply because human skin is so much cooler than the air . ) in either case , the effect will be to transfer more heat to the skin , and thus to make the air feel hotter . finally , as the hot steam rises off the stones , it will push hot air around the sauna in front of it . while this increase in air movement is slight and transient , it probably does have a noticeable effect : as the hot air flows past the people in the sauna , it will act to disperse the layer of cooler air that forms over the skin , and thus increases heat transfer to the skin . ( if you do not believe me , try blowing some air over your skin in a sufficiently hot sauna . it burns . ) the upshot is that throwing water on the stones increases heat transfer , both from the stones to the air and from the air to your skin . as long as the stones stay hot enough to supply that heat , the net effect will be that you feel hotter . however , if you throw too much water on the stones , it is possible to " kill the stones " by cooling them close to or even below the boiling point of water . at that point , throwing more water is useless , and all you can do is add more firewood or turn up the thermostat and wait for the stones to heat up again . or , if you manage to do this in a smoke sauna , go wash yourself and get dressed up because the sauna is over for the night .
ok , i think there is a mistake here : a general tensor $\varphi^i$ transforms as : $$\varphi^i\rightarrow u^i_{\phantom{1}j}\varphi^j$$ whereas $\varphi_i$ transforms as : $$\varphi_i\rightarrow ( u^\boldsymbol{\ast} ) _i^{\phantom{1}j}\varphi_j$$ where did you find these equations ? the unitary matrix element in the second line should not be a complex conjugate . i do not remember giorgi 's conventions but the customary notation i am used to is this one : $$u_i^{\phantom{i}j}=u_{ij} , \quad \varphi_i\rightarrow u_i^{\phantom{1}j}\varphi_j\\ u^i_{\phantom{i}j}=u^\ast_{ij} , \quad \varphi^\ast_i\equiv \varphi^i\rightarrow u^i_{\phantom{1}j}\varphi^j\equiv ( u_i^{\phantom{i}j}\varphi_j ) ^\ast . $$ hence , in your equations i would understand : $$ ( u^\ast ) _i^{\phantom{i}j}\equiv u^\ast_{ij}=u^i_{\phantom{i}j}$$ and it does not provide the right transformation law for $\varphi_i$ . edit : well , provided the previous comments , let me clarify some issues with the notation , that may led to confuse the meaning of these transformation laws . let us choose the convention to denote $su ( n ) $ transformations , that is $n\times n$ unitary matrices with unit determinant , with uppercase letters , like $u$ , and base states ( scalars , vectors and tensors ) with lowercase greek letters , $\psi\in \mathbb{c}^n$ . for example vector states transform as : $$\psi\to u\psi , \quad \psi_i\to u_{ij}\psi_j\equiv u_i^{\ j}\psi_j$$ note that here i followed the convention of writing base states of the fundamental or vector representation with lower indices , as georgi does and as you can find here . this is the convention i am used to , but nothing stops you to do the contrary , choosing upper indices ! note also that $u\psi$ represents the ordinary product of an $n\times n$ matrix by a vector $\psi= ( \psi_1 , \ldots , \psi_n ) ^t$ , and produce a vector of the same type . in the notation $u_{ij}$ the index $i$ represents the rows whereas the second index $j$ represents the columns . it is customary to write it like $u_i^{\ j}$ to distinguish rows and columns . $\psi_i$ is a column vector and $i$ counts its rows . you can define the conjugate representation by means of the conjugate vectors $\psi_i^\ast$ , whose transformation law is $$\psi^\ast\to ( u\psi ) ^\ast=\psi^\ast u^\ast , \quad \psi_i^\ast\to ( u^\ast ) _{ij}\psi_j^\ast=\psi_j^\ast ( u^\dagger ) _{ji}$$ since these conjugate vectors transform in a different way with respect to $\psi_i$ , it is useful to introduce upper indices to distinguish them : $$\psi^i\equiv \psi_i^\ast \to u^\ast_{ij}\psi_j^\ast\equiv u^i_{\ \ j} \psi^j . $$ as you can see , now indices are " summed on the bottom-right " . the extension to any arbitrary $ ( p , q ) $-tensor is trivial , their transformation law are those of the direct ( diagonal ) product of $p$ type $\psi^i$ vectors and $q$ type $\psi_i$ vectors : $$\psi^{i_1\ldots i_p}_{j_1\ldots j_q}\to \big ( u_{j_1}^{\ \ j'_1}\cdot\ldots\cdot u_{j_q}^{\ \ j'_q}\big ) \big ( u^{i_1}_{\ \ i'_1}\cdot\ldots\cdot u^{i_p}_{\ \ i'_p}\big ) \psi^{i'_1\ldots i'_p}_{j'_1\ldots j'_q} . $$ since upper and lower indices represents different objects it has no sense mixing them .
there are lots of different types of rock , and water can vary in it is acidity , so there are a few variables to consider . one obvious example is the formation of caves . these are usually in limestone , which is calcium carbonate . if carbon dioxide is present , calcium carbonate will dissolve in water to form calcium bicarbonate . so pure water ( ok with a bit of co2 :- ) will happily dissolve limestone . another example is in forming gorges like the grand canyon . if the surrounding rock is limestone then the gorge forms by dissolution just like a cave ; in fact many gorges are actually collapsed caves i.e. a cave forms first then the roof collapses . in the case of the grand canyon the rock contains some limestone layers but there is also a lot of sandstone . sandstone is silica ( silica grains loosely bonded ) so it is not soluble in water and would not be eroded just by solution . in this case the erosion is mostly by abrasion of rocks carried in the water , though any limestone present will dissolve and undercut sandstone layers above it . sandstone is quite soft and easily abraded . if the rock is granite this is very insoluble and very hard so it erodes only very slowly . i would guess the main water based ersion method would be freeze thaw cracking . you expanded your title to ask about any erosion of a hard material by a soft material , but rock ersion by water is not a bad place to start as it is quite varied . martin beckett mentioned water jets , and this is another good example if a bit specialised . if you fire the jet of water fast enough it is momentum means it will break off bits from a hard material . i can not think of any example where this is important in nature .
in 3d there are 7 lattice systems which are classes of lattices having the same point group . one of them is the class of cubic lattices . this class contains three different bravais lattices which are distinguished by their translation group .
gravity has played a dominant role in the development of the large-scale structure of our universe . the largest structures of matter in our universe ( most of it dark matter ) grew out of over-densities in the primordial matter distribution that emerged shortly after the big bang . dense areas began collapsing under their own gravity , and over time as their mass grew they could pull in matter from larger and larger volumes that had initially been moving away in the universe 's expansion . a powerful analytic treatment of this gravity-induced growth of structure is called cosmological perturbation theory . at small spatial scales and later times this treatment can become tedious or break down entirely , and numerical n-body simulations such as the millennium run can provide additional information about the growth of structure . essentially , the only physical process involved in these simulations is gravity ( although the initial conditions require insight from other physical processes ) . at the scales of clusters of galaxies and within single galaxies , other processes become important in addition to gravity . ordinary ( non-dark ) matter can be heated and emit light to cool itself , and this must be accounted for . gas also forms stars , which heat surrounding gas and may explode as supernovae . large black holes that grow at the centers of massive galaxies heat gas to large temperatures in their accretion disks , and t he light emitted from these disks can potentially influence the circum-galactic and even circum-cluster gas . all of these physical processes in non-dark matter can also feed back gravitationally into the evolution of the dark matter . accounting for all of these effects in cosmological simulations is a topic of ongoing research . as to whether the " big bang created gravity , " i think physicists might disagree about whether this is true , false , or a matter of semantics or philosophy that lies outside of physics . some may take a view that we as scientists are not qualified to speak about matters of the creation of physical laws at the time of the big bang , but only about observable and testable-in-principle matters that came into existence after the bang . some others would say that such questions are approachable from the point of view of the multiverse . some others might say that gravity , as an element of general relativity , " just is " and it is entirely improper to speak of its creation . there are most likely other scientific points of view .
every thermodynamic system satisfies the first law during a given process ; $$ \delta e = q-w $$ here $\delta e$ is the change in its internal energy , $q$ is the heat transferred to the system , and $w$ is the work done by the system . for a system undergoing a cyclic process , namely one for which it starts and ends in the same thermodynamic state , one has $\delta e = 0$ , and the first law then tells us that $$ q = w $$ now , suppose that the system is taking some heat $q_h&gt ; 0$ from a reservoir , and turning it into some work $w$ . let 's define $$ q_\mathrm{exhuast} = q-q_h $$ then we can write $$ q_h + q_\mathrm{exhaust} = w $$ the kelvin statement tells us that we must have $q_\mathrm{exhaust}\neq 0$ because otherwise , we would have $q_h = w$ ; the sole result of the process would have been to transform the heat it took in into work . the symbol $q_\mathrm{exhaust}$ is what one commonly calls the exhaust heat .
show me a distribution of remote mass that would provide the behavior we see for both jupiter in orbit around the sun the many moons in orbit around jupiter which both appear to be $1/r^2$ forces . now try to generalize to support all the moons and planets in the solar system . you can not do it because the system is highly over-constrained .
there are two sources of kinetic energy loss . a ) is friction and the other b ) conversion to potential energy . so $$ \frac{1}{2} m v^2 = m g \left ( k \cos\alpha + \sin\alpha \right ) s $$ in your post you have as if gravity is providing energy to the system which it would if the slope was downwards . if you correct your sign you will get the correct result .
you have to realize that depending on the dimensions of the variables under consideration the laws of physics change . when the dimensions are of order of h_bar , the planck constant , it is the framework of quantum mechanics , and this is the underlying framework from which all other physics frameworks emerge . quantum mechanics sees elementary particles , molecules and structures less than nanometers , although its results can manifest macroscopically , as in the crystal lattice order , or in superconductors and superfluids which obey large dimensions quantum mechanical equations . in addition to quantum mechanics at the particle level special relativity is also the law of nature . nature and physics which describes nature mathematically have no discontinuities . as dimensions grow statistical mechanics is the framework to describe ensembles of particles/atoms/molecules , in the beginning quantum statistical mechanics emerges as the next framework and then classical statistical mechanics where the velocities are such the newtonian mechanics emerges from special relativity . when instead of looking at the particulate nature of matter , numbers become very large macroscopically , order of 10^23 particle per mole , thermodynamics emerges as a framework to describe the macroscopic behaviors . the laws of thermodynamics were discovered long before statistical mechanics . they seemed absolute and were for this reason called " laws " because they were the postulates on which the mathematics of thermodynamics was developed . but there is continuity , and in the framework on which thermodynamics emerges , statistical mechanics , the law of entropy is shown to be probabilistic , and holds with very very high probability as long as the numbers are of order 10^23 . a common use of statistical mechanics is in explaining the thermodynamic behavior of large systems . microscopic mechanical laws do not contain concepts such as temperature , heat , or entropy , however statistical mechanics shows how these concepts arise from the natural uncertainty that arises about the state of a system when that system is prepared in practice . the benefit of using statistical mechanics is that it provides exact methods to connect thermodynamic quantities ( such as heat capacity ) to microscopic behaviour , whereas in classical thermodynamics the only available option would be to just measure and tabulate such quantities for various materials . statistical mechanics also makes it possible to extend the laws of thermodynamics to cases which are not considered in classical thermodynamics , for example microscopic systems and other mechanical systems with few degrees of freedom . one can define entropy within statistical mechanics considering the probabilities of the microsystems contained in the macrosystem , for example the gibbs entropy : the quantity k_{b} is a physical constant known as boltzmann 's constant , which , like the entropy , has units of heat capacity . the logarithm is dimensionless . so what is a law in one scale of physics is a value dependent on probabilities derived from the microstates . it is very very improbable for the atoms of the floor to align and kick the ball .
my question is , whether this is caused by an modification of the frequency/wavelength or simply by my eye combining the two incoming lights . short answer no so your second picture is accurate . frequency is not modified , the two different waves are added up by your eye to produce the light that you perceive . most computer screens operate on a rgb colour model , i.e. they only have red green and blue lights . so the " yellow " light you see coming from your screen contains zero photons of yellow frequency but the exact right proportion of red green and blue to trick your eye into firing neurons the same way it would if photons of the yellow spectrum were striking it . when " adding " light colours you start with a white wall , that is un-illuminated ( black ) . and then you add red green and blue ( the additive primary colours ) lights to get any colour you want . however when " subtracting " colours , if you start with a white sheet of paper illuminated with white light ( white ) and start adding paints ( subtracting colour ) you can make any colour by using the tree primary colours red yellow and blue ( the subtract-ative primary colours ) , or more accurately as referred to in the printing industry cmyk , cyan magenta yellow and key ( black ) , black is needed because adding enough coloured paint to make an image black is inefficient and expensive . if you are allowed to add and subtract colours you can mix ( almost ) any three different colours to generate the illusion of any desired colour . ie you get to choose any three colours as your primary colours . exactly three colours is required since there are three different types of cones in your eye ( three degrees of freedom ) . also note that the srgb does not encapsulate all different colours , so watching a movie in cinema with a reel film may display hues that cannot be rendered on a computer monitor . so far i have only dealt with the eye and completely ignored the interaction of the light with the interface . in general light is absorbed , reflected or transmitted through the medium . reflected light can be coherent light in a mirror or diffuse like a painted wall . however there are a large number of non-linear optical effects where the photons do interact with each other and may combine . there are also interactions with the medium such as those seen with a black light . links are to the respective wikipedia pages .
they are not the same in nearly any situation except for the constant force field . the two conditions are completely separate , and it is a miracle of cycloid algebra that leads them to coincide in the constant force case . but it is possible to understand both problems intuitively . isochrone/tautochrone the isochrone problem asks for a curve along which the oscillations take equal times no matter what the amplitude . the solution to this problem in the general case is to find a curve such that the potential v along the arclength of the curve is that of a harmonic oscillator $$ v ( \phi ) = {as^2 \over 2} $$ where i have absorbed the part of v linear in s into a redefinition of the origin of s , and the constant part of v into the definition of zero potential . for a constant force field , the solution is that curve whose height is proportional to the arclength squared . to see that this is the cycloid , parametrize the cycloid by the rolling circle coordinates so that $x=0$ , $y=0$ is the minimum position at $\theta=0$: $$x = r ( \theta + \sin ( \theta ) ) $$ $$y = r ( 1- \cos ( \theta ) ) $$ and verify that $$ dx = r ( 1+cos ( \theta ) $$ $$ dy = r sin ( \theta ) $$ so that the arclength is : $$ ds^2 = dx^2 + dy^2 = 2r ( 1-\cos ( \theta ) ) d\theta^2 = {2dy^2\over y}$$ from the last differential condition , the height as a function of arclength is : $$ {y\over r} = {s^2\over 8r^2}$$ and this allows you to write x as a function of s too , by solving for $\theta$ in terms of y : $$ {x\over r} = \cos^{-1} ( 1- {s^2\over 8 r^2} ) + {s\over 2r} \sqrt{ 1- {s^2\over 16r^2} ) } $$ the algebra is annoying enough , so set r to 1 by rescaling x and y appropriately , and this gives the arclength parametrization of the cycloid explicitly $$ y = {s^2\over 8} $$ $$ x = \cos^{-1} ( 1- {s^2\over 8} ) + {s\over 2} \sqrt{1- ( {s\over 4} ) ^2}$$ the fact that the height of a cycloid is its arclength squared is the central property of the cycloid which gives all the other special properties . the differential relations on a cycloid , the unit tangent , and the osculating circle , are given by differentiating the above forms $$ {dy\over ds} = {s\over 4} $$ $$ {dx\over ds} = \sqrt{1- ( {s\over 4} ) ^2} $$ the x derivative is easy to compute from the y-derivative , using the fact that the arclength derivative must have unit length . you can also take the formal derivative of x in s , but this is tedious to reduce ( but of course it works ) . the second derivative with respect to s gives the best-fit-circle direction and inverse-radius : $$ {d^2y\over ds^2} = {1\over 4} $$ $$ {d^2x\over ds^2} = -{{s\over 16}\over \sqrt{1- ( {s\over 4} ) ^2} } $$ the x equation follows from the y equation after imposing the condition that the second derivative vector with respect to arclength has zero dot product with the first derivative , so that ( switching to dot notation for arclength derivatives ) $$ \dot{x}\ddot{x} + \dot{y}\ddot{y}=0$$ the the magnitude of the second derivative with respect to arclength is the inverse-radius of a best-fit circle to the curve ( which follows by rotational invariance from the fact that this is correct for a parabola at the origin of the form $y={x^2\over 2r}$ , where r is the best fit circle ) . the resulting formula for the inverse-radius of the best-fit circle is important , because it gives the centripetal acceleration for a particle sliding down the cycloid at a velocity v . this inverse radius is $$ {1\over r} = \sqrt{\ddot{x}^2 + \ddot{y}^2} = {{1\over 4}\over \sqrt{1- ( {s\over 4} ) ^2} } $$ when a particle starts at rest at the top of the cycloid , sliding down in a field with a unit gravitational acceleration downwards , its velocity at any height is given by conservation of energy : $$v^2 = 2 ( 2-y ) = 2 ( 2- {s^2\over 8} ) = 4 ( 1- ( {s\over 4} ) ^2 ) $$ this means that the centripetal acceleration at any point is given by : $$ {v_0^2\over r} = \sqrt{1- ( {s\over 4} ) ^2} = \dot{x}$$ in other words , the centripetal acceleration inwards is equal to the component of the tangent vector in the x direction , which is also the component of gravity outward . when you slide along a cycloid , the centripetal acceleration is equal and opposite to the pull of gravity out . this condition is very important--- this is the brachistochrone condition . brachistochrone the brachistochrone problem is much more involved than the isochrone . the isochrone is solved by imposing the condition that the force along the curve is linear in arclength , which is conceptually simple , and does not involve the force perpendicular to the curve . the isochrone condition involves only the force perpendicular to the curve , and the force along the curve is only involved to the extent that it determines the particle velocity at any position . to solve the brachistochrone in a maximally insightful way , consider a particle traveling with velocity $v_0$ along the segment of the x axis between $-\delta$ and $\delta$ . this particle makes the trip in a time which is twice $\delta\over v_0$ . deform the straight line into a parabola , keeping the endpoints fixed : $$ y = {1\over 2r} ( x^2 - \delta^2 ) $$ where r is chosen to parametrize the parabola by its best-fit circle , and the form is determined by the boundary conditions , and $\delta$ is thought of as infinitesimal . then there is a little bit of extra length in this curve , which is : $$ \int_{-\delta}^{\delta} \sqrt{1- ( {dy\over dx} ) ^2} dx= \int_{-\delta}^{\delta} {x^2\over 2r^2} = {\delta^3\over r^2}$$ which gives a little bit of extra time for the particle to cross $$ \delta^3 \over r^2 v_0 $$ supposing there is a force with components $f_x , -f_y$ acting on the particle in this region ( the sign on $f_y$ is only so that positive $f_y$ match the mental picture of a downward pulling gravity ) , the potential is $-f_x x - f_y y$$ . the velocity is determined by conservation of energy to be $$ v = \sqrt{v_0^2 + 2 ( f_x x - f_y y ) } = v_0 + {f_x x - f_y y \over v_0} $$ the inverse velocity is , to lowest relevant order $$ {1\over v} = {1\over v_0} - {f_x x - f_y y \over v_0^3}$$ which , when integrated , gives a little bit of time gained $$ \int_{-\delta}^{\delta} {1\over v} dx = {2 f_y \delta^3\over 3 v_0^3 r} $$ the sum of the extra time from the arclength and the time gained from the force gives the time difference for the parabolic arc : $$ \delta^3 ( {2 f_y \over 3 v_0^3 r} - {1\over 3 r^2 v_0} ) $$ which attains its minimum when $$ {1\over r} = {f_y \over v_0^2} $$ this is the brachistochrone condition--- to have an extremum , the local curvature must be such that the centripetal force equals the external force perpendicular to the curve . $$ {v^2\over r} = f_y $$ this is a local condition , consistent to the lowest relevant infinitesimal order in $\delta$ , so it must be satisfied by the normal force at each point along the brachistochrone . this condition is satisfied by a cycloid , as was shown above . note that the brachistochrone does not care about $f_x$ , it only cares about matching $f_y$ to the centripetal force . the force tangent to the curve is only used to find the instantaneous velocity . deforming the isochrone the brachistochrone condition is determined only by the force perpendicular to the curve , while the isochrone condition only cares that the force along the curve is linear in the arclength parametrization . so it is easy to break the brachistochrone condition while keeping the isochrone condition satisfied . take any force law with a given isochrone curve x ( s ) , y ( s ) and add a vector field which is everywhere orthogonal to the isochrone . this vector field will ruin the brachistochrone condition , but not do anything to the isochrone condition-- the curve will have a longitudinal force linear in arclength , but it will no longer have a transverse force which matches the centripetal force , so it will no longer be a brachistochrone . most damning of all , if you look at the isochronous motions of the cycloid , most of them are not brachistochrone motions . if you start a particle sliding from rest anywhere except at the top of the cycloid , it is motion is isochronous with the particle sliding from the top , but it is not extremal for the brachistochrone problem . the relevant brachistochrone cycloid for a particle at rest always has the cycloid kink at the starting position--- the particle is at the tippy-top of the cycloid . so the deformations which define the isochrone nature are not in the solution space of the brachistochrone problem . if you look at a harmonic oscillator ( like motion in the center of the earth ) , there are fine isochrones along any straight line , since the restriction of a quadratic potential to a line is quadratic . but these lines are straight , so they have no centripetal force , but they have much perpendicular force , so they are not brachistochrones . if you start with a brachistochrone , and you want to deform it to not be an isochrone , this is a little more involved , because the deformation changes the instantaneous velocity . but it can be done as well . add an arbitrary potential v ( s ) to the potential on the isochrone , and adjust the perpendicular component of the force on the curve to match the new centripetal force appropriate to the velocity of the particle sliding in this potential . this adjustment will keep the brachistochrone property , but it will break the isochrone property . for an example of a nice brachistochrone which is nothing like an isochrone , consider the potential $$ v ( x , y ) = x + x^2+y^2 - ( \sqrt{x^2+y^2}-1 ) *x$$ the unit circle is a brachistochrone for a particle with just enough energy to be at rest at $x=1$ . the potential energy at angle $\theta$ along the circle is $$ v ( \theta ) = cos ( \theta ) + {1\over 2}$$ the velocity squared of this particle is $$ v^2 = 2 ( 1-cos ( \theta ) ) $$ which is also the centripetal force . the outward force on the unit circle has been engineered to match the force required by the brachistochrone condition . solving arclength equations one thing is still unsatisfying in the discussion above . the brachistochrone condition was not used to find the cylcoid solution , only to check that the cylcoid works . this is not reasonable , since similar differential equations pop up all the time , and there should be a good way of finding the exact relations they obey without knowing them in advance . but the method for solving the brachistochrone is not difficult , given the above . you start with the arclength parametrization conditions $$ \dot{x}^2 + \dot{y}^2 =1 $$ $$ \dot{x} \ddot{x} + \dot{y}\ddot{y} = 0$$ and you write the arclength version of the brachistochrone condition : $$ 2 ( y_0-y ) \sqrt{\ddot{x}^2 + \ddot{y}^2} = \dot{x} $$ substitute $\dot{x}=\sqrt{1-\dot{y}^2}$ and $\ddot{x}=-{\dot{y}\over\dot{x}}\ddot{y}$ , and you obtain the brachistochrone height-arclength equation in terms of the variable $u=y_0-y$ , $$ 2u \ddot{u} - \dot{u}^2 = 1 $$ this equation looks formidable , because it is nonlinear , but it possesses a nonobvious extra scale symmetry . when you scale x and y by a factor r , the arclength scales by the same factor , so that the derivative $\dot{u}$ is invariant . the product $u\ddot{u}$ is also invariant . the reason for choosing the variable u , and not y , is that the natural brachistochrone scaling is around the axis $y=y_0$ . the scale symmetry suggests a transformation of variables , and the right one is $$ {d\over ds} ( {\dot{u}\over \sqrt{u}} ) = {1\over 2u^{3/2}}$$ or , using $v=\sqrt{u}$ , $$ \ddot{v}= - {1\over 2v^3} $$ this is the equation for one dimensional motion in a $1/r^2$ potential , which can be solved by conservation of energy ( the x-translation symmetry of the brachistochrone ) . the general solution makes v a circle function , which makes $y$ quadratic , recovering the cycloid . the reason the equation looks complicated is because of the multiplicative way in which the integration constant appears in the general solution : $$ y ( s ) -y_0 = r ( {s\over r} - s_0 ) ^2 $$ i hope that this explanation declaws newton to some extent , because his reasoning regarding this was always mysterious to me . it looks as if an oracle provided him with the correct answer so that he could solve the problem . you can solve a whole family of related differential equations in the same way , changing the coefficient "2" to another value , which gives different exponents for the change of variables which transform the problem to a 1d mechanics motion . it is also clear that the special algebraic differntial relations of arclength problems are made manifest using the arclength parametrization .
the student is right in that energy is the analog of momentum in the " time direction " but i would not go so far as to call it " momentum in the direction of time " . it is analogous in two ways that i can think of off the top of my head : it is the time-component of the 4-dimensional energy-momentum vector in special relativity . noether 's theorem relates a symmetry in the laws of physics with respect to a coordinate to a conserved quantity . momentum is conserved because the laws of physics are invariant with respect to translations in space , and energy is conserved because the laws of physics are unchanging in time . i would not call it " momentum in the direction of time " because that phrase , at least to me , implies that energy is more momentum-like than it really is .
i apologise , this answer as it stood originally was sloppy and flat out wrong on key points , and probably still needs attention from a physicist . when you travel close to the speed of light , you experience less subjective time , according to the lorentz transform . moreover , external distances are contracted by the same transform . the factor by which time slows down for a rapidly moving object is $\frac{1}{\sqrt{1-\frac{v^2}{c^2}}}$ . as velocity $v$ increases it approaches equality with the speed of light , $c$ , and the factor approaches infinity . this means that if you had an arbitrarily fast spacecraft ( i assume you do not ) you could tour the universe in ( from your reference frame ) a few years , however you may return to find humanity extinct and continents in unexpected locations . you would actually perceive yourself as travelling at arbitrarily high speeds , in terms of the subjective time it takes to travel from point to point . however , in making measurements against objects you move past , you would see that your subjective velocity does not exceed the speed of light as the rest of the universe appears length-contracted . there are other limitations in your ability to enjoy your tour of the universe - if you set your sights on a star that is millions of light-years away it may well rapidly age ( again , from your reference frame ) and die before you get there . references : lorentz factor at wikipedia relativistic doppler effect at wikipedia the 3rd book of tipler physics also has a great primer on relativity , albeit with neglect of acceleration , which is critical in generating discrepant clocks between two observers a la the twin paradox .
the derivation in landau and lifschitz is making some additional implicit assumptions . they assume that all forces come from pair-interactions , and that the pair forces are rotationally invariant . with these two assumptions , the potential function in the lagrangian is $v ( x1 , . . . , xn ) = \sum_{\langle i , j\rangle} v ( |x_i - x_j| ) $ and then it is easy to prove newton 's third law , because the derivative of the distance function is equal and opposite for each pair of particles . this type of derivation is reasonable from a physical point of view for macroscopic objects , but it is not mathematically ok , because it omits important examples . no rotational invariance , no third law dropping the assumption of rotational invariance , but keeping the assumption of pair-wise interaction , one gets the following counterexample in 2 dimensions , with two particles ( a , b ) with position vectors $ ( a_x , a_y ) $ $ ( b_x , b_y ) $ respectively : $v ( a_x , a_y , b_x , b_y ) = f ( a_x-b_x ) + f ( a_y - b_y ) $ where f is any function other than $f ( x ) =x^2$ . this pair potential leads to equal and opposite forces , but not collinear ones . linear momentum and energy are conserved , but angular momentum is not , except when both particles are on the lines $y=\pm x$ relative to each other . the potential is unphysical of course , in the absence of a medium like a lattice that breaks rotational invariance . many-body direct interactions , no reflection symmetry , no third law there is another class of counterexamples which is much more interesting , because they do not break angular momentum or center of mass conservation laws , and so they are physically possible interactions in vacuum , but they do break newton 's third law . this is the chiral three-body interaction . consider 3 particles a , b , c in two dimensions whose potential function is equal to the signed area of the triangle formed by the points a , b , c . $v ( a , b , c ) = b_x c_y - a_x c_y -b_x a_y - c_x b_y + c_x a_y + a_x b_y$ if all 3 particles are collinear , the forces for this 3-body potential are perpendicular to the common line they lie on . the derivative of the area is maximum by moving the points away from the common line . so you obviously cannot write the force as any sum of pairwise interactions along the line of separation , equal and opposite or not . the forces and torques still add up to zero , since this potential is translationally and rotationally invariant . many body direct interaction , spatial reflection symmetry , crappy third law if the force on k particles is reflection invariant , it never gets out of the subspace spanned by their mutual separation . this is because if they lie in a lower dimensional subspace , the system is invariant with respect to reflections perpendicular to that subspace , so the forces must be as well . this means that you can always cook up equal and opposite forces between the particles that add up to the total force , and pretend that these forces are physically meaningful . this allows you to salvage newton 's third law , in a way . but it gives nonsense forces . to see that this is nonsense , consider the three-particle triangle area potential from before , but this time take the absolute value . the result is reflection invariant , but contains a discontinuity in the derivative when the particles become collinear . near collinearity , the forces perpendicular have a finite limit . but in order to write these finite forces as a sum of equal and opposite contributions from the three-particles , you need the forces between the particles to diverge at collinearity . three body interactions are natural there is natural physics that gives such a three-body interaction . you can imagine the three bodies are connected by rigid frictionless struts that are free to expand and contract like collapsible antennas , and a very high-quality massless soap bubble is stretched between the struts . the soap bubble prefers to have less area according to its nonzero surface tension . if the dynamics of the soap bubble and the struts are fast compared to the particles , you can integrate out the soap bubble degrees of freedom and you will get just such a three-body interaction . then the reason the bodies snap together near collinearity with a finite transverse force is clear--- the soap bubble wants to collapse to zero area , so it pulls them in . it is then obvious that there is no sense in which they have any diverging pairwise forces , or any pairwise forces at all . other cases where you get three body interactions directly is when you have a nonlinear field between the three objects , and the field dynamics are fast . consider a cubically self-interacting massive scalar field ( with cubic coupling $\lambda$ ) sourced by classical stationary delta-function sources of strength g . the leading nonlinear contribution to the classical potential is a tree-level , classical , three-body interaction of the form $v ( x , y , z ) \propto g^3 \lambda \int d^3k_1 d^3k_2 { e^{i ( k_1\cdot ( x-z ) + k_2\cdot ( y-z ) ) } \over ( k_1^2 + m^2 ) ( k_2^2 + m^2 ) ( ( k_1+k_2 ) ^2 + m^2 ) }$ which heuristically goes something like ${e^{-mr_{123}}r_{123}\over r_{12}r_{23}r_{13}}$ where the r 's are the side lengths of the triangle and $r_{123}$ is the perimeter ( this is just a scaling estimate ) . for nucleons , many body potentials are significant . the forces from the crappy third law are not integrable if you still insist on a newton 's third law description of three-body interactions like the soap bubble particles , and you give a pairwise force for each pair of particles which adds up to the full many-body interaction , these pairwise forces cannot be thought of as coming from a potential function . they are not integrable . the example of the soap-bubble force makes it clear--- if a , b , c are nearly collinear with b between a and c , closer to a , you can slide b away from a towards c very very close to collinearity , and bring it back less close to collinear . the a-b force is along the line of separation , and it diverges at collinearity , so the integral of the force along this loop cannot be zero . the force is still conservative of course , it comes from a three-body potential after all . this means that the two-body a-b force plus the two-body b-c force is integrable . it is just that the a-c two body force is not . so the separation is completely silly . absence of multi-body interactions for macroscopic objects in empty space the interactions of macroscopic objects are through contact forces , which are necessarily pairwise since all other contacts are far away , and electromagnetic and gravitational fields , which are very close to linear at these scales . the electromagnetic and gravitational forces end up being linearly additive between pairs , and the result is a potential of the form landau and lifschitz consider--- pairwise interactions which are individually rotationally invariant . but for close packed atoms in a crystal , there is no reason to ignore 3-body potentials . it is certainly true that in the nucleus three-body and four-body potentials are necessary , but in both cases you are dealing with quantum systems . so i do not think the third law is particularly fundamental . as a philosophical thing , that nothing can act without being acted upon , it is as valid as any other general principle . but as a mathematical statement of the nature of interactions between particles , it is completely dated . the fundamental things are the conservation of linear momentum , angular momentum , and center of mass , which are independent laws , derived from translation invariance , rotational invariance , and galilean invariance respectively . the pair-wise forces acting along the separation direction are just an accident .
you are double counting the degenacy in $e_{k}$ by multiplying by $2$ and summing from $-4$ to $4$ . either you sum from $0$ to $4$ $2e_{k}$ or you sum from $-4$ to $4$ $e_{k}$ but not both
let 's consider the specific type whistle shown in the question . when we blow the whistle , air is forced to rush out through the narrow opening . the flow of air at the center of the stream is significantly faster than the neighboring air close to the main stream . if the air stream is easily deflected ( unstable ) , vortexes are generated . if the same thing happens repeatedly , many more vortexes with similar properties will be generated . these vortexes cause air pressure to vary in a periodic way , so sound wave is produced . the frequency of this sound wave is related to the rate at which the vortexes are shed . since the process is rather chaotic , many different rates or frequencies are produced at a time . as you can see in the picture , the stream is divided into two parts . one part coming out of the opening and the other part stays inside . sound wave trapped inside will interfere with each others . if the frequency of sound does not match any of the resonant frequencies of the chamber , the waves will interfere destructively and vanish quickly . however if the frequency matches the resonant frequency of the cavity , the wave 's amplitude will increase overtime . the rate of increasing will decreases as the amplitude builds up . eventually it will reach a steady state . at this point the amplitude of sound wave is strong enough that the sound becomes very audible . the sound wave come out of the hole , get dispersed strongly , and finally reaches our ears . some whistles have a small ball bounces around inside the cavity . the ball changes the shape of the cavity and at the same time the resonant frequencies . thus it allows us to hear wider range of sound frequency .
yes , it does . when an object floats , its mass is not affected . it only affect the force experienced by it , as the water exerts a " buoyant force " on the object : basically , there is a pressure difference between the top and bottom surfaces , and this corresponds to a force difference , leading to a net upwards force : however , remember that the force exerted by the water on the object leads to an equal and opposite force exerted by the object on the water . since at equilibrium , $m_{obj}g=\text{buoyant force}$ , a force equal to the weight of the body is exerted on the water . when the beaker is weighed , this extra force is balanced by the normal force . here are some free body diagrams . bf is the buoyant force , t is the string tension . note that n is the weight that the weighing platform measures .
at least for $\omega_1=\omega_2$ it is possible to solve the system exactly . our hamiltonian can be written $$\hat h = \frac{\delta }{2}{\hat \sigma _z} + {\omega} ( \hat a_1^\dagger {\hat a_1} + \hat a_2^\dagger {\hat a_2} ) + ( {g_1}{\hat a}_1+{g_2}{\hat a}_2 ) {{\hat \sigma }_ + } + ( {g_1}{\hat a}_1^\dagger+{g_2}{\hat a}_2^\dagger ) {{\hat \sigma }_ - }$$ we apply a change of variables $$a_1&#39 ; =a_1 cos\theta-a_2sin\theta$$ $$a_2&#39 ; =a_1 sin\theta+a_2cos\theta$$ this change of variables preserves the $\delta$ and the $\omega$ terms in the hamiltonian but rotates the $ ( g_1 , g_2 ) $ vector . by choosing an appropriate $\theta$ we can achieve $g_2&#39 ; =0$ . this means the $ ( a_1&#39 ; , \sigma ) $ system decouples from $a_2&#39 ; $ . the former is an ordinary jaynes-cummings model whereas the later is a harmonic oscillator
the mass of jupiter is about $10^{27}$ kg which , via $e=mc^2$ , translates to $10^{44}$ joules . if one turned the planet into thermonuclear fuel in some way and detonated it immediately , about 1% or $10^{42}$ joules would be released . because the diameter of jupiter is about 130,000 km , the blast would last at least half a second or so . so we have $10^{42}$ joules per half a second . it is $2\times 10^{42}$ watts . the sun only releases $4\times 10^{26}$ watts of power , so the blast would be $2\times 10^{16}$ times stronger than the sun . however , looking at the effects on the earth , we must realize that jupiter is about 5 times further from the earth than the sun , reducing the energy flux by a factor of $5^2=25$ . so the half-second blast seems about $10^{15}$ times stronger than the sunshine . the equilibrium temperature is , because of the $\sigma t^4$ law , about $10^4$ times higher than that from the sunshine , about a million degrees . the sun warms the earth by a degree in hours or so . a source that is $10^{15}$ times stronger obviously needs a tiny fraction of a second to reach thousands of degrees and evaporate the matter on the surface . so no doubt about it , the thermonuclear blast of jupiter would burn and evaporate all nearby sides of all the planets – all of them are comparably far from the ground zero . on the other hand , would the incoming energy be able to evaporate the whole earth ? we would be getting $10^{15}\times 342\times 4\pi \times 6,378,000^2\sim 2\times 10^{32}$ watts for half a second , about $10^{32}$ joules per the blast and per the surface of the earth . the specific heats of materials are comparable to $1,000$ joules per celsius degree and kilogram so we have $10^{29}$ kilogram-degrees to be heated . divide it by the earth mass below $10^{25}$ kg to see that you may still heat the material by tens of thousands of degrees by the incoming light . so i do think that this could evaporate the whole earth but not the largest planets like saturn . needless to say , the sun itself would be pretty much untouched . its surface already has 6,000 degrees or so . the strong radiation from jupiter could bring it to a million of degrees , by the calculation above , but it is the same as the temperature of the interior layers . so the sun would get destabilized a bit but it would quickly converge back to the sun we know , i guess . the calculations above are completely unrealistic because at most , one could think about turning jupiter into a small star that would still burn very slowly and would be far weaker than the sun .
you wish to estimate the flux density ( power per unit area ) at a location based on a reading of the received power at a mobile device situated there . the signal transmitted from the router will result in an energy density of radiated power , decreasing with distance . the mobile device has a receive antenna which collects some of this rf power in its vicinity . this receive antenna is characterized by a parameter called its " effective area " , or " effective aperture " . this has the dimensions of an area , and if its value is $a \ cm^2$ , then at a location where the rf power had a flux density of $f \ mw/cm^2$ , the power captured by the antenna would be $fa \ mw$ now the effective aperture is related to a parameter called the antenna gain by $$ g ( \theta , \phi ) = \frac{4\pi a ( \theta , \phi ) }{\lambda^2} $$ where $\lambda$ is the wavelength . both the antenna gain and the effective aperture are functions of direction ( specified by the two parameters $\theta$ , $\phi$ ) . antennas have varying degrees of directionality ( for example a parabolic reflector is highly directional ) . the formula shows that the direction in which the gain is maximum also maximises the effective aperture . now for a few numbers - say the frequency $f = 2.4ghz$ ( depends which wifi band you are using ) , we get a wavelength $\lambda = 12.5cm$ antennas on mobile devices are actually quite lossy , so lets assume a gain of -3db in the given direction ( i.e. . 0.5 in linear terms ) . this gives us $a = 6.2cm^2$ your reading of -50dbm corresponds to $10^{-5} mw$ , so the flux density is $10^{-5}/6.2 = 1.6^{-6}mw/cm^2$
to formalize michael brown 's comment : suppose we want to change the units that we are working in to ones that are naturally set by the problem . we can write the dimensional variables $x$ and $t$ as $x = s_x \bar{x}$ and $t = s_t \bar{t}$ where $s_x$ and $s_t$ are the length and time scales respectively . these scales carry the dimensions of the quantity and the variables with bars are dimensionless variables , ie just pure numbers . obviously , in the si system $s_x$ is 1 meter and $s_t$ is 1 second . let us substitute our expressions for $x$ and $t$ into the equation that you found : $$mv^2+kx^2 = m \big ( \frac{dx}{dt}\big ) ^2 +k x^2 = m \frac{s_x^2}{s_t^2} \big ( \frac{d \bar{x}}{d \bar{t}}\big ) ^2 + k s_x^2 \bar{x}^2 = m \frac{s_x^2}{s_t^2} \bar{v}^2 + k s_x^2 \bar{x}^2 . $$ we are free to use whatever unit system that we like , so let 's choose $s_x = \lambda$ and $s_t = \sqrt{m/k}$ . substituting this above gives $ k \lambda^2 \bar{v}^2 + k \lambda^2 \bar{x}^2 = k \lambda^2 $ or $$ \bar{v}^2 + \bar{x}^2 = 1 , $$ which is just the equation of a circle of radius 1 . so , just by changing the units we have moved the distance between the foci to 0 . this implies that the distance between the foci itself can not be a physical quantity because we set it to 0 just by rescaling . we cannot ever recover the original answer in the si unit system from this one . however , the distance between the foci can be related to a physical quantity . if the semimajor axis of the ellipse is labeled $a$ and the semiminor axis labeled $b$ then the area enclosed by the ellipse is $\pi a b$ . in your equation in the original units ( assuming without loss of generality that $k/m &lt ; 1$ ) $a = \lambda$ and $b = \lambda \sqrt{k/m}$ so the area is $ \pi \lambda^2 \sqrt{\frac{k}{m}}$ . this area is known as the symplectic area , and it is a conserved quantity for hamiltonian systems . interestingly , even for hamiltonians that are time dependent , the symplectic area is still conserved . we can write the area of an ellipse in terms of the distance between the foci . if the distance between the foci is $c$ , then the semiminor axis is defined to be $b^2 \equiv a^2 - c^2$ , so the area can be written as $\pi a b = \pi a \sqrt{a^2 - c^2}$ . you can check that by using the scales we considered earlier , you can convert the symplectic area for the original case to the symplectic area for the rescaled case ( the circle ) meaning that the symplectic area is a perfectly physical quantity . if you are looking for something you can measure , then the period of motion , $t$ , of the object along the curve can also be found from the symplectic area . $ t = \frac{da}{de}$ where $a$ is the symplectic area and $e$ is the energy . in short , the locations of the foci and the distance between them are themselves not physical quantities . however , they can be related to some physical quantities , namely the symplectic area and the period of motion .
there are bigger things one can collect relating to nuclear weapons . how about this big chunk of metal from the trinity site in 1945 ? http://www.panoramio.com/photo/53676180 near an engineering school full of people smart at physics , no one is concerned about radiation from this . it would not be on public display if it had any measurable radiation much above background . in either case , your coin or jumbo , it is not the unstable fissile material but part of the ( former ) container or shell surrounding the dangerous stuff . yes , a few particles from the nasty core may have kicked some iron and other atoms into unstable isotopes , but not enough to be a problem . you get more radiation from flying in a jet at 35000ft or eating a banana , or just standing there since your body naturally contains carbon 14 , potassium 40 , and other nuclear goodies . perhaps someone with more expertise on radiation and its health effects could kindly give more precise numbers than " a banana " . but wikipedia ( at the moment ) does state this : " in a human body of 70 kg mass , about 4,400 nuclei of 40k decay per second . " also , just to become familiar with radiation amounts in everyday ( and noneveryday ) life , see this infographic : http://xkcd.com/radiation/
more smaller triangles are better than a few big ones ( more stiff ) . also think about which triangle shape distributes the loads for evenly . how are you constrained dimensionally ? ideally you create a vertical structure that distributes the weight over $n$ columns of noodles . if you can not do this , then you find which configuration yields the more vertical orientations . also it is important to know if the noodles can rotate about their support making them a 2-force member , or are they fully loaded with the ends fixed in location and orientation ? think of stiffness here . how would you make the truss as stiff as possible ? would long flat triangles be better , or short tall ones ? i guess without more details we cannot qualify an answer .
the length scale $l$ has to be present in the denominator for dimensional reasons – only logarithms of dimensionless quantities are really " well-defined " unless one wants to introduce bizarre units such as the " logarithm of a meter " . on the other hand , the dependence on $l$ is largely trivial and unphysical for most purposes . replace $l$ by $k$ and you will get $$ v_k ( x , y ) = -\ln \left ( \frac{|\vec x|}{k} \right ) = v_l ( x , y ) +\ln ( k/l ) $$ which only differs by the additive shift , $\ln ( k/l ) $ from the original potential you mentioned . shifts of potentials by a constant are largely inconsequential . in particular , the gradient of $v$ , $\nabla v$ , is not changed at all . to derive the simple claim about the shift above , i only used $\ln ( a/b ) = \ln ( a ) -\ln ( b ) $ a few times . the fourier transform of your potential may be derived by realizing what the laplacian of the potential is . the laplacian is the two-dimensional delta-function . in the momentum basis , it is equivalent to the identity $$ ( p_x^2+p_y^2 ) \tilde v ( p_x , p_x ) = 1 $$ which is easily solved by $\tilde v =1/ ( p_x^2+p_y^2 ) $ . however , the behavior of $\tilde v$ is not quite well-defined for the point $p_x=p_y=0$ where one can add a multiple of a delta-function . this is because $$ ( p_x^2+p_y^2 ) \delta ( p_x ) \delta ( p_y ) = 0$$ so $\tilde v \to \tilde v + k \delta ( p_x ) \delta ( p_y ) $ transforms a solution into another solution . of course , the two-dimensional delta-function in the momentum space is nothing else than the fourier transform of the constant term $\ln ( k/l ) $ we discussed in the position basis so the two ambiguities are the same . now , you could think that the momentum-basis form of the potential , $1/ ( p_x^2+p_y^2 ) $ , is unique because it has no length scale in it and no delta-function in it while we do not see a corresponding unique form of the position-basis potential – because the expressions with any length scale are equally good . but this is really an illusion . as a distribution , $1/ ( p_x^2+p_y^2 ) $ is ill-defined ( in the very same sense as $\ln ( x^2+y^2 ) $ would be in the position basis ) and we must specify what its behavior near the origin is . this ambiguity is the two-dimensional generalization of the subtleties connected with the one-dimensional " principal value " of $1/x$ as a distribution . $1/x$ multiplied by a test function is well-defined if we agree that the symmetric region $x\in ( -\epsilon , +\epsilon ) $ is removed . that is what we mean by the principal value . on the other hand , if you compute the two-dimensional integral of $1/ ( p_x^2+p_y^2 ) f ( p_x , p_y ) $ where $f$ is continuous near the origin , you may switch to the polar coordinates where $r$ in $r\ , dr\ , d\phi$ is beaten by $1/r^2=1/ ( p_x^2+p_y^2 ) $ so you still have a divergent integral that has to be regulated . a way to regulate it is to cut if off and remove the disk $r&lt ; p_{\rm min}$ for some small $p_{\rm min}$ . such a cutoff induces an additive shift dependence that is logarithmic in the cutoff . for the same dimensional reasons as before , one has to take the logarithmic dimensionless so what we need to subtract ( or add ? ) to erase most of the dependence on the cutoff is something like $$ f ( 0 ) \ln ( p_{\rm min} / l_p ) $$ where $l_p$ is the counterpart of $l$ , the length scale you started with . sorry if i omitted some dimensionless coefficients . clearly , the change of $l_p$ is equivalent to redefining the distribution by an additive shift by $\delta^{ ( 2 ) } ( \vec p ) \times l_p$ and $l_p \sim 1/l$ plays the same role of the scale we had before , in the position basis .
in your first reference , page $58$ , equation $ ( 3.55 ) $ , there is a personal definition by the author of what it calls " spinor adjoint of a matrix": $\overline m \stackrel {def}{\equiv} \gamma_0 m^\dagger \gamma_0$ with this definition , as you noticed , you have obviously $\overline {\gamma^\mu}= \gamma^\mu$ the above definition of " spinor adjoint of a matrix " is compatible with the definition of the adjoint $ ( 3.54 ) $: $\overline \psi = \psi ^\dagger \gamma_0$ in the following way : $\overline {m\psi} = ( m\psi ) ^\dagger \gamma_0 = \psi^\dagger m^\dagger \gamma_0 = ( \psi^\dagger \gamma_0 ) ( \gamma_0 m^\dagger \gamma_0 ) = \overline {\psi}~ \overline {m}$
the speedometer on an airplane measures air speed , that is speed relative to a big block of air , not ground speed . so if it is traveling at an air speed of 60 miles per hour , that means if there were two balloons 60 miles apart , it could travel between them in one hour . however , if in that hour , the entire block of air moved 10 miles the other way , then at the end of the hour , the plane would still have covered 60 miles of air , but only 50 miles of ground . the speedometer on an automobile measures ground speed , that is speed relative to the ground , not air speed . so if the car is doing 60 miles per hour , that means after one hour , it will have traveled 60 miles measured on the ground . regardless of the wind .
the standard book is introduction to solid state physics ( 8th ed . 2005 , isbn 0-471-41526-x ) by charles kittel . your question should be answered in chapter 13 . all that is said there should in principle be applicable to semiconductors . but since their bandgap is lower than in dielectrics you might have a problem measuring this ( they might just be too conductive an the effect is lost ) . on the other hand in your prototypical gaas crystals there is a rather strong effect in the ( 111 ) direction due to the stacking of ga and as layers and their asymmetric response to stress . . . note though that this has nothing to do with electron/hole pairs . this is strictly a polarisation phenomenon .
in 1958 dac was developed in nist by weir , lippincott , van valkenburg , and bunting . this dac article shows an image of a hand palm sized dac in the nist museum . the diamond anvil cell ( dac ) is a refined device , based on the bridgman cell . the latter was developed in the beginning of the first half of the 20th century . your question aims on the updated dac technique . what is the technical distinctive feature ? the bridgman cell has a quatro-plate geometry for applying force . this first developed technique has the feature to allow rays or wires between a pair of pressure plates and is available in a wide pressure range . polished diamond flat-tops allow beam transmission through the sample . this is depicted in the wikipedia article . however beam intensity is extenuated due too high fresnel losses . refractive index $n_{diamond}\approx 2.4$ is high . spectroscopic measurement may be influenced by natural lattice defects in diamond lattice . the refined method , called dac , allows higher pressure . to archive this the bridgman anvil made of tungsten-carbon alloy was replaced by a single-crystal diamond . the problem of breaking anvils was solved and a new pressure range was available . fun fact diamonds used by nist often were destroyed due to high pressure . the diamonds were former property of smugglers . government agents confiscated the gem diamonds . they were given for scientific purpose to the researchers .
since this question is still open and therefore not definitely answerable at present , i save the valuable discussion of the topic in the comments as an answer such that it does not get lost : this is just an accident of 10 dimensions--- there is too much supersymmetry to have a full susy superspace . it is a very good question , but research level , if you answer it fully , everyone will breathe easier . – ron maimon apr 12 at 2:11 3 up voted there are superspace formulations for 4d n=4 theories , the problem is that they only are on-shell formulations . the n=4 multiplet contains the n=2 hypermultiplet and there is a no-go theorem saying that there is no off-shell formulation with a finite number of auxiliary fields . thus you get projective and harmonic superspaces with infinite numbers of auxiliary fields . this works for n=2 , but in n=4 all constraints to reduce the unconstrained superfields down to the physical multiplets force the fields on-shell . – simon apr 12 at 3:47 3 up voted and as @ron says , the reason why it is so difficult to construct such formulations is an open research-level question . if the reason was known , then we had either have a workable n=4 superspace formulation or a no-go theorem by now . . . – simon apr 12 at 3:48 3 up voted dear dilaton , good question . ron and simon have already answered to some extent and i will only offer a different extent , extending ron 's comment in particular . if you want to make n=1 susy in 4d i.e. 4 real supercharges manifest , you need 4 superspace fermionic coordinates . with 16 supercharges , you would probably need at least 16 fermionic coordinates in the superspace but then the fields would have 216=256 components which is pretty high give that you only need 16 on-shell components only . most of the component fields would have to be auxiliary , linked to deritives of others , etc . hard – luboš motl apr 12 at 5:37 4 up voted there is also an interesting twistor-like transform for the 10d super yang-mills by a guy called witten – luboš motl apr 12 at 5:39 thanks guys for these valuable comments and the cool links therein . i would " like " and appreciate them as " partial " answers ( since as you say there is no full answer on this yet ) too . . . :- ) . – dilaton apr 12 at 8:45 @lubošmot l must admit that twistors are one of my black holes of ignorance ( i did not get it from roger penrose 's " road to reality" ) :-/ . . . so i am checking from time to time if i can find a nice " pedagogical " introduction to this on trf ; - ) – dilaton apr 12 at 8:50 1 up voted @dilaton : this is the problem with research level stuff , i can not answer because i do not feel confident enough in my biases about what the answer could or should be to put them in writing , and i would mention a bunch of things that i tried and did not work to answer this , and are not interesting , and i think everyone else is hesitant to answer too for similar reasons . maybe you could copy the comment thread into an answer box , and accept your own answer . – ron maimon 2 hours ago 1 up voted i mean , if you want a little more on this--- there is the question of whether superspace is fundamental in the first place--- it is just a trick for writing multiplets in a way that takes the susy off shell naturally , but the physical reasoning has always eluded me . i tried nicolai maps as an alternative , but it never worked , and it always is tantalizingly close to working , and i tried learning harmonic superspace for on-shell n=4 , but although it is correct , its so annoyingly complicated to work with ! and the s-matrix is simple , so there is a better language out there , i do not know what . – ron maimon 1 hour ago thanks @ron maimon , that is a good idea to save the discussion into an answer . i`m somehow intrigued too by the question if superspace itself could have some physical meaning . . . – dilaton 2 mins ago even though the question is still open , it could probably nevertheless be worthwhile to know what you tried and why it did not work . i mean similar to some kind of " null results " who can be interesting too . . . ? – dilaton
your question gets at two related points : what allows materials to conduct , and what allows materials to be transparent . conductivity requires easy movement of charge through a material . typically this charge is in the form of electrons but in an electrolyte the charge movement is primarily ions moving in the fluid . the quantum-mechanical nature of electrons means they can only exist in certain , discrete energy levels . the specific levels available depend on the electron configuration of the atoms or the ( more complicated ) electron configuration of molecules . good conductors have many closely-spaced energy levels available to electrons and very little energy is required to move them between these levels . this is the band theory of conductivity . the opacity of a material also depends on the electron configurations in the material and the energy levels available . the energy of a photon is proportional to its frequency which means materials tend to act as a low-pass filter . high energy ( high frequency ) electrons have enough energy to kick electrons into higher energy states which cause the photon to be absorbed . if the material has big gaps between the energy bands then photons will not have enough energy to force an electron energy transition so the light will pass through without being absorbed . generally conductivity and transparency are at odds with each other . conductors need closely spaced energy levels and transparency needs widely spaced levels . in the case of indium tin oxide has a mix of both properties . it has electron configurations that are closely spaced at low energies and then a large bandgap at visible light energies . this causes ito to be opaque to low energy light ( infrared ) but transparent to visible light .
if you consider a standard differential operator $b$ working on functions defined in $\mathbb r^n$ , like $\partial/\partial x_i$ or a polynomial of partial derivatives , and pick out a sufficiently smooth function $f$ vanishing in a neighbourhood $\omega$ , you see that also $bf$ vanishes therein . this is the relevant notion of locality for operators . in the rhs of the equation you wrote down an operator shows up which does not fulfil locality in the sense i said . that equation is , in fact , the equation satisfied by the positive energy solutions of klein-gordon equation . the operator in the rhs cannot be defined by formal taylor expansion ( it works only formally ) , but one has to use spectral theory . in the considered case it is equivalent to translate that equation in fourier transform . non locality arises here due to a known property of the operator $a:= \sqrt{-\delta + ai}$ and , more generally , for $ ( \delta + ai ) ^\nu$ with $\nu \not \in \mathbb z$ . this property is called anti locality ( i.e. . segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ) and is related to the famous reeh and schlieder property in qft . anti locality means that if both $f$ and $af$ vanish in a bounded region $\omega \subset \mathbb r^3$ then $f$ is everywhere zero . if $f$ has support included in a bounded open set $\omega$ , then , remarkably and very differently from what happens for standard differential operators , $af$ does not identically vanish outside $\omega$ otherwise $f$ would be the everywhere zero function .
i will try to answer from what i have understood so far . every hermitian operator has a set of linearly independent eigenvectors and hence we can use it construct a basis ( provided it spans the space ) . lets say say operator is $ \hat a $ and their eigenvector set $ \{\vert a_i \rangle\} $ with the eigenvalue equation , $$ \hat a \vert a_i \rangle = a_i \vert a_i \rangle\ $$ now we have an arbitrary state $ \vert \psi \rangle\ $and expand in the basis $ \{\vert a_i \rangle\} $ $$ \vert \psi \rangle\ = \sum_i c_i\vert a_i \rangle $$ $$ \langle \psi \vert = \sum_i c_i^*\langle a_i \vert $$ now normalising $ \psi $ would require the condition $$ \sum_i |c_i|^2 = 1 $$ with that now we can what $ c_i $ would mean physically , $|c_i|^2$ is probability of finding $\vert \psi \rangle\ $ in the eigen state $\vert a_i \rangle\ $ . hence the sum of probabilities is one ( from the above equation ) . when you do a measurement on $\vert \psi \rangle\ $of the observable $ \hat a $ , i.e. $$ \hat a \vert \psi \rangle\ = \hat a \sum_i c_i\vert a_i \rangle =\sum_i c_i \hat a \vert a_i \rangle = \sum_i c_ia_i\vert a_i \rangle$$ and $$ \langle \psi \vert \hat a \vert \psi \rangle = \sum_i |c_i|^2a_i $$ with the interpretation that $|c_i|^2 $ as the probability this would become the average value of the observable . it is important to realise , when you do a single measurement the outcome is such that you will obtain a value $a_i$ with a probability $|c_i|^2$ . remember your measurement will yield only a single value , this value is obtained by a number of measurements and averaging over them . now if the wavefunction is in an eigenstate of the observable , say $ \vert \psi \rangle = \vert a_k \rangle $ , if you do a measurement of the operator $\hat a$ you will always obtain the value $a_k$ .
there are two that i know of in the context of state estimation . the first is for estimating the mean of $p$ and is a metropolis-hasting mcmc algorithm here : optimal , reliable estimation of quantum states . the second is also mainly for computing the mean ( but can do other functions -- including the characteristic function of the region you are interested in ) . it is a sequential monte carlo algorithm and is here : adaptive bayesian quantum tomography .
the hilbert space of the bilayer system , which is spanned by the four-component basis , can be divided into the low-energy subspace and the high-energy subspace . the low-energy subspace is spanned by $|\psi_{a1}\rangle$ and $|\psi_{b2}\rangle$ , while the high-energy subspace is spanned by $|\psi_{a2}\rangle$ and $|\psi_{b1}\rangle$ . it is the interlayer coupling $\gamma'$ that makes the four basis different . $|\psi_{a2}\rangle$ and $|\psi_{b1}\rangle$ are coupled to each other , forming the bounding and anti-bounding states : $|\psi_{a2}\rangle\mp|\psi_{b1}\rangle$ ( of the energy $\mp\gamma'$ ) , and hence pushed away from the fermi surface ( or the zero-energy level ) . while $|\psi_{a1}\rangle$ and $|\psi_{b2}\rangle$ are not coupled , and remain zero energy in the $k\to0$ ( long-wave-length ) limit . we wish to consider the effective hamiltonian for the low-energy electrons near the fermi level ( because transport and many other physical properties are dominated by them ) . these low-energy electron states are mainly made up of $|\psi_{a1}\rangle$ and $|\psi_{b2}\rangle$ , so we seek to reduce the 4-by-4 hamiltonian to its upper 2-by-2 block . this can be done in terms of a 2nd-order perturbation . the resulting 2-by-2 hamiltonian is acting in the low-energy subspace spanned by the basis $|\psi_{a1}\rangle$ and $|\psi_{b2}\rangle$ . here are the details of how it works . to simplify our notation , we introduce the pauli matrices $\sigma_x$ , $\sigma_y$ and $\sigma_z$ . the 4-by-4 bilayer hamiltonian can be written as $$h_\text{bilayer}=\left ( \begin{matrix}0 and v\vec{k}\cdot\vec{\sigma}\\ v\vec{k}\cdot\vec{\sigma} and \gamma'\sigma_x\end{matrix}\right ) , $$ where $\vec{k}\cdot\vec{\sigma}=k_x\sigma_x+k_y\sigma_y$ . around the momentum $k\to 0$ , $v \vec{k}\cdot\vec{\sigma}$ term can be treated as the perturbation . explicitly , $h_\text{bilayer}=h_0+h_1$ , with $h_0$ being the 0th-order hamiltonian and $h_1$ being the 1st-order perturbation , $$h_0=\left ( \begin{matrix}0 and 0\\ 0 and \gamma'\sigma_x\end{matrix}\right ) , h_1=\left ( \begin{matrix}0 and v\vec{k}\cdot\vec{\sigma}\\ v\vec{k}\cdot\vec{\sigma} and 0\end{matrix}\right ) . $$ now we want to consider the perturbative correction brought to the upper 2-by-2 block of $h_0$ by $h_1$ . according to the 2nd-order perturbation formalism , the correction ( in the upper block ) reads $$h_2'=- ( v\vec{k}\cdot\vec{\sigma} ) ( \gamma'\sigma_x ) ^{-1} ( v\vec{k}\cdot\vec{\sigma} ) =-\frac{v^2}{\gamma'} ( ( k_x^2-k_y^2 ) \sigma_x+2k_xk_y\sigma_y ) . $$ the effective hamiltonian in the upper block should be given by $h_0'+h_1'+h_2'+\cdots$ , but $h_0'=h_1'=0$ , so we eventually have $$h_\text{bilayer}'\simeq h_2'=-\frac{v^2}{\gamma'}\left ( \begin{matrix}0 and ( k_x-ik_y ) ^2\\ ( k_x+ik_y ) ^2 and 0\end{matrix}\right ) , $$ as expected ( with the effective mass $m$ properly defined to make $\hbar^2/ ( 2m ) =v^2/\gamma'$ ) .
the calculation is described in detail in the wikipedia article on recombination . if you consider the ionisation of hydrogen as a reaction : $$ p + e \rightarrow h + \gamma $$ then you can write down an expression for the equilibrium constant as a function of temperature using the saha equation : $$ \frac{n_pn_e}{n_h} = \left ( \frac{m_ek_bt}{2\pi\hbar^2} \right ) ^{3/2} \exp \left ( \frac{-e_i}{k_bt} \right ) $$ if you take 50% ionisation you can work out the corresponding temperature and it turns out to be about 4,000k . so now it is just a matter of relating the temperature of the universe to the time after the big bang . once we are past the various phase transitions that happened in the first few instants after the big bang the temperature is inversely proportional to the scale factor . sadly there is not a simple equation to give the scale factor as a function of time , however it is a straightforward numerical calculation , and the result is that the temperature was 4,000k about 380,000 years after the big bang . that is how the figure of 380,000 years is calculated .
mathematically spoken , since you want your wave functions to be square integrable , your wave functions must be in $l^2$ or some subspace thereof . however , you will not find a function in this space that has a support on a countable set of points , since the lebesgue integral cannot see countable sets ( measure 0 ) , hence there cannot be a function ( i.e. . no wave function ) with support in a single point ( incidentally , the delta function is not a " function " in a way for that reason ) . this tells us that a wavefunction for a particle that is fully localized cannot be defined in the usual setting of square lebesgue-integrable functions , which is not too tragic , because we do not really think it makes physical sense anyway .
sorry , i was not being very clear . what i mean is , why do photons interact with electrons ? what we have discovered up to now with our studies in physics is that there exist 4 fundamental interactions of elementary particles . both the photon and the electron are elementary particles and interact with the electromagnetic interaction . now the electrons can be free , as for example in an accelerator beam , or bound with the electromagnetic interaction in an atom , as in the hydrogen atom . if they are free , a photon hitting them will scatter elastically , or might give up part of its energy to the electron and go away with a smaller energy/frequency ( $e=h\nu$ ) . if bound , it is in an energy level about the nucleus which has a unique , quantized energy , $e_1$ . over it will be unoccupied energy levels . an incoming photon , if it has an energy that corresponds to the difference between an empty energy level $e_2$ , i.e. it has energy $e_2-e_1$ can transfer its energy to the electron kicking it up and disappearing as an individual photon . the electron will probably decay from that energy level emitting a photon of energy $e_2-e_1$ but it will be a different photon . in nuclei with large $z$ there may be cascades of photons if the energy of the initializing photon is large and there exist intermediate energy levels . now if we go to second quantization , the photon interacts with the electron because it is the carrier of the electromagnetic force . this covers both the bound and the unbound state of the electron , except that in the bound case still the energy has to be $e_2-e_1$ to give a large enough probability of interaction .
it will actually weigh $f_b+\rho_lv_lg$ which is $\rho_lv_bg+\rho_lv_lg$ , and not one of the two options you say . consider the liquid as the system and the block as an external body . now we know that the liquid applies a buoyant force on the block . according to newton 's third law , the block will apply a reaction force on the liquid , equal in magnitude and opposite in direction . thus total force on liquid is $f_b+f_g$ which gives $\rho_lv_bg+\rho_lv_lg$ . note that this is the weight when the block is attached to the spring balance , and suspended in the liquid . if the block is kept on the floor not attached to the spring balance , the reading weight will be different . edit : in the case when the block is resting on the floor , the weight will simply be $\rho_lv_bg+\rho_lv_lg$ , because when you consider the block and liquid as a system , the buoyant force will become an internal force and cancel out on the whole system . so the only force responsible for the weight will be gravity .
using the method of images , you can calculate the force between the ring of charge and the sphere . assume the sphere is on the z axis with it is center on the point $z$ , a radius of $r_s$ and the ring 's radius is $r_r$ with a charge density $\lambda$ . so $z$ denotes the center of the sphere . to calculate the force , you can replace the sphere with a charged ring ( method of images ) with a charge density $\lambda'$ placed at a distance $d$ below the sphere 's center and a point charge $q'$ at the center of the sphere to make the sphere electrically neutral : $$\lambda'=-\lambda\frac{ \sqrt{r_r^2+z^2}}{r_s}$$ $$d=\frac{zr_s^2}{z^2+r_r^2}$$ $$q'=q\frac{r_s}{\sqrt{z^2+r_r^2}}$$ now the problem reduces to finding the force between the main ring and the induced ring and point charge . the image below shows the choose of parameters : i assume that $r_{\text{ring}}&gt ; r_{\text{sphere}}$ . the equations for the case of $r_{\text{ring}}&lt ; r_{\text{sphere}}$ are the same , except that the motion is restricted to $z&gt ; \sqrt{r_s^2-r_r^2}$ . the field of the main ring differs for points with $r&gt ; r_r$ and $r&lt ; r_r$: ( $q$ is the ring 's total charge ) $$\phi ( r , \theta ) =\frac{q}{4\pi\epsilon_0}\sum_{l=0}^\infty \mathrm{p}_{2l} ( \cos \theta ) \cases{\frac{r_r^{2l}}{r^{2l+1}}\mathrm{p}_{2l} ( 0 ) \ , \ , \ , \ , \ , \ , \ , \ , \ , \ , \text{$r&gt ; r_r$}\\\frac{r^{2l}}{r_r^{2l+1}}\mathrm{p}_{2l} ( 0 ) \ , \ , \ , \ , \ , \ , \ , \ , \ , \ , \text{$r&lt ; r_r$}}$$ and $$\mathbf{e}=-\frac{\partial \phi}{\partial r}\hat r-\frac{1}{r}\frac{\partial \phi}{\partial \theta}\hat \theta$$ because of the rotational symmetry of the induced charges , on which we want to find the force , the force will be in the $z$ direction : ( i omitted the lengthy calculations ) $$\mathbf{f}_{\text{total}}=\hat z \frac{q^2}{4\pi \epsilon_0} \left [ \frac{z}{ ( r_r^2+z^2 ) ^2}-\frac{1}{r_r^2\sqrt{z^2+r_r^2}}\cases{{\times f_{\text{out}} ( r , \theta ) }\\{\times f_{\text{in}} ( r , \theta ) }} \right ] $$ where $f_{\text{out}}$ is used when $r&gt ; r_r$ and $f_{\text{in}}$ is used when $r&lt ; r_r$ . these are dimensionless functions that appear when differentiating the above potential : $$f_{\text{out}}=\sum_{l=0}^\infty\mathrm{p}_{2l} ( 0 ) \left ( \frac{r_r}{r}\right ) ^{2l+2}\left ( ( 2l+1 ) \mathrm{p}_{2l} ( \cos \theta ) \cos\theta +\mathrm {p}^1_{2l} ( \cos \theta ) \sin\theta \right ) $$ $$f_{\text{in}}=\sum_{l=0}^\infty-\mathrm{p}_{2l} ( 0 ) \left ( \frac{r}{r_r}\right ) ^{2l-1}\left ( 2l\mathrm{p}_{2l} ( \cos\theta ) \cos \theta-\mathrm{p}^1_{2l} ( \cos \theta ) \sin \theta \right ) $$ and $r$ and $\theta$ are functions of $z$: $$r=\left| z-\frac{r_s^2}{\sqrt{z^2+r_r^2}} \right|$$ $$\cos \theta=\frac{z-\frac{zr_s^2}{z^2+r_r^2}}{r}$$ $$\sin \theta=\frac{\frac{r_rr_s^2}{z^2+r_r^2}}{r}$$ $\mathrm {p}^1_{2l} ( \cos \theta ) $ s are associated legendre functions , and appear when differentiating $\mathrm {p}_{2l} ( \cos \theta ) $ w.r.t. $\theta$ . the reason of summing over even terms ( indexes are $2l$ instead of $l$ ) is that $\mathrm {p}_{l} ( 0 ) $ is nonzero for even terms , as is equal to : $$\mathrm {p}_{l} ( 0 ) = \frac 1 {2^l} \sum_{k=0}^l {l\choose k}^2 ( -1 ) ^{l-k}$$ it seems impossible to say anything intuitively , but the above equations can be used to simulate the system ( with a rather good accuracy , since the higher order terms in the above series go as $r^{2l-1}$ instead of $r^l$ ) .
no . it does not necessarily mean that the acceleration of $a$ is greater than the acceleration of $b$ . here 's an explicit counterexample : object $a$ is moving at $10\ , \mathrm{m/s}$ with constant velocity while object $b$ is moving at $5\ , \mathrm{m/s}$ with an acceleration of $1\ , \mathrm m/\mathrm s^2$ . in this case , the acceleration of $a$ is zero , so $b$ 's acceleration is greater , but it is velocity is lower . note that the initial conditions of the motions of the two objects are irrelevant ; we are talking about instantaneous velocities and accelerations , and given any two objects , one can completely independently pick their velocities and accelerations .
it is important to distinguish different phases of the material . while it is true that water and snow consists of the same building blocks of $h_2 o$ , those individual blocks are actually not as important as the resulting material . you can build a storage room and pyramids just out of bricks ( at least in principle ) . gases let 's start with the simplest case . simplest in the sense that there is not much going on , just molecules flying around . let us just assume ideal gas where molecules do not interact with each other . then to explain everything it suffices to look at just one molecule . the color of the molecule of course arises because of its absorption spectrum . this in turn depends on the molecule 's energy levels . for atoms these are pretty nice discrete levels . for more complicated molecules you also have rotational and vibrational degrees of freedom to take into account and besides discrete levels you will also see continuous strips that consist of very fine energy levels corresponding to that . for illustration , see the wikipedia article on $h_2$ hydrogen . in any case , if you are somehow able to obtain that energy spectrum you can then investigate the macroscopic properties by the means of the usual boltzmann statistics $$\hat w = z^{-1} \exp ( -{\beta \hat h} ) = z^{-1} \sum_n\exp ( - \beta e_n ) {\hat p}_n $$ with $z = {\rm tr} \exp ( -{\beta \hat h} ) $ being the partition function , $\beta$ the inverse temperature and $p_n$ projector on $n$-th energy level . using this you can see that as you increase temperature the energy level distribution will change to occupy higher levels and this in turn will change the probability of individual absorption processes between certain levels . now , the only place where you need to talk about qed is the connection between absorption spectrum and energy levels . recall that in quantum mechanics energy levels are stable . they do not change , so there would be neither emission nor absorption . to resolve this apparent paradox we have to recall that we forgot to quantize the electromagnetic field . if you take this into account then atom 's excited energy levels are no longer stable because of the fluctuations of electromagnetic field . or in particle terms : because the number of particles is not conserved and it is easy to create photon out of nothing . of course , energy is conserved so only photon 's corresponding to the energy difference are possible . and the same can be said for absorption . liquids and solids for now i will not talk about these phases because the answer is both becoming long and because optical properties of solids would constitute a book of its own . as for liquids , i have a feeling that the same analysis as for gases should hold except it is of course no longer sufficient to talk about individual molecules because of non-negligible interactions . but i guess starting from ideal gas 's statistics and just switching temperature should give a reasonable first approximation . remark : looking back on the answer , i did not really explain your question completely . but i guess it is because it is quite hard to encompass everything that is going on . maybe another split and specialization ( e . g . to color of solids ) of the question would be in order ? :- )
i assume that your question is : " how do i compute the helmholtz free energy given the dependence of the entropy on the intensive parameters $s ( e , n , v ) $ ? " in this case you start by inverting $s ( e , n , v ) $ to get a relation for $e ( s , n , v ) $ . then you can perform the legendre transform , $$ f ( t , n , v ) = e ( s ( t , n , v ) , n , v ) - s ( t , n , v ) \ , t \ , , $$ where the function $s ( t , n , v ) $ is defined through the relation $$ \frac{\partial e}{\partial s} ( s , n , v ) \equiv t \ , . $$
there are not mechanical oscillators that have the same frequencies as visible light . the most common oscillators with the same frequencies as visible light are the electrons orbiting atoms , which is why atomic transitions emit and absorb visible light . if you are willing to accept a powered system , you could absorb all the light into an image-processing system and then project that image in another direction , possibly with much more light intensity than the original image had . for instance , folks with telescopes sometimes use video cameras to observe objects too faint to see naked-eye in real time .
it is a functional combination of other constants : the energy ( another constant of motion ) and the initial condition . this would be the same as proving that , in classical mechanics , $e^2 + \log ( l ) $ with $l$ being the total angular momentum is a constant . it does not hold any new physical meaning , further than what you got . if you did not know all this , you could use the fact that $u$ is a constant to show that the amplitude or the energy are constants .
the assumption here is that the water is more incompressible than a balloon , so that the density variations in the water are less than the density variation in a balloon . this is true for a balloon , but false for a submarine made of steel . the compressibility of steel is roughly 80 times less than the compressibility of water , so if the submarine has thick steel walls , it will make a stable depth equilibrium using bouyancy alone . you can maintain a depth with a submarine by adjusting the density very slightly to the one appropriate for water at a given depth . for the balloon , the equilibrium is unstable , but for there to be an equilibrium at all requires that the balloon skin be made of a material significantly denser than water , and the depth where there is balance , the air will be compressed enormously . edit : submarine compressibility ( in response to the comments of zassounotsukushi ) when a submarine is submerged in water , the compressibility is not determined by the bulk compressibility of steel , because the pressure stress has to travel through the much thinner hull to get from one side of the submarine to the other . the actual stress in the hull is increased . to see how much it is increased , roughly , consider a model submarine which is a square-box-cylinder , with walls of thickness w and side-length l . the pressure is a flow of momentum per unit length equal to pl from one side ( times the length ) of the box to the other , through the sides of the wall , and if the thing were perfectly solid , the compressibility would be roughly 80 times less than water . but the momentum must flow from right to left through a region of width 2w instead of the full width l , as it would be in a solid steel cylinder , so the actual momentum current in the side of the box is increased by a factor of l/2w . for a round cylinder , up to a factor of order unity ( it will be between . 5 and 2 ) , the analogous factor is r/w . for a typical submarine , i found a hull width w of 30 cm , while a spatious radius is 6m . the ratio is 1/20 , so the submarine is about 20 times more compressible than bulk steel . but this still makes the submarine 4 times less compressible than water , and makes the equilibrium stable . steel balloon in order to make a hollow steel balloon neutrally bouyant , ignoring the density of air , the ratio of unoccupied volume to occupied volume is as the ratio of the density of steel to air volume is as the ratio of the density of steel to water , about 8 to 1 . since this ratio is much smaller than the bulk modulus ratio of 80 to 1 , a neutrally bouyant steel balloon will acheive a stable equilibrium . this was the model i originally had in mind for a submarine , and for this model , the walls are extremely thick , and the bulk compressibility is only changed by a factor of order 10 at most , not 80 . but this model is nonsense : it is assuming an empty submarine ! the accurate description is above .
the etymology of limb for an astral object comes from the original latin root . limbus means border in latin , a language in which all good students of last centuries were proficient , including astronomers ; when describing heavenly bodies viewed through the telescopes they shortened it to limb . ( astronomy ) the apparent visual edge of a celestial body . ( on a measuring instrument ) the graduated edge of a circle or arc now limb as an appendage of a body or tree has a different etymology in english . it comes from middle english lim and acquired the " b " later .
the lifetime of hydrogen bonds in room temperature water is picoseconds . the idea that they could persist long enough to drink is wrong by a factor of trillions .
the pairs of lines are the same phase and at the same voltage - they are really just a single thick wire split into two thinner ones . it is easier to install two smaller wires to double the current capacity than a single thicker wire . it is easier to handle the lighter cable and you can stock just a single gauge of wire and handling equipment . it also provides some redundancy if one wire fails . there is an effect with ac electricity that the current mostly flows near the surface of the conductor . a number of thinner wires have more area-near-the-surface and so a larger effective cross section area than a single thick one . at the 50/60hz frequencies used by ac transmission this only affects wires more than a cm thick .
what about positivity ? the product of bounded positive operators is positive if they commute ( see proof below ) , otherwise there is no guarantee . if your initial povms are not compatible , in general , the operators of the final candidate povm is not made of positive operators and thus they do not define a povm . proposition . if $a , b \geq 0$ where $a , b :\cal h \to \cal h$ are bounded with $\cal h$ hilbert space and $ab=ba$ then $ab \geq 0$ . proof . it is known that if $a\geq 0$ is bounded , then there is a unique positive bounded operator , $\sqrt{a}$ , such that $\sqrt{a}^2 =a$ . moreover that operator commutes with all bounded operators commuting with $a$ . in the present case $ab= \sqrt{a}\sqrt{a}b$ and , since $a$ and $b$ commute , $ab= \sqrt{a}\sqrt{a}b= \sqrt{a}b \sqrt{a}$ . finally , using the fact that a bounded positive opertor is self-adjoint , $$\langle x , ab x \rangle= \langle x \sqrt{a}b \sqrt{a} x \rangle = \langle \sqrt{a} x , b \sqrt{a} x \rangle = \langle y , b y \rangle \geq 0$$ because $b\geq 0$ . since $x\in \cal h$ is arbitrary , it implies that $ab \geq 0$ . qed
to add to carl witthoft 's answer : your proposed device would violate conservation of optical extent aka optical étendue unless it were an active device ( i.e. one needing a work input to " uniformise " a given quantity of light ) . the law that optical extent can only be held constant or increased by a passive optical system is equivalent to the second law of thermodynamics for light , because the optical extent of a light source is its volume in phase space . the optical extent $\sigma$ for the light radiated from a surface $s$ is : $$\sigma = \int_s \int_\omega i ( x ) \cos ( \theta ( x , \omega ) ) \ , {\rm d} \omega\ , {\rm d} s$$ where we integrate the intensity $i$ at each point $x\in s$ over all solid angles $\omega$ taking account of the angle $\theta$ each component of the radiation from point $x$ makes with the surface 's unit normal . then we integrate this quantity over all points on the surface $s$ . so , the $\sigma$ for your output would be nought , whilst it would be large for your input , so no passive imaging device can do what you ask . so , another way of putting carl 's answer would be that the proposed device would have to " forget " the state encoded in the input light 's wavefront direction at each point . thus your proposed device , if at all possible , would needfully be an active device , needing work input of $k_b\ , t\ , \log 2$ joules for each bit of light state forgotten in accordance with the landauer principle form of the second law of thermodynamics . i say more about this in my answer here .
to remain on the merry-go-round , the person must be accelerated towards the center . how is the force applied to the person to provide that acceleration ? the static friction provides a way for the floor of the merry-go-round to force the person along the circular path . if the floor were to suddenly become frictionless and the person was not otherwise attached , the person would continue moving along a line tangent to the merry-go-round .
a gravitational wave will distort space-time and the light that is on a path affected by such a wave will be similarly affected , but it will still take longer ( or shorter ) to travel that path . imagine a car travelling along the surface of a trampoline , a wave on the trampoline could cause its path to become longer , but it will not be impossible to detect whether that path was longer . the key is to monitor path lengths in two orthogonal directions and compare them continuously , a gravitational wave will make one shorter and the other longer for long enough to be detectable by a laser . the wavelengths will not necessarily be affected , but the time of flight and thus the returning phase of the waves will be , and that can be measured very precisely using interferometric techniques similar to a michelson-morley device . as far as the energy to distort matter , that is irrelevant , it is space-time that is being distorted by gravitational waves , the energy calculations for compressing solid matter do not apply . pick up a single coin , that coin is distorting all of space-time in , effectively , the entire universe ! now move the coin around , now it is sending out gravitational waves which affect space-time in the entire universe as well . even , eventually , distant neutron stars or entire galaxies . granted , for a single coin the perturbations are so tiny as to be utterly indetectable and inconsequential , but they still happen . phenomena on a much larger scale such as closely orbiting neutron stars will send out much , much stronger gravitational waves , which we might be able to detect with sufficiently precise instruments . however , they still compress and expand the space-time all around us , but this is not a process that requires a continuous input of energy anymore than it takes a continuous input of energy to keep an object in orbit of another .
the mass of the black holes in galactic centres are typically of a few million solar masses , while the galaxies have masses that are hundreds , thousands or even tens of thousand times larger . so while the central supermassive black hole of a galaxy may have played a part in forming the galaxy and determining the central dynamics of the galaxy , the overall dynamics of the galaxy is dominated by the surrounding dark matter halo , not by the central black hole . the apparent motion of galaxies has two components : one which is due to the cosmological expansion of space , and one which is due to local gravitational interactions with other galaxies . neither , however , are correlated with the galaxies ' proper rotation or alignment .
but why is not sr taught with an imaginary time coordinate as standard ? from " gravitation " , page 51 , via google books . i will type up a paraphrase later .
the earth goes around the sun kind of like a ball on a string goes in a circle when you swing it around . instead of the string holding the earth , the sun 's gravity holds it . as the earth goes around the sun , it also spins . this makes day and night . you can see this with a flashlight on the ball when you spin it with your hand . part 2: since gravity is a little flexible , in a sense , the earth 's orbit around the sun is not a perfect circle . sometimes we are closer to the sun , and sometimes we are farther away . the earth 's spin is not directly lined up with the sun . the earth is tilted . so sometimes , the sun hits our part of the earth more directly . when this happens , we get more heat and that makes summer . that is why in summer the sun is higher in the sky at noon .
no . notoriously , supersymmetric string theory has to be formulated in 10 dimensions in order to be consistent . another example is supergravity , which can be formulated in a maximum of 11 dimensions otherwise it predicts particles with spins higher than two .
no , this is talking about correlations between s random particles . the s-particle distribution function is a 2*d*s ( so 6s in 3 dimensional space ) dimensional pdf that statistically describes s particles . for s=1 , this is just the normal density in phase space . for s=2 , this might show , for example , that more often than not two particles are traveling away from each other ( maybe they just collided ) . a good source for this is ch . 2 in " the statistical physics of particles " by mehran kardar .
i suspected that one needed to go back to the definition of the currents and indeed , in doing so one can derive the result . here 's a short version . the electron current is defined as [ see equation ( 6.6 ) in 1 ] $$\tag{1}j_\mu ( x ) = -e\bar{u}_f\gamma_\mu u_i \times\mathrm{exp} [ ( p_f-p_i ) \cdot x ] $$ which we write as $$\tag{2}j_\mu ( x ) = j_\mu\mathrm{exp} [ ( p_f-p_i ) \cdot x ] . $$ we will also need to use $$\tag{3}q = p_i^a-p_f^a = p_f^b-p_i^b$$ then the integral $ ( 1 ) $ in the original post can be written $$ \tag{4} t_{fi} = -i\int \ ! dt_ad^3x_a d^3x \ , \ , j^aj^b e^{i ( p_f^{a0}-p_i^{a0} ) t_a}e^{i ( p_f^{b0}-p_i^{b0} ) t_a}\frac{1}{|\vec{x}|}e^{i\vec{q}\cdot\vec{x}} . $$ now shifting $\vec{x}=\vec{x}_b-\vec{x}_a$ with $d^3x=d^3x_b$ and using $ ( 3 ) $ and $$ ( \vec{p}_f^a-\vec{p}_i^a ) \cdot ( \vec{x}_b-\vec{x}_a ) = - ( \vec{p}_f^b-\vec{p}_i^b ) \cdot\vec{x}_b- ( \vec{p}_f^a-\vec{p}_i^a ) \cdot\vec{x}_a , $$ equation $ ( 4 ) $ becomes $$\tag{3} t_{fi} = -i\int \ ! dt_a\int d^3x_a\int d^3x_b \ , \frac{j_0^a ( t_a , \vec{x}_a ) \ , j_0^b ( t_a , \vec{x}_b ) }{4\pi|\vec{x}_b-\vec{x}_a|} , $$ where $j_0^a ( t_a , \vec{x}_a ) $ corresponds to the op 's $a ( t_a , \vec{x}_a ) $ and so on . references : see appendix of j . h . field , classical electromagnetism as a consequence of coulomb 's law , special relativity and hamilton 's principle and its relationship to quantum electrodynamics
the fraction of baryonic matter to dark matter is not deduced only from galactic dynamics . it is also derived from big bang nucleosynthesis and from the higher multipole acoustic peaks in the cmb spectrum . i would say that the element abundance is a far more important indicator of the fraction between baryonic and dark matter . big-bang nucleosynthesis theoretical overview of cosmic microwave background anisotropy : 1.2 . results
the resistivity of any material is related to the mobility of the charge carriers within it by : $$ \rho = \frac{1}{ne\mu} $$ where $\mu$ is the mobility , $e$ is the electronic charge and $n$ is the number density of charge carriers . i have deliberately used the term charge carriers rather than electrons because in semiconductors like diodes the carriers can be holes as well as electrons . obviously the electron charge $e$ is a constant , so changes in the resistance arise either from changes in the mobility or the carrier density . in a metal neither of these change with applied voltage , so in a metal the resistance is independant of voltage . by contrast in a reversed biased diode the carrier density is not constant . as you increase the voltage you increase the energy of the electrons flowing across the depletion layer . at some point the energy gets high enough to excite bound electrons into the conduction band and we get an avalanche beakdown . the carrier density rises rapidly and hence the resistivity falls rapidly , so in a diode the resistance is strongly dependant on voltage . this argument applies to any material . if you see a resistance that varies with voltage , temperature or whatever else , this will be due to changes in the carrier mobility and/or density .
actually it is not that difficult ( but a neat problem ) , there is only one crucial step in the development that i will show you , but let 's start from a bit earlier . let 's first write out the two forces on interest here ( in terms of magnitudes , as we know already they are on parallel trajectories ) : the coulomb repulsion between the charges : $$f_c = \frac{q^2}{4\pi \epsilon_0 r^2} $$ now since we have moving charges , ( hence a current ) , each charge will experience a lorentz force in the magnetic field induced by the other charge , using biot-savart 's law , we have for the magnetic field $b$ : $$b = \frac{\mu_0 i dl}{4\pi r^2}$$ which should be interpreted as ( by analogy to a electrical circuit ) " the magnetic field felt at a distance $r$ , induced by a wire length $dl$ carrying current $i$" . note $\mu_0$ is the permeability of vacuum , $\epsilon_0$ is the corresponding permittivity . the lorentz force : $$f_l = qbv = \frac{\mu_0 q^2 v^2}{4\pi r^2}$$ where the $i dl$ term in b field is replaced using : $i=q/dt$ and $dl/dt = v$ , which gives finally $$i dl=qv . $$ the two forces together ( with $f_c$ the repulsive force here ) : $$\sum f =\frac{q^2}{4\pi \epsilon_0 r^2} -\frac{\mu_0 q^2 v^2}{4\pi r^2}$$ it is clear that the resulting effect of the two forces depends on the strength of the $b$ field which in turn then depends on the velocity of the two charges , if they are slow , the repulsion dominates . so in order to be able to compare their relative strengths , we need a slight rearrangement for the $\sum f$: $$f_{tot} =\frac{q^2}{4\pi \epsilon_0 r^2} \left ( 1-v^2 \epsilon_0 \mu_0\right ) $$ where $$v^2 \epsilon_0 \mu_0=\frac{f_l}{f_c}$$ but we know that $\epsilon_0 \mu_0 = c^{-2}$ the inverse squared of speed of light in vacuum , substituted in the total force expression : $$f_{tot}= f_c \left ( 1-\frac{v^2}{c^2}\right ) = \frac{f_c}{\gamma^2} $$ where $\gamma$ is the relativistic factor $ ( 1-v^2/c^2 ) ^{-1/2}$ or often called lorentz factor , which relates the measurements performed in different inertial frames moving at $v$ with respect to one another . last note : if the charges are not in vacuum , the relative permittivity $\epsilon$ and permeability $\mu$ are included in the expressions .
photoelastic constant i have typically seen this as part of the optical property list of glass or other optical materials . it predicts the birefringence . it is also called the stress-optic constant ( defined in mueller , the theory of photoelasticity , 1938 ) : $$ b=\frac{n_p - n_n}{p} $$ where $p$ is the pressure , $n_n$ is the index of refraction normal to the direction of pressure ( with a ${\bf{k}}$-vector normal to the pressure direction ) , and $n_p$ is the index of refraction parallel to the pressure direction . this equation is for a non-crystalline material , you get two constants and two equations for a uniaxial crystal for example . photoelastic coefficient i found a few papers where this was used to mean photoelastic constant . i also found a definition for photoelastic coefficient in properties of group-iv , iii-v and ii-vi semiconductors by adachi : $$ \alpha_{pe} = \frac{\delta\epsilon_{ij}}{x} $$ for a uniaxial crystal where "$\delta\epsilon_{ij}$ is the change in the dielectric constant parallel and perpendicular to the direction of stress $x$ . " in this definition they are using the dielectric constant ( aka the permittivity ) instead of the index of refraction . this means that the units will be different between the two definitions , but they are basically measuring the same thing in the optical regime ( where $\mu \approx 1$ , $\mu$ is the permeability ) . a lot of materials ( other than glasses ) papers seem to use this definition . acousto-optic coefficient the only reference that i could find to the above term was in a conference paper for cleo/pacific rim 2001 : sound field measurement through the acousto-optic effect of air by using laser doppler velocimeter by nakamura . this paper is so terse that i cannot figure out what he is actually calling the acousto-optic coefficient . i have never heard this term before . i found another definition from a thesis from moscow state university here : morozov thesis $$ \gamma = \frac{\partial n}{\partial p} $$ where $n = n ( p ) $ is the pressure dependent index of refraction and $p$ is the pressure , which is the differential case of the stress-optic coefficient .
the longest night for you would be for which midnight is closest to the exact moment of the winter solstice . think of it this way : the winter solstice moment is when earth 's polar axis for your hemisphere is tilted farthest away from the sun . the longitude at which it is local midnight will experience the absolute longest period of night symmetrically around that point . for example , say that ##°w longitude happens to be the longitude that is directly opposite the sun at the winter solstice moment . for people north of the equator on the ##°w longitude line , their night will be the longest it can be . if you are a little east of that line ( you will get daybreak slightly earlier ) , then your nighttime will be very slightly shorter . if you are a little west of that line ( you will get daybreak slightly later ) , then your nighttime will also be very slightly shorter because you did not have your local midnight at the time of the winter solstice . if you go farther from that line of longitude , then while the night you had closest to that midnight point for ##°w will be the longest for you of the year , it will not quite be as long as it would have been if you were at ##°w longitude . so if we take this and go back to your actual question , the winter solstice was on dec . 22 at 05:30 am utc in 2011 for the northern hemisphere . greenwich is defined as 0° e/w longitude , and there are 15° longitude per hour . so we need to go back into night ( west ) by 15°x5.5 hours = 82.5° w longitude ( somewhere in the western us/canada ) . they experienced the absolute longest night dec . 21/22 . people 82.5°w and ±90° of them would also get their longest night dec . 21/22 . people 97.5°e ( opposite side of the planet ) and ±90° of them would get their longest night dec . 22/23 .
this is the lcao approximation i.e. approximating the wavefunction of a hydrogen molecule as the sum/different of the two atomic orbitals . if $\phi_a$ is the wavefunction of one hydrogen atom and $\phi_b$ the wavefunction of the other then we guess that the wavefunction of the hydrogen molecule can be approximated as : $$ \psi_{h_2} = c_1\phi_a + c_2\phi_b $$ where $c_1$ and $c_2$ are constants to be determined . from symmetry we know that the ground state must have $c_1$ and $c_2$ equal in magnitude , so the only possibilities are $c_1 = c_2$ and $c_1 = -c_2$ . that is why we can write the two molecular wavefunctions as : $$ \psi_+ = c\phi_a + c\phi_b = c \left ( \phi_a + \phi_b \right ) $$ and : $$ \psi_- = c\phi_a - c\phi_b = c \left ( \phi_a - \phi_b \right ) $$ where i have dropped the subscript $1$ because it is not needed . the molecular orbitals $\psi_+$ and $\psi_-$ look like : to actually calculate the value of $c$ you use the fact that $\psi$ must be normalised so : $$ c = \frac{1}{\sqrt{2 ( 1 + s ) }} $$ where $s$ is the overlap integral $\langle\phi_a|\phi_b\rangle$ .
sure , many tall buildings are hit by lightning several times per year . it would be quite expensive to replace the cables each time . it has to withstand currents in the range between 10ka and 100ka for a few milliseconds . a copper wire with a diameter of several centimeters will survive lightning easily . a thin wire on the other hand is usually vaporized as the joule heating and low heat capacity combined is too much for the material , even though the current is flowing only for milliseconds . a superconductor will not help significantly here as it still has a non-zero resistance for ac , that is changing , currents .
when discussing an ideal parallel-plate capacitor , $\sigma$ usually denotes the area charge density of the plate as a whole - that is , the total charge on the plate divided by the area of the plate . there is not one $\sigma$ for the inside surface and a separate $\sigma$ for the outside surface . or rather , there is , but the $\sigma$ used in textbooks takes into account all the charge on both these surfaces , so it is the sum of the two charge densities . $$\sigma = \frac{q}{a} = \sigma_\text{inside} + \sigma_\text{outside}$$ with this definition , the equation we get from gauss 's law is $$e_\text{inside} + e_\text{outside} = \frac{\sigma}{\epsilon_0}$$ where " inside " and " outside " designate the regions on opposite sides of the plate . for an isolated plate , $e_\text{inside} = e_\text{outside}$ and thus the electric field is everywhere $\frac{\sigma}{2\epsilon_0}$ . now , if another , oppositely charge plate is brought nearby to form a parallel plate capacitor , the electric field in the outside region ( a in the images below ) will fall to essentially zero , and that means $$e_\text{inside} = \frac{\sigma}{\epsilon_0}$$ there are two ways to explain this : the simple explanation is that in the outside region , the electric fields from the two plates cancel out . this explanation , which is often presented in introductory textbooks , assumes that the internal structure of the plates can be ignored ( i.e. . infinitely thin plates ) and exploits the principle of superposition . the more realistic explanation is that essentially all of the charge on each plate migrates to the inside surface . this charge , of area density $\sigma$ , is producing an electric field in only one direction , which will accordingly have strength $\frac{\sigma}{\epsilon_0}$ . but when using this explanation , you do not also superpose the electric field produced by charge on the inside surface of the other plate . those other charges are the terminators for the same electric field lines produced by the charges on this plate ; they are not producing a separate contribution to the electric field of their own . either way , it is not true that $\lim_{d\to 0} e = \frac{2\sigma}{\epsilon_0}$ .
your process will be reversible only if it is a ) quasi-static and b ) non-dissipative . it will be quasi-static if it is carried out infinitely slowly in such a manner that the pressure on either sides of the piston varies only infinitesimally . it will be non-dissipative if the piston is frictionless and there is no viscous heating of the gas as it expands .
euler 's rotational theorem only states , that we can determine a unique axis of the rotation for any given moment . that does not necessarily mean that the axis of rotation 's direction is fixed forever . quite opposite , let 's suppose that we have body with no external force acting upon it . if axis of rotation at some ( starting ) moment does not coincide with one of the three principal axis of moment of inertia , the axis of rotation shall change and you shall have complex rotation of that body ( precession ) . you can see that directly from euler 's equations by putting torque to be zero .
the word comes from the ancient greek meaning " pertaining to man ; " ' man ' here means human . the etymologyonline dictionary is helpful . the anthropic principle is so-named because it is fundamentally based on the fact of a human observer .
the velocity of the orbiting space junk is a vector , with both a radial and a tangential component . $$\vec{v}_f = \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}$$ ( my $r_f$ is your $r$ ) the equation for conservation of angular momentum involves only the tangential component of velocity , because it comes from the cross product of the radius vector and the velocity . $$\vec{l}_f = m\vec{r}_f\times\vec{v}_f = mr_f\hat{r}\times\bigl ( \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}\bigr ) = mr_f ( r_f\dot{\theta}_f ) \hat{z} = mr_fv_f\hat{z}$$ but the equation for energy conservation involves both components . $$e_f = \frac{1}{2}mv_f^2 - \frac{gmm}{r_f} = \frac{1}{2}m\dot{r}_f^2 + \frac{1}{2}mr_f^2\dot{\theta}_f^2 - \frac{gmm}{r_f}$$ the combination $r_f\dot{\theta}_f$ corresponds to your $v_f$ , but the equation as you have written it in the question is missing the $\frac{1}{2}m\dot{r}_f^2$ term , which corresponds to the energy of motion toward or away from the moon . at apapsis and periapsis ( furthest and closest points of the orbit ) , $\dot{r}_f = 0$ momentarily , so you can ignore this term , as is done in the problem you are asking about . but for other points on the orbit , that term is nonzero , which means you have an extra variable . that prevents you from finding a unique solution without specifying which point in the orbit you are at .
having given it some more thought , there is an unambiguous philosophical difference , with practical implications . the two-slit experiment provides a good example of this . in a classical universe , any particular photon that hits the screen either went through slit a or slit b . even if we did not bother to measure this , one or the other still happened , and we can meaningfully define p ( a ) and p ( b ) . in a quantum universe , if we did not bother to measure which slit a photon went through , then it is not true that it went through one slit or the other . you might say it went through both , though even that is not entirely true ; all we can really say is that it " went though the slits " . ( asking which slit a photon went through in the two-slit experiment is like asking what the photon 's religion is . it simply is not a meaningful question . ) that means that p ( a ) and p ( b ) just do not exist . here 's where one of the practical implications comes in : if you do not understand qm properly [ i am lying a bit here ; i will come back to it ] then you can still calculate a probability that the particle went through slit a and a probability that it went through slit b . and then when you try to apply the usual mathematics to those probabilities , it does not work , and then you start saying that quantum probability does not follow the same rules as classical probability . ( actually what you are really doing is calculating what the probabilities for those events would have been if you had chosen to measure them . since you did not , they are meaningless , and the mathematics does not apply . ) so : the philosophical difference is that when studying quantum systems , unlike classical systems , the probability that something would have happened if you had measured it is not in general meaningful unless you actually did ; the practical implication is that you have to keep track of what you did or did not measure in order to avoid doing an invalid calculation . ( in classical systems most syntactically valid questions are meaningful ; it took me some time to come up with the counter-example given above . in quantum mechanics most questions are not meaningful and you have to know what you are doing to find the ones that are . ) note that keeping track of whether you have measured something or not is not an abstract exercise restricted to cases where you are trying to apply probability theory . it has a direct and concrete impact on the experiment : in the case of the two-slit experiment , if you measure which slit each photon went through , the interference pattern disappears . ( trickier still : if you measure which slit each photon went through , and then properly erase the results of that measurement before looking at the film , the interference pattern comes back again . ) ps : it may be unfair to say that calculating a " would-have " probability means that you do not understand qm properly . it may simply mean that you are consciously choosing to use a different interpretation of it , and prefer to modify or generalize your conception of probability as necessary . v . moretti 's answer goes into some detail about how you might go about doing this . however , while this sort of thing is interesting , it does not appear to me to be of any obvious use . ( it is not clear that it gives any insight into the disappearance and reappearance of the interference pattern as described above , for example . ) addendum : that has become clearer following the discussion in the comments . it seems that it is thought that the alternative formulation may have advantages when dealing with more complicated scenarios ( qft on curved spacetime was mentioned as one example ) . that is entirely plausible , and i certainly do not mean to imply that the work lacks value ; however , it is still not clear to me that it is pedagogically useful as an alternative to the conventional approach when learning basic qm . pps : depending on interpretation , there may be other philosophical differences related to the nature or origin of randomness . bayesian statistics is broad enough , i believe , that these differences are not of any great importance , and even from a frequentist viewpoint i do not think they have any practical implications .
there exists a variety of options for this task but let me stress first that this is an extremely complicated and difficult issue that is still subject of current research because analytical continuation is an ill posed problem ! 1 ) the ' analytical ' analytical continuation can be performed when the function $f ( \mathrm i\omega ) $ under consideration is a rational function of $\mathrm i\omega$ . so $$f ( \mathrm i\omega ) =\frac{1}{\mathrm i\omega}$$ can be continued to the complex plane $\mathrm i\omega \rightarrow z\in\mathbb c$ while $$f ( \mathrm i\omega ) =\frac{e^{\mathrm i\omega\beta}}{\mathrm i\omega}$$ is not a rational function of $\mathrm i\omega$ and making the replacement here is a mistake . instead one needs to evaluate the exponential first and find $e^{\mathrm i\omega\beta}=\pm 1$ depending on the statistics . 2 ) directly inferred from this replacement rule comes the expansion of a function in a finite laurent series $$f ( z ) =\sum^{m_2}_{n=m_1} a_n z^n , \ ; \ ; m_1 , m_2\in\mathbb z$$ where the coefficients may be calculated from the numerical values known at $m_2-m_1$ matsubara energies . 3 ) one of the oldest methods to do a numerical analytical continuation is the pade approximation . the function in question is expanded in a continued fraction $$f ( z ) =b_0+\frac{a_1z}{1-\frac{a_2z}{1-\frac{a_3z}{1- . . . }}} . $$ the coefficients can be computed from a pade table , see http://en.wikipedia.org/wiki/pad%c3%a9_table method 1 ) is exact and other than for almost trivial calculations of little practical value . 2 ) and 3 ) suffer from cutoff effects due to the limited amount of available matsubara points at which the function value might also have a numerical error as is the case for data from quantum monte carlo calculations . but in fact analytical continuation is very volatile towards cutoff and noise effects . this is where physical considerations have to be accounted for . to tackle the cutoff one can approximate the tail ( large $\mathrm i\omega$ or respectively $z$ expansion ) of the function with an analytical form that can often times be computed exactly from the many body problem or general physical necessities , e.g. the 1-particle green 's function of a fermionic system always has the form $\frac{1}{z}+\frac{a_1}{z^2}+ . . . $ . the tail can be used to compute an arbitrary number of expansion coefficients but keep in mind that the interesting low energy spectrum of your system is strongly influenced by small matsubara energies and less so by the tail so from computing a large number of coefficients from the tail one gains little to nothing . the treatment of statistical noise is even more delicate than the cutoff and the reason why a lot of people try to avoid calculating on the matsubara axis altogether . 4 ) a prominent method for noisy data is the maximum entropy method about which you can read more here http://arxiv.org/pdf/1001.4351v1.pdf where you will also find references to alternative techniques .
in the $s'$ frame , your variables are $x ' = x - t\cdot u \cos\theta $ and $y ' = y - t\cdot u \sin\theta$ . if you do the change of variable , you get that the motion now is described by $$x ' = 0$$ $$y ' = -\frac{g}{2}t^2$$ so in your new frame of reference you have vertical free fall from rest . this is not very helpful in finding out when or where does the projectile hits the ground , but is very relevant if you want to know where will the projectile be after releasing it from a plane moving at constant velocity : right below it all the time . disregarding air resistance , of course . edit the system with a prime is moving with velocity $ ( u \cos\theta , u\sin\theta ) $ , so if you have a velocity in the unprimed system , to convert it to the primed system , you have to substract the velocity of the origin : $$\vec{v'} = \vec{v} - ( u \cos\theta , u\sin\theta ) $$ integrating this , you can get the relation for the position vector : $$\vec{r'} = \vec{r} - ( u \cos\theta , u\sin\theta ) t + \vec{r}_0$$ where $\vec{r}_0$ is the position of the origin of the primed system for $t=0$ . both systems share origin for $t=0$ , so $\vec{r}_0=\vec{0}$ . now replace $\vec{r'}= ( x ' , y' ) $ and $\vec{r}= ( x , y ) $ and you will get the equations above .
use the directions up the ramp and perpendicular to the ramp as the simplest coordinate system . find the component of the vertical force of gravity that acts down the ramp . find the component of the horizontal force f that acts up the ramp . find an expression for the net force up the ramp , and equate to the mass of the cart times the acceleration of the cart up the ramp . solve for f .
the random character of the process itself does not depend on any specific virtual particles - instead , the random character of all microscopic processes in the world is a basic consequence of quantum mechanics . the existence of virtual particles is a consequence of quantum mechanics , too . obviously , virtual particle pairs are important - in a particular computational scheme - to calculate the actual decay rate of a particular unstable object . so the virtual pairs are not the cause of the randomness itself - they are a consequence of the randomness - but in any calculation , the virtual pairs are a part of the cause of the actual numerical value of the decay rate .
which units are fundamental and which are derived is pretty much a matter of arbitrary convention , not an objective fact about the world . you might think that the number of fundamental units would be well-defined , but even that is not true . take electric charge for example . in the si system of units ( i.e. . , the " standard " metric system ) , charge cannot be expressed in terms of mass , length , and time : you need another independent unit . ( in the si , that unit happens to be the ampere ; the unit of charge is defined to be an ampere-second . ) but sometimes people use different systems of units in which charge can be expressed in terms of mass , length , and time . by decreeing that the proportionality constant in coulomb 's law be equal to 1 , $$ f={q_1q_2\over r^2} , $$ you can define a unit of charge to be ( if i have done the algebra right ) $ ( ml^3/t^2 ) ^{1/2}$ , where $m , l , t$ are your units of mass , length , time . whether charge is defined in terms of mass , length , time , or whether it is an independent unit , is a matter of convenience , not a fact about the world . people can and do make different choices about it . similarly , some people choose to get by with fewer independent units than the three you mention . the most common choice is to decree that length and time have the same units , using the speed of light as a conversion factor . you can even go all the way down to zero independent units , by working in what are often called planck units . in summary , you can dial up or down the number of " independent " units in your system at will . one more example , which seems silly at first but is actually of some historical interest . you can imagine using different , independent units of measure for horizontal and vertical distances . that'd be terribly inconvenient for doing physics , but for many applications it is actually quite convenient . ( in aviation , altitudes are often measured in feet , while horizontal distances are measured in miles . in seafaring , leagues are horizontal and fathoms are vertical . yards are pretty much always used for horizontal distance . ) it sounds absurd to think of using different units for different directions , but in the context of special relativity , using different units for space and time ( different directions in spacetime ) is sort of similar . if we had evolved in a world in which we were constantly zipping around near light speed , so that special relativity was intuitive to us , we had probably think that it was obvious that distance and time " really " came in the same units .
i think that the best way to justify the logarithm is that you want entropy to be an extensive quantity -- that is , if you have two non-interacting systems a and b , you want the entropy of the combined system to be $$ s_{ab}=s_a+s_b . $$ if the two systems have $n_a , n_b$ states each , then the combined system has $n_an_b$ states . so to get additivity in the entropy , you need to take the log . you might wonder why it is so important that the entropy be extensive ( i.e. . , additive ) . that is partly just history . before people had worked out the microscopic basis for entropy , they had worked out a lot of the theory on macroscopic thermodynamic grounds alone , and the quantity that they had defined as entropy was additive . also , the number of states available to a macroscopic system tends to be absurdly , exponentially large , so if you do not take logarithms it is very inconvenient : who wants to be constantly dealing with numbers like $10^{10^{20}}$ ?
i am not sure why you are using the displacement-time formula if you have the shape of the graph . the distance covered by the particle is given by the area under the graph from point 0 . after t = 2s , the distance the particle would have covered is the area under the trapezium , that is : $$\dfrac12 ( 1+2 ) ( 20 ) = 30$$ the position of the particle becomes $8 m + 30m = 38m$ . maybe you can work out the second part now . explanation : your formula assumes constant acceleration , which is not the case ! for the first 1 s , the acceleration is $0ms^{-2}$ , the next 1s , the acceleration is $-20 ms^{-2}$ ! if you used your formula for $0&lt ; t&lt ; 1$ and then $1&lt ; t&lt ; 2$ , then you would have obtained the desired answer . but it is just making the problem more complicated than it actually is at that point : )
you are right . the field will indeed extend outside the electric conductor as well . however , since there are very few electrons affected outside the conductor , the field strength will quickly drop outside the conductor . however , inside the conductor , where the field does move many additional electrons , those movements will contribute to the field strength . the field strength is not directly related to its speed , though .
it is a direct consequence of equilibrium . it can be proved mathematically that proving the moment is zero at one point of a sytem in equilibrium ensures it is zero everywhere . have a look at the first two pages of this , you can find its proof
you are not wrong , the symmetries of a theory are essential to finding the right space of states . the space of states must carry a representation of all symmetries of the theory ( though it might be the trivial one ) . for example , for a quantum system that is invariant under rotation ( think of the hydrogen atom ) , the fact that we must represent the rotation group $\mathrm{so} ( 3 ) $ on the space of solutions of the schrödinger equation , since they are , as wavefunctions , the states , is naturally reflected in the fact that the solutions are ( linear combinations of ) the spherical harmonics $y^l_m$ , which are the basis vectors of all irreducible representations of $\mathrm{so} ( 3 ) $ , labeled by $l\in\mathbb{n}$ . if $h_l$ denotes the representation of a certain $l$ , the full space of states is $\bigoplus_{l\in\mathbb{n}}h_l$ . so , you could have guessed the space of states by looking at the symmetry alone instead of solving the schödinger equation ! ( i have neglected the radial and spin part in the above , but it gives the general idea , i think ) but a theory is ( almost ) always more than its symmetries . many field theories have an action determining classical equations of motion and the quantum path integral , though not all . quantum mechanics ( almost ) always has the hamiltonian determining the time evolution , and the quantomorphism symmetry ( that post is not directly related , but urs schreiber tells a great story about how the passing from classical to quantum mechanics is intrinsically motivated by lie theory , i think it might interest you ) is not enough to fix it , it must be given . the closest you get to determining the whole theory just by its symmetry are pure quantum gauge theories in low dimensions , where , in 2d , the topological structure of the spacetime together with the gauge group completely fixes the qft and all observables ( which are not that many ) . i am not certain i have answered your precise question , so feel free to point it out if i missed the mark .
so you know about how to get the effective moment of all the forces $$ \vec{m} = \sum_{i} \vec{r}_i \times \vec{f}_i $$ and the total forces $$ \vec{f} = \sum_i \vec{f}_i $$ to get the location where the moments balance out ( the line of action of the combined force ) you do the following $$ \vec{r} = -\frac{\vec{m} \times \vec{f}}{\vec{f} \cdot \vec{f}} $$ for example a force $\vec{f}= ( 1,0,0 ) $ located at $\vec{r}= ( 0 , y , z ) $ creates a torque of $\vec{m}= ( 0 , z , -y ) $ . to recover the location of the force do $$r = -\frac{ ( 0 , z , -y ) \times ( 1,0,0 ) }{ ( 1,0,0 ) \cdot ( 1,0,0 ) } =- \frac{ ( 0 , -y , -z ) }{1} = ( 0 , y , z ) $$
the ratio of energy to mass would arguably be lower than h-h or d-t fusion , so it would hardly be a preferred fusion reaction for space travel applications for production purposes ( be them energy , manufacture , etc . ) we humans usually prefer more result ( i.e. : more energy output ) as a result of less investment ( i.e. : less energy expenditure ) to improve the efficiency of the conversion . as such , any fusion reaction for heavier elements will undoubtly be less energetic per dollar spent having it working , regardless if the purpose is weaponry , energy production or propulsion
the technical term for what you are looking for is " fully developed flow " . this means that the velocity does not change in the direction of the flow . i am going to explain for a horizontal pipe here , but it is just as valid for a vertical pipe . when the flow just enters the pipe from a region of significantly larger cross section , the layers of the flow close to the wall are decelerated due to friction with the wall . this retardation creates a shear layer , where the fluid is slower than the rest of the fluid . as the flow " develops " , this shear layer gets bigger until the entire area of the pipe has a velocity gradient . consider $p_1$ , $p_2$ , $p_3$ , and $p_4$ to be four probes in the flow , which can measure the velocity without affecting it . $p_1$ measures a uniform velocity profile entering the pipe . you can see the profile developing at $p_2$ and $p_3$ . the blue area is the area with the unretarded fluid . the grey area is the shear layer , where there is a velocity in the radial direction ( to maintain continuity ) . the green area is the fully developed area . in a horizontal pipe flow , the flow is kept running by a pressure gradient . it becomes stable when the force due to the pressure gradient on the fluid element is equal to the force due to the shear forces on it . at this condition ( in the green zone ) , because there is no net force on the fluid element , its velocity can not change with time . $$\therefore \frac{\partial \vec{v}}{\partial t} = 0$$ additionally , because the element moves along the pipe ( i am going to call this the $z$ direction ) , and its velocity does not change with time , its velocity stays the same with $z$ . $$\therefore \frac{\partial \vec{v}}{\partial z} = 0$$ when the pipe is vertical , gravity replaces ( or supplements ) the pressure gradient as the driving force . the shear force still balances this out in fully developed flow . when we say ignore the " entrance effects " , we mean ignore the blue and grey zones , and consider the flow in the green zone only .
there are several ways of knowing what states should be there . for simple cases such as this , the easiest way is just by counting all the possibilities or micro-states . since you have 2 " equivalent " electrons in $p^2$ ( equivalent meaning that they share quantum numbers $n$ and $l$ , related to the energy of the system ) there are $$\left ( \begin{array}{l} 6 \\ 2 \end{array} \right ) = 6 \cdot 5/2 = 15$$ micro-states ( possible ways of assigning $n$ , $l$ , $m_l$ and $m_s$ to the outer ( valence ) electrons . know you have to count all the possible ( allowed by pauli 's principle ) different arrangements of $m_{l , 1}$ , $m_{l , 2}$ , $m_{s , 1}$ , $m_{s , 2}$ . you can find them explicitly in any physical chemistry book ( e . g . mcquarrie and simon ) . writing $m_{l , 1} , m_{l , 2}$ and $m_s$ omitted ( $m_s = 1/2$ ) or with an over bar ( for spin $m_s = -1/2$ ) , the microstates are : $ ( 1 , \bar{1} ) , ( 1,0 ) , ( 1 , \bar{0} ) , ( 1 , -1 ) , ( 1 , -\bar{1} ) $ , $ ( 0,1 ) , ( 0 , \bar{1} ) , ( 0 , \bar{0} ) , ( 0 , -1 ) , ( 0 , -\bar{1} ) $ , $ ( -1,1 ) , ( -1 , \bar{1} ) , ( -1,0 ) , ( -1 , \bar{0} ) , ( -1 , -\bar{1} ) $ now you have to group these states by characterising $l$ and $s$ ( since , barring spin-orbit coupling , for a given $l$ and $s$ the $m_l$ and $m_s$ states form a manifold of degenerate states ) . there are several ways of doing so , but i will be sketchy to avoid lengthiness . for instance you can first think in those cases with $m_{s , 1} \neq m_{s , 2}$ where $m_{l , 1}$ can be equal to $m_{l , 2}$ . then $m_s = m_{s , 1} + m_{s , 2} = 0$ ( $s$ can still be 1 -triplet or 0 - singlet ) . $m_l = m_{l , 1} + m_{l , 2} = 2,1,0$ . so you can have states with $l = 2,1,0$ . the states with $l = 2$ must be $s = 0$ since , as you realised , $m_{s , 1}$ cannot be equal to $m_{s , 2}$ . thus you identify 5 micro-states ( $m_l = +l , \ldots , 0 , \ldots -l$ ) corresponding to a single level $^1$d . of the remaining 10 states you can clearly see from the listing of micro-states that your $l=1$ level is a triplet ( you can find microstates with $m_s = 1$ and $m_l = 1$ so the remaining $m_s = 0 , -1$ and $m_s = 0 , -1$ have to be there too ) . for a $^3$p level there are $3\times 3 = 9$ microstates . finally , the remaining micro-state must correspond to a $^1$s state . regarding your question of the wave function . again , think only on your last 2 electrons ( it is not difficult to " enlarge " your slater determinant with the other electrons ) . each of your previous micro-states would correspond to a single slater determinant . for example , for $ ( 1 , \bar{0} ) $ you would have the wave function $$ \frac{1}{\sqrt{2}} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right|$$ where $2p_{m}$ is the wave function in atomic orbital $2p_{m}$ and $\alpha$ and $\beta$ are the spin functions . some states belonging to the levels with well-defined $l$ and $s$ correspond directly to the micro-states . for instance $ ( 1 , \bar{1} ) $ is exactly $|l=2 , s=0 , m_l=2 , m_s=0\rangle$ . however , most states must be written as linear combinations of slater determinants . for instance , to the state $|l=2 , m_l=1 , s=0 , m_s=0\rangle$ corresponds the wave function $$\frac{1}{2} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{2} \left| \begin{array}{cc} 2p_0 ( 1 ) \alpha ( 1 ) and 2p_1 ( 1 ) \beta ( 1 ) \\ 2p_0 ( 2 ) \alpha ( 2 ) and 2p_1 ( 2 ) \beta ( 2 ) \end{array} \right|$$ whereas for the state $|l=2 , m_l=0 , s=0 , m_s=0\rangle$ the wave function would be $$\frac{1}{2\sqrt{3}} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_{-1} ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_{-1} ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{\sqrt{3}} \left| \begin{array}{cc} 2p_0 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_0 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{2\sqrt{3}} \left| \begin{array}{cc} 2p_{-1} ( 1 ) \alpha ( 1 ) and 2p_{1} ( 1 ) \beta ( 1 ) \\ 2p_{-1} ( 2 ) \alpha ( 2 ) and 2p_{1} ( 2 ) \beta ( 2 ) \end{array} \right|$$
using this from my book ( physics for scientists and engineers with modern physics 9th ed ) : if more than one force acts on a system and the system can be modeled as a particle , the total work done on the system is just the work done by the net force . if we express the net force in the x direction as o fx , the total work , or net work , done as the particle moves from xi to xf is σw = the integral of σf from xi to xf a proof that d ) the speed of the particle must be unchanged is true : if σw = the integral of σf from xi to xf = 0 , then δx = 0 or σf = 0 ( this alone is proof that e ) there must be no displacement is false ) . in the case that δx = 0 , the particle is not displaced and the speed is unchanged . if σf = ma0+ma1+ . . . + man = m ( σa ) = 0 and m ≠ 0 , then σa = 0 and the speed is again unchanged . therefore , if σw = 0 , then it is necessarily true that the speed remains unchanged .
significant digits is a convention that only affects how you write numbers , not what the numbers actually are . so you only round when you are asked to drop down to a given number of significant digits - that is , at the end . think of it like this : there is a difference between a number , which is an abstract idea , and a written representation of a number . some numbers have exact written representations ; all numbers have approximate written representations , which represent another , nearby number . for example , the notation $5.82876$ is an exact representation of a particular number , and $5.8$ is an approximate written representation , to two significant figures , of the same number . $5.8$ is also an approximate written representation ( to two significant digits ) of many other numbers , such as $5.810394$ and $5.79928129$ . this is the idea behind uncertainty , and significant digits : if you are given the written representation $5.8$ , you do not know which actual number it represents - it could be anything between $5.75$ and $5.85$ . the only exception is if you are told that $5.8$ is an exact representation , which uniquely specifies which number you are supposed to take it to mean . when you calculate the product $1.8\times 2.01\times 1.542$ , you start with three written representations which you are supposed to assume are exact . then you multiply the first two of them , and get a number which is exactly represented by the notation $3.78$ . now , it is true that $3.8$ is an approximate written representation of that number . but does that fact change what the number is ? no . if you do the intermediate rounding , you are effectively deciding to replace one number , the one which is exactly represented by $3.78$ , which another number , the one which is exactly represented by $3.8$ . and the operation " replace one number with another number " is not part of the mathematical expression you are supposed to simplify . so do not do it .
no it is not . this is a mysterious thing in quantum field theory on curved space , as first noted by ' t hooft . if you assume there is a certain amount of entropy in the quantum fields surrounding the black hole , due to their thermal nature , you might estimate that there is a local contribution to the entropy from each approximate mode at the correct local hawking temperature of the black hole . this entropy is divergent in quantum fields in curved space , because the time dilation factor makes it that at a fixed energy , the number of modes diverges as you approach the horizon . this is one of the paradoxes that led t'hooft to the holographic principle . within ads/cft models , it is easy to give an answer-- the entropy of a black hole is the entropy of it is cft description . this includes systems like stacked branes , in which case , the entropy of the black hole is the number of vacuum states . this is strominger and vafa 's famous calculation of 1995-96 . this entropy coincides with the extremal horizon area ( although in this case , the black hole is extremal , so the temperature is zero ) . within string theory , this mystery is essentially resolved . the entropy is the entropy of the microscopic constituents of the black hole . it is not resolvable in curved-space qft because of the ' t hooft divergence , and it is not well resolved in an agreed upon manner in any other approach ( this means loops ) .
tl ; dr they shift but only if you have non perfect system , phase difference is compensated in a perfect system . first how to get no shift . imagine it is an ideal transformer . you apply the induction law once and get the $b ( t ) $ . $$u ( t ) =\int_\ell e ( t ) \mathrm{d}\ell=-\frac{\mathrm{d}b ( t ) }{\mathrm{d}t} $$ now the $b ( t ) $ is shifted in relation to $u ( t ) $ because of the derivation , but when we do the calculation of $\hat u$ we shift it back : $$\hat u ( t ) =-\frac{\mathrm{d}b ( t ) }{\mathrm{d}t} $$ so two shifts in opposite directions give zero phase difference . perfect . ( you should now think : but the second conductor influences the first . . . in some way , so i think this may be wrong ! that is what i was thinking , then i got over it . explanation : you would use the same formula over and over again ( ping-pong ) always getting net zero shift . ) if you really want to have a shift , you can look at it as a non-ideal transformer ( if you like add capacitances ) . $$\hat u ( t ) =\hat l\frac{\mathrm{d}\hat i ( t ) }{\mathrm{d}t}+ m\frac{\mathrm{d} i ( t ) }{\mathrm{d}t}+r\hat i$$ $$u ( t ) =l\frac{\mathrm{d} i ( t ) }{\mathrm{d}t}+ m\frac{\mathrm{d}\hat i ( t ) }{\mathrm{d}t}+ri$$ for a constant frequency $\omega$ you can write : $$\hat u ( t ) =j\omega \hat l \hat i+ j\omega m i+r\hat i$$ $$u ( t ) =j\omega l i+j\omega m\hat i+ri$$ if you know how to solve simple circuits these equations should not be a problem for you . the question remains , what are the values of $\hat l , m$ and $l$ ? the derivation of these formulas is something i can not remember , but you will find it for sure somewhere . the resistances can be calculated easier . what would happen if the resistances were the same ? if you add the capacitances you will have a characteristic impedance rating . even more fun stuff to think about !
this should be a comment as i am not knowledgeable about the lhc refrigeration , but it is too long for a comment and i can hazard a pretty good guess . sheer heat capacity likely accounts for a great deal of this time . assuming the total amount of kit that needs to be cooled is , say $2\times10^4$ tonnes , and if it has roughly a $1{\rm kj k^{-1} kg^{-1}}$ heat capacity , that means we have to extract $20{\rm gj}$ for every degree k that we cool . once we get down below $100{\rm k}$ were going to see some serious multipliers happenning . an ideal heat pump needs work input $w = q_{lhc} \left ( \frac{t_{out}}{t_{lhc}} - 1\right ) $ to pump out heat $q_{lhc}$ from the kit at temperature $t_{lhc}$ and dump it to the environment at $t_{out}$: this is the reversible heat pump . so if we are drawing this heat out and dumping it at $300k$ , say , the energy needed to get from $300k$ to $5k$ we get as a rough estimate ( assuming heat capacities stay constant , which they will not , but there will not be any phase changes of most of the kit ) : $$w_{total} = \sigma \int\limits_{t_{lhc}}^{t_{out}} \left ( \frac{t_{out}}{t} - 1\right ) \ , {\rm d}t$$ where $\sigma$ is the $20{\rm gj k^{-1}}$ total heat capacity i estimated above . plugging in the numbers $t_{lhc} = 5k$ ( not everything will need to be cooled all the way down to $1.9k$ ) and $t_{out} = 300k$ we get : $$w_{total} = \sigma \left ( t_{out}\left ( \log\left ( \frac{t_{out}}{t_{lhc}}\right ) -1\right ) +t_{lhc}\right ) = 933 \sigma \approx 20{\rm tj}$$ this is the total output of a $5{\rm gw}$ power station for over an hour , roughly the energy released by the first of the only two nuclear weapons brought to bear in combat . $5gw$ electricity generation is the electricity consumption of two million australians , and we are extremely greedy electricity users by world standards , so i do not know how many normal people this would represent . the lhc site quotes a peak power consumption of $180mw$ and about $30mw$ is used for cryogenics . $30{\rm tj}$ at $30mw$ is about ten days . another factor is stresses in the kit induced by too swift cooling or warming . i am slightly familiar with the design of some of the magnetic beamsteering hardware , and much of this kit is toleranced to within tens of microns . one can not brook even the tiniest of any irreversible , plastic deformations of the kit and still have it work properly . so it is likely that heat transfer could not go much faster than this even if the refrigeration capacity were there . there will also be economic considerations too . even if one can cool faster than with $30mw$ refrigeration and still meet technical / engineering constraints , refrigeration capacity is expensive , particularly if you are only using this full capacity for cooldown during maintenance . the rest of the time the refrigeration needs are much less , so you have an economic tradeoff between capital spent on capacity that is unused most of the time and the cost of project delays arising from downtime . i am absolutely sure exactly this calculation has been done , as it ones like it are done for all soundly managed engineering projects .
diamond dust ( or dust of any other material ) will not conduct heat anywhere close to as well as the solid material . at a molecular level the dust is not in very good contact with other grains of dust . there is plenty of separation and air in between the particles that will retard heat conductivity . if you were to compress the dust so significantly that it did conduct as well , i am certain the pressure would be great enough to cause the dust to bond with other dust into a larger solid . for more a more technical treatment of the thermal conductivity of powder beds , see this paper .
assuming the first was also tap water at the same temp and the pot was room temperature , then all that can be said given your question is " less than 20 minutes " . it depends on the thermal capacity of the pot . put another way , the first time you boil the water you have to do two things : heat the pot to boiling temp heat the water to boiling temp the second time all you have to do is : heat the water to boiling temp what really happens when you put the tap water in the hot pot for the second time is that thermal energy from the pot flows into the cooler water and warms it up . this lowers the temperature of the pot and raises the temp of the water until they are roughly equal temperatures ( thermal equilibrium ) . after this balancing the whole system starts out warmer than the first system did and less energy must be put into the system to heat it to the same boiling point . if the first one took 20 minutes then the second one will take less time . the actual amount of time saved depends on how much heat energy was stored in the pot and that depends on the size of the pot , what it is made out of , etc .
you may have encountered it in a different context , but i recognize it from the topic of singularities of correlation functions in quantum field theory . in massless field theories such correlation fucntions becomes singular for points which are lightlike separated , and the structure of such singularities is determined by good physical principles such as locality and unitarity . then again , i may be completely off the mark . in any event , except for the linguistic similarity , i do not think it has to do with null singularities in spacetime .
the notation always refers to the total angular momentum . a source of confusion may be that in nuclear physics , we talk about the " spin " of a nucleus as a whole , even though the spin is only partly due to the intrinsic spin 1/2 of the neutrons and protons .
energy ( n . ) 1590s , " force of expression , " from middle french énergie ( 16c . ) , from late latin energia , from greek energeia " activity , operation , " from energos " active , working , " from en " at " ( see en- ( 2 ) ) + ergon " work , that which is wrought ; business ; action " ( see urge ( v . ) ) . used by aristotle with a sense of " force of expression ; " broader meaning of " power " is first recorded in english 1660s . scientific use is from 1807 . energy crisis first attested 1970 . http://www.etymonline.com/index.php?term=energy huygens ( 1650 's ) was the first to develop the terminology , stating that : energy is not like matter energy does not have size , shape or occupy space energy does not have inertia instead , it was defined that energy is a measure of the ability of a physical system to perform work http://abyss.uoregon.edu/~js/ast122/lectures/lec03.html
the rectangular prism is a rigid body . the equations of motion of a rigid body around its center of mass are given by : ( please , see for example : marsden and ratiu , ( page 6 ) . $$i_1\dot\omega_1= ( i_2-i_3 ) \omega_2\omega_3$$ $$i_2\dot\omega_2= ( i_3-i_1 ) \omega_3\omega_1$$ $$i_3\dot\omega_3= ( i_1-i_2 ) \omega_1\omega_2$$ where $\omega_1 , _2 , _3$ are the angular velocity components around the body axes and $i_1 , _2 , _3$ are the corresponding moments of inertia . given that the moments of inertia are different , we may assume without loss of generality that : $i_1&gt ; i_2&gt ; i_3$ . the fact is that the steady motion around the intermediate axis $2$ is not stable , while around the two other axes , the motion is stable . this fact is explained by marsden and ratiu on page 30 . also , various other explanations are given in the answers of a related question asked on mathoverflow . here i will describe the details of a linearized stability analysis . a steady state in which the angular velocity vector has only one nonvanishing constant component is a solution of the equations of motion . for example : $$\omega_1=\omega = const . $$ $$\omega_2=0$$ $$\omega_3=0$$ is a solution describing rotation around the first axis . also $$\omega_1=0$$ $$\omega_2=\omega = const . $$ $$\omega_3=0$$ is also a solution describing rotation around the second axis . now , we can analyze the stability of small perturbations around these solutions . a perturbation of the first solution is given by : $$\omega_1=\omega + \epsilon \omega_1$$ $$\omega_2=\epsilon \omega_2$$ $$\omega_3=\epsilon \omega_3$$ with $\epsilon&lt ; &lt ; 1$ . substituting in the equations of motion and keeping only terms up to the first power of $\epsilon$ , we obtain : $$i_2\dot\omega_2=\epsilon \omega ( i_3-i_1 ) \omega_3$$ $$i_3\dot\omega_3=\epsilon \omega ( i_1-i_2 ) \omega_2$$ taking the first derivative of the second equation with respect to time and substituting the second equation , we obtain : $$i_2i_3\ddot\omega_3=\epsilon ^2 \omega^2 ( i_3-i_1 ) ( i_1-i_2 ) \omega_3$$ since $i_3&lt ; i_1$ and $i_1&gt ; i_2$ , the coefficient on the right hand side is negative and the perturbation satisfies a harmonic oscillator equation of motion of the form : $$\ddot\omega_3 + k^2 \omega_3 =0$$ repeating the perturbation analysis for the second solution ( rotation about the second axis ) we obtain : $$i_2i_3\ddot\omega_3=\epsilon ^2 \omega^2 ( i_2-i_3 ) ( i_1-i_2 ) \omega_3$$ since $i_3&lt ; i_2$ and $i_1&gt ; i_2$ , this coefficient is now negative and the solution describes a harmonic oscillator with a negative spring constant of the form : $$\ddot\omega_3 - k^2 \omega_3 =0$$ which is an unstable perturbation .
the following fact lies at the heart of this and many similar issues with sizes of things : not all physical quantities scale with the same power of linear size . some quantities , like mass , go as the cube of your scaling - double every dimension of an animal , and it will weigh eight times as much . other quantities only go as the square of the scaling . examples of this latter category include muscle strength ( a longer muscle can exert no more force than a shorter one of equal cross sectional area ) , heart pumping ability ( the heart is not solid but rather hollow , so the amount of muscle powering it goes as the surface area ) , the compression/tension that can be safely transmitted by a bone ( material strength is intrinsic and independent of size , so the pressure that can be supported is constant , so the force - cross sectional area times pressure - that can be supported goes as the square of size ) , and the ability to exchange material and heat the the environment ( single cells for example have a hard time growing large because their metabolism goes as the cube of the size , but their ability to transport nutrients across their outer membranes only scales as the area of those membranes ) , at least to a first approximation . you could also come up with other quantities that scale differently with size . as a result , simply scaling up an organism will undo the balance that has been achieved for that particular size . its muscles will likely be too weak , its bones will likely break , and it will generate so much internal heat ( if it is warm blooded ) that the only equilibrium achievable given its comparatively small surface area would be at a high enough temperature to denature many proteins . for a completely non-biological example , consider the fact that airplanes cannot be made arbitrarily large , and in fact different sizes of planes have very different shapes and engineering requirements . the surface area of the wings does not scale the same way as the total mass , and the stresses and pressures the material needs to withstand will not stay constant as you enlarge the plane .
so i am not an expert in limit cycles by any means but i am intrigued by this problem so here is what i came up with . let 's treat the nonlinear term perturbatively . this will not be enough to prove the existence of the limit cycle for large values of $\mu$ , but given that apparently there is a proof that this works perturbatively , it will be enough for us . let 's take an ansatz \begin{equation} x ( t ) =\bar{x} ( t ) +\mu \delta ( t ) \end{equation} where \begin{equation} \bar{x} ( t ) =a\cos ( \omega_0 t ) \end{equation} here $\mu\delta ( t ) $ is a small perturbation . just to be clear , i am thinking of $\mu$ as the small parameter , $\delta$ is not a small function . i am assuming the perturbation will scale like $\mu$ ( as opposed to $\mu^2$ or $\mu^3$ ) , this will be justified later . ( ok i do not actually explicitly justify it later . the point is that the $o ( \mu ) $ equation below would not have given any useful information had this scaling been wrong ) . by the way , if we could calculate the form of the limit cycle for large $\mu$ we could generalize the analysis by making $\bar{x}$ equal to the limit cycle and then running through all the steps below . the point of the small $\mu$ approximation is that the limit cycle must be approximately the harmonic oscillator path in this limit . i do not know much about this stuff but i would not be surprised if there was a way to calculate the limit cycle curve . what we expect to happen is there to be a special value of $a$ such that this ansatz is stable ( meaning that $\delta$ will not blow up ) . numerically you have discovered that this value is $a=2x_0$ , we would like to see if we can see this peturbatively as well . so we expand out the equation . at $o ( \mu^0 ) $ , we find the harmonic oscillator equation , of course . at $o ( \mu ) $ we get an equation for $\delta$: \begin{equation} \ddot{\delta}+\omega_0^2 \delta = ( x_0^2 -\bar{x}^2 ) \dot{\bar{x}} \end{equation} after subbing in the form for $\bar{x}$ and using some trig identities we find \begin{equation} \ddot{\delta}+\omega_0^2 \delta = a \omega_0 x_0^2 \sin ( 3\omega_0 t ) + a\omega_0 ( a^2-4x_0^2 ) \cos^2 \omega_0 t \sin \omega_0 t \end{equation} this is a forced harmonic oscillator : the right hand side has two forcing terms . let 's look at the second one : \begin{equation} \cos^2 ( \omega_0 t ) \sin ( \omega_0 t ) = \sin ( \omega_0 t ) - \sin^3 ( \omega_0 t ) = \frac{1}{4}\sin ( \omega_0 t ) + \frac{1}{4} \sin ( 3 \omega_0 t ) \end{equation} there are multiple terms here , but the problem is that there is a term with frequency $\omega_0$ . this drives the oscillator at its resonant frequency , creating an instability . so the fluctuations are unstable so long as that second term is present . but precisely when $a=2x_0$ , the dangerous resonant driving term vanishes , and the fluctuations are stable . voila .
the idea that nature is described by a nonlinear system of equations was the idea that einstein had in the 1920s , and motivated his search for a unified field theory . it does not work , and it is philosophically less worthwhile than current theories anyway , so even if it did work , it would not be simpler than string theory , or as elegant . the idea that you can describe what is going on with local equations is false , as is demonstrated conclusively by bell 's inequality violations . the bell inequality tells you that you can send electrons to far-away locations with spins that can be measured in 3 directions , a , b , c . the spin of the two electrons in each direction are 100% correlated ( it is actually anti-correlated , but same difference for the argument ) , so if you measure the spin in direction a , and one electron is up the other is 100% certain to be up . same for direction b and c , the two electrons always report the same spin in any of the three directions . the spin in directions a and b are 99% correlated , meaning if you measure a on one of the electrons is up , then b on the other electron is up 99% of the time , and b is down 1% of the time . the spin in directions b and c are 99% correlated , so if you measure b is down on one electron , c is up on the other electron 1% of the time . from the 100% correlation of the electrons , you conclude that the nonlocal field state ( hidden variable ) on one electron has the property that a and b are 99% the same , 1% different b and c are 99% the same , 1% different from this you deduce that a and c must be at least 98% the same meaning that whatever field configuration is happening to make a , the field configuration for c can only give different results 2% of the time , the sum of those times when it gives different answers than b plus the times b gives different answers than a . this bound is called bell 's inequality , and it is violated by quantum mechanics . a and c are different 96% of the time . this means any type of local-in-space description , linear , nonlinear , complicated , simple , whatever , will never ever work to describe nature . your description is either nonlocal in the sense of faster-than-light communication , or nonlocal in the sense of having a global notion of state which is entangled nonlocally by measurements . this is why nobody looks for nonlinear field equations to describe nature anymore . it can not possibly work . but the main ideas of einstein 's nonlinear field theories have survived to inspire developments in later physics . the pions are excitations of a sigma-model , which is a type of nonlinear field theory . they are small oscillations of the quark condensate in the vacuum . the proton can be thought of as the topological soliton of the sigma model . in quantum mechanics , it can still be a fermion even though the sigma model has no fundamental fermionic variables . the field equations of 11-dimensional supergravity , which are a central part of string theory , generalize general relativity in pretty much the only nontrivial ways known--- they give the biggest extension of spacetime symmetry possible , and they include a new field , constrained by the supersymmetry . so these ideas are not a dead end , but they cannot work without quantum mechanics by themselves . if you want to understand quantum behavior emerging from some sort of nonlinear dynamics underneath , this dynamics can not be local .
victor stenger 's stuff is vague unquantitative musings with no significant value , about back and forth in time particle paths and such things . it is not important to this question , and it is not important for anything at all . if quantum mechanics is deterministic underneath , and the number of variables is reasonably bounded , for example a number of bits less than the area of the cosmological horizon in planck units , about $10^{138}$ , then the the exponentially growing search ability of a quantum computer will crap out . the point at which factoring necessarily fails can be easily estimated from the number of independent multiplications performed in shor 's algorithm . to do $10^n$ multiplications in superposition at once , you need $10^n$ classical states ( it is actually nlogn in the exponent , but same difference ) , so when n is larger than around 130 , so around 130 digit numbers , you exceed this bound . since the entire universe can not be expending all it is effort on your dinky quantum computer , the actual bound will be closer to 100 digits , on the same the order of the numbers used for cryptography today . to be on the absolute safe side , perhaps nature realizes something about the multiplications and reduces the operations accordingly , you should have n to some power less than 1 in the exponent , because of redundancies , and you should have n to some power less than 1 in the exponent . this power cannot be as small as 1/2 , because the best classical algorithm scales exponentially in a higher exponent than this , and nature is not as clever as mathematicians trying hard to do factoring . so you get a firm bound at around 10,000 digits , and a quantum computer factoring larger numbers than this is definite evidence that quantum mechanics is exponentially large . if you do not say there is a limit to the amount of classical computation in the deterministic model , if you allow exponentially large deterministic descriptions , you can mimic quantum mechanics exactly using bohmian mechanics . you make a lattice description of some field theory , you make a wavefunction for the fields , and you have definite field values that wander around according to the bohmian quantum force law . this type of thing can not be ruled out experimentally , but the description is not unique , it is different depending on which fields you choose to make bohmian , and in which time-slicing you define the quantum-force dynamics , so it is certainly not the right description . bohmian mechanics might as well be quantum mechanics , which is why bohm 's theory is more often called an interpretation of quantum mechanics rather than a new theory . there is no point in asking whether it is deterministic underneath if shor 's algorithm works , the determinism in this case would be the universe simulating quantum mechanics and fooling us into thinking it is correct . in this case , you might as well believe quantum mechanics is exact with no substructure . so the only options , modulo philosophy , are that quantum mechanics fails to factor 10,000 digit numbers , or that it succeeds and is exact . this a wonderful experimental distillation of the hidden-variable question , and for those that have full confidence that quantum computers will work , this definitively excludes hidden variable theories . this is shor 's answer to this question : why do people categorically dismiss some simple quantum models ? .
for a fixed muzzle velocity the time the cannonball stays in the air depends on the vertical component of the velocity , so trajectory a would stay in the air longest . the trajectory with the longest duration is firing directly upwards . if the angle to the ground is $\theta$ , then the vertical velocity is $v_0sin\theta$ , and the time in the air ( neglecting air resistance ) is $2v_0sin\theta/g$ , where $g$ is the acceleration due to gravity .
yes , you can make a unitary asymptotic s-matrix ( so asymptotic measurements ) when the intermediate states do not evolve in a unitary way . this is what ghost fields do--- the intermediate states in ghost-descriptions include negative probability objects , but when you make asymptotic measurements you do not see the ghosts , you only see the positive probability objects . in cases where you have a ghost description , there are often no-ghost formulations , like light-cone or axial gauges . in these formulations , the hamiltonian is well defined , so that you can ask about measurements on the intermediate states and get well defined answers . these formulations have a reduced symmetry compared to the ghost formulation , but they are manifestly unitary . in ghost formulations , you assume that every measurement is made on asymptotic states which have no ghosts . even if it is not true that every measurement is of an s-matrix quantity , the existence of the unitary formulation guarantees that anything you build out of asymptotic states will only end up measuring a quantity which has a reasonable positive probability interpretation .
note that friction opposes ( or tends to oppose ) relative motion and ceases to act when there is no tendency of relative motion . friction acts on lower point of disc till it comes at rest with respect to ground . with respect to ground the lower point can considered as superposition of 2 velocities : $v_{cm}$ ( linear ) and $w_{cm}r$ ( due to rotation of disk about centre ) in opposite directions where $cm$ stands for centre of mass of disc which is its centre ( here ) . friction provides force and torque to adjust them together till $v'=w'r$ where v ' and w ' are new speeds and angular speeds respectively . now no friction acts . so , would it matter if friction can act or not ? $no . $
the most general answer is " we engineer things with relativity . " the most striking example i know is the cebaf accelerator at jlab . like all modern accelerators it uses rf linacs to add energy to the beam . that only works if the size of the cavities and the frequency of the rf match the speed of beam . now , cebaf is special , beams of widely differing energy can propagate simultaneously through the linac at high efficiency , which only works because the speed of the beam is insensitive to the kinetic energy in the ultra-relativistic regime in which the machine works . pre-upgrade a 50 mev beam could coexist with a 4.5 gev beam . post upgrade those numbers will roughly double .
that is , it will eventually change its direction of travel , but just because it is accelerating in the opposite direction of the current vector does not mean that it has changed direction ; yet . i think this is the core of your confusion . the object is not accelerating in the opposite direction of the current velocity . try drawing a diagram of the two vectors and you will see what i am talking about .
apart from the spatial translations corresponding to the momentum operator , the other symmetries ( that i can think of ) that are relevant in particle physics i.e. things like spatial rotations phase transformations flavour transformations colour transformations are represented by the action of compact lie groups . the irreducible unitary hilbert space representations of compact lie groups are finite dimensional and this is reflected in the discrete labelling .
it depends on the hamiltonian , i.e. , the interactions the beam is subject to after the first measurement . if the beam is allowed to propagate freely , then a pure state , say $|0\rangle$ , will remain in that state always because the hamiltonian only contains a momentum operator that will not affect the spin component of the state vector . if on the other hand , after the first measurement , there are other interactions like a magnetic field , then it can lead to a non-trivial schrodinger evolution of the pure state ( spin part ) which will change the outcome at the second measurement .
it seems your three equations are not linearly independent and so check your math . you should have 3 independent equations to solve for 3 variables like $t$ , $f$ and $a$ . in general you solve for one variable in one equation and substitute it into the others . like equation #2 $t= m ( g-a ) $ to get $$ f = - m g \sin\theta - ( m+m ) a + m g $$ . then repeat for the next variable . in your case pluggin the above $f$ into equation #1 returns $m a=m a$ which is not helpful and hence i stated that your equations should be independent to solve correctly . can you post a diagram in order to check your equations also ?
i think there may be a better forum to ask this question in and it will likely be closed , but information theory is important to many branches of physics in , so here 's a quick answer . the bandwidth of a channel is simply the number of symbols you can send through it per unit time . by symbol , i mean here a single , real number , and this meaning arises through the shannon sampling theorem . see the wikipedia page for this theorem , and go through the proof so you will understand exactly what i mean . now , just one lone noiseless real number can in theory encode as much information as you like . there are $\aleph_0$ digits in a real number ! write out the whole of wikipedia as 0s and 1s and call it a binary fraction between 0 and unity and the whole of wikipedia is still a finite precision , rational binary number ! so you can see in theory that you can send heaps of data over channels that can send only a low number of symbols each second . this theoretical ideal is , of course , limited by noise . it effectively " coarse grains " the real numbers . if i have noise with an amplitude of 0.1units , and can send symbols with an amplitude of up to 1 units , then i roughly have 10 amplitude levels i can encode data on . otherwise put , i can tell apart ten levels . so i can encode $\log_2 10$ bits per symbol in this example . if my noise amplitude is 0.01 units , i can tell apart roughly 100 different levels per symbol . so i can encode $\log_2 100$ bits per symbol in this example . i think you should now be able to see what is going on : the number of bits you can send per unit time is roughly $$b \log_2 s/n$$ the actual shannon-hartley theorem is a little more complicated , but that is the idea . edit : for interest : 64-qam modulation is commonly used for digital communications . this is essentially where the " symbols " are one of 64 points on a regularly spaced grid in the argand plane representing the amplitude and phase of the signal . so this scheme has a spectral efficiency of six bits ( $\log_2 64$ ) per hertz ( i.e. . symbol per second ) . the ultimate spectral efficiency of a typical optical fibre link is of the order of 20 to 25 bits per hertz : see my answer here .
you prove the equality of operators by applying them to a function , we have $$ h = - \frac{\hbar^2}{2 m} \frac{d^2}{dx^2} + v ( x ) $$ ergo : $$ hp f ( x ) = h f ( -x ) = ( - \frac{\hbar^2}{2 m} \frac{d^2}{dx^2} + v ( x ) ) f ( -x ) = - \frac{\hbar^2}{2 m} f&#39 ; &#39 ; ( -x ) + v ( x ) f ( -x ) $$ and $$ ph f ( x ) = p ( - \frac{\hbar^2}{2 m} \frac{d^2}{dx^2} + v ( x ) ) f ( x ) = p ( - \frac{\hbar^2}{2 m} f&#39 ; &#39 ; ( x ) ) + p ( v ( x ) f ( x ) ) . . . $$ $$ . . . = - \frac{\hbar^2}{2 m} f&#39 ; &#39 ; ( -x ) + v ( -x ) f ( -x ) $$ when you use $$ v ( -x ) = v ( x ) $$ you see that both expressions are equal .
become another unstable element that will again go through beta decay ? yes , this happens . if you start far from the line of stability , many beta decays are needed in order to get to something stable . for heavy elements , you can also have beta and alpha decays intermixed in the chain . could that element ( hypothetically speaking ) go through beta decay , then , once it has too many protons could it immediately go through electron capture to become that same radioactive element there are three processes : $\beta^+$ decay , $\beta^-$ decay , and electron capture . when people refer to beta decay , it means all of these , not just $\beta^-$ . anyway , the kind of process you are referring to is not possible if all the decays we are talking about are beta decays of ground states . first off , the way you are describing it you seem to be imagining the nucleus oscillating back and forth across the line of stability . since almost all mass numbers have at least one stable isotope , this can not happen ; once you hit stability ( in the ground state ) , you can not decay due to conservation of energy . even if you do not cross and recross the line of stability , you can not have a chain like this due to conservation of energy . if the ground state of a can decay to b without violating conservation of energy , then conservation of energy prohibits b from decaying to a . it might be possible for this to happen if a decayed from a state that was not its ground state , went to b , and then b decayed back to a 's ground state . however , this does not seem likely to me , and i do not know of any examples . typically , excited states in nuclei decay electromagnetically on very short time-scales ( nanoseconds or picoseconds ) , so weak decay branches can not compete . to get competition , you typically have to have a state whose electromagnetic decay to the ground state is hindered by the very low excitation energy ( gamma decay is slower if the gamma-ray energy is lower ) , as well as possibly a large difference in spin . this is fairly common in odd-odd-nuclei . however , this only works if you have a low excitation energy in a , which then makes it unlikely that you had have enough energy left in b to get back to the ground state of a . furthermore , the staggering of binding energies between even-even and odd-odd nuclei of the same mass number makes it extremely unlikely that the even-even nucleus b would decay back , away from the line of stability , to the ground state of the odd-odd a .
the direct calculation of the derivatives is not that hard . but one can also quickly see the values of the riemann tensor for a sphere – and similar simple shapes – by using the definition of the riemann tensor via the parallel transport of vectors . $$\delta v^\alpha = r_{\alpha\beta\gamma\delta}v^\beta d\sigma^{\gamma\delta}$$ around a point of the sphere $s^d$ , the transport around an area given by $d\sigma^{\gamma\delta}$ for fixed values of the indices ( locally orthonormal basis ) allows you to see that all of this is happening on an $s^2$ only . the other dimensions are unaffected . that is why you get $r_{\alpha\beta\gamma\delta}$ equal to $ ( 1/a^2 ) $ times $g_{\alpha\gamma}g_{\beta\delta}-g_{\alpha\delta}g_{\beta\gamma}$ . effectively , the antisymmetrized pair of indices $\alpha\beta$ has to be the same as the pair $\gamma\delta$ . i did not assume anything special about the point ; all points on a sphere are equally good by a symmetry . so the ansatz for the riemann tensor has to hold everywhere . note that it is multiplied by $1/a^2$ , the inverse squared radius of the sphere . many of your formulae omit it ; moreover , you are using a confusing symbol $r$ for the radius which looks like the ricci scalar – a different thing . in $d=2$ , the riemann tensor only has one independent component and the formula for the riemann tensor in terms of the metric tensor above actually holds for any surface if $1/a^2$ is replaced by $r/2$ . note that the two-sphere has ( ricci scalar ) $r=2/a^2$ . also , the ricci tensor is $r_{ij}=rg_{ij}/2$ in $d=2$ so that the vacuum einstein equations are obeyed identically . for $d=3$ , the riemann tensor has 3 independent components , just like the ricci tensor , so the riemann tensor may be written in terms of the ricci tensor . that is not true for higher dimensions , either .
i do not want you to think we are ignoring your question ; i suspect the reason no-one has answered so far is that your question is not as clear as it could be . i will try to answer what i think you are asking , but please ask for more details if my answer here does not help . i think the problem is that the word " energy " has lots of different meanings and it is very easy to mix them up and end up comparing two different things . for example " energy " could mean the energy of a moving bullet or it could mean the energy in a compressed gas cylinder , and these are rather different ( although they are interchangable ) . dark energy is more like a pressure , though actually it is a negative pressure rather than the positive pressure you get in a compressed gas cylinder . dark energy is ( probably ) a property of space itself , so it is not like the energy you get from letting off an atomic bomb . every cubic metre of space has a certain amount of dark energy associated with it and this energy is a constant so it is always there and always the same ( strictly speaking this only applies if dark energy behaves like a cosmological constant , and there are other theories like quintessence where the dark energy is not constant ) . when you are trying to solve the equations of general relativity to find out how the universe behaves you have to count the dark energy , and when you do this you end up with the result that 72% of the total mass/energy of the universe is in the form of dark energy . but that does not mean you could convert the dark energy into matter , or that you could convert matter to dark energy . in fact , as far as we know you can not do this . you are quite correct that matter can be converted into energy and back , and indeed the lhc does this all the time . actually you do this every time you light a fire or fire a gun , though in these cases the percentage of matter converted into energy is so tiny you had struggle to measure it . we tend to lump matter and energy together and put them in the 28% of the universe that is not dark energy . i hope this clarifies things a bit . if you want to ask anything further please comment to this answer or post a new question .
for a star of a given mass , we can calculate , based on theoretical models , how long it has to live . for example , the sun is currently 5 gyr old and will live another 5 gyr . so if you observe the sun from , say , andromeda , the light would be about 2 million years old and you could therefore conclude that the sun is still alive even though you are observing " old " light . if you flip it around , if we see a sun-like star in andromeda , we could safely say it is still alive . broadly , while fusing hydrogen in their cores , more massive ( and therefore brighter , hotter ) stars live shorter lives . on the one hand , it means that we observe some small stars whose lives will be longer than the current age of the universe . on the other hand , we could theoretically observe stars whose lifetimes are shorter than the distance to them , in light-years . so , a star 80 times as massive as the sun might have a lifetime of a few million years , so if we currently observe it in andromeda near the end of its life , that star is probably gone now . off the top of my head , i would say most of the stars we presently observe still exist as we see them now . we just can not resolve stars that far away . for what it is worth , there are some distant phenomena that we see that happen much faster than the light takes to reach us . for example , the 2011 nobel prize went to the leaders of two teams that observe type ia supernova at redshifts up to about $z=1$ . these are events that lasted less than a year that happened nearly 8 billion years ago . who knows what they look like now !
your intuition about the charge repulsion and strong force acting on protons more is less important that you think . the strong nuclear force is a few orders of magnitude greater than electromagnetism so coulomb repulsion just does not contribute much . what matters most is the nuclear binding energy to separate a proton from the nucleus . if the resulting system is below the proton separation energy it is possible for the proton to tunnel out . see proton emission and proton drip line for more information about this . it does happen but remember neutron emission is also rare . $\beta^+$ and $\beta^-$ and alpha emission are much more common .
it is not really a single principle - it is a philosophy and in the context of philosophical discussions about science , it is usually known as positivism . http://en.wikipedia.org/wiki/positivism as any philosophy , it cripples the penetrating power of science if it is extended too far - and every philosophy ultimately fails . the thought experiment about the earth in the universe is just one among millions of examples . positivism could have a problem with the whole concept of thought experiments . while it was very useful and important for the development of quantum mechanics to realize that science does not have to talk about things that can not be measured , i.e. that theories that deny the existence of things that can not be observed are just fine , it still remains true that science also can talk about concepts that can not be observed , such as quarks . it is up to the scientific method to decide whether an auxiliary concept or a theory that is not accessible to observations has an explanatory power that justifies its validity - and the answer may be different in each individual situation .
i am in part trying to understand this myself . the berry phase is computed from differential forms , such as the one-forms $\omega$ constructed from states $$ \omega~=~\langle\psi|d\psi\rangle $$ and with the covariant differential $d~=~d~+~\omega$ the two-forms $$ \omega~=~d\omega~=~d\omega~+~\omega\wedge\omega $$ the tensor components of the 2-form $f$ are elements of a self-adjoint principal bundle $p$ . the determinant of these elements $$ det\big|1~+~\frac{ixf}{2\pi}\big|~=~\sum_nc_jx^n $$ which is a characteristic polynomial which represents the chern class . each $c_n ( p ) $ is an element of $h^{2n} ( m ) $ . so the curvature form for the berry phase , or the fubini-study metric $\omega=~dz\wedge d{\bar z}/ ( 1~+~|z|^2 ) ^2$ is evaluated $\int\omega~=~2\pi i$ and gives $c_1~=~1$ so there is a nontrivial cocycle on the “2-level . for this projective geometry there are alternating betti numbers $1 , ~0$ for even and odd . if you had some product of states $\prod_n |\psi_n\rangle$ , say in an entangled state etc , you could apply the differential $d$ up to $n$ times and form and $n$-form . for instance the product $|\psi_1 , ~\psi_2\rangle$ $=~|\psi_1\rangle|\psi_2\rangle$ defines the one-form $$ \omega~=~d|\psi_1 , ~\psi_2\rangle~=~d|\psi_1\rangle|\psi_2\rangle~+~|\psi_1\rangle d|\psi_2\rangle $$ and one could then build up a system of differential forms on various chains . the analogue of the projective geometry for this is a $g_2 ( v ) $ grassmannian and this continues up for n-product spaces .
the two images are equally ' real ' . one is produced in each of your eyes , so they are similar , but viewed from slightly different angles . this means that a reflection of a bright light , which requires a precise alignment of light source , eye and reflecting surface , may be visible in one image and not the other .
ball lightning could definitely be some atmospheric pressure plasma phenomenon . you can make a pretty impressive ball plasma by discharging a kilojoule-scale capacitor bank into a bucket of salt water . check out free-floating atmospheric pressure ball plasma . in most of those pictures they are using a copper sulfate solution , but that is not essential ( sodium chloride also works ) . these ones only last a ( significant ) fraction of a second , but i am sure if you made a larger one ( e . g . by a lightning strike ) , they could last longer . btw , this was the subject of a killer science fair project : http://www.youtube.com/watch?v=se6sbanskoc
your lower bound has certainly been considered . even wikipedia does the calculation in a number of places , noting that a black hole will be in equilibrium with the $2.7\ \mathrm{k}$ cmb at about the mass of the moon . the thing about black holes , though , is that there are multiple channels for them to be produced in nature . the remnants of collapsed stars should have masses around $1\ m_\odot$ . this number probably varies far less than the masses of stars themselves vary , but it is set by very complicated physics involving degenerate matter and stellar evolution as a whole . on the other hand , supermassive black holes in the centers of galaxies have masses well over $10^9\ m_\odot$ . so right there you are looking at at least $9$ orders of magnitude of observed variation . add to that another $7$ or so on the lower end if you believe lunar-mass black holes exist , and you can see why there is not a standard $1\ m_\mathrm{bh}$ unit . it should be noted that most all black hole properties of interest scale perfectly in proportion to some power of the mass , usually just the first power . thus one often normalizes quantities to the mass of the particular black hole in question .
when the notions of electric and magnetic fields were conceptualized , they imagined that there was an invisible fluid being pushed around by charges , and they leveraged some of the equations and terminology of fluid mechanics . modern understanding of field have largely gotten rid of this picture , but some colorful langauge like " electric flux " remains . if you want to picture positive charge as " amount of fluid added to region per unit time " and negative charge as " amount of fluid removed from region per unit time " , you can , but this thinking only gets you so far . safer to just think of it as an abstract mathematical definition .
you might be interested in skimming the expanding confusion paper by davis and lineweaver , which i have referenced many times on this site . in particular , the last sentence of the abstract states : we analyze apparent magnitudes of supernovae and observationally rule out the special relativistic doppler interpretation of cosmological redshifts at a confidence level of 23$\sigma$ . the basic idea is looking at the luminosity distance using type ia supernovae ( the " standard candles " of cosmology ) . the results you had predict from a doppler shift - even a special relativistic one - do not agree with reality . you had have to have some pretty ad hoc additions to your special relativistic cosmology to get these results . section 4.2 of the paper covers this . it is not surprising , though , that there is so much confusion . the authors themselves got confused , and an earlier version of section 4.1 stated that you could differentiate between doppler shifts and gr based on timing with standard clocks , but in fact the two effects are indistinguishable . perhaps the best part of the paper is appendix b , which gives some 25 examples of misleading or outright erroneous statements on this and related topics made by well-known physicists . i also should add that we also have the cmb radiation to observe , and it provides a unique local at-rest reference frame . that is , if you are moving with respect to the cmb , you will know because it will appear hotter in the forward direction and cooler in the opposite direction . now we have a small velocity relative to it , but nothing close to the speed of light . so if you try to explain redshifts with just sr and the doppler effect , you have to explain why we happen to live in one of the 100 or so galaxies barely moving with respect to the cmb , and why the billions of other galaxies we observe are all moving quite fast with respect to it . in gr , everyone can be locally " at rest " and see an isotropic cmb , while still seeing each other redshifted . so there is a copernican argument for you - the sr description forces you to believe you are in a special location in the universe . if you want to intuitively understand the cosmological redshift , i will shamelessly plug my own excessively verbose answer to a related question .
this depends a lot on the heating and cooling mechanisms you have in your model . certainly initially neutron stars cool when neutrinos created in nuclear reactions escape . however , this is highly sensitive to such effects as superconductivity of the nucleon species ( which both suppresses nuclear reactions and reduces heat capacity ) and the density profile ( which is in turn determined by the endlessly debated equation of state for the matter ) . at some point , the star likely reaches a thermal equilibrium of sorts , cooling by blackbody radiation from the surface . of course cooling is not the only process . just a few of the heating ideas that have been floated over the years include : friction between superfluid and non-superfluid layers sapping energy out of the star 's rotation ; the latent heat of crystallization of the crust being released whenever the crust " cracks " due to its equilibrium obliquity being reduced as the star spins down ; conversion of magnetic field energy into heat via , e.g. , pair production ; redistribution of species as the centrifugally-induced chemical potentials change with spindown ; interaction with the interstellar medium ; and even colliding and interacting with the occasional free-floating magnetic monopole , if you believe in such things . a rather extensive review of all this and more can be found in this review by tsuruta , if you have access . the paper is replete with cooling curves for all sorts of models - too many to summarize here . extrapolating to $12~\mathrm{gyr}$ leads to temperatures 1 of anywhere from a few kelvin ( obviously the lower limit based on the temperature of the cmb reservoir in which the system sits ) to $10^4~\mathrm{k}$ ( blackbody peaks in the uv ) and higher . 1 often the axes are labeled by luminosity rather than temperature , the conversion being simple enough given the model 's radius . also note that all observables in the paper are properly gravitationally redshifted ; if you want to know about the radiation field just above the surface , you have to blueshift the photons back into potential well .
what do you mean by " prove " ? if you mean in a strict mathematical sense , then looking for such assurances is a lost cause . there are quite a wide variety of papers on this matter . curie attacked this particular problem in 1913 with radium . they immersed a radium source in liquid hydrogen for more than an hour and did not find a change of more than 0.1% in its activity . you can read more from the paper by curie and kamerlingh onnes entitled , " the radiation of radium at the temperature of liquid hydrogen " in knaw , proceedings , 15 ii , 1912-1913 , pp . 1430-1441 . people even claimed , from russia , that polonium 's activity varied depending on geography . hardly the case . more recently , work has been done on the half-life decay rate of $^{97}ru$ without seeing a noticeable temperature dependence near 20k compared with rt . see the paper by goodwin , golovko , iacob and hardy entitled , " half-life of the electron-capture decay of $^{97}$ru : precision measurement shows no temperature dependence " in physical review c ( 2009 ) , 80 , 045501 . it could be that there is a small dependence , but not even the russian paper mentioned above by martin agrees there is a measurable temperature dependence .
you make artificial light with the same spectrum as the sun , and shine it on the crops .
lets suppose that all n magnets are identical . assume that they are quite far from each other so we can replace them with n magnetic dipoles with dipole moment $\overrightarrow{m}$ . also assume that the dipoles are placed along the x axis in such a way that they repel each other and all n moments $\overrightarrow{m}$ are parallel . the first step is to find the interaction force between two dipoles . fortunately , wikipedia has a formula for the force so that we can save a large amount of work . link here i write only its x axis component which we need , to save space : $$f=\frac{6\mu_0 m^2}{4\pi x^4}$$ now , to find " spring rate " between two magnets , it is sufficient to find the differential $df$: $$df=-\frac{6\mu_0 m^2}{\pi x^5}dx$$ so for the small displacement the " spring rate": $$k ( x ) = \frac{6\mu_0 m^2}{\pi x^5} $$ now , since we know $k$ it is easy to find the effective spring rate for the top magnet . let $f$ be the force on the top magnet , other end magnet fixed . then the displacement of the top magnet is $\delta x= ( n-1 ) \frac{f}{k}$ so the effective spring rate : $$k_e ( x ) = \frac{6\mu_0 m^2}{ ( n-1 ) \pi x^5} $$
the explosion certainly is hemispherical , see , for instance , this explosion caused by the trinity bomb : the gas cloud that you posted , and what many would consider is synonymous to the nuclear weapons , comes after the explosion . nuclear bombs are actually usually ignited above ground for " maximum destruction . " since the nuclear reaction is immensely hot ( about 4000 k whereas the surface of earth is sitting pretty around 300 k ) , the gas rises much the same way a hot-air balloon rises . at some point , the cold air from around the explosion gets sucked under the mushroom cap and causes the thin column you see : thus , for the most part , it is the extreme temperatures that cause the explosion " bubble " to rise in the first place . and it is the convective air currents under the bubble that cause the column to form .
interesting , i was just studying the fourier decomposition of vowels for half an hour yesterday . first , you must distinguish vowels and consonants . words like " bee " ( which is how we spell the letter " b" ) of course are not uniform sounds . they start with a consonant , in this case one created by lips . depending on the way how they are created , consonants are divided to many groups . see a table of consonants here . in general , consonants are types of noise because they do not have a well-defined basic frequency . it means that the fourier series for a consonant is composed pretty much of all sufficiently high frequencies . the color of the noise – whether higher frequencies tend to be more strongly represented than the lower one etc . – determines the type of the consonant . there are consonants with a throat sound added , like l , m , n , which may be fourier decomposed similarly to the vowels , and noise-based consonants such as b , d , g , v , z which are the sound-equipped cousins of p , t , k , f , s , and so on , and so on . the most monochromatic sounds are the vowels . they can be sung so they have a well-defined base frequency . whether one gets u , o , a , e , i – or , in english , oo , aw , ah , eh , ee etc . – depends on how the mouth is opened . this modifies the shape of the resonance cavity and therefore the preferred additional frequencies that are excited by the action of the base frequency coming from the throat . the presence of higher harmonics is essential for the difference between vowels . i recommend you to look at a page about it , for example this one : http://hyperphysics.phy-astr.gsu.edu/hbase/sound/vowel.html the vowel u ( oo in english ) has the lowest representation of the higher harmonics . it is the closest one to the harmonic sound and it is achieved by changing one 's mouth into a passive tube through which the sound penetrates . on the other hand , a ( ah ) and i ( ee ) have a huge , important contribution of higher harmonics and when i say higher , i do not mean the 2nd or 3rd . the 20th harmonic etc . etc . are very important . in fact , it is more accurate to talk about the absolute frequencies . the vowel a ( ah ) has lots of those higher harmonics that are close to 1,000 hz ( cycles per second ) which are already suppressed in u ( oo ) and partly suppressed in o ( aw ) . as you continue to go towards e ( eh ) and i ( ee ) , the contribution from frequencies close to 3,000 hz starts to increase . these frequencies are calculable from the size of the mouth opened in the right way , from the length of the resonant cavity that becomes comparable to the wavelength of the sound waves that become important . in principle , all vowels may be emulated by the fourier expansion using just the base frequency ( pressure goes like $\sin \omega t$ ) plus higher harmonics but the very harmonics including $\sin 20\omega t$ are still very important for the character of the vowel . phonetics is the portion of linguistics that studies how language sounds ; the experts partially learn some physics although fourier series are not their primary tool . still , to understand phonetics , one has to accept various basic things . i found out that native english speakers misunderstand phonetics because they do not really decompose the language into " pure sounds " . your representation of " b " as a sound analogous to " a " may be an extreme example because you may have meant " b " in the sense of " bee " which is clearly composed of two sounds , the consonant " b " and the vowel " ee " . but even when it comes to vowels only , english ( and french and some other languages ) is deliberately obscuring the reality as it pronounces many vowels in a variable way . for example , " my " is pronounced as " mai " where the vowel gradually changes from " ah " at the beginning to " ee " at the end . some native english speakers do not even realize that this " y " in " my " is not a single uniform vowel . there are many other examples , of course .
he means that since it is a second order differential equation , it is completely determined by two sets of information , namely the value of $\phi$ and ${\dot \phi}$ at some time $t=t_0$ . in other words , $\phi ( t_0 ) $ and ${\dot \phi} ( t_0 ) $ paramaterizes the solution . we can therefore choose them to take any values , some which will imply a negative value of $\rho$ .
thermal neutrons capture on hydrogen and carbon with reasonable ( i.e. . not large , but significant ) cross-sections ( this is the delayed event detection methods of most organic liquid scintillator anti-neutrino detectors--i . e the one that do not dope their scintillator with gadolinium ) . so though a " cloud"--meaning a localized diffuse gas--of neutrons can develop in the neighborhood of a strong source ( size of the cloud is driven by how far they go as they thermalise ) , their dissipation is driven by their mean capture time , not their half-life . confession : here i am presuming that the mean capture time is significantly shorter than the half-life , but i have not measured it in a " near the laboratory " setting . in organic liquid scintillator the capture time is on order of $200\text{ }\mu\text{s}$ , but air has a lot less hydrogen and carbon in it . note that the neutrons also go into the ground , the building , nearby vehicles and passers-by ( if any ) where they may find things to interact with . at my grad-school we had a 2 curie ( i.e. . huge ) ambe source . the source vault would register unusually high back-grounds on a survey meter for a few minutes after it was returned from the moderator tank to the shielded vessel , so that may be a rough measure of the time scale . it also says something about the strength of the radiation field : a few times the in-the-basement background level . shielding methodology for strong neutron sources generally incorporates a great deal of boron in various layers to help suck up the thermal neutron flux ; not incidentally this means that most of the capture gammas are generated inside the shielding . borated plastics are common as are borated concretes . these days gadolinium is cheep enough that i imagine we will start seeing it used in shielding design . the source vault in grad school was built of borated cinder block---two layers with a meter air-gap between . another not-very-quantitative story that might shed some light on this . i was friends with one of the radiation safety guys at jlab . part of his job was monitoring the radiation level at the fence around the secure area with the accelerators , experimental halls , etc . mostly they just put out general purposes detectors and compared the results with background reading from nearby , but early on they built a more sophisticated detector out there to understand the various contributions to the dose ( probably trying to tune their monte carlos , those guys are really big into modelling ) . he told me two interesting things if they ran the accelerator at high current and high duty cycle they could about double the dose at the fence ( i.e. . the accelerator related dose was as big as the background at the fence ) . neutron sky-shine was the single biggest contributor . sky-shine means that the neutrons got out through the lightly shielded roofs of the halls ( only 50 cm of concrete and 2 meters of packed earth ) , and their detectors saw radiation coming from the captures/decays that occurred above them . the fence was about 40 meters from the beam dumps .
it depends on the fan , but i would guess the majority of domestic fans will use less power at lower speeds . i can state with authority that the fan in my car ( a ford focus ) uses roughly the same power regardless of speed because i have just had to replace the ballast resistor that is uses to control fan speed . when you select a lower speed the fan dissipates power as heat in the ballast resistor so the speed setting makes little difference to the power drawn . i can not be sure about domestic fans , but in the car fan the heat dissipated in the ballast resistor is very noticable and indeed theresistor gets too hot to touch . the fan on my desk does not get hot when used at a lower speed , so i think it is very likely it does not simply dissipate power to lower the speed and therefore it will use less power at lower speeds . it is probably significant that the car fan is dc while domestic fans are ac . it is much easier to control power in ac circuits because you can use a thyristor or something similar to control the power delivery in a lossless way . the only way to be sure is , as energynumbers suggests , to measure the power drawn . a simple power meter like this one is all you need . unfortunately i am working away from home this week otherwise i could measure the power drawn by my own fan and give you a definitive answer . however i am sure there must be some fan owning , power meter armed , experimental physicists reading this :- )
well the answer is that the body will indeed loose the momentum . but since the mass of the body will decrease as well due to radiation , the velocity should not change .
the problem is a statics one , where the sum of the forces on the beam equal zero and the sum of the moments about any point equals zero also . let say the hinge is point a at the coordinate origin . the rope is at a point b with coordinates $\vec{r}_b = ( 6\cos ( 30^\circ ) , 6\sin ( 30^\circ ) ) $ . the center of gravity is at a point c with coordinates $\vec{r}_c = ( 4\cos ( 30^\circ ) , 4\sin ( 30^\circ ) ) $ . if the weight is $w$ , the tension is $t$ and the pivot reaction $a_x$ and $a_y$ then the equations of statics are $$ a_x - t \cos ( 10^\circ ) = 0 \\ a_y + t \sin ( 10^\circ ) - w = 0 \\ 6\cos ( 30^\circ ) t \sin ( 10^\circ ) + 6\sin ( 30^\circ ) t \cos ( 10^\circ ) - 4\cos ( 30^\circ ) w = 0 $$ which you can solve for $t$ , $a_x$ and $a_y$ . i can confirm that the tension you found is indeed $t=13,203\ , {\rm n}$ conceptually you are stating that a steady state solution means balance of forces with zero accelerations .
in this case the index can vary on the number of spatial dimensions ( three if you are in 3d ) . the $i_n$ notation refers to the fact that you are " repeating " the scalar product "$\mathbf{a}\cdot\nabla$" n times : $$ ( \mathbf{a}\cdot\nabla ) ^n = ( a_i\nabla_i ) ^n = ( a_{i_1}\nabla_{i_1} ) ( a_{i_2}\nabla_{i_2} ) \dots ( a_{i_n}\nabla_{i_n} ) = a_{i_1}a_{i_2}\dots a_{i_n}\nabla_{i_1}\nabla_{i_2}\dots \nabla_{i_n} $$
provided that these 2 perpendicular forces are the only forces acting on the body , there resultant can be easily found by parallelogram law of vectors . using it i got the acting force as 94.34n and dividing by mass i got the acceleration as 3.77m/s2 . note : fnet^2 = f1^2 + f2^2 ( from parallelogram law when angle between vectors is 90degrees )
a string is a " particle with a complicated internal structure " . to see the rough emergence of particle species , you may start with a hydrogen-string analogy . the hydrogen atom is a bound state of a proton and an electron . it may be in various energy eigenstates described by the quantum numbers $ ( n , l , m ) $ . they have a different angular momentum and its third polarization and different energies that mostly depend on $n$ . it is similar for a string . a string may be found in various states . the exact " spectrum " i.e. composition of these states depends on the background where the string propagates and the type of string theory ( more precisely , the type of the string theory vacuum ) . but for the rough picture , consider string theory in the flat space , e.g. in the 26-dimensional spacetime . take an open string . its positions $x^\mu ( \sigma ) $ may be fourier decomposed and each of the fourier modes , labeled by a positive integer $n$ , produces coordinates of a 24-dimensional harmonic oscillator . so an open string is equivalent to a $24\infty$-dimensional harmonic oscillator ( yes , it is twenty-four times infinity ) . each of the directions in this oscillator contributes $nn/ \alpha'$ to the squared mass $m^2$ of the resulting particle where $n$ is the total excitation level of the harmonic oscillators that arise from the $n$-th fourier mode . at any rate , the possible values of the squared mass $m^2$ of the particle are some integer multiples of $1/\alpha'$ . this dimensionful parameter $1/\alpha'$ is also called $1/l_{string}^2=m_{string}^2$ . the ground state of the string , $|0\rangle$ of the harmonic oscillator , is a tachyonic particle with $m^2=-1/\alpha'$ in the case of bosonic strings . these tachyons are filtered away in the superstring . the first excited state of an open string is $\alpha^\mu_{-1}|0\rangle$ which carries one spacetime lorentz vector index so all these states behave as a vector with $m^2=0$ . they give you a gauge boson . and then there are massive modes with $m^2\gt 0$ . closed strings of similar masses have twice larger number of indices , so for example , the massless closed string states inevitably produce a graviton . so different masses of the resulting particles arise from different values of $nn$ – and the very fact that the values may be different for different excitations is analogous to the same feature of the hydrogen atom or any other composite particle in the world . in string theory , however , one may also produce states with different values of the angular momentum – also somewhat analogous to the hydrogen atom which is a sufficient model – or different values of the electric charge and other charges . for example , in some kaluza-klein-like vacua , the number of excitations of $x^5_{n}$ , the fourier modes of the ( circular ) fifth dimension $x^5$ , will be interpreted as the electric charge and it will behave as the electric charge in all physical situations , too . there are other ways how $u ( 1 ) $ electric-like charges and other charges arise in string theory . see e.g. this popular review http://motls.blogspot.com/2012/08/why-stringy-enhanced-symmetries-are.html?m=1 of ways how yang-mills gauge groups and charges may emerge from different formulations and vacua of string theory . if even this review is too technical , you will have to be satisfied with the popular brian-greene-like description stating that particles of different mass , spin , or charges emerge from strings vibrating in different ways . i am sort of puzzled about your question – and afraid that my answer will be either too simple or too off-topic given your real question – because you must have heard and read these basic insights about string theory about hundreds of times already .
realistically , because the light emitted from the infalling object is quantized , you will observe the " last " photon emitted from the infalling object in ( very much ) finite time . if you keep waiting after that , you will eventually observe the black hole hawking-radiate away , and any " information " carried by the infalling object will be " encoded " in the hawking radiation . here 's an additional argument : when the infalling object passes through the event horizon , it will contribute to the mass of the black hole , which will cause the event horizon to expand outward even more . this ensures even further that the infalling object will no longer be observable from the outside in a relatively short time , compared with the lifetime of the black hole .
in addition to bms answer , i want to point out the integration part as i have seen , in the comments , you have some problems in the integration part . first you should have written the unit vectors in the expression of the electric field . the electric fields are $\vec{e}_{\text{in}} ( \vec{r} ) =\dfrac{q}{4\pi \epsilon_0 r^3}r\hat{r}$ and $\vec{e}_{\text{out}} ( \vec{r} ) =\dfrac{q}{4\pi \epsilon_0 r^2}\hat{r}$ $\ $ ( obviously , electric field is radially outward due to spherical symmetry symmetry ) $\vec{e}=-\vec{\nabla}\phi$ $\vec{e} \cdot d\vec{r}=-\vec{\nabla}\phi \cdot d\vec{r}= -d\phi$ $\int_\vec{a}^\vec{r}\vec{e} \cdot d\vec{r}=-\int_\vec{a}^\vec{r}d\phi$ $\phi ( \vec{r} ) -\phi ( \vec{a} ) =-\int_\vec{a}^\vec{r}\vec{e} \cdot d\vec{r}$ out of many possible path , we are going to take our path radially , due to conservative nature of electric field . so $d\vec{r}= d ( r \hat{r} ) =dr \hat{r}$ , since $\hat{r}$ remains same along the radial direction , we have $d\hat{r}$=0 . this would not be case if we would have taken any other path between the points $\vec{a}$ and $\vec{r}$ . so , $\int_\vec{a}^\vec{r}\vec{e_{in}} \cdot d\vec{r}=\int_\vec{a}^\vec{r}\dfrac{q}{4\pi \epsilon_0 r^3}r\hat{r} \cdot dr \hat{r}=\int_\vec{a}^\vec{r}\dfrac{q}{4\pi \epsilon_0 r^3}r dr=\int_a^r\dfrac{q}{4\pi \epsilon_0 r^3}r dr $ see at the last step vector sign has been dropped because the integration depends only on r , but not on ( $\theta , \phi$ ) . so , $\int_\vec{a}^\vec{r}\vec{e_{in}} \cdot d\vec{r}=\dfrac{qr^2}{8\pi \epsilon_0 r^3} -\dfrac{qa^2}{8\pi \epsilon_0 r^3}$ so , $\phi_{in} ( \vec{r} ) -\phi_{in} ( \vec{a} ) =- ( \dfrac{qr^2}{8\pi \epsilon_0 r^3} -\dfrac{qa^2}{8\pi \epsilon_0 r^3} ) =-\dfrac{qr^2}{8\pi \epsilon_0 r^3} +\dfrac{qa^2}{8\pi \epsilon_0 r^3}$ $\phi_{in} ( \vec{r} ) =-\dfrac{qr^2}{8\pi \epsilon_0 r^3} +\dfrac{qa^2}{8\pi \epsilon_0 r^3}+\phi_{in} ( \vec{a} ) $ $\phi_{in} ( \vec{r} ) =-\dfrac{qr^2}{8\pi \epsilon_0 r^3} +c1$ where c1=$\dfrac{qa^2}{8\pi \epsilon_0 r^3}+\phi ( \vec{a} ) $ note : i think you have some problem with vector analysis . you can look at the " vector analysis ( schaum 's outline ) by spiegel " . it is an excellent book .
it is not non-sensical at all , except that there should not be a minus sign ( as mentioned in the comments ) and that you took an operator outside of an expectation value , which i think worked out ok in this case but in general should be avoided . more conservatively , $$ \hat x = i \hbar \frac{d}{d \hat p} $$ it follows that $$ \langle q \mid \hat x \hat p \mid q ' \rangle = q ' \langle q \mid \hat x \mid q ' \rangle = i \hbar q ' \langle q \mid \frac{d}{d \hat p} \mid q ' \rangle = i \hbar q ' \langle q \mid \frac{d}{d q'} \mid q ' \rangle $$ remember , an actual physical state is never an idealized momentum ( or position ) eigenket . a slightly more realistic description of a physical state is a wave packet with a narrow width around some momentum : $$ \mid p , \sigma \rangle = \int \frac{dq}{2 \pi \hbar} \ , \left ( \frac{2 \pi \hbar^2}{\sigma^2}\right ) ^{1/4} e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \mid q \rangle $$ you can check that this is normalized to unity , because $$ \langle p , \sigma \mid p , \sigma \rangle = 1 $$ remember that $\langle p \mid q \rangle = 2 \pi \hbar \delta ( p-q ) $ . now , the expectation value of $\hat x \hat p$ for this physical state is $$ \langle p , \sigma \mid \hat x \hat p \mid p , \sigma \rangle = \frac{1}{\sqrt{2 \pi} \ , \sigma } \int dq \ , dq ' \ , e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} e^{- \frac{1}{4} \left ( \frac{q'-p}{\sigma} \right ) ^2} \frac{\langle q \mid \hat x \hat p \mid q ' \rangle}{2 \pi \hbar} $$ using the above result , we can integrate the $q'$ integral by parts , yielding $$ \frac{-i\hbar}{\sqrt{2 \pi} \ , \sigma } \int dq \ , dq ' \ , e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \frac{d}{dq'} \left ( q ' e^{- \frac{1}{4} \left ( \frac{q'-p}{\sigma} \right ) ^2} \right ) \frac{\langle q \mid q ' \rangle}{2 \pi \hbar} $$ since $\langle q \mid q ' \rangle = 2 \pi \hbar \delta ( q-q' ) $ , we can integrate out q ( and then relabel q ' back to q ) , making this a single integral $$ \frac{-i\hbar}{\sqrt{2 \pi} \ , \sigma} \int dq \ , e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \frac{d}{dq} \left ( q e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \right ) $$ we can integrate by parts again , ending up with $$ \frac{i\hbar}{\sqrt{2 \pi} \ , \sigma} \int dq \ , q e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \frac{d}{dq} \left ( e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \right ) $$ which is a bit easier to evaluate since we do not have to use the product rule . the derivative brings down a factor $- ( q-p ) /2\sigma^2$ , and the two exponentials combine , leaving us with $$ \frac{-i\hbar}{ 2\sqrt{2 \pi} \ , \sigma^3} \int dq \ , q ( q-p ) e^{- \frac{1}{2} \left ( \frac{q-p}{\sigma} \right ) ^2} $$ the first $q$ in the integrand can be rewritten as $ ( q-p ) + p$ . then there are two terms , the latter of which has an odd integrand ( of the form $ ( q-p ) \exp ( \alpha ( q-p ) ^2 ) $ and vanishes . so , changing the integrand to $x = ( q-p ) /\sigma$ , we are left with $$ \frac{-i\hbar}{ 2\sqrt{2 \pi}} \int dx \ , x^2 e^{- \frac{1}{2} x^2} $$ the remaining integral is a textbook gaussian integral , equal to $$ \int dx \ , x^2 e^{-\frac{1}{2} x^2} = \sqrt{2\pi} $$ so , the expectation value of the operator $\hat x \hat p$ for the wave packet is simply $$ \frac{-i\hbar}{2} $$
i simply misfactorised the quadratic - i knew it was a stupid mistake . i am amazed that i did not see it , but even more amazed that nobody else did ! here is the correct solution . $$\phi_{01} ( t , x , y , z ) = \frac{1}{2\pi i}\oint d\xi \frac{\xi}{ ( x^{01'} ) ^2 ( \xi -\xi_1 ) ^2 ( \xi - \xi_2 ) ^2}$$ the residue at $\xi_1$ is \begin{align*} r_1 and = \rho_{\xi_1}\frac{d}{d\xi}\frac{\xi}{ ( x^{01'} ) ^2 ( \xi - \xi_2 ) ^2} \\ and = \frac{1}{ ( \xi_1 - \xi_2 ) ^2 ( x^{01'} ) ^2} -2 \frac{\xi_1}{ ( x^{01'} ) ^2 ( \xi_1-\xi_2 ) ^3} \\ and = \frac{\xi_1 - \xi_2 - 2\xi_1}{ ( x^{01'} ) ^2 ( \xi_1-\xi_2 ) ^3} \\ and = -\frac{x ( y+iz ) ^2}{2r^3 ( y+iz ) ^2} \end{align*} this is now the correct answer : ) .
for someone with perfect vision , the lens in their eye focuses a point source in their field of vision to a point source on their retina . for someone with less than perfect vision , the focus lies either before or behind the retina , resulting in a large spot on the retina , instead of a point . see the top illustration . when you squint , or roll your fingers into a tube in front of your eye , you are placing an aperture in the way of the light . see the bottom illustration . as the diagram shows , the focus stays the same distance away from the retina , but since the angles are smaller , the spot on the retina is also smaller . a smaller spot is closer to a point , and therefore seems " sharper " - even though the vision is just as bad . the camera obscura works on a similar principle .
hint : the divergence theorem tells us that the divergence of a vector field integrated over a region $r$ with boundary $\partial r$ equals the integral of that vector field dotted with the outward-pointing normal along the boundary ; \begin{align} \int_r dv\ , \nabla\cdot \mathbf v=\int_{\partial r} ds\ , \mathbf v\cdot\mathbf n . \end{align} if we are integrating over all space , then this is like integrating over the inside of a sphere but in the limit that this sphere is infinitely big . so we have \begin{align} \int_{\mathbb r^3} dv\ , \nabla\cdot\mathbf v = \lim_{r\to\infty}\int_{b_r}dv\ , \nabla\cdot \mathbf v = \lim_{r\to\infty} \int_{s_r}ds \ , \mathbf v\cdot \mathbf n \end{align} where $b_r$ denotes the closed ball of radius $r$ ( namely the inside of a sphere of radius $r$ along with its boundary ) and $s_r$ denotes the sphere of radius $r$ . now , on the surface of a sphere of radius $r$ , the area element and outward-pointing normal are \begin{align} ds = r^2\sin\theta d\theta d\phi , \qquad \mathbf n = \hat{\mathbf r} , \end{align} so we get \begin{align} \int_{\mathbb r^3} dv\ , \nabla\cdot\mathbf v = \lim_{r\to\infty} \int_{s_r} d\theta\ , d\phi\ , \sin\theta\ , r^2 v_r \end{align} if $v_r$ decreases sufficiently rapidly with $r$ , then the expression on the right will be zero . in other words , we have found that provided the radial component of a vector field falls off sufficiently rapidly with $r$ , the integral of the divergence of the vector field over all of space vanishes . try to use this fact along with things you know about how the electric field of a finite distribution of charge behaves very far from the distribution .
the answer is in front of you $$ a = \frac{{\rm d} ( u ) }{{\rm d} t} = \frac{{\rm d}}{{\rm d} t} \left ( \frac{{\rm d} ( r ) }{{\rm d} t} \right ) = \frac{{\rm d}^2 ( r ) }{{\rm d} t^2}$$ as a matter of convention . it is just the way we write 2nd derivatives .
yes , though i do not think that we will see d-wave factoring even 20-bit numbers anytime soon . one of their tutorials shows how to model a nand gate using 4 qubits . with a handful of those , i can make a carry-save multiplier cell , though surely it can be built more optimally . if i want to factor an n-bit number , i could use an n/2 by n/2 array of the carry save adder cells , and constrain the n-bit output to equal the number i want to factor , and have no weights on the inputs . run quantum annealing , and in theory with probability approaching 100% as noise goes to zero and run time get 's longer , and the inputs will settle to the input factors , in one of the two acceptable states , for example 3x5 = 15 vs 5x3 = 15 . the title " better than shor " may simply mean that with their new 512 qubit qao , they believe they can factor 35 = 5x7 , or maybe 51 = 3x17 . i really do not see factoring a 512 bit number with a 512 qubit quantum annealer . since building the multiplier takes o ( n^2 ) qubits , we will probably need over a million to factor 2048 bit rsa . a modified booth encoded multiplier saves you a factor of over 2 . if d-wave continues doubling qubits every year or so , and if they continue showing true quantum annealing performance , we may need to switch to a post-quantum-computer encryption algorithm . note that this technique also works for finding sha-1 collisions . it is super cool stuff . i just found a paper describing the algorithm from 2002: http://arxiv.org/abs/quant-ph/0209084
entropy never increases during this " mixing process " as it normally would , because it is not really a mixing process . the only thing that happens is a huge distortion of the distribution of the colors , but no information about it is lost . compare it to performing a fourier transform on some function : this also results in something that looks completely different , sometimes chaotic and high-entropy-like , but it is really just an invertible transformation .
tldr : cheap optics - the image gets worse with each new element added , pretty quickly . high end optics - the image may get better , may stay pretty much the same , or may get worse ; if it does get worse , it is by such a small amount that you can usually ignore it . this assumes a scope that is in good shape , otherwise all bets are off . now the long version : a t-adapter normally has no optical components , so there is zero loss of quality there . anyway , some actual answers : it is all a matter of price . use cheap mass-produced optics , and the quality will degrade pretty quickly . use more expensive elements and the image will stay amazingly clean even after passing through a large number of optical elements . a cheap plossl eyepiece has only a few lenses and the image looks like junk . a tele vue ethos eyepiece has 9 different lenses in 5 groups and the image looks incredibly sharp . keep in mind the quality of the primary mirror is also important . many mass-produced scopes come with primary mirrors that barely pass the rayleigh criterion . some are randomly better . the secondary mirror also matters . the corrector plate again introduces its own problems in catadioptric designs . " amateur " does not mean much . a mass-produced gso-made dob is " amateur " . but so is a clone made with optics figured by a reputable optician . yet one might be a terrible less than λ/4 instrument , end to end , while the other may well be above λ/15 and pretty high strehl factor . looking at tight double stars , the loose instrument may not even resolve the pair , while the good optics may put a clean sharp sliver of black between the two airy disks . one will show a few stripes on jupiter and not much else . the other will show plenty of detail . their performances will be very different . but so will be their prices . each element in the optical chain matters , and their individual performance levels are very much dependent on price . a single cheap element may ruin the image , while you could add tons of high quality optics and the image stays sharp and contrasty . this is all with amateur components that you will actually see used by various folks when your local astro club throws a star party over the hill south of the city . and then there is collimation . there are folks out there who ask " what is collimation ? " : ) for cryin ' out loud , people , that is not acceptable . the best optics in the world will mean nothing if the scope is not well collimated . the best car in the world will not work well if you forget to change the oil . then there is seeing . you take your expensive hand-figured scope outside , and seeing is terrible . you may as well look through a pirate 's spyglass , that is how bad the turbulence is . and then the next day the jet stream moves over some other place , and you get clean , pristine , sharp views . then there is thermal stabilization . a cheap mass-produced dob with a fan blowing over the primary mirror may actually beat the custom-made clone with no fan . well , after a few hours the fanless clone may catch up thermally and its view may improve a lot ( or not ) , but you get my point . goes on and on . there are no simple answers . edit : sometimes the image is improved when you add more glass . this is obvious in fast newtonians ( f/5 or faster ) , where coma is visible in the eyepiece . but then add a coma corrector , like a paracorr , to the optical chain , and the image is dramatically better . in fast newts , only with a coma corrector will the high end eyepieces achieve their whole potential . low end eyepieces need not apply , their own problems are worse than coma . same idea for astrographs : the field of virtually every scope is curved . the sensor of virtually any camera out there is flat . that is a mismatch . add a field corrector and the mismatch disappears . the image gets better when you add glass . heck , if you are ambitious and design a short achromat refractor , you will get chroma trouble in the eyepiece . but then filter out the offending wavelengths , and voila , no more chromatic aberration . a lowly filter has improved the image . sure , the image is tinted , but the overall resolution is better . ( this is a reason why you do not see short achromats too often ) edit2: let me confuse you a little . all of the above was a " whole field " discussion . if we are talking about strictly on-axis performance ( for folks who do planetary observations in tracking scopes ) , then less glass tends to be better . again , if the glass is low quality then it is no good , but as a rule , if you watch a very small low contrast target like a planet , and keep it on axis , then there are eyepieces dedicated for this kind of thing that manage to eek out a tiny bit more performance from the scope by using clever minimalist designs . some people have even tried ball lenses , extracted from laser couplers , which are just simple balls of glass or sapphire , and reported a very small improvement for on-axis planetary observations , compared to any eyepiece . these are not mere glass marbles , but high quality optics ; a tiny 9mm ball like that might go for \$20 . . . \$40 online . but this is kind of a fringe issue ( unless you have a very special thing for observing planets , in which case you will see this discussion in a very different light , so to speak ) . the question you asked is very complex .
i am going to assume that you have some experience with information theory as it relates to computation theory . this book : http://books.google.com/books/about/optical_computing.html?id=zffraaaamaaj is a good foundational look at how optical computing works , combining the physical processes with the computational understanding . this paper will provide a more up-to-date overview of where the state of optical computing is today : http://www.hindawi.com/journals/aot/2010/372652/ and more importantly provide further direction on where to look for more information .
yes , coherent radiation means that the phases of two ( or more ) waves representing the radiation differ by a known constant . incoherence means that the phase differences are unknown/random . laser radiation is coherent because stimulated emission assures phase differences are constant . radiation from an incandescent lamp is incoherent because the electromagnetic waves are generated in a statistically random manner depending on which atoms are excited and de-excited . to develop intuition think of the classical example of soldiers marching in step . their motion is coherent and so are the vibrations they set up . they break step over ancient bridges so as to become incoherent , there were cases where ancient bridges resonated to the step and were damaged .
you are operating under a misconception . when a real image is formed , we can see it , provided that our eye is positioned in a location such that rays from the image can enter your eye . compared to a hologram , the situation is different for a couple of reasons . ( 1 ) the possible locations of your eye are more restricted . ( 2 ) you may pick up psychological cues such as the framing of the field of view , which cause your brain not to interpret the real image as being where it actually is . by the way , the word is spelled " lens , " not " lense , " and it is not always true that a convex lens produces a real image .
the density of electrons / holes depends on both the effective mass of the electron / holes ( which , as you correctly realize , affects the density of states ) and the fermi level . if the fermi level moves towards the conduction band , you get more electrons and fewer holes ; if the fermi level moves towards the valence band , you get fewer electrons and more holes . therefore , there is some possible location ( in the band gap ) of the fermi level where the number of electrons equals the number of holes . for a big piece of undoped semiconductor , that is exactly where you will find the fermi level . if the electron and hole effective masses are different , then the intrinsic ( # holes = # electrons ) fermi-level depends on temperature . therefore , as you change the temperature of the semiconductor , the fermi level will shift up or down a bit . it will not be exactly halfway between the conduction and valence bands ; it will have a temperature-dependent offset . there is no law that says that the fermi level has to be independent of temperature .
others correctly state that the trajectory is no longer circular . your second equation $$v^2=\frac{gmm}{r}$$ was probably derived from uniform circular motion : $$\frac{mv^2}{r}=\frac{gmm}{r^2} . $$ if motion is not circular ( as is the case ) , the above equation will not hold . as for your first equation , the more general form is $$e_t=-\frac{gmm}{2a} , $$ where $a$ is the semi-major axis . for circular orbits , $a=r$ ( the radius of the orbit ) . your previous reasoning still works here : the total energy of this two-body system arguably increases , $a$ must increase as well since an increase in $a$ would correspond to an energy closer to zero ; that is , higher than before .
yes , p will observe a thermal radiation that is locally identical to the hawking radiation and it is called the unruh radiation . its temperature is $a/2\pi$ in the $\hbar=c=k_b=1$ units . http://en.wikipedia.org/wiki/unruh_radiation a few more words about the relationship of unruh and hawking radiation . unruh found his derivation after hawking found his - even though unruh 's radiation is kind of simpler and more elementary . this anomaly shows that hawking did not have any problems with doing more complicated calculations than others . also , the magic of the black hole background is that the killing vector field $\partial/\partial t$ where $t$ is the ordinary schwarzschild time coordinate has the character of a " non-accelerating " observer at infinity ( away from the black hole ) but near the horizon , it is severely accelerating ( not to fall to the black hole ) . the latter fact means that in this reference frame , the observer sitting above a black hole horizon ( by using his jets ) will see the what looks like the unruh radiation , but because his $t$ coordinate is continuously interpolated to the " static time " at infinity , this radiation will become real at infinity . it is important to realize that a star without an event horizon does not emit any hawking radiation . the boundary conditions at the star surface are completely different than at the event horizon . in the case of a star , we can not argue that a " freely falling " observer will see a nice local empty vacuum - because a freely falling observer gets killed when he collides with the star 's surface . finally , let 's mention that op 's original picture - a way of dividing the minkowski space by an event horizon for an accelerating observer - is known as the rindler space : http://en.wikipedia.org/wiki/rindler_coordinates the event horizon for the observer is observer-dependent and is also known as the rindler horizon .
i think this is largely just terminology . strictly speaking the coordinate is $ct$ not $t$ , and of course $d ( ct ) = cdt$ . in any case we usually choose units where $c = 1$ and just ignore it .
the " track " is the intended travel path over the ground from the origin to the destination . cross-track wind is the wind perpendicular to this line ( crosswind ) , and along-track wind is the headwind/tailwind component .
few remarks consider the whole system , this is , gun and bullet . this is an isolated system , so the net momentum is constant . in particular before firing the gun , the net momentum is zero . the conservation of momentum reads $$0=m_{1}v_{1}+m_{2}v_{2} $$ if the call $1$ to the gun and $2$ the bullet : $$\boxed{v_{2}=-\displaystyle\frac{m_{1}v_{1}}{m_{2}}} $$
you are right . in a space-time with one time dimension and $d$ spatial dimensions , finding possible different statistics is equilalent to look at the fundamental group ( first homotopy group ) of $so ( d ) $ for $d=1$ , the fundamental group is trivial . for $d=2$ , the fundamental group is $\mathbb{z}$ . for $d&gt ; =3$ , the fundamental group is $\mathbb{z}_{2}$ so , it explains , why , in 3 spatial dimensions , there are only 2 kinds of statistics ( fermions and bosons ) , while the situation is different with 2 spatial dimensions . for quantum point of view , you will have to find unitary representations of this fundamental group .
the jiles-atherton model of ferromagnetism is used in some circuit analysis programs . it may be overkill for this question , but it does give pretty pictures . i am going to work in mks units exclusively . the model equations are : $$ b= \mu_0 m \quad , \quad m = m_{rev} + m_{irr} $$ where $b$ is the magnetic flux density , and the magnetization $m$ is composed of reversible ( $m_{rev}$ ) and irreversible ( $m_{irr}$ ) components . physically , during the magnetization process : $m_{rev}$ corresponds to ( reversible ) magnetic domain wall bending ( the s-shaped magnetization curve , but without hysteresis ) $m_{irr}$ corresponds to ( irreversible ) magnetic domain wall displacement against the pinning effect ( the hysteresis ) . these components are calculated according to : $$ m_{rev} = c ( m_{an} - m_{irr} ) $$ $$ m_{an} = m_s \left [ \coth \left ( \frac{h + \alpha m}{a}\right ) -\frac{a}{h+ \alpha m}\right ] $$ $$ \frac{dm_{irr}}{dh} = \frac{m_{an}-m_{irr}}{k \delta - \alpha ( m_{an}-m_{irr} ) } $$ here the anhysteretic magnetization $m_{an}$ represents the magnetization for the case where the pinning effect is disregarded . ( this case corresponds to $c=1$ , where $m=m_{an}$ and $m_{irr}$ therefore does not contribute to $m$ . ) the quantity in the expression for $m_{an}$ in square brackets is the langevin function $\mathcal{l}$: $$ \mathcal{l} ( x ) = \coth ( x ) - \frac{1}{x} \quad , \quad \mathcal{l} ( x ) \approx \left\{\begin{array}{ccc} \frac{x}{3} and , and |x|&lt ; &lt ; 1 \\ 1 and , and x &gt ; &gt ; 1 \\ -1 and , and x&lt ; &lt ; -1 \end{array} \right . $$ and $\delta$ is the sign of the time rate of change of the applied magnetic field $h$: $$ \delta = \left\{ \begin{array}{ccc} +1 and , and \frac{dh}{dt}&gt ; 0 \\ -1 and , and \frac{dh}{dt}&lt ; 0 \end{array} \right . $$ $m_{rev}$ can be eliminated from this system of equations to reduce their number by 1: $$ m = c m_{an} + ( 1-c ) m_{irr} $$ the equations for $m , m_{an} , $ and $m_{irr}$ are inter-dependent and so are to be solved simultaneously . there are 5 parameters ( listed here together with sample values ) : $m_s$ , the saturation magnetization [ 1.48 ma/m ] $c$ , the weighting of anhysteretic vs . irreversible components [ 0.0889 ] $\alpha$ , the mean field parameter ( representing interdomain coupling ) [ 0.000938 ] $a$ sets the scale for the magnetic field strength [ 470 a/m ] $k$ sizes the hysteresis [ 483 a/m ] for the values listed , a crude spreadsheet produced this plot : the horizontal axis is the applied magnetic field h , in a/m , sweeping from 0 up to 2500 , then down to -2500 , and then up again to 2500 . the vertical axis is the flux density b in t . this example comes from a 1999 ieee paper by lederer et al , " on the parameter identification and application of the jiles-atherton hysteresis model for numerical modelling of measured characteristics " . it appears that choosing the parameters to match a given material is a chore , but that is another story . . .
let $x$ denote the length of the rope that is on the table , then $$ m ( x ) = \frac{m}{l}x $$ is the mass of the rope on the table . it follows that the force of friction on the rope on the table is $$ f ( x ) = \mu_k m ( x ) g = \mu_k\frac{m}{l}xg $$ if the rope moves an amount $dx$ then the work done by friction is $$ dw = f ( x ) dx = \mu_k\frac{m}{l}gx dx $$
unlike an electron beam , a wire carrying a current contains positive charges as well , and these charges move with respect to the moving observer .
there does not have to be eddy currents to obtain diamagnetic . in fact you can have eddy currents in iron plate , which is ferromagnetic . diamagnetics can be insulators , so there do not have to be any currents at all . in some cases it is energetically beneficial to align so that magnetic field is lowered . there are always diamagnetic and paramagnetic tendencies struggling , and in some materials one is just stronger than the other . magnetic field can enter diamagnetic and is only slightly ejected ( most of the times ) . it means that inside it magnetic field is lowered . for reasons described above diamagnetism is usually quite small . in case of superconductors magnetic field can not enter at all . that is provided it is small enough and superconductor is classical type one . magnetic field inside superconductor is zero . you can think of superconductors as ideal diamagnetics . but keep in mind that superconductors are more than that .
fundamental particles are identical . if you have two electrons , one from the big bang and the other freshly minted from the lhc , there is no experiment you can do to determine which one is which . and if there was an experiment ( even in principle ) that could distinguish the electrons then they would actually behave differently . electrons tend to the lowest energy state , which is the innermost shell of an atom . if i could get a pen and write names on all of my electrons then they would all fall down into this state . however since we can not tell one electron from another only a single ( well actually two since there are two spins states of an electron ) electron will fit in the lowest energy state , every other electron has to fit in a unique higher energy level . edit : people are making a lot of comments on the above paragraph and what i meant by making electrons distinguishable , so i will give a concrete example : if we have a neutral carbon atom it will have six electrons in orbitals 2s1 2s2 2p2 . muons and tauons are fundamental particles with very similar properties to the electron but different masses . muons are ~200 times more massive than electrons and tauons are ~3477 times more massive than an electron . if we replace two of the electrons with muons and two of the electrons with tauons all of the particles would fall into the lowest energy shell ( which can fit two of each kind because of spin ) . if in theory these particles only differed in mass by 1% or even 0.0000001% they would still be distinguishable and so all fit on the lowest energy level . now atoms are not fundamental particles they are composite , i.e. composed of " smaller " particles , electrons , protons and neutrons . protons and neutrons are themselves composed of quarks . but because of the way that quarks combine , they tend to always be in the lowest energy level so all protons can be considered identical , and similarly with neutrons . to take the example of carbon , there are several different isotopes , different number of neutrons , of carbon ( mostly $^{12}c$ but also ~1% $^{13}c$ and ~0.0000000001% $^{14}c$ {the latter which decays with a half life of ~$5,730$ years [ carbon dating ] but is replaced by reactions with the sun 's rays in the upper atmosphere} ) . if we take two $^{12}c$ atoms , and force all of the spins to be the same . this is not too difficult for the electrons of the atom since the inner electrons do not have a choice of spin because every spin in every level is already full . so only outer electrons matter . the nucleons also have spin . with our two $^{12}c$ atoms with all of the same spins , we now have two indistinguishable particles which if you set up an appropriate experiment ( similar in principle to the electrons not being able to occupy the same state ) we will be able to experimentally prove that these two atoms is indistinguishable . answer time : are atoms unique ? no . do atoms have any uniquely identifying characteristic besides their history ? their history of a particle does not affect it* . no particles are unique . atoms may have isotopes or spin to identify one from another , but these are not unique from another particle with the same properties . would it contain information with which we could positively identify that they two are the same ? yes only because we could positively identify that this carbon atom is the same as almost every other carbon atom in existence . *unless it does , in which case it may be considered a different particle with different properties .
this is a statement about a congruence of null geodesics . we are looking for a conjugate point , which is just a place where the null geodesics cross each other . the theorem is putting a bound on how far you can advance the affine parameter $\nu$ along the geodesics before the conjugate point occurs ( this is what is meant by affine parameter distance ) . to derive the bound , you need to assume the null energy condition , which says that $r_{\mu \nu} l^\mu l^\nu \geq 0$ for all null vectors $l^\mu$ . you are also assuming that the geodesics you are working with are hypersurface orthogonal , which means the twist $\omega_{ij}$ vanishes , and does not contribute to the raychaudhuri equation . by noting also that the shear $\sigma_{ij}$ is a spatial tensor , so will have a positive definite norm , $\sigma_{ij}\sigma^{ij}\geq 0$ , we find that this equation is saying $$\frac{d\rho}{d\nu}\geq\rho^2 . $$ this differential equation is easily solved , and you find that $$\rho\geq\frac{1}{\rho_0^{-1}+\nu_0-\nu} . $$ when $\nu-\nu_0=\rho_0^{-1}$ , the rhs of this inequality goes to infinity , which means that $\rho$ diverged at some value of $\nu$ before that . when $\rho$ diverges , it means there was a conjugate point .
an engine operating in a cycle can operate continuously i.e. for any number of cycles . if you just pull out one step in the cycle you do not have a useful engine because it can only operate once and then only for a short time . an isothermal process is reversible by definition because temperature is not defined in an irreversible process . so an isothermal process does not increase ( total ) entropy . incidentally this does not violate the second law since the second law says only that entropy cannot decrease . it does not forbid energy staying the same .
one revolution per minute is one sixtieth of a revolution per second . you have it the opposite way .
there were many models overturned throughout history , i will list some of the most salient ones . i will ignore the ones that predate modern science , the most prominent one being the geocentric model of the solar system , and i will confine myself to wrong ideas that were scientifically accepted as probably true at some point in history . phlogiston : this is the fluid that carries heat . it was imagined to be a quantity , like electric charge , that is conserved . experiments with cannon-boring machines showed that the amount of heat that you can make by doing mechanical work is only limited by your patience and force , so that the phlogiston fluid was discredited . this work is associated with the name of joule . the replacement for phlogiston is the modern notion of energy , which includes both mechanical and heat energies as one , and heat is no longer a quantity conserved separately from mechanical energy . lumineferous ether : this is the idea that light propagates in a medium , which fills all of space , and picks out a preferred frame , in which the speed of light is equal in all directions . relativity showed that such a frame does not exist--- that light moves in the same speed in all directions regardless of the motion of the source or the observer . this new symmetry removed the need for a non-relativistic light ether . modern etherlike ideas are relativistically invariant , and include the qcd condensates and the higgs mechanism . the drude model : when electrons were known , but quantum mechanics did not exist , drude proposed that a current is a drift of electrons . the idea was that electrons make a gas inside a metal , and for some reason they drift along as if they were in free space . the result predicted a very slow drift velocity of electrons , the drude velocity . drude 's model was made quantum mechanical , and this required that the electrons make a quantum fermi gas . the fermi gas is very cold at room temperature , since the typical temperature at which it becomes classical is of the order of the melting temperature of the metal . the behavior of a degenerate fermi gas explained the specific heat of metals , and gave a correct velocity for the current carrying electrons . the ether-knot theory : this idea was due to kelvin , that atoms are vortices in the ether , different atoms are different kinds of knots , and molecules are links . it was very popular at the turn of the century , because it could explain why atoms were discrete , but was shown to be wrong when bohr 's atom qualitatively and quantitatively explained the periodic table and the spectrum of hydrogen . the plum-pudding model : this imagined that electrons were embedded in a big diffuse positive charge , which was the atom . this theory predicted why atoms can have special resonance frequencies where they scatter light strongly . these frequencies were the resonant motions of the electrons inside the plum-pudding . the theory is incorrect , and the correct laws of resonant scattering are only provided by quantum mechanics . the bohr-sommerfeld quantum theory : this is the old quantum mechanics , where the action was given integer values . it was an important intuition for developing modern quantum mechanics , but it is now recognized as just an approximation to the actual quantum mechanical laws . the modern quantum mechanical laws were intuited from the bohr-sommerfeld laws by heisenberg in 1925 , and the same laws were derived by a different path , using wave-particle duality by debroglie , einstein and schrodinger . the bohr-sommerfeld theory is still a useful approximation , but it is not considered fundamental anymore . bohr-kramers-slater theory : this idea was that electron orbits are quantized , but the electromagnetic field is not . the theory predicted that energy is not conserved . the reason is that electromagnetic waves which do not come in photons can excite electrons in quantized orbits on many atoms at once , even if there are too few photons in the wave to do that . the modern quantum theory , plus the observation of compton scattering , demonstrated that photons are real , and killed the theory . nuclear electrons : in the 1920s and 1930s , before the neutron was discovered , the mass of nuclei were known to be approximately integer multiples of the mass of the proton . since it was considered unlikely that there would be a neutral particle with the same mass as the proton , people assumed that there were electrons tightly bound in the nucleus . this idea was inconsistent with quantum mechanics , which predicted that an electron confined to a nucleus would be about as massive as a proton , and when the neutron was discovered , the idea was jettisoned . you see the theory of nuclear electrons in old papers . integer charged sakata quarks : in 1957 , japanese physicist sakata explained the structure of the known strongly interacting particles by assuming they were all made of the proton , neutron , and lambda baryon . this idea is roughly successful , but only because the proton , neutron and lambda are surrogates in this theory for the up , down , and strange quark . the idea was fixed up to the quark model by gell-mann and zweig , but sakata is strangely neglected in this story , perhaps because of his strong marxist political leanings . landau mean-field exponents : in the field of critical phenomena , second order phase transitions are those which have power-law divergences in the correlation length and the averaged field fluctuations at the transition . the power laws for the divergences were predicted by landau using general principles of analyticity , later made more precise by thom in catastrophe theory . these predictions fail , and landau recognized that this was a sign of an important new discovery to be made . the discovery was modern renormalization , due to widom , kadanoff , wilson , fisher and developed by many others . kolmogorov turbulence theory : kolmogorov proposed an approximation to turbulence which derived the energy in each mode from an argument which does not require much more sophistication than dimensional analysis . the same theory was reproduced by onsager and heisenberg , probably completely independently , during the wwii years . the k41 theory predicts the velocity-velocity correlation functions in fully developed turbulent flow . the theory was first thought to be exact , but in the 1960s , it was slowly shown to be only a rough first approximation , with new phenomena of intermittency altering the correlation function power laws . steady state cosmology : this was the idea that the expansion of the universe is produced by a field with a positive cosmological constant , so that we live in a desitter vacuum . further , as the universe expands , new h atoms are produced from the expansion so that the universe is in a steady state . this was killed by evidence for a hot big bang , the cosmic microwave background , plus the observation that old galaxies are noticibly different in their statistical properties from modern ones , they are immature and irregular , contrary to steady state predictions . frozen star black-holes : before the black hole theory was advanced , many people , including einstein , proposed that something terrible happened at the horizon , which either led to blow ups in energy , or to freezing matter on the exterior . this is somewhat true , in that it takes forever for an object to cross the horizon from an exterior point of view , but the modern understanding requires that objects have an interior to go into . black hole remnants : this was the short-lived idea the black holes leave behind a small pointlike object with a huge entropy when they decay in the final stages . this was designed to fix the black-hole information paradox . black holes make other universe : this idea was associated with the information loss puzzle--- how can black holes lose information ? they must be making a new universe . cosmological constant suppression due to diverging mode in quantum gravity path integral : this idea is due to coleman , and unlike many of his other brilliant contributions , it turned out to be wrong . the idea there was that the diverging scale-factor path-integral factor in the quantum gravity path integral leads to a zero cosmological constant . this theory was killed by the observation of a nonzero cosmological constant . one could continue giving examples , but it is easier to look at the old literature and find all claims . many of these claims are incorrect , and each one is usually an example of this sort . it is good to know the wrong turns , so as not to rediscover an old new idea . for the most challenging physical phenomena , those which are currently the most difficult to understand , i will have to choose one of the many mysteries . i would probably say : high temperature superconductivity : what makes ceramics superconduct ? it is clearly a purely electronic thing , nothing to do with phonons , but the attraction mechanism is not completely understood . this has been an active subject for 20 years , but i do not see a good answer in the literature . regge theory : how do you produce a regge trajectory from a confining field theory ? this quetion lies at the intersection of string theory and quantum field theories , like qcd . how do clouds form ? how do they separate charges ? the phenomena in clouds is completely ill understood , and important for giving climate science more precise predictions . there are many approximate models here again , there are too many to list , you just explore the literature and find open questions .
so , what is antimatter ? even from the name it is obviously the " opposite " of ordinary matter , but what does that really mean ? as it happens there are several equally valid ways to describe the difference . however , the one that i think is easiest to explain is that in antimatter , all of the electrical charges on all of the particles , at every level , have been switched around . thus ordinary electrons have negative charges , so their antimatter equivalents have positive charges . protons are positive , so in antimatter they get the negative charges . even neutrons , which have no overall charge , still have internal parts ( quarks ) that very definitely have charges , and those also get flipped around . now to me the most remarkable characteristic of antimatter is not how it is differs from ordinary matter , but how amazingly similar it is to ordinary matter . it is like an almost perfect mirror image of matter -- and i do not use that expression lightly , since it turns out that forcing ordinary matter into becoming its own mirror image is one of those other routes i mentioned for explaining what antimatter is ! the similarity is so close that large quantities antimatter would , for example , possess the same chemistry as ordinary matter . for that matter there is no reason why an entire living person could not be composed of antimatter . but if you do happen to meet such a person , such as while floating outside a space ship above earth , i strongly recommend that you be highly antisocial . do not shake hands or invite them over , whatever you do ! the reason has to do with those charges , along with some related factors . everyone knows that opposite charges attract . thus in ordinary matter , electrons seek out the close company of protons . they like to hang out there , forming hydrogen . however , in ordinary matter it also turns out that there are also all sorts of barriers -- i like to think of them as unpaid debts to a very strict bank -- that keep the negative charges of electrons from getting too close to the positive charges of the protons . thus while the oppositely charged electrons and protons could in principle merge together and form some new entity without any charge , what really happens is a lot more complicated . except for their opposite charges , electrons do not have the right " debts " to pay off everything the protons " owe , " and vice-versa . it is like mixing positive apples with negative oranges . the debts , which are really called conservation laws , make it possible for the powerfully attracted protons and electrons to get very close , but never close enough to fully cancel out each other 's charges . that is a really good thing , too . without that close-but-not-quite-there mixing of apples and oranges , all the fantastic complexity and specificity of atoms and chemistry and biochemistry and dna and proteins and us would not be here ! now let 's look at antimatter again . the electrons in antimatter are positively charged -- in fact , they were renamed " positrons " a long time ago -- so like protons , they too are strongly attracted to the electrons found in ordinary matter . however , when you add electrons to positrons , you are now mixing positive apples with negative apples . that very similarity turns out to result in a very dangerous mix , one not at all like mixing electrons and protons . that is because for electrons and positrons the various debts they contain match up exactly , and are also exactly opposite . this means they can cancel each other 's debts all the way down to their simplest and most absolute shared quantity , which is pure energy . that energy is given off in the form of a very dangerous and high-intensity version of light called gamma rays . so why do electrons and positrons behave so very badly when they get together ? here 's a simple analogy : hold a rubber band tightly at its two ends . next , place an aaa between the strands in the middle . ( this is easier for people with three arms . ) next , use the battery to wind up the rubber band until it is quite tight . now look at the result carefully . notice in particular that the left and right sides are twisted in opposite directions , and in fact are roughly mirror images of each other . these two oppositely twisted sides of the rubber band provides a simple analog to an electron and a positron , in the sense that both store energy and both have a sort of defining " twistiness " that is associated with that energy . you could easily take the analogy a bit farther by bracing each half somehow and snipping the rubber band in the middle . with that more elaborate analogy the two " particles " could potentially wander off on their own . for now , however , just release the battery and watch what happens . ( important : wear eye goggles if you really do try this ! ) since your two mirror-image " particles " on either side of battery have exactly opposite twists , they unravel each other very quickly , with a release of energy that may send the battery flying off somewhere . the twistiness that defined both of the " particles " is at the same time completely destroyed , leaving only a bland and twist-free rubber band . it is of course a huge simplification , but if you think of electrons and positrons as similar to the two sides of a twisted rubber band , you end up with a surprisingly good feel for why matter and antimatter are dangerous when placed close together . like the sides of the rubber band , both electrons and positrons store energy , are mirror images of each other , and " unravel " each other if allowed to touch , releasing their stored energy . if you could mix large quantities of both , the result would be an unraveling whose accompanying release of energy would be truly amazing ( and very likely fatal ! ) to behold . now , given all of that , how " real " is antimatter ? very , very real . its signatures are everywhere ! this is especially true for the positron ( antimatter electron ) , which is the easiest form of antimatter to create . for example , have you ever heard of a medical procedures called a pet scan ? pet stands for positron emission tomography . . . and yes , that really does mean that doctors use extremely tiny amounts of antimatter to annihilate bits of someone 's body . the antimatter in that case is generated by certain radioactive processes , and the bursts of radiation ( those gamma rays ) released by axing a few electrons help see the doctors see what is going on inside someone 's body . signatures of positrons are also remarkably common in astrophysics , where for example some black holes are unusually good at producing them . no one really understands why certain regions produce so many positrons , unless someone has has some good insights recently . positrons were the first form of antimatter predicted , by a very sharp fellow named paul dirac . not too long after that prediction , they were also the first form of antimatter detected . heavier antimatter particles such as antiprotons are much harder to make than positrons , but they too have been created and studied in huge numbers using particle colliders . despite all of that , there is also a great mystery regarding antimatter . the mystery is this : where did the rest of the antimatter go ? recall those debts i mentioned ? well , when creating universes physicists , like other notable entities , like to start the whole shebang off with pure energy -- that is to say , with light . but since matter has all those unbalanced debts , the only way you can move smoothly back and forth between light and matter is by having an equal quantity of antimatter somewhere in the universe . an amount of antimatter that large flat-out does not seem to exist , anywhere . astrophysicists have by now mapped out the universe well enough to leave no easy hiding places for large quantities of antimatter . recall how i said antimatter is very much like a mirror image of matter ? that is an example of a symmetry . a symmetry in physics is just a way of " turning " or " reflecting " or " moving " something in a way that leaves you with something that looks just like the original . flipping a cube between its various sides is a good example of a " cubic symmetry , " for example ( there are fancier words for it , but they mean the same thing ) . symmetries are a very big deal in modern physics , and are absolutely critical to many of our deepest understandings of how our universe works . so matter and antimatter form an almost exact symmetry . however , that symmetry is broken rather spectacularly in astrophysics , and also much more subtly in certain physics experiments . exactly how this symmetry can be broken so badly at the universe level while being only very subtly broken at the particle level really is quite a bit of a mystery . so , there you have it , a mini-tutorial on both what antimatter is and where it occurs . while it is a bit of overkill , your question is a good one on a fascinating topic . and if you have read through all of this , and have found any of what i just said interesting , do not just stop here ! physics is one of those topics that gets more fascinating as you dig deeper you get into it . for example , some of those cryptic-looking equations you will see in many of the answers here are also arguably some of the most beautiful objects ever uncovered in human history . learning to read them well enough to appreciate their beauty is like learning to read great poetry in another language , or how to " hear " the deep structure of a really good piece of classical music . for physics , the reward is a deep revelation of structure , beauty , and insight that few other disciplines can offer . do not stop here !
unfortunately for you the authority says no . compound prefix symbols , that is , prefix symbols formed by the juxtaposition of two or more prefix symbols , are not permitted . this rule also applies to compound prefix names . source however , you might use $145\times10^3\text{ mpa}$ .
measure the angular distance between a star and the distant background stars . repeat 6months later when the earth is on the opposite side of the sum if you know the length of the baseline ( the earth 's orbit ) and the angle then you know the distance to the star . in fact we define the distance to stars in terms of this angle and the earth 's orbit - see http://en.wikipedia.org/wiki/parsec because of the blurring effects of the atmosphere it is difficult to measure angles much less than 1 arcsec , and so determine the distance to stars more than a few parsecs away directly by this method . the hipparcos satelite was able to make much more accurate measurements ( less than 1 milli-arcsec ) and so measured distances 1000x further
from a purely algebraic standpoint , you could never have a state which is perfectly correlated with respect to every basis , simply because it is incompatible with the probabilistic formulation of quantum mechanics . specifically , if you accept the current linear formulation of quantum mechanics , any two-qubit state which had perfect positively correlated measurement outcomes for each basis would have a density operator whose eigendecomposition includes negative probabilities . so , you would either have to explain what negative probabilities correspond to , or you must abandon the linear structure of quantum mechanics . either alternative is likely to raise people 's hackles , and give rise to exotic phenomena such as superluminal signalling . the proof is not difficult , if you are comfortable with pauli ( spin ) operators . on two-qubit states , the fact that the singlet is anticorrelated in all bases is related to the fact that $$ \rho \ ; :=\ ; |\psi^-\rangle\langle\psi^-| \ ; \ ; =\ ; \ ; \tfrac{1}{4}\bigl ( \mathbf 1 \otimes \mathbf 1 \ ; -\ ; x \otimes x \ ; -\ ; y \otimes y \ ; -\ ; z \otimes z\bigr ) $$ which indicates at least that the outcomes of x , y , and z observable measurements will be anticorrelated : for each observable $\mathcal o \in \{x \otimes x , \ ; y \otimes y , \ ; z \otimes z\}$ , we have $\mathop{\mathrm{tr}} ( \mathcal o \rho ) = -1$ . this is the maximum negative value such an expectation value may have by virtue of the fact that $\rho$ has eigenvalues bounded between zero and one , and that each observable $x \otimes x$ , &nbsp ; $y \otimes y$ , and $z \otimes z$ have eigenvalues $\pm 1$ . this also means that the anticorrelation is the greatest possible ; that is , the outcomes of the measurements will be opposite with certainty . suppose instead that you wanted a hermitian operator $\varrho$ , whose eigenvalues have magnitude at most one and which sum to exactly one , representing a state which was perfectly positively correlated in each basis . in particular , such an operator would have to satisfy $\mathop{\mathrm{tr}} ( \mathcal o \varrho ) = +1$ for each $\mathcal o \in \{x \otimes x , \ ; y \otimes y , \ ; z \otimes z\}$ , so we could decompose it as $$ \varrho \ ; \ ; =\ ; \ ; \tfrac{1}{4}\bigl ( \mathbf 1 \otimes \mathbf 1 \ ; +\ ; x \otimes x \ ; +\ ; y \otimes y \ ; +\ ; z \otimes z \ ; +\ ; r\bigr ) , $$ where $r$ is some operator consisting of a linear combination of distinct pauli operators $p \otimes q$ for $p \ne q$ ( taking advantage of the fact that we may use the pauli operators as a basis with real coefficients , for hermitian operators on two qubit state vectors ) . in particular , the operators $ ( \mathbf 1 \otimes \mathbf 1 ) r$ , $ ( x \otimes x ) r$ , $ ( y \otimes y ) r$ , and $ ( z \otimes z ) r$ have zero trace . because $\varrho^2$ has trace bounded above by one , and as $\mathbf 1 \otimes \mathbf 1$ is the only two-qubit pauli operator with non-zero trace , we may expand $\varrho^2$ to obtain $$\begin{align*} 1 \ ; and \geqslant\ ; \mathop{\mathrm{tr}} ( \varrho^2 ) \ ; =\ ; \cdots \\ and = \tfrac{1}{16}\bigl ( \mathop{\mathrm{tr}}\bigl ( ( \mathbf 1 \otimes \mathbf 1 ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( ( x \otimes x ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( ( y \otimes y ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( ( z \otimes z ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( r^2\bigr ) \bigr ) \\ and = \tfrac{1}{16}\bigl ( 4 + 4 + 4 + 4 + \mathop{\mathrm{tr}} ( r^2 ) \bigr ) . \end{align*}$$ then $r^2$ must be traceless ; but as the square of a hermitian operator , it can only be traceless if all of the eigenvalues of $r$ were zero , that is if $r = 0$ . thus it follows that $$ \varrho \ ; \ ; =\ ; \ ; \tfrac{1}{4}\bigl ( \mathbf 1 \otimes \mathbf 1 \ ; +\ ; x \otimes x \ ; +\ ; y \otimes y \ ; +\ ; z \otimes z\bigr ) \ ; =\ ; \tfrac{1}{2} ( \mathbf 1 \otimes \mathbf 1 ) - \rho , $$ where $\rho$ again is the density operator for the singlet state which we wrote above . because the bell states form an eigenbasis for both $\rho$ and $\mathbf 1 \otimes \mathbf 1$ , they form an eigenbasis for $\varrho$ as well , yielding the following eigen-decomposition of $\varrho$: $$ \varrho \ ; =\ ; \tfrac{1}{2}|\phi^+\rangle\langle\phi^+| \ ; +\ ; \tfrac{1}{2}|\phi^-\rangle\langle\phi^-| \ ; +\ ; \tfrac{1}{2}|\psi^+\rangle\langle\psi^+| \ ; -\ ; \tfrac{1}{2}|\psi^-\rangle\langle\psi^-| . $$ this is not a positive operator , and thus not a density operator . in particular , if you performed a bell measurement , there is not any well-defined meaning to the probability distribution you would obtain , if you supposed that you could have a system in a state described by $\varrho$ .
this is a difficult concept to talk about without using a few proper definitions . unfortunately these radiometric definitions all sound very similar and have similar meanings but important differences . intensity is the rate of energy transfer per unit area . intensity does not have direction and thus we cannot even talk about radiation having ' different ' intensities in different directions by definition . the radiant intensity , however , is the amount of energy transfer per unit solid angle . the radiance is the amount of energy transfer per unit area per unit solid angle . lambert 's cosine law states that the radiant intensity of an ideally diffuse emitter ( e . g . a perfect black body ) is proportional to $\cos^2\theta$ , where $\theta$ is the angle from surface normal . however , the apparent surface area of a flat object is also reduced by a factor of $\cos^2\theta$ when viewed from an angle $\theta$ from normal . thus the radiance does not vary with angle . when you use the term intensity in your question , you probably mean radiance , which is the most natural term to talk about . it is the same as talking about how bright an object appears . to quote wikipedia : radiance is useful because it indicates how much of the power emitted by an emitting or reflecting surface will be received by an optical system looking at the surface from some angle of view . in summary : is the radiance the same in every direction ? yes . does the radiance in each direction have the same spectral distribution ? yes .
the diagram looks like the fat man bomb that was dropped on nagasaki . the wikipedia article gives lots of info on the design if you are intereted in pursuing it further . the casing is just to make it aerodynamically stable so it fell in a controlled way . the bomb itself is spherical so the case could be spherical as well if it were not for aerodynamic requirements .
if you observe a star of radius $r$ from the distance $l$ , you will see it as a small disk under the angle $2r/l$ so the solid angle the photons from the star will cover will scale like $ ( r/l ) ^2$ . that is the percentage of the retina that will be receiving photons : the solid angle measures the " percentage of directions " in which the photons from the star are flying . the number of photons from the star that hit your eyebulb scales like $ ( r/l ) ^2$ as well ( the dependence $1/l^2$ is what i care about here ) , because they are divided to all points on the sphere of area $4\pi l^2$ , so by dividing these two expressions , you may easily see that the number of photons per unit area of the retina is actually independent of $l$ . the star will look smaller as it gets further but the number of photons per unit time that hit a small area of the retina is $l$-independent . if you are really worried that the star does not emit enough photons to satisfy your eyes , note that the sun emits roughly $4\times 10^{44}$ photons each second . the earth-sun distance is roughly 150 million km which is 15 trillion times the radius of the eyebulb . square it and you will still get that the number of eye-sized areas on the surface of the 1-au-radius-large sphere is just of order $10^{26}$ , still giving you $10^{18}$ photons to each eyebulb per second . so if you allow 100 photons per eyebulb to be enough to see it , you may still allow the star to be $10^{8}$ times further than the sun . the sun is 8 light minutes and if multiplied by 100 million , you get something like 200 light years . so with this minimal required number of photons per eyebulb ( 100 per second ) , you may see stars up to hundreds of light years away ( i can not ) . of course , telescopes are collecting starlight from a much larger area than the eyebulb ( and they may also patiently collect the photons for a much longer time ) so they may see stars much further than that .
your idea is a good one . when i taught this course we actually used a metallic ( but non-magnetic ) pendulum which moved through a magnetic field during its whole swing . it turns out that to a good approximation the damping is proportional to the velocity . what causes the damping is this : by moving a metal around in a magnetic field you create currents in the metal which create their own magnetic field . these two fields interact and can create weird effects -- usually resistance . for example , if you drop a magnet down a copper tube , the magnet will fall very slowly . the concept is the same . the reason your particular experiment is not a good approximation is because the damping agent is only present during a part of the swing . also , it is not proportional to velocity , but to how close the magnets are . luckily this last part is not so bad because the velocity is roughly proportional to how close the magnets are ( because the pendulum moves fastest when its at equilibrium . ) as far as i know there is not really a good closed form solution to your problem . but this " answer " ( if you can call it that ) should give you a good idea of how to make an actual damped pendulum .
the formula is just a force balance . if the contact line is stationary the forces at it must balance so taking the horizontal component of the forces gives you : $$\sigma_{gas-liquid} cos \theta + \sigma_{liquid-solid} = \sigma_{gas-solid}$$ and hence the formula you quote . if you replace the gas with a liquid the force balance calculation is just the same , so the formula remains valid .
if you mean special conformal transformation x-> 1/x conformal invariance of maxwell equations is known since 1909 . see here : http://cts.iisc.ernet.in/personnel/pages/asinha/draft1shouvik.pdf or here : http://arxiv.org/pdf/hep-th/9701064
oozing honey through pipes the solution below is for a very viscous fluid which has negligible inertia and large viscosity . it is wrong for water in real pipes , because it neglects the pressure drop which comes with the changing velocity of water . this term is higher order in v , but it is obviously relevant for real water pipers . i leave it , because it is an interesting exercise with a direct analogy to resistive current flow , the correct solution is at the end . the way to do this is to note that the pressure at the divergence point is equal for all 4 pipes , and that there is a given law for pressure drop along a pipe per unit length at any a given flow rate . the answer is different depending on whether you have a fixed pressure forcing the water through the pipes ( as you do in a water main system ) or whether you are forcing a given volume of water through per unit time , as you suggest , and which is appropriate when you have a large pressure drop along a very long pipe before you get to your splitter . i will assume that the 4 pipes have a given length , and that they empty at atmospheric pressure , which i will label as 0 , and that the water flow is sufficient to keep the pipes filled until near the exit point , otherwise the problem requires more information . consider the fixed flow rate problem first . if the imposed flow rate is f units of water per second , the first equation is the mass conservation equation $$\sum_i f_i = f $$ where $f_i$ are the flow rates along the pipes . phill . zitt gave this formula , but it is not enough--- it is analogous to the current kirchhoff law . you also need the analog of the voltage kirchhoff law . the voltage law tells you that the flow rate $f_i$ is proportional to the pressure drop along pipe i . i will call the proportionality constant the " flow conductance " $c_i$ ( it is the analog of the reciprocal of resistance in an electrical circuit ) : $$ f_i = c_i \delta p $$ for the four pipes , $\delta p$ is equal , so that $$ f_i \propto c_i $$ and along with the sum rule , you find : $$ f_i = {c_i f \over \sum_i c_i } $$ so the only thing you need to know are the $c_i$ , just as in a resistor network . two pipes with flow conductances $c_1 , c_2$ connected in series have a flow conductance c given by the formula : $$ {1\over c} = {1\over c_1} + {1\over c_2}$$ for the same two pipes in parallel , $$ c = c_1 + c_2 $$ so that conductances add in series and parallel just like the reciprocal of the resistance ( the electrical conductance ) in circuits . you have a problem of 4 parallel resistors connected in series to an input resistor , just like a resistor connected to 4 resistors in parallel . for a cylindrical pipe of length l and radius r , the laminar flow profile is exactly parabolic in the radial cylindrical coordinate r : $$ v ( r ) = v ( 1 - {r^2\over r^2} ) $$ so that the total flow as a function of r is $$ f ( r ) = \int_0^r v ( r ) 2\pi r dr = {\pi v r^2\over 2}$$ the navier stokes equations reduce to something very simple in the laminar pipe flow case--- all the terms drop out except the viscosity term , which tells you the diffusion of momentum out of the pipe , and so the pressure drop per unit length . ( see here : is there an analytical solution for fluid flow in a square duct ? ) the equation is $$ \nu \nabla^2 v = \delta p $$ so that $$ 2\nu {v\over r^2} = {\delta p \over l} $$ this gives you the flow rate as a function of r and l , $$ f = {\pi v r^2 \over 4} = {\pi r^4\over 8\nu l} \delta p$$ so that the conductance is $$ c ( r , l ) = {\pi r^4 \over 8 \nu l} $$ and this determines the flow through the i'th pipe in terms of the total flow and the geometry : $$ f_i = {f {r_i^4\over l_i} \over \sum_k {r_k^4\over l_k}} $$ this solves the constant flow-rate problem purely geometrically . the limit of constant flow rate is achieved when there is a long pipe feeding into the whole thing with a much larger pressure drop than the pressure drop after the split . the total flow is determined by the total conductance , which is essentially equal to the conductance of the long pipe , so no matter what you attach at the end , so long as the part at the end has much more conductance than the initial pipe . the same problem can be solved at a fixed pressure at the divergence point , the outgoing flow is just the conductance times the shared pressure . for question 2 , the issue of constant pressure or constant flow rate is essential . at constant pressure , if you attach the contraption to the side of a wide water main at high pressure , closing one pipe does nothing to the flow in the other pipes . at constant flow rate , closing pipe number 4 increases the flow through the other 3 by the factor $$ c_1 + c_2 + c_3 + c_4 \over c_1 + c_2 + c_3 $$ for non-rigid pipes , you just need to know the r as a function of the pressure . this will be a fine approximation if the pressure drops are slow in the pipe as usual , so that the radius change slowly with length . in normal pipes , the radius does not change hardly at all with the pressure , so i did not bother to calculate anything , but you can split up the pipe into slices with a radius r ( p ) , giving a conductance , which you add according to the series rule . water in pipes i will assume the flow is laminar in the pipes , but that the pipes are short , so that the pressure drop due to viscosity is negligible between the two ends . this is the correct limit for water pipes . the pressure does work on the water which is not dissipated significantly in the pipes , and comes out as kinetic energy in the water , not as heat in the pipe . given a pressure drop from p to atmospheric pressure 0 , the water in each of the four pipes will adjust it is velocity so that the bernoulli principle is obeyed--- the work done by the pressure is the energy gained by the water . the energy flow in a cross section of the pipe is : $$ \int {\rho v ( r ) ^2\over 2}v ( r ) 2\pi r dr $$ with the laminar profile ( the flow f is as before ) , and this gives $$ f {\rho v^2\over 4} $$ where v is the velocity at the center , as before . the work done by the pressure difference at the two ends is $pf$ , so you get a version of bernoulli 's equation for laminar pipes : $$ p + {\rho v^2\over 4} = {\rho v_0^2\over 2}$$ the velocity in the pipes are then $$ v= \sqrt{{4p\over \rho} + {v_0^2\over 2}} $$ and they are equal . so that the flow rate in this limit ( the right limit for water ) is proprotional to the cross section area of the pipe , to r^2 . if you have a fixed flow rate , the pressure rises to the point where the total outflow is equal to the inflow , and the water flow is partitioned according to the cross section area : $$ f_i = {f r_i^2\over \sum_k r_k^2 }$$ this neglects the incoming velocity $v_0$ , assuming the water coming out is significantly faster than the water coming in . the answer for 2 and 3 is not changed in the water case compared to the honey .
i will once again state that string theory , any theory , cannot be proven right by any experiment . the experiment might validate the theory , i.e. come as a result of a prediction from the theory . at the moment there does not exist one string theory in the manner that there exists one general relativity theory . there are many models based on string theory , though . why such an interest ? because at the moment string theories are the only theories that can accommodate the standard model of particle physics and at the same time allow for the quantization of gravity , which has been the holy grail of theoriticians the past fifty years . that is they promise a " theory of everything , toe ) . what might disprove the usefulness of string theories for a toe would be if supesymmetry were falsified at the lhc , for example . if nothing is seen other than the higgs at the lhc , ss would seem as a nice try but bad luck . then the usefulness of strings becomes doubtful . if ss is seen in the lhc and studied as well as the sm in the international linear collider to be built in the future , then strings will be good as candidates of a toe .
0 . caveat lector : this was done before i drank my morning coffee , so there may be some errors in the reasoning ( well , the physical reasoning , the mathematics should be kosher ) . 1 . perfect fluid . so we have two stress-energy tensors here . one is the stress energy tensor for a perfect fluid $$\tag{1}t^{\alpha\beta}_{\text{fluid}} = \rho \ , u^\alpha \ , u^\beta + p \ , h^{\alpha\beta}$$ where we have the worldlines of the fluid 's particles have velocity $u^\alpha$ the projection tensor $h_{\alpha\beta} = g_{\alpha\beta} + u_\alpha \ , u_\beta$ projects other tensors onto hyperplane elements orthogonal to $u^\alpha$ the matter density is given by the scalar function $\rho$ , the pressure is given by the scalar function $p$ . we had need extra terms if there were heat flow or shear involved . 2 . scalar field . now , we have another distinct stress-energy tensor for a massless scalar field : $$\tag{2}t^{\mu \nu}_{\text{scalar}} =\partial^{\mu}\phi\ , \partial^{\nu}\phi-\frac{1}{2}g^{\mu \nu}\partial_{\rho}\phi\ , \partial^{\rho}\phi$$ we would use this equation when modeling , e.g. , massless pions ( or some other massless spin-0 field ) . 3 . problem : are these two related ? now if we take our matter density to be , in the appropriate units , $$\tag{3a} \rho = 1 + \frac{1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ and the pressure $$\tag{3b} p = \frac{-1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ then ( 2 ) resembles ( 1 ) . this is after pretending $\partial^{\mu}\phi=u^{\mu}$ , which terrifies the original poster ( but that is what condensed matter physicists do , so i suppose i could end here content ) . is this kosher ? we should first note if we wanted to take the derivative of some function along the worldline $x^{\mu} ( s ) $ with respect to the " proper time " ( length ) $s$ we have $$\tag{4} \frac{\mathrm{d}f}{\mathrm{d}s}=\frac{\mathrm{d}x^{\mu}}{\mathrm{d}s}\frac{\partial f}{\partial x^{\mu}}$$ by the chain rule . for general relativity , we use the " comma-goes-to-semicolon " rule , but for a scalar quantity $f$ we have $$ \nabla_{\mu}f = \partial_{\mu}f . $$ ( if this is not obvious , the reader should consider it an exercise to prove it to him or herself . ) the punchline : identifying $\partial^{\mu}\phi=u^{\mu}$ is kosher . how ? observe in equation ( 4 ) the guy in front , the $\mathrm{d}x^{\mu}/\mathrm{d}s$ is just some vector . so in the very , very special case that equations ( 3a ) and ( 3b ) hold , and $\mathrm{d}x^{\mu}/\mathrm{d}s= ( 1,0,0,0 ) $ , we see that we can indeed recover the first stress-energy tensor as a special case of the scalar field 's stress-energy tensor .
in the first experiment , work done is 0 as volume is constant . using the first law of thermodynamics $q=u-w$ , $q=\delta u$ . in the second case extra heat is needed due to the work done which is $\delta ( pv ) =p\delta v$ , as pressure is constant . using the ideal gas equation $p \delta v = nr \delta t=2r\delta t$ . note the change in internal energy depends only on the change in temperature and is same in both the cases .
yes there is . i use the summation convention throughout ; repeated indices are summed over 1,2,3 . begin with the canonical commutation relations ( ccrs ) \begin{align} [ x_i , p_j ] = i\hbar\delta_{ij} i , \qquad [ x_i , x_j ] = 0 , \qquad [ p_i , p_j ] = 0 . \end{align} define the components of orbital angular momentum as follows : \begin{align} l_i = \epsilon_{ijk}x_jp_k \end{align} prove your desired identities by applying the definition of the angular momentum components and by repeatedly using the ccrs . i will leave the details to you ; it is a good exercise to get comfortable with using the ccrs to prove stuff . it actually turns out to prove useful to first prove the following identities which encode the fact that the $x_i$ are components of a " vector operator " and so are the $p_i$ . \begin{align} [ x_i , l_j ] =i\hbar\epsilon_{ijk}x_k \end{align}
i have not read that paper , but here is a physical reason for why the arrival of photons at a detector is modelled as a poisson process - assuming a source of photons ( say , for example a tungsten lamp ) and a photodetector , there is no predetermined way of predicting when a photon is going to reach the detector , or with what energy . the emission of photons in the direction of the detector is a random ( stochastic ) process , which is described perfectly by a poisson process . however , if you have ever seen the distribution of intensities from a photodetector ( although the same will be true for almost any stochastic process ) the resulting distribution is gaussian . this is a result of the central limit theorem . this is mostly because the photodetector does not show you the output of a single photon , but it averages over the arrival of several photons . therefore for a certain period of averaging , the variance of the resultant distribution is deterministic .
the operator $\sigma_z$ is the $z$ component of the total spin , and therefore is the sum of the quantities $s^{ ( 1 ) }_z$ and $s^{ ( 2 ) }_z$ , whereas the operator $s^{ ( 1 ) }_z\cdot s^{ ( 2 ) }_z$ is their product . you can easily check that for the matrices in your question the identity $$\sigma_z=-2 ( s^{ ( 1 ) }_z+ s^{ ( 2 ) }_z ) $$ holds ; the factor of $-2$ is due to differing conventions in sign and normalization between the ones in your post and the ones in the link . for your particular system you want the product , as that will have the states $|\uparrow\rangle|\uparrow\rangle$ , $|\downarrow\rangle|\downarrow\rangle$ , $|\uparrow\rangle|\downarrow\rangle$ and $|\downarrow\rangle|\uparrow\rangle$ as eigenstates , with eigenvalues $+1 , +1 , -1$ and $-1$ resp . , which is how you want to model ( anti ) ferromagnetic interactions in the first place . the sum is useful in situations where you want the total number of excitations . this is the case , for example , in the jaynes-cummings model , where you use spin matrices to model two-level atoms even though the states are not necessarily spin states ( and more probably electronic eigenstates ) . thus taking $|\uparrow\rangle=|\text{excited}\rangle$ and $|\downarrow\rangle=|\text{ground}\rangle$ , the operator $s^{ ( 1 ) }_z+ s^{ ( 2 ) }_z$ counts the total number of excited atoms , which is of course the atomic part of the jaynes-cummings hamiltonian . you should also notice that your hamiltonian simplifies to $h= ( j_\text{af}-j_\text{f} ) s^{ ( 1 ) }_z\cdot s^{ ( 2 ) }_z$: only a ferromagnetic or an antiferromagnetic interaction is possible between any two spins . they either like being parallel or not . the two edges in your graph combine to a single one , of either f or af character . the graph formalism is useful when you have multiple spins with varying interactions .
your equation is almost correct . updated using x-axis along ab taking the x-axis along ab yields $$ 50 t \sin ( \theta-15^\circ ) = 26 t \sin ( 40^\circ ) $$ $$ \sin ( \theta-15^\circ ) = 0.52 \sin ( 40^\circ ) $$ $$ \theta = 34.527^\circ $$ $$ \cos ( \theta-15^\circ ) = \sqrt{1-\sin^2 ( \theta-15^\circ ) } $$ and the y-axis perpendicular to ab $$ 50 t \cos ( \theta-15^\circ ) = 20 + 26 t \cos ( 40^\circ ) $$ $$ t = 0.735 $$ using x-axis along ac ( interception pt ) taking y-axis perpendicular to ac $$ 50 t = 20 \cos ( \theta-15^\circ ) +26 t \cos ( 55^\circ-\theta ) $$ $$ t = \frac{20 \cos ( \theta-15^\circ ) }{50 - 26 \cos ( 55^\circ-\theta ) } $$ taking x-axis along ac $$ 20 \sin ( \theta-15^\circ ) =26 t \sin ( 55^\circ-\theta ) $$ which when expanded you need to solve an equation of the form $$a\cos \theta + b \sin \theta = c$$ for $\theta$ with the same results as above .
re . 2nd question:- " if i plug in my memory stick/usb into my laptop and load it up with documents etc . will its mass/weight increase ? " its mass will not necessarily increase or decrease . it depends on the prior state of the memory . if there are the same average numbers of binary ones and zeros before and after the weight would be approximately the same . this is due to how the ones and zeros are stored on the memory stick . placed electrons create charges on the chip gates to store information , but if you are just rearranging them there is no extra weight . ref . wiki:- floating gate transistor " because the floating gate ( fg ) is electrically isolated by its insulating layer , any electrons placed on it are trapped there and , under normal conditions , will not discharge for many years . "
the morally correct answer is that the measurement of one spin in the epr-entangled pair eliminates the entanglement as it picks a particular factorized basis vector for the measured spin , and the total wave function therefore has to factorize to $\psi_\text{just measured}\otimes \psi_{\rm something}$ . if you parameterize the multi-fermion states in terms of " fermion 1" and " fermion 2" , you will have to antisymmetrize , so no multi-particle wave function will ever tensor factorize . ( this is true even for bosons with the exception of the states where all the bosons are placed to the same one-particle state . ) however , as you say , this obstacle to factorization ( in the form of the required antisymmetrization , and similarly symmetrization for bosons ) is sort of " trivial " . there is a technical way to support the claim that this state is really not entangled . if you write the " subsystems " not as " fermion 1" and " fermion 2" but as " region around $r_a$" and " region around $r_b$" , using your notation , then you will see that the wave function ( al ) for the whole space is almost accurately tensor factorized to the wave functional that only depends on the fields near $r_a$ ( where a spin-down electron excitation is added ) and the fields around the point $r_b$ ( where the spin-up electron excitation lives ) . $$ \psi_{qft} = c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) \otimes c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) $$ of course , the formula above is just a suggestive way to show that the ordinary , correct state of a quantum field theory $$ c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) |0\rangle $$ is really a simple tensor product of a sort . for this reason , the notion of " entanglement " is usually modified for identical particles so that the unentangled states are not just the states having the form of the simple tensor product but all states obtained as ( anti ) symmetrization of a tensor product .
the maxwell equations do not need to " take into account " that the proper time of light-like paths is zero . the definition of the minkowski metric as $$\mathrm{d}s^2 = c^2\mathrm{d}t^2 - \mathrm{d}x^2 - \mathrm{d}y^2 - \mathrm{d}z^2$$ lorentz invariance means that all physical laws are invariant under the isometries of this metric , which are the lorentz group . the maxwell equations are manifestly lorentz invariant if you simply write them as $\partial_\mu f^{\mu\nu} = 0$ . they also give as a result that the speed of electromagnetic waves is $c$ . since they are lorentz invariant , this is true in all lorentz-related frames . now , if you plug any path that represents motion with the speed $c$ into the metric , it is zero . why ? parametrize the path as $t\mapsto ( t , ct , 0,0 ) $ ( you can certainly lorentz transform so that that is possible ) . plug it in . now , $$\mathrm{d}s^2 = c^2\mathrm{d}t^2 - \mathrm{d} ( ct ) ^2 = c^2\mathrm{d}t^2 - c^2\mathrm{d}t^2 = 0$$ and so the proper time along that path is $\tau = - \int ds^2 = 0$ .
a tsunami is basically a shallow-water wave , even in deep seas . this means its velocity is $v=\sqrt{gh}$ , where $h$ is the water depth and $g$ is the gravitational acceleration . the energy of the tsunami scales as the square of its amplitude $a$ , and thus the energy flux $s$ goes as $s\sim a^2 \sqrt{h}$ . conservation of energy then implies that the wave amplitude depends on the sea depth as $$ a \sim h^{-1/4}$$ a result known as green 's law . for example , green 's law predicts that a tsunami with amplitude $a=1$m at $h=5000$m will run up to $a=4$m if the depth becomes $h=20$m .
you dont give magnitude of pounds , you give magnitude of , say mass ( as here ) . so the magnitude is 120 pounds .
assume that the point mass , $m$ has two tiny thrusters , mounted so as to exert purely tangential force in the plane of the circular motion , one clockwise , and the other counter-clockwise . the magnitude of the constant velocity of the mass is $v$ , and the radius of the circle is $r$ . measure the position of the point mass in the standard cartesian coordinate way : angles are measured from the positive x-axis , counter-clockwise positive . at the point where the mass is at a position angle $\theta$ . the total radial force inward on the mass , $f_r$ is given by the centripetal force equation:$$f_r=\frac{mv^2}{r}$$ there are two forces that supply this radial force : the tension , $t$ in the string , and the inward radial component of the force of gravity:$$f_{g-r}=mg\sin ( \theta ) $$so:$$\frac{mv^2}{r}=t+mg\sin ( \theta ) $$and:$$t=\frac{mv^2}{r}-mg\sin ( \theta ) $$note that this implies that:$$v &gt ; =\sqrt{rg}$$ or the string tension will become negative near the top of the circle , an impossibility . the conditions of the question also require that at all times the net tangential force , $f_t$ , be zero . the tangential component of the force of gravity , $f_{g-t}$ is given by:$$f_{g-t}=-mg\cos ( \theta ) $$where a positive force implies counter-clockwise force . the thrusters are needed to supply the exact opposite force to the mass at all times .
maybe you can think about it like this : the equation of motion for a physical system ( here , an object ) which obeys newtonian mechanics is $$f ( t ) = m\frac{\mathrm{d}^2y}{\mathrm{d}t^2}$$ that is , given $f ( t ) $ and a fixed set of initial conditions , you can determine $y ( t ) $ . now suppose you have two arbitrary functions $f_1 ( t ) $ and $f_2 ( t ) $ . for each of these functions , imagine subjecting the object to a force that follows that function , assuming the same initial conditions , and then using the above equation to determine the resulting motion of the object , $y_1 ( t ) $ and $y_2 ( t ) $ respectively . $$\begin{align} f_1 ( t ) and = m\frac{\mathrm{d}^2 y_1}{\mathrm{d}t^2} \\ f_2 ( t ) and = m\frac{\mathrm{d}^2 y_2}{\mathrm{d}t^2} \end{align}$$ by adding these two equations , you get $$f_1 ( t ) + f_2 ( t ) = m\frac{\mathrm{d}^2 ( y_1 + y_2 ) }{\mathrm{d}t^2}$$ which shows that $y_1 ( t ) + y_2 ( t ) $ is the motion that results from the combined time-dependent force $f_1 ( t ) + f_2 ( t ) $ . if you are analyzing some physical system that is more complicated than a simple object , then the equation of motion may be something more complicated than $f_\text{net} = ma$ . but as long as the underlying equation of motion specifies a linear relationship between $y ( t ) $ and $f ( t ) $ , then you can use green 's functions .
current cosmological theorists suppose that the universe is exactly identical , no matter where it is viewed from , so long as it is viewed at the same time . at the time of the big bang , the distances between any two given points seems to shrink to zero ( or some nonzero value that we supposedly will derive from quantum mechanics ) . the conclusion is that the big bang happened everywhere , all at once . this is also how you get out of the ' was the big bang a black hole ? '-type questions : even though you had large concentrations of matter at times close to the big bang , they were spread out over all space , which is different than just having a clump of matter with finite extent ( the second thing would collapse to a black hole ) .
distributions occur in many areas in physics . here are two interesting cases : in general relativity , the einstein equations admit distributional ( and even more general ( colombeau algebras ) ) solutions . the following review article by : steinbauer and vickers describes some of these solutions . in relativistic quantum mechanics , the wave functions belong to distributional valued hilbert spaces , for example the sobolev space $\mathfrak{s}_{-1/2}$ in the case of the massive klein-gordon equation . see , for example the the following lecture notes by arthur jaffe . this phenomenon is characteristic to unitary representations of non-compact groups ( the lorentz group in the case of the klein-gordon equation ) .
from wiki : approximately 90% of the power consumed by an incandescent light bulb is emitted as heat , rather than as visible light . so , yes , the incandescent bulb can be used as a heater and , in fact , has been used as a heater . for example , see : easy-bake oven . the original toy used an ordinary incandescent light bulb as a heat source
if you take the simplest form of capacitor , two parallel plates , the the capacitance is proportional to the area of the plates and inversely proportional to the distance between the plates : $$c \propto \frac{a}{d}$$ when you are making a capacitor out of a snapple bottle you are actually making something similar to the simple " two plate " capacitor but the " plates " are curved round the surface of the bottle , with the foil as the external plate and the ( conducting ) salt water in the bottle acting as the internal plate . the $d$ in the equation above is the thickness of the glass . so you can make a capacitor out of any bottle , jar or anything similar . all that matters is the area of the foil and the thickness of the glass . if you want to increase the total capacitance you can just link any number of bottles in parallel i.e. link the external foil covers as one electrode and the internal brine solution as the other electrode . when you join up capacitors in this way , " in parallel " , you get the total capacitance just by adding up the individual capacitances of all the bottles you have joined . you ask about the effect of the increased voltage : the charge stored in a capacitor is given by : $$q = cv$$ where $c$ is the capacitance and $v$ the voltage , so using 12kv instead of 9kv just means you get 33% more charge for a given capacitance , or alternatively you can get away with a smaller capacitance to hold the same charge . do i sound a bit like a grandma if i point out you need to be careful with this experiment . a typical tesla coil can not generate enough current to kill you , but if you gang together enough capacitors the stored charge in them will kill you ! finally , i normally point people to wikipedia if they want to learn more , so see http://en.wikipedia.org/wiki/capacitor and http://en.wikipedia.org/wiki/leyden_jar for the sort of capacitor you are making . however be warned that the wikipedia article on the capacitor is a bit technical .
the scatter direction depends on the size of the particle and the wavelength . very small particles ( e . g . nitrogen atoms of the atmosphere ) scatter isotropically . there is still an effect on the polarisation of the scattered light ( bees use that to locate the sun if they can not see it directly ) . often gaussian beams are used to describe how the intensity propagates in an optical train ( a system of lenses and mirrors ) http://en.wikipedia.org/wiki/gaussian_beam . note that this describes the intensity of a laser with a gaussian intensity profile ( this is a good approximation for many lasers , especially if you focus them through a pinhole ) . if you have an extended light source you will have to add the intensities of several such beams . once you have a numeric intensity profile ( i guess 2d is enough for your usage ) , you can try an exponential decay law to estimate the effects of scatter . your focus spot will look more intense the higher the numerical aperture of your lens is . if you use a lens that has 2.5 cm diameter and f=10cm the spot will not look as intense as with a lens that has f=3cm . have you thought of using fluorescence ? you could use dissolve some colour in water and use laser protection goggles to see the fluorescent light . then you do not have to cope with scattering . you can get polygonal mirrors out of old laser printers . that way you can scan the beam in one direction . if you use a laser diode , you can modulate the intensity very fast . i recently purchased a 405nm laser with 120mw for $120 from lasever.com. 120mw is very dangerous . if you do not have protection goggles or you share your space with other people do not use lasers> 0.5mw !
i kept wondering about the same question for quite a time , it makes sense to me now it is true that he has first ionization potential ( or energy ) of 24.6 ev while o2 has a value of 12.6 ev for the same number . yet experimentally igniting he discharge is much easier than igniting o2 discharge in dbd mode . the reason in simple words is the mean free path . think of having two identical discharges one with he as operating gas and one with o2 . assume the local electric field is identical and the gas density is identical , the mean free path of electrons in helium is much longer than what it is in o2 , which basically means that the electrons are accelerated by electric field to higher velocities ( energies ) in he compared to o2 before they experience a collision . so the electrons in he have larger energy than they have in o2 under similar circumstances . the difference in energy gain overcomes the difference in ionization energy . if you wanted to test it your self , you can use a freely available software called bolsig+ . what this software basically does is computing the electron energy distribution function ( eedf ) given the cross section data of the gas ( which is experimentally obtained data ) . for the same electric field to gas density ratio , the mean energy of an electron in he gas is much larger than what it is in o2 . i did the following plot of mean electron energy as function of reduced electric field in both air and helium . the reduced electric field is the electric field devided by number density . its unit is townsend so for example at 300 td , mean electron energy of an electron in helium is higher than the first ionization energy of helium , while in air it is lower than first ionization energy of either o2 or n2 . the reason the mean free path differs significantly is that n2 and o2 are more chemically active compared to he , which means the electrons have too many possible ways of spending their energy in rotational and vibration excitation , dissociation and excitation to metastable states while in he those ways are very limited . also it is true that the mass of a helium atom is much smaller than the mass of o2 or n2 molecule , being the basic unit in the gas . so from a rough geometrical perspective , the he atoms are smaller in size than o2 or n2 molecules . the geometrical size is not really relevant but it helps to explain the concept . i hope that made it clear .
the basis of the hilbert space in schrödinger 's picture is assumed to be time-independent regardless of any properties of the hamiltonian . the hamiltonian is just another operator . if the hamiltonian is time-dependent , its eigenstates and eigenvalues are obviously time-dependent , too . both equations you write down only express the fact that the basis of eigenstates of $h ( t ) $ is still a basis , so a general ket vector , including the actual state vector of the system , may be expanded as a linear superposition of these basic vectors with some general complex coefficients $c_n ( t ) $ . the two expansions only differ by the phase one includes into the coefficients $c_n ( t ) $ or into the basis vectors $|n ; t\rangle$ . one convention includes the phase $\exp ( i\theta_n ( t ) ) $ , another one does not , and so on . obviously , there is no " universally mandatory " rule that would dictate the right phase of these vectors so there is some freedom about the notation . note that a phase factor times an eigenstate is still an eigenstate . whatever your convention for the phases is , if you carefully follow the maths and remember what the symbols mean – the defining equations – you will be able to derive the invariant claims about the adiabatic theorem . the wikipedia-sakurai conventions treat the phases wisely and naturally , to speed up the derivations .
$\partial_t\equiv\frac\partial{\partial t}$ and $\partial^\mu\equiv g^{\mu\nu}\frac\partial{\partial x^\nu}=\left ( \sum_{\nu=0}^3g^{\mu\nu}\frac\partial{\partial x^\nu}\right ) _{\mu=0}^3$ are differential operators . $\partial^\mu$ is formally contravariant ( upper index ) and obeys the corresponding transformation laws . $\partial_t$ has a lower index and is ( up to a constant factor ) a component of the formally covariant operator $\partial_\mu$ via $\partial_0=\frac1c\partial_t$ , which , in general , is not equal to $\partial^0$ , the zeroth component of $\partial^\mu$ . the differential operator $\partial^\mu$ is known as gradient , which derives vector fields from potential functions . the gradient is not a natural operation on arbitrary manifolds and only available if there is a metric . its dual $\partial_\mu\equiv\frac\partial{\partial x^\mu}$ on the other hand is a natural operation corresponding to the differential $\mathrm d$ , taking potentials to 1-forms ( covectorfields ) . as a side note , $\partial_t$ can also be understood as a local vector field , as one of the intrinsic definitions of vectors on manifolds is via their directional derivatives . in mathematical literature , it is common to write the basis of the tangent space as $\{\frac\partial{\partial x^\mu}\}$ and its dual space as $\{\mathrm dx^\mu\}$ .
i will translate your post into the language of translations . then i will answer this question about translations . then i will answer your original question . translation question i am confused about a trivial concept . let the displacement of a rigid body be described by the equation $\vec{x} ( t ) =\delta \vec{x} ( t ) +\vec{x} ( 0 ) $ , with $\delta \vec{x} ( 0 ) =0$ . then , at each instant there is only one unit vector in the direction of displacement that we may call $\hat{w} ( t ) $ and which we may take to be normalized . that vector $\hat{w} ( t ) $ is what geometrically we would call the ( instantaneous ) direction of velocity [ note : this sentence is wrong ] . kinematically , however , the instantaneous direction of velocity $\vec{v}$ is the derivative $\dot{\vec{x}} ( t ) =\dot{\delta \vec{x}} ( t ) $ . that is the direction of $\vec{v} ( t ) $ . as is obvious ( for example an object not moving in a straight line ) , in general $\hat{w} ( t ) $ and $\vec{v} ( t ) $ are not parallel . so , why are there two directions for velocity , and does $\hat{w} ( t ) $ play any role in the kinematics/dynamics of the motion ? answer to translation question you are wrong that $\hat{w}$ is the direction of velocity . $\hat{w}$ was defined as the direction of $\delta \vec{x}$ , that is , the direction of the displacement . as kevin said the total displacement is not needed because you only need to know a objects current position and velocity to get its motion in the future . answer to the rotation question now we are ready to answer the rotation question . we just translate the answer of the translation question to rotation language . you are wrong that $\vec{v}$ is the direction of angular velocity . $\vec{v}$ was defined as the axis of rotation for $r$ , that is , the direction of the rotation between the initial and final orientation . as kevin said this rotation is not needed because you only need to know a objects current position and velocity ( here we are talking about the velocity everywhere in the object , which for a rigid object can be summarized by a linear velocity and an angular velocity ) to get its motion in the future .
the rotation operator normalization you have chosen , $ r_{\boldsymbol n} ( \theta ) =e^{-i\theta \boldsymbol \sigma\cdot \boldsymbol n/2} $ , means that for a rotation by $2\pi$ about the $z$ axis , $$ r_{\hat z} ( 2\pi ) =e^{-i\pi \sigma_z} = ( -1 ) ^{\sigma_z}=-1 , $$ because $ ( -1 ) ^1= ( -1 ) ^{-1}=-1$ . thus , your rotation operator has indeed rotated you back to the same point on the bloch sphere ( because the operator is scalar ) but you have accumulated a global phase . there is not , strictly speaking , an error , because global phases can be ignored . on the other hand , this ' global ' phase is tremendously useful if you are in contact with some other degree of freedom . it is an example of a geometric phase ( i.e. . if you rotate around some path in the bloch sphere and return to the same path , you will accumulate a phase of $$\text{ ( area enclosed by the path in steradians ) }/2 , $$ where the total area to be had is of course $4\pi$ ) . it is the foundation of what are called phase gates , which are very popular basic entangling gates between two qubits . the idea there is that you perform a controlled rotation : leave qubit a alone if b is in the down state , and rotate it once around the bloch sphere if b is in the up state . either way , you come back to the same state , but in one of the two paths you have accumulated a phase , which can be used to generate entanglement .
even without a graph , the answer is straightforward . the potential energy stored in a spring is proportional to the square of the difference b/w stretched and unstretched length , $$v = \frac{1}{2}kx^2 $$ thus , the work required to stretch it ( mind you , you have to do this slowly so that the kinetic energy is not changed ) would be : $$w = \frac{1}{2}k ( 2d ) ^2 - \frac{1}{2}k ( d ) ^2 = \frac{3}{2}kd^2 = 3w_0$$ this is just straightforward conservation of energy .
the spiral arms do not mean that the mass is getting sucked to the center . they are just wave-like density patterns . the bodies in orbit around the center of the galaxy are in stable orbit ; just like the earth around the sun and the moon around the earth . what happens is that gravity accounts for the centripetal force ( in the orbiting frame , gravity is balanced by the centrifugal force ) , so there is no net radial acceleration " left over " to suck the body in . the only reason things would fall into the center is if they were headed there . this can happen if two stars pass by each other and are slingshotted in opposite directions , one of which gets sent to the center of the galaxy .
provided that $\mathcal{l}$ is a lorentz scalar , the quantity $\partial\mathcal{l}/\partial ( \partial_{\mu}\phi ) $ has to carry an upper index . since $\mathcal{l}$ is a function of $\phi$ and $\partial_{\mu}\phi$ , the only object that can give such an index is $\partial^{\mu}\phi$ . hence \begin{equation} \frac{\partial\mathcal{l}}{\partial ( \partial_{\mu}\phi ) } \propto \partial^{\mu}\phi . \end{equation} then , \begin{equation} \begin{split} \frac{\partial\mathcal{l}}{\partial ( \partial_{\mu}\phi ) } \omega^{\sigma}{}_{\mu}\partial_{\sigma}\phi \ , and \propto \ , \omega^{\sigma}{}_{\mu}\ , \partial_{\sigma}\phi \ , \partial^{\mu}\phi\\ and =\omega^{\sigma\mu} \partial_{\sigma}\phi\ , \partial_{\mu}\phi\\ and =0 . \end{split} \end{equation} the last expression vanishes because $\partial_{\sigma}\phi\ , \partial_{\mu}\phi$ is symmetric under the interchange of indices while $\omega^{\sigma\mu}$ is antisymmetric . i actually do not understand why tong did not simply write \begin{equation} \delta \mathcal{l} = -\omega^{\mu}{}_{\nu} x^{\nu}\partial_{\mu}\mathcal{l} . \end{equation} after all , $\mathcal{l}$ should have the same transformation rule as $\phi$ because they are both lorentz scalars . one can verify the above equation by noting that \begin{equation} \delta \mathcal{l} = - \partial_{\mu} ( \omega^{\mu}{}_{\nu}x^{\nu}\mathcal{l} ) = -\omega^{\mu}{}_{\nu} x^{\nu}\partial_{\mu}\mathcal{l} - \omega^{\mu}{}_{\mu}\mathcal{l} , \end{equation} and that \begin{equation} \omega^{\mu}{}_{\mu} = \eta_{\mu\rho}\omega^{\mu\rho} = 0 \end{equation} because $\eta_{\mu\rho}$ is symmetric and $\omega^{\mu\rho}$ is antisymmetric .
comment to the question ( v1 ) : consider the composed function $$s ( a ) ~:=~ s [ x_a ] , $$ where $$x_a ( t ) ~=~ x_0 ( t ) + a \beta ( t ) , $$ for fixed $\beta$ . it should be stressed that the function $a\mapsto s ( a ) $ is not necessarily independent of $a$ , or equivalently , the derivative $s^{\prime} ( a ) $ is not necessarily zero for all $a$ , even if $x_0 ( t ) $ is a stationary path . however , if $x_0 ( t ) $ is a stationary path , then $s^{\prime} ( 0 ) =0$ by definition . the full derivation of euler-lagrange equations from the stationary action principle is done in many textbooks and websites , e.g. wikipedia . for more information , see also e.g. this phys . se post .
if the charge in the capacitor is large enough , you will get a nice little shock:- ) , as the capacitor will discharge through you . i remember grabbing a rectifying valve disconnected from the mains a long time ago - that was not pretty ( apparently , it contained a capacitor ) . i was " clever " enough to grab it again:- )
the energy you seem to refer to is the electric part of the poynting energy expression for some volume $v$: $$ e_{\text{poynting}} ( t ) = \int_v \frac{1}{2}\epsilon_0 \left|\mathbf e ( \mathbf x , t ) \right|^2 + \frac{1}{2\mu_0}\left|\mathbf b ( \mathbf x , t ) \right|^2 \ , d^3\mathbf x . $$ the vector $\mathbf e ( \mathbf x , t ) $ in this expression is the electric vector at position $x$ at time $t$ . there is no integration over time in this expression . if you want to express this electric part of energy with help of the fourier amplitude $\tilde{\mathbf e} ( \mathbf x , \omega ) $ defined by $$ \mathbf e ( \mathbf x , t ) = \int_{-\infty}^{\infty}\tilde{\mathbf e} ( \mathbf x , \omega ) e^{i\omega t} \frac{d\omega}{2\pi} , $$ you can simply substitute in the above expression : $$ e_{electric} ( t ) = \int_v \frac{1}{2}\epsilon_0 \left|\int_{-\infty}^{\infty}\tilde{\mathbf e} ( \mathbf x , \omega ) e^{i\omega t} \frac{d\omega}{2\pi}\right|^2 d^3\mathbf x . $$ energy is a function of $t$ only , and you can try to find a formula for its frequency dependent fourier components $e_{electric} ( \omega ) $ by calculating ft of the last expression with respect to time $t$ .
i do not think it is that tough to analyse . if a conductor is present in a uniform electric field then there will be redistribution of charges to counter electric field inside the conductor ( so that the net field inside the conductor is zero ) . however in uniform electric field this redistribution of charges will not cause any net force on the conductor . why ? because the amount of +ve charge on the conductor is equal to the -ve charge . hence f = q*e will be countered ( or balanced ) by equal and opposite force ( -q*e ) . the geometry on conductor will not play any role at all . ( nature of coulomb in force . ) so centre of mass will not experience any acceleration . what about torque ? it turns out torque = r×f . ahh . . . " r " . interesting . so will it experience any angular acceleration ? : )
hints to the question ( v1 ) : let us parametrize the problem wrt . an arbitrary world-line parameter $\tau$ ( which does not have to be the proper time ) . the lagrange multiplier $\lambda=\lambda ( \tau ) $ depends on $\tau$ , but it does not depend on the canonical variables $x^{\mu}$ and $p_{\mu}$ . similarly , $x^{\mu}$ and $p_{\mu}$ depend only on $\tau$ . the lagrange multiplier $\lambda=\frac{e}{2}$ can be identified with an einbein$^1$ field $e$ . see below where we outline a simple way to understand the appearance of the on-shell constraint $$\tag{1}p^2+m^2~=~0 . $$ here the minkowski signature is $ ( - , + , + , + ) $ . start with the following lagrangian for a massive relativistic point particle $$\tag{2}l_0~:=~ -m\sqrt{-\dot{x}^2} , $$ where dot means differentiation wrt . the world-line parameter $\tau$ . here the action is $s_0=\int \ ! d\tau~ l_0 $ . introduce an einbein field $e=e ( \tau ) $ , and lagrangian $$\tag{3}l~:=~\frac{\dot{x}^2}{2e}-\frac{e m^2}{2} . $$ show that the lagrangian momenta$^2$ are $$\tag{4}p_{\mu}~=~\frac{1}{e}\eta_{\mu\nu}~\dot{x}^{\nu} . $$ show that the euler-lagrange equations of the lagrangian ( 3 ) are $$\tag{5} \dot{p}_{\mu}~\approx~0 , \qquad \dot{x}^2+ ( em ) ^2~\approx~0 . $$ show that the lagrangian ( 3 ) reduces to the original lagrangian ( 2 ) when integrating out the einbein field $e$ . perform a legendre transformation$^2$ of the lagrangian ( 3 ) , and show that the corresponding hamiltonian becomes $$\tag{6}h~=~ \frac{e}{2} ( p^2+m^2 ) . $$ this hamiltonian ( 6 ) is precisely of the form lagrange multiplier times constraint ( 1 ) . show that hamilton 's equations are precisely eqs . ( 4 ) and ( 5 ) . the arbitrariness in the choice of the world-line parameter $\tau$ leads to reparametrization symmetry $$\tau^{\prime}~=~f ( \tau ) , \qquad d\tau^{\prime} ~=~ d\tau\frac{df}{d\tau} , \qquad \dot{x}^{\mu}~=~\dot{x}^{\prime\mu}\frac{df}{d\tau} , \qquad e~=~e^{\prime}\frac{df}{d\tau} , \qquad $$ $$\tag{7} p_{\mu}~=~p_{\mu}^{\prime} , \qquad l~=~l^{\prime}\frac{df}{d\tau} , \qquad h~=~h^{\prime}\frac{df}{d\tau}\qquad s~=~s^{\prime} , $$ where $f=f ( \tau ) $ is a bijective function . thus one may choose various gauges , e.g. $e={\rm const . }$ references : j . polchinski , string theory , vol . 1 , section 1.2 . -- $^1$ an einbein is a 1d version of a vielbein . $^2$ strictly speaking , in the singular legendre transformation , one should also introduce a momentum $$\tag{8}p_e~:=~\frac{\partial l}{\partial \dot{e}}~=~0$$ for the einbein $e$ , which leads to a primary constraint , that immediately kills the momentum $p_e$ again . note that $\frac{\partial h}{\partial e}\approx 0$ becomes one of hamilton 's equations .
there is nothing wrong with your calculations . from the wikipedia article on supermassive black holes : " the average density of a supermassive black hole ( defined as the mass of the black hole divided by the volume within its schwarzschild radius ) can be less than the density of water in the case of some supermassive black holes " given that black hole masses scale with linear size , while objects we encounter in daily lives have a mass proportional to the cube of their linear size , makes it inevitable that beyond a certain size black holes are characterized by mass densities that we label as ' small ' . in other words : when growing an object while keeping it is mass density fixed , there is a maximum to how far you can grow such an object . beyond a certain size , the object acquires a gravitational horizon that starts expanding proportional to the object 's mass , thereby reducing the object 's mass density .
approximately $10^{15}$ . see this : photon flux of 540 nm light from the mechanical equivalent of light and the integrated spectral sensitivity of the human eye : $3.8&#215 ; 10^{15}\ photons/s$ ( photons per second ) $6.3&#215 ; 10^{-9}\ mol/s$ ( moles of photons per second ) also see this reference . note : this summarises robert 's answer in the question comments and is set to cw .
if a wind hits a wall directly ( in a 90 degree angle ) does any of it bounce back ? no , because air behaves like a continuous fluid , it can not rebound and flow back through itself without interacting with the fluid behind it . the air will all be displaced sideways . there will be a higher pressure in front of the wall . are there any similarities with , say , light rays hitting a mirror ? none that i can think of . vortices in a fluid can bounce off walls . ref . ref . a stream of fluid can bounce off a fluid surface . ref . but none of these are like wind hitting a wall or like light-rays hitting a mirror .
calories are the energy released when food is burnt . they are not a very accurate measure of the energy you get form eating them because they do not consider the actual biological process - just what happens if you burn them . if you drink cold water or eat ice then your body must use energy to heat it to body temperature - so eating enough ice could be a diet ! see how much more energy does it take for a human body to heat 0c ice vs 0c water ?
the practical lowest orbit is around 300km and a velocity of 8km/s - around 26 times the speed of sound at the altitude of the balloon jump . so imagine the mass of a 300km long rope and even with a thin atmosphere there is quite a lot of friction on something moving at mach 26
i do not think anyone here has really answered your question . in this case , the sound is " focused " using phased arrays . the face of the audio spotlight has multiple transducers : flickr the same signal is output from each of them , but delayed slightly by different amounts , so that the wavefronts all reach the same point in front of the device at the same time . this " virtual focus " is called beamforming . ref ref this is how modern radars focus their beams , too . instead of spinning a satellite dish around , they have lots of little elements that do not move , but the signals are delayed to produce different beam shapes .
when it is in water the buoyant pressures are distributed more evenly over the whale 's natural surface contour , resulting in less internal strain in the whale 's body . on land , the pressures are all concentrated in a planar surface at the bottom . the whale 's body is not naturally planar , so significant strain develops as the body attempts to conform to the planar surface in order to distribute the forces resisting gravity .
your problem is deeper than it might seem at the first glance . apart from numbers , it involves a remarkable phenomenon , which was studied in detail only a few years ago in the paper phys . rev . lett . 90 , 248302 ( 2003 ) . suppose the puck is an infinitely thin disc , and it is sent across the ice sliding and rotating . what will stop first , translational or rotational motion ? the answer is they will stop simultaneously , and this does not depend on the initial translational and angular velocities . the origin is the intrinsic friction-mediated coupling between rotation and sliding . if rotation is too fast for a given speed $v$ , the translational friction will be very small . and vice versa , if rotation is too slow , the rotational deceleration is small . thus , there exists a " magic " value of $\epsilon=v/r\omega$ towards which every initial condition is attracted . in the paper cited above this magic value was found to be approximately 0.653 . this value fully characterizes the asymptotic motion of the puck and allows you to calculate the friction force distribution , deceleration and finally the path ( which is a straight line in this approximation ) . in fact , a part of that was already found in the paper cited above . however , this concerns only an infinitely thin disc , for which the pressure distribution is equal everywhere under the puck . with a thick disc you have more pressure under the front part of the puck , which additionally complicates the problem ( still , see some discussion in the paper ) . finally , if you change the shape of the pack ( as you in fact did by taking a ring instead of a disc ) , the magic value of $\epsilon$ also changes . see another paper that discusses this : phys . rev . lett . 95 , 264303 ( 2005 ) . see also this comment to this old paper on puck motion on the ice .
in the simplest approximation , an explosion is a shockwave moving out from some locus . the shockwave may be a compression front in a ambient medium , or may be a wave of gas propagating from the explosive into a vacuum . so that is the first thing you need to tell us : in air , water , vacuum , or what ? when the shockwave arrives at some material thing , it is the pressure exerted by the shockwave that transfers momentum ( i.e. . applies a force ) to the target . the target object then accelrates as per newton 's law : $$ \vec{f} = m\vec{a} . $$ the vector part of the above is the trigonometry that you show . i am simply going to assume that you have your coordinate system squared away . however , we still have not said how much force . to a first approximation it goes by the shock pressure $p$ times the area $a$ the object presents to the shock wave . so that gets us to $$ \vec{a} = \frac{p a}{m} \hat{n} $$ where the unit vector $\hat{n}$ is normal to the surface of the shockwave . we are still not done because we do not know $p$ . again , we will take the simplest approximation . assume you know the shock pressure $p_0$ at the surface of the explosive ( this is presumably something you can look up for chemical explosives ) . when the shockwave has total area $\mathcal{a}$ , it is pressure is $$ p = p_0 \frac{\mathcal{a}_0}{\mathcal{a}} $$ where $\mathcal{a_0}$ is the area over which $p_0$ applied . for small sources ( like a bomb or stick of dynamite ) this will give you a $1/r^2$ type dependence for the pressure . for long linear sources you get a $1/\rho$ dependence ; and for large surface charges a pressure that is independent of distance . that is not a good approximation , because you can expect the shockwave to spread out of it is own accord over time , and i have not yet modeled that . the really , really simple thing to do here is to choose a maximum range $l = \tau v_s$ equal to some time interval times the speed of shock propagation , and apply a hard cutoff at that point . now we know how much force as a function of distance from the explosive and geometry of the explosive , but to get to change in velocity we need to know for how long the force is applied . obviously that will be related to the depth of the shockwave and to it is speed of propagation : $ \delta t \approx v_s * d $ . unfortunately i am hazy on how to estimate $d$ . for a simple explosive i might go with half the " size " of the charge ( radius , thickness , whatever ) . the speed of propagation depends on the medium . again , for game purposes you could simple chose some number . once we have $\delta t$ we get $$ \delta v = \delta t * a . $$ since you say that your engine works on a applied impulse , you get $$ i = \delta t * p * a = \delta t * \frac{p_0 \mathcal{a}_0}{\mathcal{a}} * a , $$ which is actually pretty simple . and that should get you on your way .
with the given $m$ and $k$ you indeed cannot calculate the damping coefficient $c$ . remember that you just have a model where you put some constants in and you can only derive other constants which somehow depend on them . the question concerning an actual measurement was answered in investigating the dampening of a spring . greets
hint : use repeatedly : the virasoro algebra . the condition for being a lowest weight representation .
first of all , the 21-centimeter line is " cold enough " so that its presence is universally connected with cold hydrogen , namely with interstellar gas . this spectral line may appear both as emission and as absorption . we see emission lines from the interstellar gas if there is no source behind the interstellar gas ; we may also see absorption lines if there is a source of all frequencies behind the interstellar gas . in the latter case , the interstellar gas ' absorption exceeds its emission . this is the dominant process in the case of the cas-a spectrum on the image so ( 2 ) is basically correct . otherwise the lines ' frequencies are not sharply determined in the graph . in the context of astrophysics , virtually all deviations from the " precisely right frequency " of the 21-centimeter line are due to the doppler shift i.e. the relative motion of the interstellar gas with respect to us . the graph mostly shows absorption and one may calculate the radial speeds of the clouds that are reducing the function in the graph in this way ( from the frequencies ) . the explanation ( 1 ) contains almost no component of the truth because cas a is a remnant of a type iib supernova that exploded a few centuries ago ( more precisely , that is when the light describing the explosion reached the earth ) . such supernova remnants have virtually no hydrogen in the shells – because even the star that led to this explosion , probably a red giant , had already have just the helium core and almost no hydrogen envelope . so pretty much all the spectral features of the hydrogen have to be due to the interstellar gas . it is not true that the emission and absorption always cancels . once again , emission dominates when the interstellar gas is " warmer " at the given frequency than the environment behind it ; absorption wins when the source behind the interstellar gas is " warmer " at the frequency ( literally think about the intensity as if it were temperature and realize that the heat goes from a warmer body to a cooler one ) .
can the imaginary part of the energy be interpreted as the lifetime of an unstable state in the case of a vortex-vortex situation . . . yes . vortices have repulsive interactions between them and cannot form bound states . so the result is physically reasonable . and complex energy eigenvalues do measure decay rates of resonances . this is shown in this article : " a pedestrian introduction to gamow vectors " . gamow vectors are the eigenvectors corresponding to the complex energy eigenvalues . [ george gamow was also a famous physics writer with books such as one , two , three infinity and physics : foundations and frontiers . respect . ]
on your first question : absolutely , energy gravitates ( or induces curvature in spacetime ) the same way that mass gravitates . if you read general relativity , you will learn that it is in fact the stress-energy tensor that is the source of gravitational interaction ( or equivalently spacetime curvature ) . energy can be localized very easily ; a parallel-plate capacitor has electrical energy localized between its parallel plates ( and that electrical energy gravitates ! ) . in fact celestial bodies have most of their masses owing to the energy of the bonds that keep their fundamental constituents ( quarks ) together in their atoms ' nuclei . you can imagine any of these bonds as a spring connecting two fundamental constituents ; every spring has potential energy localized on it and this potential energy looks like mass to macroscopic observers that do not see the tiny springs inside nuclei . in any case , thinking of the bond energy as mass or energy does not really matter ; what matters is that the bonds ( springs ) contribute to the stress-energy tensor , and the stress tensor gravitates . the answer to your last question is negative . the energy is conserved in an annihilation process . take the annihilation of a pair of electron-positron to form a photon as an example . the energy of the annihilating electron and anti-electron ( positron ) equals the energy of the resulting photon . since energy gravitates , the out-going photon gravitates as much as the in-going electron and positron were gravitating .
anti-matter is produced all the time in the world 's particle accelerators . it is also produced natrually during air shower cascades caused by cosmic ray particles and gamma-rays interacting in the atmosphere and when these same particles interact in particle detectors . one of the simplest methods to show it exists is used by the cosmic ray detector on the pamela satellite . in this detector , there is a target that the cosmic rays ( and gamma rays ) interact with . then the particles pass through a strong magnetic field . since the path of moving charged particles curves in the presense of a magnetic field , they can see where the particle entered and where it hit the final target which is a calorimeter that measures the energy . electrons come into the detector and curve one direction . positrons , the positive anti-particle to the electron , enter the detector and curve the other direction . they have the same mass and energy but opposite charges and this is easily measured showing that positrons exist . the same principle is used to detect them in the particle accelerators around the world .
another very fresh paper presented at dark attack yesterday , one by hektor et al . , http://arxiv.org/abs/1207.4466 also claims that the signal is there – not only in the center of the milky way but also in other galactic clusters , at the same 130 gev energy . this 3+ sigma evidence from clusters is arguably very independent . all these hints and several additional papers of the sort look very intriguing . there are negative news , too . fermi has not confirmed the " discovery status " of the line yet . puzzles appear in detailed theoretical investigations , too . cohen at al . http://arxiv.org/abs/1207.0800 claim that they have excluded neutralino – the most widely believed identity of a wimp – as the source because the neutralino would lead to additional traces in the data because of processes involving other standard model particles and these traces seem to be absent . the wimp could be a different particle than the supersymmetric neutralino , of course . another paper also disfavors neutralino because it is claimed to require much higher cross sections than predicted by susy models : http://arxiv.org/abs/1207.4434 but one must be careful and realize that the status of the "5 sigma discovery " here is not analogous to the higgs because in the case of the higgs , the " canonical " null hypothesis without the higgs is well-defined and well-tested . in this case , the 130-gev-line-free hypothesis is much more murky . there may still exist astrophysical processes that tend to produce rather sharp peaks around 130 gev even though there are no particle species of this mass . i think and hope it is unlikely but it has not really been excluded . everyone who studies these things in detail may want to look at the list ( or contents ) of all papers referring to weniger 's original observation – it is currently 33 papers : http://inspirehep.net/search?ln=enp=refersto%3arecid%3a1110710
i am not an expert in 2d cft . however i hope following manipulations are valid . assume that your second equation follows from first one . then on rhs of your first equation taylor expansion of $o_2 ( w ) $ at point $z$ gives : $o_2 ( w ) =o_2 ( z ) + ( w-z ) \partial_z o_2 ( z ) + . . . $ taking derivative wrt $w$ on both sides we get $\partial_wo_2 ( w ) =\partial_zo_2 ( z ) + . . . $ using these two results in your first equation we get $o_1 ( z ) o_2 ( w ) = \displaystyle\frac{o_2 ( z ) }{ ( z-w ) ^2}+regular\:terms$ subtracting it from your second equation , multiplying with $ ( z-w ) ^2$ and taking limit $w\rightarrow z$ we conclude that $o_2$ and $o_1$ should be equal . since to begin with we did not assume any such thing regarding fields $o_2$ and $o_1$ so in general your second equation should not follow from the first one . i think equality of $o_2 ( w ) o_1 ( z ) $ and $o_1 ( z ) o_2 ( w ) $ ( assuming fields are ' bosonic' ) within time ordered product only implies that their ope should be symmetric under exchange of z and w . so if your first equation for ope can be realized for some ( bosonic ) fields , then by exchanging z with w on rhs you should get the same result within a regular term .
the acceleration equation needs to include force terms for air drag $f_a$ , and runner friction $f_f$ in addition to the gravity term $g \sin ( \theta ) $ where $\theta$ is the slope of the luge run and $g$ is gravitation . as singh pointed out , gravitation exerts a force proportional to mass , so the total acceleration is $$a = g \sin ( \theta ) - \frac{f_f + f_a}{m}$$ runner friction is more or less proportional to mass , so we can replace it with a constant , i.e. the mass of the rider is not a consideration , giving $$a = g \sin ( \theta ) - k - \frac{f_a}{m} $$ air drag depends on frontal surface area , $a$ and the square of velocity $v^2$ , so $$f_a = dav^2$$ where d is a drag coefficent to account for the rider 's aerodynamic smoothness ( or lack thereof ) . user11865 's answer points out that the rider 's surface area is proportional to $\sqrt{m}$ and this is what gives heavier riders an advantage , especially at higher speeds . let 's wrap the density and shape of the human body into a constant , say , $b$ , and the acceleration equation now looks like $$a = g \sin ( \theta ) - k - \frac{dbv^2}{\sqrt{m}}$$ the acceleration lost to air drag is the only term that depends on mass and having more mass makes it smaller .
i am assuming that this section of the book is talking about the ultraviolet catastrophe , where an ideal black body in thermal equilibrium will emit an infinite amount of power through radiative means . the source goes on to say : the ultraviolet catastrophe results from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes ( degrees of freedom ) of a system at equilibrium have an average energy of kt . following that : according to classical electromagnetism , the number of electromagnetic modes in a 3-dimensional cavity , per unit frequency , is proportional to the square of the frequency . this therefore implies that the radiated power per unit frequency should follow the rayleigh–jeans law , and be proportional to frequency squared . thus , both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered : this is clearly unphysical as the total radiated power of a cavity is not observed to be infinite , a point that was made independently by einstein and by lord rayleigh and sir james jeans in the year 1905 . essentially , if in a cavity there are an infinite number of electromagnetic modes possible ( think standing waves ) , the equipartition theorem says that a system in equilibrium has an average of $k_{b}t$ worth of energy per mode . this is not what is actually observed , since we do not see an infinite amount of power radiated . how was this solved : max planck solved the problem by postulating that electromagnetic energy did not follow the classical description , but could only be emitted in discrete packets of energy proportional to the frequency , as given by planck 's law . the radiated power eventually goes to zero at infinite frequencies , and the total predicted power is finite . ( 1 ) the formula for the radiated power for the idealized system ( black body ) was in line with known experiments , and came to be called planck 's law of black body radiation . based on past experiments , planck was also able to determine the value of its parameter , now called planck 's constant . the packets of energy later came to be called photons , and played a key role in the quantum description of electromagnetism . ( 1 )
as you are aware , thermal stratification is occurring . once the upper surface of the tub is hot the driving force ( temperature and therefore density difference ) for the thermal siphoning effect is reduced . mixing the tub will definitely help here . all the tubs i have seen are very well insulated ( they take days to cool down . . ) so burying it will not have a great reduction in heat loss . your lid sounds like the weak link as far as reducing heat loss is concerned . any fouling inside the steam pipe can significantly reduce heat transfer to the water . in extreme cases by an order of magnitude . to maintain the ' off the grid ' aesthetic while still improving heat transfer a non-electric pump could be used . a water pump from a dishwasher or washing machine could be rigged up to be powered in another way . some ideas : a wind up mechanism , pedal power , a wind mill ( or a ' fire mill ' powered from the fires exhaust flow ! ? ) , or any other harebrained scheme you can come up with , it will not take much to dramatically improve flow rate from the current thermal siphon rate . motorcycle speedometer cables are a really flexible ( pun intended ) way to get rotational energy from one place to another .
your friends are correct . if there is no force in the left-right direction , then linear momentum will be conserved in that direction . because the new composite object has more mass than the original object , it will have a lower speed to the right . what about energy ? kinetic energy is not conserved in this case , because the collision is inelastic . the kinetic energy lost in the collision ( and the downward momentum lost , for that matter ) is absorbed by whatever wall is preventing the composite object from continuing to move downward .
there are two su ( 3 ) symmetries you can come across . basically , su ( n ) emerges everywhere when you have n quantum states and some physics does not distinguish these states - then all quantum superpositions of these states make a fundamental representation of su ( n ) significant to that physics ( but maybe insignificant to some other ) . thus in particle physics there are two su ( 3 ) discussed often . the su ( 3 ) of colors represents the exact , unbroken symmetry . then there is the su ( 3 ) of flavours , which mixes $u$ , $d$ , $s$ quarks in strong interactions . it is very rough and it is broken by the masses of quarks ( $m_s$ is about 150 mev , that is dozen percent of the qcd scale of about 1 gev ) . after breaking it leaves the flavour su ( 2 ) of only $u$ , $d$ quarks , and that is much more accurate ( $m_d-m_u$ in the order of unities of mev ) , though still approximate . nevertheless , these symmetries are used to classify hadron states , and to descibe physics on the hadron level ( which is the effective theory with respect to sm ) . it may be credited for the close masses of hadron multiplets , but this is ( almost ) another way to say " hadrons have close masses because quarks inside them have close masses " . also , $su ( 3 ) _c$ is a gauge ( local ) symmetry , while $su ( 3 ) _f$ is a global symmetry .
in principle yes . but to make a good $\text{zn}_3\text{n}_2$ homojunction led you need the capability to incorporating both p-type and n-type dopants ( normally oxide materials are naturally n-type ) which might not be possible . from what i have read , this material has been proposed as a way of making p-type zno ( which is naturally n-type ) by a post growth annealing step . this means that via $\text{zn}_3\text{n}_2$ you maybe able to make a p-zno/n-zno homojunction led . more info here , http://pubs.rsc.org/en/content/articlehtml/2013/ra/c3ra46558f .
liquid nitrogen boils when it comes in contact with skin , so small amounts of spatter are no danger at all-- the droplets just bounce off . i regularly pour a liter or so ( a bit at a time ) out on a lab table when i do liquid nitrogen demos , with no problems or safety gear . the biggest risk from the low temperature is getting it into fabric of some sort , which will hold it in closer proximity to skin for longer than the drops by themselves will . i have , on occasion spilled some on my pants , which is annoyingly cold , but not too bad unless you are wearing really tight clothing . really , the biggest hazard from any of the nitrogen demos i do is not the temperature but the expansion . when it boils , it expands to something like 700 times the volume of the liquid , so if you put some in a sealed container , it can make a big bang . i know of a case where a grad student at mit destroyed a bathroom with a 2-liter bottle of liquid nitrogen . you can use this expansion for a kind of cool demonstration if you take one of those little dropper bottles with the angled spouts that are common in chem labs , and seal a little nitrogen inside . put it down on the floor , and it spins like a firework . the problem with that is , you almost always end up getting a little water vapor condensing in the spout , which plugs it up , and then the bottle will go bang . i had a student a few years ago who had one go off in his hand , and he said it stung pretty badly . i have seen videos of people ( jearl walker in particular ) " drinking " liquid nitrogen by taking a small amount into their mouth , and holding it there for a second or so-- the instant boiling will keep it from giving you oral frostbite for a little while , and you can spit the liquid out after breathing out over it , which makes an enormous plume of steam . it is really cool to see , but kind of risky to do . i have never had the guts to try it myself .
i think a very good orignally german qm book is : straumann : " quantenmechanik : ein grundkurs über nichtrelativistische quantentheorie " there is also a second volume : straumann : " relativistische quantenfeldtheorie " another good book is from g . grawert : quantenmechanik you may also have a look at thirrings books about qm ( it is mathematically more advanced ) . i think straumanns book is not " canonical " . often used by students are the books by nolting and greiner . from the experimental point of view you may have a look at demtröders book .
if op wants to evaluate $ [ a , e^b ] $ in terms of $ [ a , b ] $ , there is a formula $$\tag{1} [ a , e^b ] ~=~\int_0^1 \ ! ds~ e^{ ( 1-s ) b} [ a , b ] e^{sb} . $$ proof of eq . ( 1 ) : the identity ( 1 ) follows by setting $t=1$ in the following identity $$\tag{2} e^{-tb} [ a , e^{tb} ] ~=~ \int_0^t\ ! ds~e^{-sb} [ a , b ] e^{sb} . $$ to prove equation ( 2 ) , first note that ( 2 ) is trivially true for $t=0$ . secondly , note that a differentiation wrt . $t$ on both sides of ( 2 ) produces the same expression $$\tag{3} e^{-tb} [ a , b ] e^{tb} , $$ where we use the fact that $$\tag{4}\frac{d}{dt}e^{tb}~=~be^{tb}~=~e^{tb}b . $$ so the two sides of eq . ( 2 ) must be equal . remark : see also this related phys . se post . ( it is related because $ [ a , \cdot ] $ acts as a linear derivation . )
firstly , block b is the heavier of the two and coefficient of friction is lower on b 's side . if either of the blocks moved , it has to be b . given that b has a tendency to move first and its side has lower coefficient of friction , friction on that side would reach a limiting value before a . this is what the author must have assumed .
your mistake is in forgetting that the verictial component of force f holds up half the stick , so the friction force only needs to hold up the other half of the stick 's weight , not the whole weight .
if i understand the question , you are wondering how to justify the statement that a ( reverible ) adiabatic process is isentropic from the point of view of statistical mechanics ( the classical thermodynamics definition makes sense to you ) . let us then start with the entropic fundamental relationship , s = s ( u , v , n ) , where u stands for energy , v for volume , n for number of particles . in many a statistical mechanics texts you will find the explicit definition of s for a system of particles ( under usual simplifying assumptions ) . inyour example n is constant , but u and v are not : i would be glad to help further if needed , but if you looked at the expression for s as a function of v and n this would answer your question alone . i believe the discussion at isentropic processes be useful .
answering my own doubts in order:- no . it got to the correct answer , but was wrong . i think she was getting confused between cause ( force ) and effect ( acceleration ) . when the train brakes , the ball and train acquire a relative acceleration . no other force comes into play when one 's inertial frame of reference is the train 's frame . she was basically saying $ a_{iron} = a_{rubber} $ and $ m_{iron} &gt ; m_{rubber} $ , so therefore $ f_{iron} &gt ; f_{rubber} $ which is correct . but then she also says that the iron ball will move a longer distance which implies that both the balls will eventually come to a stop while simultaneously claiming to ignore air resistance and friction . so then which force eventually stops the balls ( as otherwise they will keep moving at the constant speed otherwise as dictated by one of newton 's laws of motion ) ? none that i could think of ( which are probably all of them considering this is such a simple question ) . not mentioned , but probably not . in most idealized physics questions ( especially textbook ones ) , air resistance is not something to be considered . this is to some extent implied by the phrase " on a smooth floor " . if we do not consider it , then no external forces will be acting on the ball ( barring friction ) and both the balls will move at the same speed till they like slide off the train 's surface or hit a wall or something meaning this was a trick question ( which seems atypical as it is not the modus operandi of the crappy , unenlightening educational system ) . if we do consider it , then b'coz of the reasons in the ( my ) original answer , the iron ball will have a slight greater speed than the rubber ball and thus will move a bit further . probably not as the question mentions the phrase " a smooth floor " . although it could be said that while this would affect the absolute values of friction , it would not change the relativity , i.e. , the fact that the rubber ball will experience more friction that the iron one . also , considering it would mean we should probably also consider air resistance . and then the question becomes unsolvable as there will be unknown variables required then . for that and the reason above , it should probably not be considered . 10x , @nathaniel and @tromik .
given a arbitrary metric $g_{\mu\nu}$ you can introduce a reference ( background ) metric $\bar{g}_{\mu\nu}$ ( in the paper notation it is just the minkowski metric $\eta_{\mu\nu}$ ) in a way that $\delta{}g_{\mu\nu} = g_{\mu\nu} - \bar{g}_{\mu\nu}$ is small ( in some sense ) . you can reintroduce the background metric in a way that the perturbation remains small , this action is parametrized by an small diffeomorphism ( you can see why in the mukhanov 's review of perturbations : dx . doi . org/10.1016/0370-1573 ( 92 ) 90044-z ) generated by an arbitrary vector field $-\xi^\alpha$ , thus , the background change by $$\bar{g}_{\mu\nu}\rightarrow\bar{g}_{\mu\nu}-\mathcal{l}_\xi\bar{g}_{\mu\nu} , $$ where $\mathcal{l}_\xi$ is the lie derivative with respect to $\xi^\alpha$ . given this transformation , the perturbation transform as $$\delta{}g_{\mu\nu}\rightarrow\delta{}g_{\mu\nu} + \mathcal{l}_\xi\bar{g}_{\mu\nu} . $$ if you choose a covariant derivative compatible with $\bar{g}_{\mu\nu}$ , say $\bar{\nabla}_\alpha\bar{g}_{\mu\nu} = 0$ , the lie derivative can be written as $$\mathcal{l}_\xi\bar{g}_{\mu\nu} = 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$ in the paper the background metric is just the minkowski metric , then in cartesian coordinates $$\mathcal{l}_\xi\eta_{\mu\nu} = 2\partial_{ ( \mu}\xi_{\nu ) } = 2\xi_{ ( \mu , \nu ) } , $$ where the comma represent the partial derivative . so in general you can write a metric in terms of the background , perturbation and a gauge transformation as $$g_{\mu\nu} = \bar{g}_{\mu\nu} + \delta{}g_{\mu\nu} + 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$
hints to the question ( v5 ) : op correctly imposes two conditions because of the delta function potential at $x=-a$ , but op should also impose the boundary condition $\psi ( x\ ! =\ ! 0 ) =0$ because of the infinite potential barrier at $x\geq 0$ . there is zero probability of transmission because of the infinite potential barrier at $x\geq 0$ . ( recall that transmission would imply that the particle could be found at $x\to \infty$ , which is impossible . ) hence there is a 100 percent probability of reflection , cf . the unitarity of the $s$-matrix . see also this phys . se answer . as op writes , away from the two obstacles , one has simply a free solution to the time-independent schrödinger equation , namely a linear combination of the two oscillatory exponentials $e^{\pm ikx}$ . this solution is non-normalizable over a non-compact interval $x\in ] -\infty , 0 ] $ . to make the wave function normalizable , let us truncate space for $x&lt ; -k$ , where $k&gt ; 0$ is a very large constant . so now $x\in [ -k , 0 ] $ . one may then define and calculate the probability $p ( -a \leq x\leq 0 ) $ of finding the particle between the two barriers via the usual probabilistic interpretation of the square of the wave function . if we now let the truncation parameter $k\to \infty$ , then we can deduce without calculation that this probability $p ( -a \leq x\leq 0 ) \to 0$ goes to zero .
obviously things vary from subfield to subfield , so canonical advice is probably to much to hope for . with that in mind , let me try to give you something of an answer . in principle when a person finishes a phd , if they remain in academia , there are a number of ways this might happen . they might end up with : a teaching job with little or no research emphasis , a postdoc supported from a faculty members grant , a personal fellowship , or a junior faculty position . now , ( 4 ) is essentially unheard of these days , so you can more or less forget that . of the rest , what is feasible will depend on a number of factors , but the impact of your research is going to play a big role in your success in applying for either ( 2 ) or ( 3 ) . i presume , since you are only looking at what to apply for now , that you have not finished the phd yet , and have something on the order of six months to a year left ( do correct me if i am wrong ) . with this in mind , it seems quite possible that you will have more than one publication by the end of the phd . if this is possible , you should put in the effort to write up results . if you get 3 or 4 papers ( preprints definitely count , so make sure you are using the arxiv and including them on your cv ) , this will substantially alter your odds of getting a postdoc from someone you have not previously worked with . personal fellowships are the most sought after form of postdoc , as they usually allow substantial autonomy , and so provide something of a springboard for a research career . this means that they are competitive and realistically , i can not see someone getting one on a single publication . i was rejected from some of the ones i applied for with 8 publications including a number of prls . postdocs supported from someones grant are easier to get , but still competitive . if you have only one publication , it might prove hard to convince someone you do not already know of your potential , though it is perhaps still worth trying , especially if your paper is particularly interesting . the best way to get a position if you are really limited on publications is as a postdoc on a grant supported by someone you have worked with before , or who knows your work well . these people are more likely to hire you on potential or other factors , rather than on your publication record . the scope for this is of course limited , so as pieter points out the more people who know you and your work , the better . that said , my advice would definitely be to start applying and get your results written up as soon as possible .
i do not think it is fair to say " the higgs looks like it is going to be at higher energies then anticipated " . in fact , my money was on 160 gev , based on models coming from noncommutative geometry . but the basic constraints on the higgs mass from the standard model were really not very good ( the following comes from the review article of djouadi , 0503172v2 ) . short story : unitarity starts failing around 900 gev , perturbation theory fails around 700 gev . a lower bound can be gotten by requiring that the higgs quartic coupling remain positive , which gives $m_h&gt ; 70$ gev . this depends on the cutoff-scale ; the 70 gev comes from assuming a 1 tev cutoff scale . if the sm is valid up to gut scales , this rises to $m_h&gt ; 130$ gev . so , although the current values for higgs are actually just below that 130 gev , i think it is not fair to make any statement except that " it seems fine for the standard model " - it is too early to say " the higgs mass implies new physics " . all of these estimates are based on measured parameters such as the top mass , which has it is own uncertainty associated with it . there is also the fine-tuning problem , but the above bounds generally give the same or slightly better estimates then that . if someone wants to mention susy implications of a $\sim 125$ gev higgs , be my guest - there certainly are some . but susy can not possibly be real anyway , so i am ok not knowing them ; - )
electroweak symmetry breaking requires that $\partial v/\partial h_{u , d} = 0$ . combining the two expressions , we find the superpotential bilinear $\mu$ , $$ |\mu|^2 = \frac12 \left [ \frac{|m_{h_d}^2 - m_{h_u}^2|}{\cos 2\beta} - m_{h_u}^2 - m_{h_d}^2 - m_z^2\right ] _{\text{ew-scale}} $$ in the focus-point region $|m_{h_u}^2|$ and $|m_{h_d}^2|$ are " focused " to $\lesssim m_z^2$ at the electroweak scale ; this is insensitive to their values at the high-scale . careful with the sign of $m_{h_u}^2$ , though ; it is parameter unto itself , rather than the square of a real parameter , and it is typically negative at the electroweak scale . as a result $|\mu| \lesssim m_z$ , and certainly $|\mu| \ll m_1 , m_2$ . ( note that in minimal models like the cmssm , $\mu$ is calculated in this way , but in more general models , we can trade e.g. a free parameter $m_{h_d}^2$ for $\mu$ , and have $\mu$ as an input parameter and $m_{h_d}^2$ calculated . ) the lightest neutralino/chargino is therefore higgsino-like , with $$ m_{\chi_{1,2}} \approx |\mu| , \\ m_{\chi_{3}} \approx m_1 , \\ m_{\chi_{4}} \approx m_2 , \\ m_{\chi^\pm_{1}} \approx |\mu| , \\ m_{\chi^\pm_{2}} \approx m_2 . $$
yes , $$\sum_{\mu} f_{\mu}{}^{\mu}~:=~\sum_{\mu , \nu}f_{\mu\nu} g^{\nu\mu}~=~0$$ vanishes because it is a trace of a product of a symmetric and an antisymmetric tensor . it is irrelevant for the argument that $f_{\mu\nu}$ is lie algebra valued .
a wave can propagate in any medium that is : a ) elastic b ) less than critically damped neither homogeneity nor isotropy are necessary . any elastic system will return to it is original state when deformed , the question is just whether the deformation can propagate , and this is down to how quickly the energy of the deformation is dissipated . if the damping is high enough , this is critical damping , the material will return to its original state with a $e^{-\alpha t}$ dependance on time and no wave will propagate . for example , in water common experience tells a gravity wave ( i.e. . a wave ) propagates just fine , and a longitudinal wave ( i.e. . sound ) propagates just fine . however shear waves will not propagate because they are too rapidly damped .
the book answer seems correct ( it can be obtained from the equations for the bottom body ) . i do not know how you obtained your result , but there is a possibility that it is also correct and actually coincides with the book answer , just because the equations for the top body provide an extra relation between $\alpha$ and $\beta$ .
i will assume you want to know the equilibrium points . the lagrangian tells you everything you need to know about the system . because variation of generalized momentum is : $$ \frac{dp_k}{dt} = q_k + \frac{\partial t}{\partial q_k} = -\frac{\partial v}{\partial q_k}+ \frac{\partial t}{\partial q_k} = \frac{\partial l}{\partial q_k} $$ then : $$ \frac{dp_k}{dt} = 0 \quad\longrightarrow\quad \frac{\partial l}{\partial q_k}=0 $$ in your question , i think it makes more sense to talk about equilibrium in terms of potential . you are using generalized coordinates $\theta_r , \theta_r$ , and the lagrangian tells us the potential involves a $\cos\theta_r$ , because the other terms are cleary kinetic related . the points of equilibrium is when the gradient of the potential equals to zero , which means , when the force equals zero . thinking in terms of variation of generalized momentum , it must be zero . we then have : $$ \frac{\partial l}{\partial q_k} = \frac{\partial l}{\partial\theta_r} = \frac{\partial}{\partial\theta_r} \cos ( \theta_r ) = -\sin\theta_r $$ . the equlibrium points $\theta_r$ occurs , when $\sin\theta_r = 0$ . simple equation to solve .
the geometry of special relativity is called lorentzian geometry , or in full : the " pseudo-riemannian geometry of minkowsk spacetime " . this is also the cartan geometry of the lorentz group inside the poincar&eacute ; group . see on the nlab at lorentzian geometry for further pointers . see the references there for introductions and surveys .
the only dimension-five operators allowed by the sm are neutrino masses , $ ( hl_i ) ( hl_j ) $ . so we mostly talk about dimension-six operators because for almost any question they are the first higher-dimension operators that can appear .
this equation is wrong . as it has been pointed out in the comments , i can not equate the integrands of two integrals just because the integration limits are the same . the equation which is of relevance in the context of probability density and quantum mechanics is perhaps the well known continuity equation , $$ \frac{\partial \rho}{\partial t} + \nabla \cdot \textbf{j} = 0 $$ , where $\rho = \vert \psi \vert^2$ and $ \mathbf{j}=\frac{\hbar}{m}\text{im}\left [ \psi^*\nabla\psi\right ] $ is the probability flux .
your second equation , $p ( \nu , t ) = \frac{2 h {\nu}^3}{c^2}$ $\frac{1}{\exp\bigl ( \frac{h \nu}{kt}\bigr ) - 1}$ is what is commonly referred to as planck 's law for radiation , although a more standard symbol used is $b_\nu ( t ) $ . this is the energy radiated per time , per area , per frequency interval , per steradian . it is a formula for the ' specific intensity ' of a source , which intuitively is the energy flux along a ray of radiation in a given direction , and so you must normalize by the solid angle subtended by that ray . to get the total energy per time per area radiated by a patch of a black body , integrate over solid angle and over frequency . be careful performing the solid angle integral , however , because you must include the geometric factor $\cos \theta$ that accounts for the projected area of the patch ( $\theta = 0$ corresponds to a ray emitted in the normal direction ) . rays leaving one side of a patch can only be directed into the upper hemisphere of the solid angle sphere . so the solid angle integral looks like this : $$ f_\nu = 2 \pi \int_0^{\pi/2} b_\nu ( \theta ) \ , \cos\theta \ , \sin \theta \ , d \theta$$ the $2 \pi$ out in front is for the azimuthal angle . here , $f_\nu$ is what is commonly referred to as the specific flux ( 'specific ' because it is still per unit frequency interval ) . then , either by reading up on the riemann $\zeta$ function , or just using a computer to tell you the answer , you can perform the frequency integral and get $$ f = \sigma \ , t^4$$ here $f$ is what we commonly think of as the flux ( energy per area per time ) , and $\sigma$ is the stefan-boltzmann constant , $$\sigma \equiv \frac{2 \pi^5 \ , k_\mathrm{b}^4}{15 \ , h^3 \ , c^2}$$
there are some rules about what happens to light rays passing through lenses , which are derived from snell 's laws . in short : a ) a ray passing through the focal point into the lens will exit the lens parallel to the optical axis . b ) a ray passing straight into the center of the lens ( at any angle with respect to the optical axis ) will exit at the same angle . c ) a ray entering the lens parallel to the optical axis will exit the lens and pass through the focal point . these are standard in any intro physics text , but what the heck i will draw a simple diagram : the object is on the left , i have labeled the three rays appropriately , and the image is on the right ( focal points are dots ) . as you can see , the rays a ) and b ) are incident with the lens at an angle with respect to the optical axis , which should answer your question . the same rules apply for concave lenses as well , and also curved mirrors if you make the appropriate adjustments .
you are right that we do not know if the universe is finite or infinite in space . cosmologists do now think that it has an infinite future because of the accelerated expansion rate due to dark energy but this does not tell us anything about the question of infinite space . to answer the question for space we first have to assume spacial homogeniety , i.e. that space looks the same everywhere on large scales . if this assumption is wrong then we are stuck because we cannot see beyond the horizon of the observable universe which is due to the finite speed of light and the finite age of the universe . if we can not assume anything about what happens beyond the horizon then obviously we cant tell if it is finite or infinite . however , within the observable universe space does appear to be homogeneous so it is usual to assume , rightly or wrongly , that it is homogeneous everywhere . if that is the case then the question of the finiteness depends on the curvature of space and this is something that can be estimated with some precision using cosmological observations , especially that of the cosmic microwave background . if the curvature of space takes a positive value then it must be finite . if it is zero or negative then it is probably infinite though the possibility of a finite universe remains if it has an unusual topology like a tessellation of a finite polyhedron . when we measure the curvature we find that it is close to zero and we cannot tell whether it is positive , zero or negative with the present error bars , so we do not know if the universe is finite or infinite in space . this flatness is expected as a prediction of inflation theory . the best we can do is set a lower limit to how small the universe can be given limits on its measured curvature and that is what the papers you linked to are trying to do . they do not claim to settle the question of whether it is finite or infinite . your final question about quantum mechanics and the outside of the universe is unrelated and should have been asked as a separate question , but the answer is no .
this is ok . we define the scalar multiplication in the ket vector space to be complex linear , $a\left|v\right&gt ; =\left|av\right&gt ; $ . the inner product $\left&lt ; w|v\right&gt ; $ is complex linear on the ket vector space , but complex anti-linear on the bra space , $$a\left&lt ; w|v\right&gt ; =\left&lt ; w|av\right&gt ; =\left&lt ; a^*w|v\right&gt ; . $$ that is almost a matter of definition for the inner product of a complex hilbert space , where in an alternative notation we would write $ ( a^*w , v ) = ( w , av ) =a ( w , v ) $ . in particular , this means that $\left&lt ; aw|av\right&gt ; $ is a positive multiple of $\left&lt ; w|v\right&gt ; $ , $\left&lt ; aw|av\right&gt ; =|a|^2\left&lt ; w|v\right&gt ; $ . perhaps the notation would be slightly less worrying if we wrote $\left&lt ; av\right|=a^*\left&lt ; v\right|$ ?
the presumed equivalence between the canonical quantization and the fock space representation is only a particular case . the canonical formalism provides only with canonical poisson brackets . the first step according to dirac 's axioms is to replace the poisson brackets by commutators and since these commutators satisfy the jacobi identity , they can be represented by linear operators on a hilbert space . canonical quantization does not specify the hilbert space . finding a hilbert space where the operators acts linearly and satisfy the commutation relations is a problem in representation theory . this task is referred to as " quantization " in the modern literature . the problem is that in the case of free fields , this problem does not have a unique solution ( up to a unitary transformation in the hilbert space ) . this situation is referred to as the existence of inequivalent quantizations or inequivalent representations . the fock representation is only a special case . some of the quantizations are called " non-fock " , because the hilbert space does not have an underlying fock space structure ( i.e. . , cannot be interpreted as free particles ) , but there can even be inquivalent fock representations . before , proceeding , let me tell you that inequivalent quantizations may be the areas where " new physics " can emerge because they can correspond to different quantum systems . also , let me emphasize , that the situation is completely different in the finite dimensional case . this is because that due to the stone-von neumann theorem , any representation of the canonical commutation relations in quantum mechanics is unitarily equivalent to the harmonic oscillator representation . thus the issue of inquivalent representations of the canonical commutation relations occurs only due to the infinite dimensionality . for a few examples of inquivalent quantizations of the canonical commutation relations of a scalar field on a minkowski space-time , please see the following article by : moschella and schaeffer . in this article , they construct inequivalent representations by means of bogoliubov transformation which changes the vacuum and they also present a thermofield representation . in all these representations the canonical operators are represented on a hilbert space and the canonical commutation relations are satisfied . the bogoliubov shifted vacuum cases correspond to broken poincare ' symmetries . one can argue that these solutions are unphysical , but the symmetry argument will not be enough in the case of quantization on a general curved nonhomogenous manifold . in this case we will not have a " physical " argument to dismiss some of the inequivalent representations . the phenomena of inequivalent quantizations can be present even in the case of finite number of degrees of freedom on non-flat phase spaces . having said all that , i want nevertheless to provide you a more direct answer to your question ( although it will not be unique due to the reasons listed above ) . as i understand the question , it can be stated that there is an algorithm for passing from the single particle hilbert space to the fock space . this algorithm can be summarized by the fock factorization : $$ \mathcal{f} = e^{\otimes \mathcal{h}}$$ where $\mathcal{h}$ is the single particle hilbert space and $\mathcal{f}$ is the fock space . as stated before canonical quantization provides us only with the canonical commutaion relations : $$ [ a_{\mathbf{k}} , a^{\dagger}_{\mathbf{l}} ] = \delta^3 ( \mathbf{k} - \mathbf{l} ) \mathbf{1}$$ at this stage we have only an ( $c^{*}$ ) algebra of operators . the reverse question about the existence of an algorithm starting from the canonical commutation relations and ending with the fock space ( or equivalently , the answer to the question where is the hilbert space ? ) is provided by the gelfand -naimark-segal construction ( gns ) , which provides representations of $c^{*}$ algebras in terms of bounded operators on a hilbert space . the gns construction starts from a state $\omega$ which is a positive linear functional on the algebra $ \mathcal{a}$ ( in our case the algebra is the completion of all possible products of any number creation and annihilation operators ) . the second step is choosing the whole algebra as an initial linear space $ \mathcal{a}$ . in general , there will be null elements satisfying : $$\omega ( a^{\dagger}{a} ) = 0$$ the hilbert space is obtained by identifying elements differing by a null vector : $$ \mathcal{h} = \mathcal{a} / \mathcal{n} $$ ( $\mathcal{n} $ is the space of null vectors ) . the inner product on this hilbert space is given by : $$ ( a , b ) = \omega ( a^{\dagger}{b} ) $$ it can be proved that the gns construction is a cyclic representation where the hilbert space is given by the action of operators on a cyclic " vacuum vector " . the gns construction gives all inequivalent representations of a given $c^{*}$ algebra ( by bounded operators ) . in the case of a free scalar field the choice of a gaussian state defined by its characteristic function : $$ \omega_{\mathcal{f} } ( e^{\int\frac{d^3k}{e_k} z_{\mathbf{k}}a^{\dagger}_{\mathbf{k}} + \bar{z}_{\mathbf{k}}a^{\mathbf{k}} } ) = e^{\int\frac{d^3k}{e_k} \bar{z}_{\mathbf{k}} z_{\mathbf{k}}}$$ where $z_{\mathbf{k}}$ are indeterminates which can be differentiated by to obtain the result for any product of operators . the null vectors of this construction will be just combinations vanishing due to the canonical commutation relations ( like $a_1 a_2 - a_2 a_1$ ) . thus this choice has bose statistics . also subspaces spanned by a product of a given number of creation operators will be the number subspaces . the state of this specific construction is denoted by : $\omega_{\mathcal{f}}$ , since it produces the usual fock space . different state choices may result inequivalent quantizations .
which superpositions are allowed or forbidden in principle is determined by so-called superselection sectors . http://en.wikipedia.org/wiki/superselection these are virtually absent from the standard textbook literature as they cannot be inferred from the traditional perturbative treatment of quantum mechanics or quantum field theory . the superselection rule easiest to understand is the one that forbids the superpositions of a boson state and a fermion state . the reason is that these states transform differently under the action of the rotation group , hence there is no consistent action of the rotation group on the superposition . ( under a 360 degree rotation the boson half would rotate back to itself , while the fermion half changes its sign . ) this shows that superselection rules are tied to inequivalent representation of lie groups or lie algebras of quantities defining the physics of a system . the charge superselection rule is associated to inequivalent representations of an infinite-dimensional heisenberg group defining the canonical commutation relations ( ccr ) of a relativistic quantum field . [ my discussion assumes the bosonic case . in case of fermions , one needs instead the car , and s similar reasoning applies . ] here the reasoning is more intricate , and requires the nonperturbative setting of algebraic quantum field theory . in this setting , observables form a c^*-algebra , and states are suitable positive linear functionals of this algebra . the analogous standard qm situation is where the c^*-algebra is the algebra of bounded linear operators on a hilbert space , and a state is the expectation mapping $\langle a\rangle =tr ~\rho a$ with a positive semidefinite density matrix $\rho$ of trace 1 . here the uniqueness theorem of von neumann guarantees that the canonical commutation relations have a unique unitary representation up to unitary equivalence . however , this theorem only holds for ccr of finitely many operators , whereas field theory deals with infinitely many of these . the ccr of field theory have infinitely many inequivalent representations , and these live in different hilbert spaces , between the elements of which no sensible inner product is defined . as it makes no sense to consider superpositions between states of two different hilbert spaces ( not embedded in a common hilbert space with a physical meaning ) , inequivalent representations imply a superselection rule . this implies the charge superselection rule for qed , as it can be shown that in qed , states of different charge must lie in inequivaent representations .
it is a common misconception to think that because the higgs mechanism is the origin of mass it is also the origin of gravity . this is a misconception because the origin of gravity is not simply mass . instead it is a quantity called the stress-energy tensor . the stress-energy tensor is usually represented as a 4 $\times$ 4 matrix containing 10 independant entries ( 10 not 16 because the matrix is symmetric ) and in most cases the only significant entry is the top left one , $t_{00}$ , which gives the energy density . the key point is that as far as the stress-energy tensor is concerned mass and energy are the same thing related by einstein 's famous equation $e = mc^2$ . immediately before the electroweak transition all particles were massless , and immediately after they had a finite mass , but this change did not make any difference to the stress-energy tensor and therefore to gravity . before and after the transition the energy density was the same ( well similar anyway ) so the contribution of the particles to the stress-energy tensor and therefore to gravity was the same . so there is no contradiction between the higgs mechanism and the idea of entropic gravity .
yes . you have probably heard that string theory predicts the universe is 10 dimensional ( and m-theory predicts it is 11 dimensional ) while we only see 4 dimensions . however this is not because the number of dimensions has changed , but because 6 ( or 7 ) of the dimensions are rolled up into a very small circle . having said this , there have been suggestions that the universe started out as 2 dimensional ( 1 space and 1 time ) then the number of dimensions increased to 10/11/whatever as the universe evolved . however the idea comes from causal dynamical triangulation , and this is pretty speculative even by the standards of quantum garvity theories . there are even wilder suggestions that the spacetime dimension might be fractal .
in special relativity , you think of a 4-dimensional space-time . the key point here is that two events , 1 , and 2 happening at $t_{1} , x_{1} , y_{1} , z_{1}$ and $t_{2} , x_{2} , y_{2} , z_{2}$ have a distance given by ${}^{1}$ $$ ( \delta s ) ^{2} = -c^{2} ( \delta t ) ^{2} + ( \delta x ) ^{2} + ( \delta y ) ^{2} + ( \delta z ) ^{2}$$ now , we can therefore give any two events a unique relation to each other : 1 ) $ ( \delta s ) ^{2} &lt ; 0$: these events are considered timelike seperated . 2 ) $ ( \delta s ) ^{2} = 0$: the events are lightlike seperated 3 ) $ ( \delta s ) ^{2} &gt ; 0$: the events are spacelike seperated the key point is that , in all reference frames , timelike and null events happen in the same order ( this is derivable from the fact that all observers follow timelike paths ) . but , you can also show that there is no unique ordering of spacelike events--events that happen in order 1 > 2 > 3 in frame a may happen in order 2 > 1 > 3 in frame b . in fact , for any two spacelike seperated events , it is possible to find a reference frame where you can reverse the order in which they happen . so , the answer to your question is " maybe " , but the trick would have to be that you throw the second dart much more quickly than the first , and it hits the dartboard a distance x a way at a time t later than the first , in such a way that $t &lt ; x/c$ . you do this , then there will be a reference frame where it appears that the second dart hits first . ${}^{1}$here $\delta t = t_{2} - t_{1}$ , etc
well , by the presentation you give , you are going to have $\frac{d^{2}x^{i}}{d\tau^{2}}\neq 0$ , because you have those $\gamma_{0i}{}^{j}$ terms . for instance ${\ddot y} + 2\frac{\dot a}{a}{\dot y}{\dot t} = 0$ ( i abuse notation and mean the obvious things with dots , but obviously , $a = a ( t ( s ) ) $ and $y=y ( s ) $ ) the condition you want is $\gamma_{\mu\nu}{}^{i} = 0$ , with at least one $\gamma_{\mu\nu}{}^{0}\neq 0$ . i am sure there are metrics that satisfy this condition , but i do not know any ( non-trivial ones${}^{1}$ ) off of the top of my head . edit : note that even the minimally coupled " perturbative spherical potential " metric $ds^{2} = - ( 1-2\phi ( r ) ) dt^{2} + dr^{2} + r^{2}d\theta^{2} + r^{2}\sin^{2}\theta d\phi^{2}$ will have a nonzero component for $\gamma_{tt}{}^{r}$ , so it might be a bit tricky to find a nontrivial example . ${}^{1}$for instance , you could define $g_{ab} = -f ( t ) dt^{2} + \delta_{ij}dx^{i}dx^{j}$ . this will have a nonzero value for $\gamma_{tt}{}^{t}$ , but the space is really just minkowski space because it can be changed to it by the substitution $t = \int \sqrt{f ( t ) }dt$
the universe does not come equipped with any " clocks at each moment " that would immediately recognize different " moments " . indeed , the laws of physics - except for cosmology - are totally invariant with respect to translations in time , which is just a different way of saying that there is no way , even in principle , to find out whether an event occurred at time $t_1$ or $t_2$: the events are guaranteed to proceed in an identical way if the initial conditions are identical , whether the events occur at $t_1$ or $t_2$ . by noether 's theorem , this symmetry is inseparably connected with energy conservation . still , spacetime exists and events occur at different times , and the distances between events in time - duration of processes etc . - are well-defined numbers and can be measured by various tools . in particular , the proper time of world lines may be measured by all kinds of clocks . in the context of cosmology , the universe is expanding and the time-translational symmetry is broken by the expansion ( and so is the corresponding energy conservation law ) . it follows that one may define a " cosmic time " - the proper time of the " static " ( a frame in which the density of momentum of the cmb radiation vanishes ) world line stretched between a point of the big bang and a present event . this cosmic time is related to the current cmb temperature and many other things . still , the cmb temperature is obviously not accurate enough to measure the time separations of some very recent events from each other . in that case , we prefer things like atomic clocks that may achieve an unbelievable precision . archeologists use radioactive isotopes to determine the age of various things because it works and they do not use a more accurate method because no method that would be more accurate is available . pharaohs could have put ( and reset ) digital clocks into the pyramids when they built them except that they failed to do so and it is very hard to prosecute them for their negligence today . ; - ) at any rate , those questions are for geologists , historians , and archeologists - not physicists . if a historian asked whether physicists may come up with a better method , the answer is almost certainly no . but this is mostly a question about the creative engineering tricks and inventions that are possible given the known laws of physics ; it is not a question about physics itself . if we measure the age of objects and materials , we must look at the time-dependent changes of these objects and materials . chemistry does not bring us too far and nuclear physics is the way to go - we are back to various types of radioactive dating . as you can see , i probably do not understand the question - and i would bet that i will end up being in the majority - so i have no way to predicting whether the text above will be found satisfactory by hde .
starting from the hamiltonian formulation of qm one can derive the path-integral formalism ( see chapter 9 in weinberg 's qft volume 1 ) , where the hamiltonian action is found to be proportional to $\int \mathrm{d}t ( pv - h ) $ . for a subclass of theories with " a hamiltonian that is quadratic in the momenta " ( see section "9.3 lagrangian version of the path-integral formula " in above textbook ) , the term $ ( pv - h ) $ can be transformed into a lagrangian $l_h = ( pv - h ) $ . then the lagrangian action is proportional to $\int \mathrm{d}t l_h$ . both actions give the same results because one is exactly equivalent to ( and derived from ) the other . $$ \int \mathrm{d}t ( pv - h ) = \int \mathrm{d}t l_h$$ moreover , when working in the interaction representation you do not use the total hamiltonian but only the interaction . the derivation of the hamiltonian action is the same , except that now the total hamiltonian is substituted by the interaction hamiltonian $v$ . again you have two equivalent forms of write the action either in hamiltonian or lagrangian form . if you consider hamiltonians whose interaction $v$ does not depend on the momenta , then the $pv$ term vanishes and the above equivalence between the actions reduces to $$ - \int \mathrm{d}t v = \int \mathrm{d}t l_v$$ where , evidently , the interaction lagrangian is $l_v = -v$ this is what happens for instance in qed , where the interaction $v$ depends on both position and dirac $\alpha$ but not on momenta . note : there is a sign mistake in your post . i cannot edit because is less than 10 characters and i have noticed the mistake in a comment to you above , but it remains .
yes , as long as $b$ does not depend on time .
unless i am missing an easy way to do this problem it seems a surprisingly hard one . this diagram shows the problem ( i have exaggerated the altitude of the satellite to make the diagram clearer ) : the satellites are in circular orbits ( dotted line ) at a distance $r$ from the centre of the earth , so their orbital velocity as ( as you say ) : $$ v = \sqrt{\frac{gm}{r}} $$ assuming the two satellites stick to each other when they collide you end up with a single 500kg mass of twisted metal moving at some velocity $v'$ that is less than $v$ . the new orbit will be an ellipse , and the question is whether the new orbit intersects the earth 's surface . if the new orbit does not intersect the earth then the fused satellites will continue to orbit , while if the new orbit does intersect the earth obviously the debris will crash into the earth . the first step is to calculate v ' , and this is done simply by conservation of momentum . at the moment before the collision the momentum of the 400kg satellite is $400v$ and the momentum of the 100kg satellite is $-100v$ ( i am taking velocity positive to the right ) . after the collision the momentum of the wreckage is $500v'$ , so : $$ 400v - 100v = 500 v ' $$ and we get : $$ v ' = \frac{3}{5}v \tag{1} $$ the hard bit is working out the new orbit . for this you need to start with the vis viva equation , and with some head scratching you can work out the equation for the perigee distance ( $r_p$ in the diagram ) : $$ r_p = \frac{r_a}{\frac{2gm}{v'^2r_a} - 1} $$ the question is then simply whether $r_p$ is less than the radius of the earth . you know $v'$ from equation ( 1 ) , and $r_a$ is just the initial orbital radius ( measured from the centre of the earth ) . i will leave it up to you you calculate $r_p$ and answer the question .
your procedure gives : $$ a_{xz} = \sqrt{a_x^2 + a_z^2} $$ then : $$ a_{total} = \sqrt{a_y^2 + a_{xz}^2} $$ but if you substitute for $a_{xz}$ in the second equation you get : $$ a_{total} = \sqrt{a_y^2 + ( \sqrt{a_x^2 + a_z^2} ) ^2} = \sqrt{a_y^2 + a_x^2 + a_z^2}$$ so you do not need to split the calculation into two steps . your accelerometer may already exclude the acceleration due to gravity . if it does not then yes you need to use the inclination to work out the three components of gravity then subtract them from $a_x$ , $a_y$ and $a_z$ . it is hard to say exactly how to do this without knowing how your phone reports it is inclination . response to comment : suppose you have your device held flat so $a_z$ = -1 . now move the device downwards at and angle of $\theta$ as shown below : assuming it is moving in the $xz$ plane the value of $a_z$ will be decreased a bit and the value of $a_x$ will increase from zero . suppose you are applying an acceleration to the phone of $2g\space cos ( \theta ) $ - you will see why i have chosen this value in a moment . now the values of $a_x$ and $a_z$ are : $$ a_x = 2g\space cos\theta \space sin\theta $$ $$ a_z = g - 2g \space cos^2 \theta $$ you now calculate $a_{total}$ by just squaring and adding as we discussed above to get : $$a_{total}^2 = 4g^2 \space sin^2\theta \space cos^2\theta + g^2 + 4g^2 \space cos^4\theta - 4g^2 \space cos^2\theta $$ and a bit of rearrangement gives : $$a_{total}^2 = g^2 + 4g^2 \space cos^2\theta \left ( sin^2\theta + cos^2\theta - 1\right ) $$ and because $sin^2\theta + cos^2\theta = 1$ the quantity in the brackets is zero so you end up with : $$a_{total}^2 = g^2 $$ that is : $$a_{total} = g $$ which is the same as when the phone is stationary . so it is possible to be accelerating the phone and still have the total acceleration come out as $g$ ( $g$ = -1 in the phone 's units ) . that is why just subtracting one is not a reliable way to tell if the phone is accelerating .
the thing " in between " is an action principle . the most accessible explanation appears in feynman 's the character of physical law . see what&#39 ; s the interpretation of feynman&#39 ; s picture proof of noether&#39 ; s theorem ?
conformal field theories do not have a mass-gap , which is one of the assumptions [ for the strong conclusions of non-mixing of poincare spacetime symmetries vs internal symmetries ] of the coleman-mandula no-go theorem . similarly , for its superversion : the haag-lopuszanski-sohnius no-go theorem . [ in the supercase , the poincare algebra is replaced with the super-poincare algebra . ]
the classical field has a straightforward interpretation in the bosonic case--- it is determined by the density and phase of the superfluid condensate of the particles and both are simultaneously measurable in the thermodynamic limit . if there is no superfluid , the field is zero . where there is a superfluid , it is the square-root of the density , with a phase whose gradient is the local superflow . you can measure the value of psi-squared at x , that is determining the superfluid condensate density , which can be done by shining light through the fluid to get the index . you can also measure the value of the velocity by the exact way light refrects ( or if it is dense enough , by putting little dust specks in the fluid ) . a gaseous bose-einstein condensate , or even liquid helium , is described precisely by the classical limit of this formalism in the thermodynamic limit . if you take the classical limit wrong , by insisting that the particle number n is fixed , you do still get hbars lurking around . in order to take a good classical field limit , you need enormous occupation numbers for the field , which requires that you take n to infinity and the mass to zero keeping the density fixed . in this limit , the conjugate variables density/phase become commuting . as for measuring the " x-projection of operators " , i could not figure out what that meant . you can measure the field momentum too , of course , by just taking the complex conjugate of the measurement of the field ( the imaginary part of the field and the real part are not independent , and you can describe the whole system using only the real part and its time derivative , as described in the wikipedia article ) .
the first thing you should realize is the fact that while $\phi$ has an equation of motion with second time derivatives , it is not the wave function , and therefore there is no problem with qm . the field is just an operator ( more or less ) , not a state . acting with the fields on the vacuum state you generate the other states which do evolve with an hamiltonian built out of the operators such as $\phi$ itself . and the operators evolve accordingly to the usual heisenberg equation of motion $ [ h , \phi ( t , x ) ] =-i\partial_t \phi ( t , x ) $ ( and by lorentz symmetry , $ [ p_j , \phi ( t , x ) ] =-i\partial_j \phi ( t , x ) $ with $p_\mu= ( h , p_i ) $ as a 4 lorentz vector ) . from this heisenberg picture you can move to the schroedinger picture which it is as in non relativistic qm mechanics , the hamiltonian gives rise to time evolution of the states , $h\rightarrow -i\partial_t$ . the fact that the theory is lorentz invariant just adds other ( important ) things , but do not change what qm says . qft implements the principles of qm for a system with infinitely many degrees of freedom that can change the number of particles . all should become very clear if you realize that the lagrangian for a free scalar boson gives the hamiltonian for a collection of harmonic oscillators , one harmonic oscillator for every momentum $k$ with frequency ( aka energy ) $\omega^2=k^2+m^2$ . in case i find more time , i will add more details to this answer .
the key here is that you think there is no skidding . in fact , there is skidding , although for normal automobiles this is barely noticeable . for normal cars , the rear wheels simply skid a lot less than would the front wheels when a turn would be fully forced . you can see this also in trucks , where it becomes necessary to have dual or triple-axle steering when doing tight turns while manoeuvring .
since you are dealing with an inelastic collision , energy is not conserved when the bullet hits the block . you should try to find a relation between the initial velocity of the bullet and the velocity of the combined system ( bullet+block ) after the collision from conservation of momentum .
okay , this was really cool and i got some help from my physics professors on this one ( apparently i will not learn this until next semester ) and to find the magnitude of the square of a complex number you take it times it is complex conjugate . so in this case $$\frac{v_0}{i \omega l +r}$$ is multiplied with $$\frac{v_0}{-i \omega l + r}$$ leaving you with simply $$\frac{v_0^2}{w^2l^2+r^2}$$ , then just take it times your $$r_2$$ giving you $$\frac{v_0^2}{\omega^2 l^2+r^2} \times r_2$$ for the average power . sorry that my equations are not very pretty , latex is not working on my ubuntu install yet . . . . . hope this helps ! !
let $u_a$ and $u_b$ be the final velocities of car a and b respectively , and let $v_a$ be the initially velocity of car a . the conservation of momentum that the final momentum is the same as the initial momentum ( if there is not any external forces ) : $$ m_av_a=m_au_a+m_bu_b \tag1 $$ since this is an fully elastic impact , the energy is conserved : $$ \dfrac{1}{2}m_av_a^2=\dfrac{1}{2}m_au_a^2+\dfrac{1}{2}m_bu_b^2 \tag2 $$ with some algebra : $$ m_av_a^2=m_au_a^2+m_bu_b^2\\ m_av_a^2-m_au_a^2=m_bu_b^2\\ m_a ( v_a^2-u_a^2 ) =m_bu_b^2\\ $$ eq ( 2 ) becomes : $$ m_a ( v_a-u_a ) ( v_a+u_a ) =m_bu_b^2 \tag3 $$ here one uses $a^2-b^2= ( a-b ) ( a+b ) $ . eq ( 1 ) can be rewritten as : $$ m_a ( v_a-u_a ) =m_bu_b \tag4 $$ eq ( 4 ) can be inserted into eq ( 3 ) to get : $$ m_bu_b ( v_a+u_a ) =m_bu_b^2 $$ or $$ v_a+u_b=u_b \tag5 $$ plugging in that $m_a=2m_b$ and $u_b=4$ into eq ( 1 ) and eq ( 5 ) yields two equations : $$ v_a+u_a=4 \tag6 $$ $$ 2v_a=2u_a+4 \tag7 $$ eq ( 7 ) can be simplify to $$ v_a=u_a+2 \tag8 $$ plugging in eq ( 8 ) into eq ( 6 ) gives : $$ 2u_a+2=4 $$ which gives the result $u_a=1$ m/s . plugging $u_a$ into eq ( 8 ) gives $v_a=3$ m/s .
duration is certainly a more physical concept than time . duration is something you may measure between timelike separated events while time is always something you compute by adding up duration measurements + an arbitrary constant to fix the origin . duration is experimental and relational while time ( e . g . gps time ) is an abstract a posteriori construction . for these reasons i think your proposal is correct . it could happen that advanced theories get rid of most of time , but at some point these advanced theories will need to have some room for durations ( even if only in a limit ) . beyond this quantitative component of time , or rather more fundamentally , there is also the more qualitative notion of ordering of timelike separated events , linked to causality . the ordering does not require a continuous flow , e.g. discontinuous " pre-time " can work for this purpose . in particular if durations are discrete in a way or another . this would be quite a different concept for time . @brandonenright : in such a domain as modern gravitation theory , it is normal to have discussions on fundamental concepts because they need to be questioned and understood to see better what is useful in the postulates of the theory under construction . i understand you want to contain the pseudo-scientific spontaneous trends of discussion , but this not the case here . and about reverting to philosophy se , i would say yes if we were into metaphysics ( discussion on non-experimentable concepts ) . but here we are still in physics as all the assertions lend themselves to experimental tests , at least in principle
your textbook or lab manual will have what method they want you to use . but without more information , i would personally report the standard error of the mean , which is neither of the two methods you have shown . this method takes into account the number of measurements ; more measurements tends to result in a smaller reported error .
the book has a minus sign typo in the previous equation . the commutation in the step you are looking at does not introduce a minus sign , but the previous line had a term of the form $$ - c^r c^i b_r k_i$$ with a minus sign in front , which requires you to commute the two c 's past each other . the next step writes $$ - c^i c^r b_r k_i $$ incorrectly , it should be a plus sign , because the c 's are commuted , and then the next step says $$ c^i k_i c^r b_r $$ which fixes the error . you assumed that the minus sign came from moving the k , which does nothing , when it actually comes from commuting the c 's past each other . thank you for linking the book , it would have been impossible to find the error otherwise . given the unmotivated and needlessly formal introduction of brst in this book , i would recommend that you read the introduction to brst in the appendices of polchinski . the exercise in question does not require formal symbol manipulation gymnastics in an operator calculus , it is a simple question .
forget the webcam . attach the secondary mirror to the wall , at a height that is near the height of the center of the primary mirror . then adjust the horizontal and vertical tilt of the primary mirror to center the multiple images of the secondary mirror within each other on the primary mirror . if you have a cheap laser pointer and a carpenter 's square , you can set up the laser pointer so that it is exactly square to the wall ( vertically and horizontally ) and then adjust the primary mirror such that the laser beam goes back to the laser .
temperature generally gets cooler as you go higher in altitude ( which is one reason why you have snow on mountain peaks long after it has melted away in the foothills ) . hail develops in thunderstorms . a thunderstorm ~by definition~ is a storm which has developed through the freezing layer . so think about this : if you see lightning or hear thunder from a storm , you can be absolutely sure that at least part of that storm is above the freezing altitude , and at least part of the storm is below it . thunderstorms often develop on warm days ; and they also often develop on the edge of a cold front , where cold air is overtaking and riding over the less dense warmer air . a typical thunderstorm case : it is a very warm day , and the air might be quite humid . the warm moist air tries to rise to get higher than the cooler drier ( and therefore heavier ) air above it . the moist air rises . as it rises it cools past its dew point and moisture droplets precipitate . these droplets can be blown up by the rising air , higher than the freezing altitude , where they freeze . they start falling through the moist air , freezing a layer of moisture onto the small iceball , then may be blown up again by updrafts . the process can repeat many times as the little iceball goes up and down past the freezing level , each time growing by adding another layer of frozen moisture . eventually the little iceball is too heavy for the updrafts to keep it aloft in the freezing zone , and it falls to earth as hail .
the answer can be calculated by referring to band emission tables of any introductory heat transfer book . i use the heat transfer book by incropera and dewitt where are concept is explained and the tables given in chapter 12 . here is how its done . the fraction of total energy emitted as wavelengths from 0 to a certain wavelength $\lambda$ can be obtained from the band emission table ( $f_{ ( 0-\lambda ) }$ ) . this way the emissions from 0 to 400 nm ( $f_{ ( 0-0.4\mu m ) }$ ) and between 0 to 800 nm ( $f_{ ( 0-0.8\mu m ) }$ ) can be obtained . the difference in these two quantities gives the emission from 400 nm to 800 nm and is the required answer . the energy fractions $f_{ ( 0-\lambda ) }$ as a function of $\lambda$ and $t$ are given in the table here $t$ = 2,573 k , while $\lambda$ is the upper limit of wavelength .
actually , the answer is a bit more subtle than just density . the principle that is behind floating objects is archimedes ' principle : a fluid ( liquid or gas ) exerts a buoyant force , opposite apparent gravity ( i.e. . gravity + acceleration of fluid ) on an immersed object that is equal to the weight of the displaced fluid . thus , if you have an object fully immersed in a fluid , the total force it feels is given by ( positive sign means down ) : $f = gravity + buoyancy = \rho_{object} v g - \rho_{fluid} v g = ( \rho_{object} - \rho_{fluid} ) v g$ thus , if the average density of the object is lower than that of the water , it floats . if the object is partially immersed , to calculate the buoyant force you have to consider just the immersed volume and its average density : $f = \rho_{object} v g - \rho_{fluid} v_{immersed} g$ note that when i was talking about density , i was talking about the average density of the object . that is its total mass divided by its volume . thus , a ship , even if it is made out of high-density iron it is full of air . that air will lower the average density , as it will increase the volume considerably while keeping the weight almost constant . if you want to understand this better you can give the following problem a try : ) what is the height an ice cube of side l floats in water ?
i think the short answer is , you do not . the reason we call the unit of force a newton and not a kg m/s$^2$ is because it is convenient and it expresses the relation you want to convey when used elsewhere ( e . g . , $f=-kx$ for a spring ) . similarly , it is convenient to " hide " the mks base units into a single term , the potential $v$ in this case , so that the formula is easier to remember and that the relation is conveyed , in this case the relation between potential difference , current , and resistance .
please be aware that plutonium cores are supposed to be plated with another metal ( nickel or silver , if my memory serves me right ) . machining plutonium is very hazardous and is done with remote manipulators , since it increases risk of inhalation . source : http://toxnet.nlm.nih.gov/cgi-bin/sis/search/r?dbs+hsdb:@term+@na+@rel+plutonium,+radioactive : absorption through the skin can occur through occupational exposure . experiments show that the skin is an effective barrier and the percentage absorbed /seldom/ exceeds 0.05% for intact skin . [ seiler , h.g. , h . sigel and a . sigel ( eds . ) . handbook on the toxicity of inorganic compounds . new york , ny : marcel dekker , inc . 1988 . , p . 724 ] peer reviewed source : plutonium anl factsheet oct 2001 plutonium metal . plutonium isotopes are primarily alpha-emitters so they pose little risk outside the body . here the plastic bag , gloves , and outer ( dead ) layer of skin would each alone stop the emitted alpha particles from getting into the body . what happens to it in the body ? when plutonium is inhaled , a significant fraction can move from the lungs through the blood to other organs , depending on the solubility of the compound . little plutonium ( about 0.05% ) is absorbed from the gastrointestinal tract after ingestion , and little is absorbed through the skin following dermal contact . after leaving the intestine or lung , about 10% clears the body . the rest of what enters the bloodstream deposits about equally in the liver and skeleton where it remains for long periods of time , with biological retention half-lives of about 20 and 50 years , respectively , per simplified models that do not reflect intermediate redistribution . the amount deposited in the liver and skeleton depends on the age of the individual , with fractional uptake in the liver increasing with age . plutonium in the skeleton deposits on the cortical and trabecular surfaces of bones and slowly redistributes throughout the volume of mineral bone with time . what is the primary health effect ? plutonium poses a health hazard only if it is taken into the body because all isotopes but plutonium-241 decay by emitting an alpha particle , and the beta particle emitted by plutonium-241 is of low energy . minimal gamma radiation is associated with any of these radioactive decays . inhaling airborne plutonium is the primary concern for all isotopes , and cancer resulting from the ionizing radiation is the health effect of concern . the ingestion hazard associated with common forms of plutonium is much lower than the inhalation hazard because absorption into the body after ingestion is quite low . laboratory studies with experimental animals have shown that exposure to high levels of plutonium can cause decreased life spans , diseases of the respiratory tract , and cancer . the target tissues in those animals were the lungs and associated lymph nodes , liver , and bones . however , these observations in experimental animals have not been corroborated by epidemiological investigations in humans exposed to lower levels of plutonium . as a note , the common myth that plutonium is the “deadliest substance known to man” is not supported by the scientific literature . it poses a hazard but is not as immediately harmful to health as many chemicals . for example , for inhalation – the exposure of highest risk – breathing in 5,000 respirable plutonium particles , about 3 microns each , is estimated to increase an individual’s risk of incurring a fatal cancer about 1% above the u.s. average “background” rate for all causes combined . ) edit : as an aside : i recommend reading eileen welsome 's the plutonium files : america 's secret medical experiments in the cold war to get some idea of what early plutonium health safety experiments really meant ( e . g . injecting a solution of plutonium salt into a patient 's leg ) .
for what it is worth : http://www.coolmagnetman.com/magmeter.htm - a home-made device based on a hall effect device - for about $40 .
as lubos motl and twistor59 explain , a necessary condition for unitarity is that the yang mills ( ym ) gauge group $g$ with corresponding lie algebra $g$ should be real and have a positive ( semi ) definite associative/invariant bilinear form $\kappa : g\times g \to \mathbb{r}$ , cf . the kinetic part of the yang mills action . the bilinear form $\kappa$ is often chosen to be ( proportional to ) the killing form , but that need not be the case . if $\kappa$ is degenerate , this will induce additional zeromodes/gauge-symmetries , which will have to be gauge-fixed , thereby effectively diminishing the gauge group $g$ to a smaller subgroup , where the corresponding ( restriction of ) $\kappa$ is non-degenerate . when $g$ is semi-simple , the corresponding killing form is non-degenerate . but $g$ does not have to be semi-simple . recall e.g. that $u ( 1 ) $ by definition is not a simple lie group . its killing form is identically zero . nevertheless , we have the following ym-type theories : qed with $g=u ( 1 ) $ . the glashow-weinberg-salam model for electroweak interaction with $g=u ( 1 ) \times su ( 2 ) $ . also the gauge group $g$ does in principle not have to be compact .
normal matter structure is entirely constructed from the electronic bindings , so it is in the realm of the possible to engineer how the atoms are binded together exactly and this is the aim of lower-level nanotechnology . however , it is with current technogy and physics , impossible to create complex structures at lower scales ( i.e. : nuclear scale ) . and i do not think that is something that is in principle possible unless some dramatic breakthrough occurs in how we obtain larger-scale nuclear matter , which from the experimental limitations that there are to obtain high z nucleus that might probe the regions of higher islands of stability , one can safely infer that this is a very challenging problem in of itself update : this is not entirely related , but it shows an example of how assumptions as the one i have made above about manipulation of small scales being out of engineering reach can be twisted : this slide about non-homogeneous diffraction crystals shows how to do something that most physicists have thought for long to be essentially impossible ; x-ray and gamma-ray optics
you surely need to consider einstein 's field equations at some point because the stress-energy tensor in general relativity is defined as the object that is set equal to the einstein curvature tensor ( with the appropriate coefficient ) . so if you vary the whole action $$ \int \left ( \frac{r}{16\pi g} +{\mathcal l}_{\rm matter} \right ) \sqrt{-g}\ , d^d x $$ with respect to the metric , you will simply obtained einstein 's equations that has a multiple of the einstein tensor and a multiple of the stress-energy tensor defined in the way you mentioned . what you could have asked is why this definition of the stress-energy tensor is equivalent to other definitions you may know – i.e. non-gravitational ones . to answer this question , you would have to mention which other definition you mean . well , you would probably mean the stress-energy tensor that is locally covariantly conserved , $\nabla_\mu t^{\mu\nu}=0$ in the context of general relativity . you could derive this stress-energy tensor by the noether procedure by considering spacetime translations in non-gravitational theory , and then by covariantizing the result that you get in some right way . why is the conserved stress-energy tensor the same thing as the stress-energy tensor obtained by varying the metric ? they have to be the same because einstein 's equations set the stress-energy tensor equal to a multiple of the einstein tensor and the latter is covariantly conserved as a matter of identity . the equation $$\nabla_\mu g^{\mu \nu}=0$$ holds identically , for any metric tensor field configuration , and because the einstein tensor is set proportional to the stress-energy tensor , the same condition ( vanishing of the covariant divergence ) has to hold for the stress-energy tensor , too . this is no accident and this fact may be formulated in other ways . for example , we say that gravity has to use the diffeomorphism symmetry ( it is doubly important as the gauge symmetry in the quantum mechanical framework to get rid of the unphysical , negative-norm polarizations of the gravitons ) . such a local symmetry has to be coupled to a " conserved current " ( conserved stress-energy tensor ) for consistency . you will therefore typically end up with the same formula for both stress-energy tensors . however , they may end up differ by certain terms whose covariant derivative is zero identically as well . for example , the non-gravitational conserved stress-energy tensor is not always quite canonical and it does not even have to be a symmetric tensor . the gr definition of the tensor is automatically symmetric .
imagine that you are on a train , traveling at a steady speed of 50 miles per hour ( mph ) . your physics textbook on the table in front of you . now , you and the textbook ( and the train ) are all moving at the same speed . to an outside observer standing next to the train tracks , you and the book are each rushing by at 50mph . but , from your point of view , the book is not moving at all . that is , it is not getting closer to or farther from you . you are moving at 50mph relative to the observer next to the tracks . you are not moving , relative to the textbook . the book , the train , and yourself are not moving at all , relative to each other .
yes , perpetual motion that does not work is possible , and has been done in a famous soviet experiment . you put a superfluid in the interior of a macroscopic donut-shaped tube in the normal state , and let the tube spin along the donut axis , fluid plus pipe , then cool the thing down so that it becomes a superfluid . then you stop the tube from spinning . the superfluid will spin with no measurable loss essentially forever , even in imperfect conditions . it spun for many years without measurable loss in the actual experiment . this is quantum perpetual motion in your sense , and it does not require such an advanced civilization , just mid 20th century humans . the statement that perpetual motion is impossible is nowadays always interpreted to mean energy producing machine , not a motion that does not decay by friction .
at certain positions in the waves , the em field is zero and thus zero energy is stored at those positions . but at other positions , the em field is at a maximum , and those points are local maxima of energy . that pattern of oscillation between zero energy and maximum energy moves in the direction of propagation of the wave but never changes - in particular , the maximum value of the em field ( the amplitude ) stays constant , and there is no time at which the em field is zero everywhere . as for your conclusion from the definition of the poynting vector that the energy disappears at certain times : it is not correct , but i could not tell you why without seeing how you did it . what i can do is show the calculation for an electromagnetic plane wave , defined by $$\vec{e} ( z , t ) = e_0\hat{x}\sin ( kz - \omega t ) $$ the corresponding magnetic field is $$\vec{b} ( z , t ) = \frac{1}{c}\hat{k}\times\vec{e} ( z , t ) = \frac{e_0}{c}\hat{y}\sin ( kz - \omega t ) $$ since i am setting the direction of propagation as $\hat{k} = \hat{z}$ . check that this satisfies maxwell 's equations if you want . the energy density is $$\begin{align} u ( z , t ) and = \frac{\epsilon_0}{2}e ( z , t ) ^2 + \frac{1}{2\mu_0}b ( z , t ) ^2 \\ and = \frac{\epsilon_0}{2}\biggl ( e_0\hat{x}\sin ( kz - \omega t ) \biggr ) ^2 + \frac{1}{2\mu_0}\biggl ( \frac{e_0}{c}\hat{y}\sin ( kz - \omega t ) \biggr ) ^2 \\ and = \epsilon_0 e_0^2\sin^2 ( kz - \omega t ) \end{align}$$ using $\frac{1}{c^2} = \epsilon_0\mu_0$ . this energy density does vary from point to point , but at any fixed time , if you take the average over one cycle , a length $\frac{2\pi}{k}$ , you get $$k\int_0^{2\pi/k} u ( z , t ) \mathrm{d}z = k\int_0^{2\pi/k} \epsilon_0 e_0^2\sin^2 ( kz - \omega t ) \mathrm{d}z = k\epsilon_0 e_0^2 \frac{\pi}{k} = \pi\epsilon_0 e_0^2$$ which does not depend on time . so the average energy density is constant , it does not ever go to zero .
why they are not on the geostationary orbit ? it is because the geostationary orbit - and indeed , there is just one such orbit , a one-dimensional curve in space - only exists above the equator while the gps has to cover the whole planetary surface , including the points closer to the poles . however , it is untrue that the gps satellites are located at the leo , either . low earth orbit is defined as 160-2000 kilometers of altitude . however , the gps satellite constellation is located roughly 20,200 km above the surface - over one half of the geostationary radius - in such a way that the position of each satellite returns to the same place twice per 24 hours . this is very convenient for synchronization and planning . the galileo satellites will be at altitude 23,222 km . it is also an intermediate circular orbit , much like for the 19,100 altitude of the glonass whose orbital period is about 11 hours .
actually that is not too far off the mark , although i am not sure " knot " or " kink " is the best word . quantum field theory , the best theory we currently have to describe particles , says that particles correspond to excitations of a field , which are kind of like waves in water ; you could consider the surface of a pond " excited " whenever it is not flat . just as with water waves , there is an infinite variety of " shapes " you can have for these excitations . for example , you could have a repeating wave , in which the surface of the water cycles up and down over a large area , or you could have just one wave front that just propagates across the water without spreading out very much . the former case is pretty typical for things like light waves , and the latter case is pretty typical for particles of matter , although technically any kind of field ( whether it is the electromagnetic field for light , or a quark field in matter , or whatever ) can have any of the different types of excitations . by the way , according to special ( and general ) relativity , even an object that is standing still is moving through time . so all of these excitations move through spacetime in one way or another . but only certain ones ( the excitations in fields corresponding to massless particles ) can move through space in such a way that they appear to us to be traveling at the speed of light .
both formulas are equivalent , if you are in the electrostatic approximation and your dipole vector does not depend on the position $\mathbf{r}$ . let 's consider the expression $\mathbf{f}=\nabla_{\mathbf{r}} ( \mathbf{p} \cdot \mathbf{e} ) $ which can be easily obtained from the potential energy function $u=-\mathbf{p} \cdot \mathbf{e}$ and its relation with the force $\mathbf{f}=\nabla_\mathbf{r} u$ . now , recall the vector identity $\nabla_\mathbf{r} ( \mathbf{a}\cdot \mathbf{b} ) = ( \mathbf{a} \cdot \nabla_\mathbf{r} ) \mathbf{b}+ ( \mathbf{b} \cdot \nabla_\mathbf{r} ) \mathbf{a} + \mathbf{a} \times ( \nabla_\mathbf{r} \times \mathbf{b} ) + \mathbf{b} \times ( \nabla_\mathbf{r} \times \mathbf{a} ) $ for $\mathbf{a}=\mathbf{a} ( \mathbf{r} ) $ and $\mathbf{b}=\mathbf{b} ( \mathbf{r} ) $ two arbitrary vectors . for $\mathbf{p}=\mathbf{a} \neq \mathbf{p} ( \mathbf{r} ) $ [ independent of the position ] and $\mathbf{b}=\mathbf{e} ( \mathbf{r}$ ) we have $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}+ ( \mathbf{e} \cdot \nabla_\mathbf{r} ) \mathbf{p} + \mathbf{p} \times ( \nabla_\mathbf{r} \times \mathbf{e} ) + \mathbf{e} \times ( \nabla_\mathbf{r} \times \mathbf{p} ) $ as the dipole vector does not depend on the position we can drop the second and the fourth terms . in the electrostatic approximation , faraday 's law reads $\partial_t \mathbf{b}=\mathbf{0}\leftrightarrow \nabla_\mathbf{r} \times \mathbf{e} ( \mathbf{r} ) =\mathbf{0} $ [ this is known as ''carn 's law'' ] so that the electric field is irrotational and the curl vanishes . then we can drop the third term and $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}$ so that your definitions agree .
i think you are misinterpreting the statement that " it does not have any effect " . this statement does not mean that the faddeev-popov methodology " does not work " , as you wrote later . instead , it means that it is completely unnecessary . if you look at the faddeev-popov ghosts ' lagrangian , you will see that for abelian groups , the structure constants $f_{abc}$ vanish and we are left with $$ {\mathcal l}_{\rm ghost} = \partial_\mu \bar c^a \partial^\mu c^a $$ which means that the ghosts are completely decoupled . they do not interact with the gauge fields ( photons ) . you may still use the faddeev-popov machinery and the brst formalism based upon it to identify the physical states as the cohomologies of $q$ , the brst operator . but what this brst machinery tells you is something you may easily describe without any faddeev-popov ghosts , too . it just tells you that the excitations of $\bar c , c$ are unphysical much like the excitations of time-like and longitudinal photons . that is why the brst problem in the case of abelinan gauge groups is " solvable " in such a way that you may simply eliminate the ghosts completely , together with 2 unphysical polarizations of the photon . and that is why qed may be taught without any faddeev-popov ghosts and one may still construct nice feynman rules for any multiloop diagrams . for non-abelian theories , the counting still works – ghosts , antighosts , and two polarizations of gluons etc . are unphysical . however , because there are interactions of ghosts with the gluons in that case , there is no easy way to describe the physical states without the faddeev-popov ghosts .
how galilean transformations which are wrong ( are approximately correct ) give the correct answer for k ? the lorentz prediction and the galilean prediction must agree in the limit that $v \to 0$ ( or in the limit that $c \to \infty$ ) . this is because $v=0$ corresponds to no transformation at all , so they had better both agree there . so if you take the transformation and evaluate it for smaller and smaller $v$ , you will find that $k=1$ still has to be true . why we should assume that there are two electric fields , one in the lab frame and one in the other , but just one magnetic field in both frames ? that is just the galilean transformation of the em field . to see how it relates to the relativistic case , the lorentz transformation of the em field is : $$\mathbf{e}' = \gamma \left ( \mathbf{e} + \mathbf{v} \times \mathbf{b} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{e} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ $$\mathbf{b}' = \gamma \left ( \mathbf{b} - \frac {\mathbf{v} \times \mathbf{e}}{c^2} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{b} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ when you take the limit that $c \to \infty$ , we know that $\gamma \to 1$ , so it just becomes : $$\mathbf{e}' = \mathbf{e} + \mathbf{v} \times \mathbf{b}$$ $$\mathbf{b}' = \mathbf{b}$$
your reasoning contains some errors and unjustified assumptions . the first error is to think that the universe started out with a size of about the plank length . this may not be the case . if it is flat and infinite now then it would always have been flat and infinite , even at the beginning , or at least as far back as the point where it makes sense to talk about space-time in such terms . it is true that the observable universe would have started from a very small point , but the whole universe is likely to be much bigger . even if the universe is curved and finite in size its initial size could have been anything from much smaller than the plank size to much larger . your second error is to think that a flat universe has to be infinite . it is true that a universe with constant positive curvature over space must be finite , but the converse is not true . a flat universe or even one with negative curvature can be finite if it repeats with periodic boundary conditions . for a flat space the simplest topology that can have this property is the 3-torus . you are also making the assumption that the cosmological principle holds on all scales no matter how large . our observations of the observable universe suggest that this principle is quite reasonable on scales up to billions of light years , but we can not say anything for sure about what the universe is like on much larger scales . the curvature of space may vary in quite dramatic ways beyond the horizon that limits how far we can see due to the finite speed of light . all four of your options are possibilities and there are too many in the " other " category to elaborate .
i would expect metals to cool faster through convection because of a related heat property of theirs : conduction . metals are generally good heat conductors so as heat energy is removed from the surface of the metal by air , internal heat energy in the metal can quickly flow to the surface . the rate heat flows between two systems is dependent on their temperature difference . when air removes heat via convection , the surface of the object is cooler than the central portion of the object . if heat in the object flows slowly from the center to the surface like in many plastics then the surface stays closer to air temperature and heat transfers to the surrounding air more slowly . if the object is a metal , the heat removed at the surface is easily replaced by heat flowing from the center to the surface and the temperature delta between the surface and the air is greater so the object cools faster .
as brandon mentioned , two small objects could not orbit each other near a significant gravitational field . the hill sphere " approximates the gravitational sphere of influence of a smaller body in the face of perturbations from a more massive body . " therefore , your pebble 's hill sphere would be too small to permit orbits near earth . the wiki article has a calculation showing that an astronaut could not orbit the 104 tonne space shuttle 300 km above the earth since the shuttle 's hill sphere was only 120 cm .
to answer briefly , i am not very confident in this answer and invite editing or downvoting as appropriate ! . the energy levels of the electrons are determined via calculations of the electric potential/field around the nucleus . the electric field is a vector field that transforms under special relativity and hence we account for any relativistic effects through moderation of this field .
suppose you had three electrons , with individual wavefunctions $\lvert \psi_1 \rangle$ , $\lvert \psi_2 \rangle$ , and $\lvert \psi_3 \rangle$ . let them all have the same $\vec{x}$ , $l$ , and $m$ , so they can only differ in intrinsic spin . since spin is a two-dimensional hilbert space , as you noted , then three vectors must be linearly dependent . that is , there exist complex numbers $\alpha$ and $\beta$ such that $$ \lvert \psi_3 \rangle = \alpha \lvert \psi_1 \rangle + \beta \lvert \psi_2 \rangle . $$ now the state of all three electrons is given by the tensor product $$ \lvert 1,2,3 \rangle \equiv \lvert \psi_1 \rangle \otimes \lvert \psi_2 \rangle \otimes \lvert \psi_3 \rangle . $$ the tensor product respects the structure of the underlying vector spaces , which is a fancy way of saying we can write $$ \lvert 1,2,3 \rangle = \alpha \lvert 1,2,1 \rangle + \beta \lvert 1,2,2 \rangle . $$ but $\lvert 1,2,1 \rangle$ , as a product state of identical fermions , is antisymmetric under interchange , in particular under interchance of the $\lvert \psi_1 \rangle$ components : $$ \lvert 1,2,1 \rangle = - \lvert 1,2,1 \rangle . $$ thus $\lvert 1,2,1 \rangle \equiv 0$ . similarly , $\lvert 1,2,2 \rangle$ vanishes . we have just shown that our 3-electron wavefunction is a linear combination of $0$-vectors , and so it too vanishes . this argument easily generalizes to more than three electrons by appending the combined wavefunction of all the others to the end of our state and just carrying it through the computations ; that is , just do the same thing splitting up $\lvert \psi_3 \rangle$ , but apply it to $$ \lvert 1,2,3 \rangle \otimes \lvert 4 , \ldots , n \rangle . $$
i would guess that the professor is explaining his/the ( ? ) theory that dark matter is neutrinos , produced via a scattering process he calls " witten 's dog " . it is funny because the neutrinos are coming out of the dog 's butt . in the standard humor classification , this is known as a " poop joke " .
i got it . $\epsilon$ anticommutes with $b_a$ .
the blades of a ceiling fan are pitched out of plane slightly . as a result , when the fan spins , the blades push air either up towards the ceiling or down towards the floor . which direction it pushes air is determined by the direction the fan is spinning , and the direction the blades are pitched . the usual convention is given by the right hand rule : if you hold your right hand so that you can curl your fingers in the direction the fan is spinning , then it will push air in the direction that your thumb is pointing . 1 when it is pushing air down on you , it will then be spinning in a counter clockwise direction as you look up at it . you can make it follow a left hand rule by reversing the pitch of the blades . once you have set the direction of the pitch of the blades , you can reverse the airflow by reversing the direction the fan spins . many ( most ? ) modern ceiling fans provide some mechanism to do this . the fans in my house have a small black switch that slides up and down . @ignacio vazquez-abreams mentions using a pull chain in a comment to another answer . in warm weather , you set the fan so that it pushes air down towards the floor . this causes you to feel a breeze , which cools you . in cold weather , you set the fan so that it pushes air up into the ceiling . you do not feel a breeze , 2 but it circulates warm air from the ceiling towards the walls and down towards the floor . box fans usually follow the right hand rule as well . you may find it easier to check some of this if you have a box fan handy . you can walk around the fan and hold a tissue in front of it from both side while watching the blades spin . you can check this with a box fan . set the box fan on the floor , and turn it on . it will pull air from one side and push it out the other . you feel a breeze on the " out " side and feel a much weaker breeze ( if any ) on the " in " side .
$\newcommand{\er}{\hat e_r} \newcommand{\et}{\hat e_\tau} \newcommand{\d}{\dot} \newcommand{\m}{\frac{1}{2}m} $ in radial coordinates , $\d\er=\d\theta \et$ , and ( useless here ) $\d\et= -\d r \er$ . $\er , \et$ are unit vectors in radial and tangential directions respectively . due to this mixing of unit vectors ( they move along with the particle ) , things get a little more complicated than plain ' ol cartesian system , where the unit vectors are constant . for your particle , writing $x+l\to r$ , the position vector is : $$\vec p= r\er$$ $$\therefore \vec v=\d{\vec p}= \d r\er + r\d\er=\d r \er + r\d\theta\et$$ $$\therefore v^2= \vec v\cdot\vec v= \d r^2+r^2\d\theta^2$$ substituting back the value of $r=x+l , \d r=\d x$ ( and mutiplying by $\m$ , we get the above expression ? as you can see in my expression for $\vec v$ , i had two components of velocity--radial and tangential . since they are perpendicular , i can just square and add , akin to $t=\m\left ( \d x^2 +\d y^2\right ) $ . the point is , it may be a scalar , but it contains a vector in its expression:$$t=\m v^2=\m|v|^2=\m \vec v\cdot \vec v=\m ( \dot x^2+\dot y^2 ) $$
by ' density ' in this case i think you just mean " something on a manifold that can be integrated to give you a scalar " . by this definition , on an $n$-manifold , a density would be an $n$-form ( since if you integrate over a form of lower dimension you get zero ) . so in your 3d case , take 3 smooth functions $f , g , h:m^3\to \mathbb{r}$ , the form $df\wedge dg\wedge dh$ is a density . now , in your example you are integrating over a scalar field multiplied by a 3-form , which is again a 3-form , which can be integrated over a 3-manifold to get you the change in the region . but the scalar field $\rho:m^3 \to\mathbb{r}$ is not a density ( not a 3-form ) , so it cannot be integrated over to find the total charge . the charge density is $\omega=\rho dx\wedge dy\wedge dz , $ and $\rho$ just tells us how ' big ' this should be . in other words , the mathematical term ' density ' can be stated as '$n$-form on $m^4$' , whereas the colloquial ' density ' for ' something per unit length/area/volume ' is shorthand for what we really mean ( $n$-form ) .
classically a non-pointlike spinning charged object possesses a magnetic dipole moment due to the fact that charged particles in the object are spinning around some axis . in contrast , the electron has a dipole moment that arises from its intrinsic spin angular momentum . as you point out , the electron has no internal structure , so the spin does not refer to actual physical spinning . the dipole moment has spatial dimensions outside of the point where the electron exists because it arises from the quantum spin property of the electron , it is not itself a property of the electron . the magnetic dipole moment of the electron is related to its spin in the way described here .
the influence of gravity is pretty much negligible . gravity is only there to guarantee that the mercury gets concentrated on the right ( lower ) side of the glass cavity and there are no bubbles in it . but in the thin glass tube , this is guaranteed by the surface tension , anyway : liquids do not want to create holes in them even in the absence of gravity . as long as the gravity is strong enough to keep the mercury down and keep the air up , it does not matter how strong it is . at least the impact is not detectable for changes of gravity we may achieve on earth . at most , a stronger gravitational field could compress the mercury a little bit but because the liquid is nearly incompressible anyway , you will not be able to see much of a decrease . the thermometer is a thermo-meter , not a gravity-meter , so it should not be shocking that gravity 's influence on its reading is negligible . it has to be so for the thermometer to fulfill its role . if your thermometer is placed horizontally , it is not a real problem because the surface tension is usually stronger than gravity and it prevents the air from penetrating " into " the mercury . the surface tension also keeps the shape of the mercury surface in the thin tube , relatively to the glass , more or less independent of the direction of the gravitational field . if you shake with a medical mercury thermometer intensely enough , you can dislocate the mercury at the end .
in my experience this is entirely possible in a ' warmer ' climate ie southern britain . i can give you advice on that basis , maybe someone else will advise on dealing with the cold ! can i suggest something like the robodome , this is an automated dome to house the scope ( upto 10" aperture is suitable but tight . ) which can be synced to a weather station and the telescope controls . operating can be done fairly simply via r232 and or usb cables , the usb will need to be powered to deal with the distance . remote operation is no different really from a wired connection while stood next to the scope . but ! remeber that a scope drive will have enough power to crush fingers , mains electricity is enough to kill and if something goes wrong you could a lot of equipment at the mercy of the weather . so some kind of safety plan is needed . . . minimum list : dome , scope , ccd , webcam , cabling , weather station , motorised mount . [ to track add a tracking camera/on ccd chip . ] setting up your own observatory is a lot of fun though , sitting in the warm is definitely the way to go !
the electric field is the vector sum $$ {\vec e}~=~\frac{1}{4\pi\epsilon_0}\big ( \frac{q_1{\bf n}_1}{2d^2}~+~\frac{q_2{\bf n}_2}{d^2}\big ) $$ so the components of the field are $$ e_x~=~\frac{1}{4\pi\epsilon_0}\frac{q_1}{2\sqrt{2}d^2} $$ $$ e_y~=~\frac{1}{4\pi\epsilon_0}\frac{q_1}{d^2}\frac{1~+~2\sqrt{2}}{2\sqrt{2}} $$ the rest is plug and grind on numbers .
the sound speed $c_s$ of any fluid in which the fluid pressure $p$ is a function of the fluid density $\rho$ is going to be given by $c_s^2 = \frac{dp}{d\rho}$ . if that expression is not already familiar to you , it is a good exercise to write down the fluid equations for conservation of mass and momentum in one dimension and then write down what happens when the density and velocity are perturbed from their equilibrium , static values . you will get the wave equation for the perturbed values with the wave speed given by the above equation . if $\rho$ can be treated as a function of $a$ , then applying the chain rule gives $c_s = \sqrt{\frac{dp}{da} \ , \frac{da}{d\rho}}$ . if $a$ is proportional to $\rho$ with the same proportionality constant everywhere , you get the expression you have asked about . even if the relationship between $a$ and $\rho$ is not exactly that simple , you had expect to get the same answer to order of magnitude since $a$ and $\rho$ should change by values comparable to themselves . edit : it turns out that you can get the same expression for the sound speed if $a$ is inversely proportional to $\rho$ . in that case both $\frac{dp}{da}$ and $\frac{da}{d\rho}$ are negative , which might be useful to keep in mind . and an inverse relationship is exactly what you had expect if the flow is steady or nearly so ( $\rho$ and $u$ at each position do not vary with time ) . in that case conservation of mass implies that $\rho u a$ is a constant at each position . and if $u$ is not varying much with position , then you indeed get $a \propto 1/\rho$ .
no , the temperature of the water is not that important for the performance of an evaporative cooler . this is basically because the energy needed to increase the temperature of liquid water ( its specific heat capacity ) is very small compared to the energy needed to evaporate the same amount of water ( its enthalpy of evaporation ) . at room temperature the specific heat of liquid water is 4.18 j/ ( g$\cdot$k ) while the ethalpy of evaporation is 44.0 kj/mol . since the molar mass of water is roughly 18 g/mol , this means that approximately 585 times as much energy is needed to evaporate an amount of water as to increase the temperature of the same amount of water by 1 k . this means that even if the water starts at freezing temperature , is heated to 40 $^\circ$c ( 104 $^\circ$f ) and then evaporates , less than 7% of the energy absorbed by it is used for increasing the temperature . the temperature of the water might effect the rate at which evaporation occurs , but i guess evaporative coolers are designed to achieve 100% relative humidity in the wet bulb regardless of the temperature of the water , so this probably does not affect the performance .
when alpha particles ( helium nuclei ) or beta particles ( electrons ) are released in a radioactive decay they carry significant kinetic energy . as they go through the surrounding material they bump here and there occasionally kicking off some electrons from the surrounding atoms which are then ionised , until they finally stop .
perhaps this is just rephrasing your last explanation , so i am not sure if you consider this as a " better argument " , but i will give you a good reference for further reading . quantum gravity may break global symmetries because the global charge can be eaten by virtual black holes or wormholes , see this paper .
a simple method to judge the chirality ( or in your words " orientation" ) of the hamiltonian is to evaluate the following quantity $$f=\frac{i}{2}\mathrm{tr}\frac{\partial h}{\partial{q_x}}\frac{\partial h}{\partial{q_y}}\frac{\partial h}{\partial{m}} . $$ the sign of this quantity $f$ gives the chirality of the hamiltonian . example : given the two hamiltonians $h_1=q_y\sigma_x-q_x\sigma_y-m\sigma_z$ and $h_2=-q_y\sigma_x-q_x\sigma_y+m\sigma_z$ , we can evaluate $$f_1=\frac{i}{2}\mathrm{tr} ( -\sigma_y ) \sigma_x ( -\sigma_z ) =1 , $$ $$f_2=\frac{i}{2}\mathrm{tr} ( -\sigma_y ) ( -\sigma_x ) \sigma_z=1 . $$ because $f_1$ and $f_2$ are of the same sign , so $h_1$ and $h_2$ are of the same chirality . the reason that this trick works is that it basically estimates the berry curvature , which is defined as $$f=\frac{i}{2}\mathrm{tr}\ , g^{-1}\mathrm{d}g\wedge g^{-1}\mathrm{d}g\wedge g^{-1}\mathrm{d}g , $$ where $g= ( i\omega - h ) ^{-1}$ is the single particle green 's function . the chern number is then simply an integral of the berry curvature , i.e. $c=\frac{1}{2\pi}\int f$ . since the berry curvature mostly concentrated around the origin of the momentum-frequency space , one just need to estimate the berry curvature at that point to determine the sign of the chern number . while in the formula for $f$ , $$g^{-1}\mathrm{d}g = ( i\omega-h ) d ( i\omega-h ) ^{-1}\sim g dh , $$ which gives the $\mathrm{d} h$ terms . and because the hamiltonian is gaped , so in the zero momentum and frequency limit , the green 's function is a constant $g\propto \partial_m h$ that is proportional to the mass term . putting all these pieces together , and to the leading order of momentum and frequency , we find the berry curvature can be roughly estimated from $f\sim f$ . so the quantity $f$ is of the same sign as the chern number , and can be used to determine the chirality of the hamiltonian . this estimate is exact around the dirac point , which is just the case of the examples you provided .
 the magnetic field lines are in the same direction as the upwards velocity of the electrons , so the v × b term in the lorentz force is 0 . magnetic field lines are curved , so they cannot be everywhere in the same direction . magnetic field in the metallic wire is not everywhere parallel to the velocity of the wire and produces electromotive intensity $\mathbf v\times \mathbf b$ circulating in the horizontal plane that is non-zero both below and above the center of the magnet . near the center of the magnet , the induced intensity is low due to fact you mention , i.e. velocity is parallel to magnetic field .
yes , everything generates a gravitational field , whether it is massive or massless like a photon . the source of the gravitational field is an object called the stress-energy tensor . this is normally written as a 4 x 4 symmetric matrix , and the top left entry is the energy density . note that mass does not appear at all . we convert mass to energy by multiplying it by $c^2$ ( as in einstein 's famous equation $e = mc^2$ ) and then put in the energy . so even a photon generates a gravitational field because although it has no mass it does have energy . it is surprising what else is in the stress-energy tensor and can therefore generate a gravitational field . for example pressure and shear stress appear . it is even been suggested that a gravitational field could be generated by gravty itself i.e. the energy of the gravitational field generates the curvature that creates the field . the resulting object is called a geon , though i should emphasise that no-one has proved these could exist and most of us think they probably can not .
the key to this problem is the fact that the planet 's mass $m$ as it appears in newton 's law of gravitation , $$f=\frac{gmm}{r^2} , $$ is not actually constant . this is because the layers of the planet that are above you cause zero net force : if you are inside of a hollow spherical shell of mass then diametrically opposite elements of solid angle exert equal forces in opposite directions . thus , the effective mass of the planet in this problem is only that of a sphere of radius $r$ and density $3m_0/4\pi r^3$ , i.e. $m ( r ) =\frac{r^3}{r^3}m_0$ . the force is then $$f=\frac{gm_0m}{r^3}r$$ and it of course causes harmonic motion , with " spring constant " $k=gm_0m/r^3$ .
the reason is that the exposure on the camera is set so that the main subject of the image is properly exposed , ie not too dim and not too bright . because the typical objects being photographed are quite bright , the image detector ( camera ) will not get enough light from the stars for them to show up .
well , immediately after the earthquake , they created the map showing when the tsunami arrives at different places : i guess that it ended up being pretty accurate but i have not checked . the speed of ocean waves actually depends on the frequency . . . your ordering " tsunami and earthquake " is somewhat bizarre . you do acknowledge and realize that the tsunami was a consequence of the earthquake , do not you ? ; - ) predicting earthquakes themselves is not really possible . i think that no one knew about the japanese earthquake until the very moment when it took place .
for a fiber to guide light , even considering the situation using only geometrical optics , there must be total internal reflection ( tir ) at the boundary of the fiber core . for tir to occur , the angle of incidence of the light must be greater than the critical angle $\theta_c$ . if the material before the boundary has index of refraction $n_1$ and the material after the boundary has index $n_2$ , then the critical angle is : $$ \theta_c = \arcsin ( \frac{n_2}{n_1} ) $$ of course , if $\frac{n_2}{n_1} &gt ; 1$ then the critical angle is undefined -- tir cannot occur . this is why tir is only possible when light encounters an interface with a material of lower index . this should be familiar to you if you think about it : you can see the bottom of a pool from above the water , but the water surface looks silvery and reflective from underneath ; the reflective face of a roof prism looks reflective when you try to look through it from the glass-to-air side , but if you look from the air side , you can see clearly . on its own , this does not require a fiber to have cladding . air has an index very near 1 , so any other material will have a higher index , and therefore guide light without a cladding . however , this situation would be very unstable . any material contacting the fiber could produce an area where tir does not occur , and suddenly the fiber would begin leaking . consider how easy it would be for this to happen ! any sort of dirt or oil on the fiber would either eliminate tir completely , or raise the critical angle such that some of the light could escape . the same would be true of most materials you might use to mount the fiber , or even any type of coating you might apply as a mechanical barrier to dirt . thus , in practice , fibers have a cladding to ensure that the optical guiding characteristics continue to work in real world conditions . they also provide engineers with another way of adjusting the properties of the fiber , so its definitely not something we are unhappy about . do keep in mind though , that there are examples of unclad " fibers " although they are usually just acrylic rods used in teaching demonstrations , so i would not really call them a fiber in the common sense of the word .
you are on the right way with your line of thought . the only part of the puzzle you are missing is , as you have correctly identified , the explanation of $$\oint{\overrightarrow{r_0} \times d\overrightarrow{f}} = \overrightarrow{r_0} \times\oint{ d\overrightarrow{f}}$$ it is actually very simple : $\overrightarrow{r_0}$ is a constant vector ( it does not depend on coordinates ) and as such you can factor it out of the integral . rewriting the vectors in their component elements does nothing for you . i am sure you know of the fact that you can factor constants out of integrals , but just for completeness , here is a link to the wikipedia entry for this rule called the constant factor rule in integration .
you are right that the result you see is due to the chain rule . the author uses either spherical or cylindrical coordinates , so \begin{equation} r = \sqrt{x^2 + y^2 + z^2} \end{equation} or \begin{equation} r = \sqrt{x^2 + y^2} \end{equation} which you can differentiate to obtain \begin{equation} \frac{\partial{r}}{\partial{x}} = \frac{x}{r} \end{equation} hence \begin{equation} f_x ( r ) = -\frac{\partial{u ( r ) }}{\partial{x}} = -\frac{\partial{u ( r ) }}{\partial{r}}\frac{\partial{r}}{\partial{x}} = -\frac{x}{r} \frac{\partial{u ( r ) }}{\partial{r}} \end{equation}
if you consider a homogeneous piece of silicon the total flow of electrons through it is : $$ i = \frac{u}{r} = n \mu \frac{s}{d} u $$ where $r$ - the resistance of the piece , $u$ - external voltage applied to it . the resistance depends on : $n$ - the concentration of electrons ( number of electrons per m$^3$ ) , $\mu$ - the mobility of electrons ( ratio of velocity of the electron and electric field that makes it move ) , $s$ and $d$ - cross-section and length of the sample . the changes of geometric size with temperature are negligible . so the values that affect the resistance are concentration $n$ and mobility $\mu$ . the mobility depends on the temperature and also on the concentration of various defects in the sample . at 100 f the temperature dependence is dominating . the concentration is the most complicated point . there are the following cases : pure silicon . all the electrons ( and the same amount of holes ) are thermally generated . their concentration depends on the temperature exponentially . if you need total current do not forget about the holes . silicon doped with donors . the amount of thermally generated electrons is negligible . the concentration does not depend on the temperature . semiconductor device with p-n junction or/and heterojunction ( connection of different materials ) . the laser/led of optical computer mouse is this case . the sample is not homogeneous and the concentration is determined mainly not by temperature but by more interesting things like voltage polarity . this case requires more formulas and exact data concerning the sample structure . ! the laser is made of gaas and similar materials not silicon . the attempts to make silicon laser never stop though . edited ( 2011/12/15 ) : for the temperature dependence of electron mobility wikipedia gives $$ \mu ( t ) \approx \mu_0 t^{-2.4} $$ where $\mu_0 = 9.46 \cdot 10^{6} \text{m}/\text{ ( v s ) }$ , hope i have calculated it correctly from first point set ( black circles ) here this formula takes into account only electron scattering on the oscillations of the ions of the crystal . this effect is dominating at room temperature and higher . the temperature must be in kelvin degrees here . for the electron $n$ and hole $n_h$ concentration in pure silicon ( case 1 . ) at room temperature and higher one can use the following formula : $$ n = n_h = n_\text{eff} \ ; t^{\ ; 3/2} \exp \left ( -\frac{e_g}{2k_b t} \right ) $$ where $n_\text{eff}$ - some constant describing the shape of conduction and valence bands of silicon ( i have not found explicit value yet ) , $t$ - temperature in kelvin degrees , $e_g = 1.12\ ; \text{ev} = 1.79 \cdot 10^{-19} j$ - energy gap of silicon , $k_b$ - boltzmann constant .
nick , do not be surprised that this is confusing . there are a lot of concepts intermixed in the discussion of the uncertainty principle that are frequently not clearly understood and are intertwined unintentionally . although one often sees that these are stated in statistical terms , the standard deviation does not directly require multiple observations of a sample to understand . traditional statistics does rely upon repeated sampling in order to develop a standard deviation , however in quantum mechanics the idea is more closely associated with properties associated with the fourier transform . to understand the fourier transform one must first understand what a fourier series is . the hyperlink will take you to a discussion about the fourier series as it relates to sound . starting at about minute two you see a representation of a saw-tooth like wave form . when they show you in the video how the saw-tooth like wave has many components , those components are determined by performing a fourier transform . in many cases , they transform time series functions into frequency functions ( which is directly proportional to energy ) but the transform is also applicable to situations where one is transforming position into momentum . essentially what happens , is that if one wants to have complete certainty in the value of momentum ( or energy ) , one must look at the entire position ( or time ) spectrum . in other words , a definite position , when transformed into the momentum domain , requires the entire momentum domain . if one allows a little uncertainty in the position , one does not require the entire momentum domain . this relationship can be well defined as it relates to fourier transforms . this is the real source of the uncertainty principle , and does not require a statistical interpretation to understand .
the momentum of the whole system is still conserved -- it is just that when you add friction between the cannon and the ground , you have to include the ground ( and in fact the whole planet that it is attached to ) as part of " the system " . when the cannon ball flies off in one direction , the cannon is pushed in the other . but due to friction the cannon pushes in turn on the earth , which is not fixed to anything , so its velocity changes very slightly . however , the mass of the earth is very large , so the change in velocity is minute . a typical cannonball would have a mass of the order $10\:\mathrm{kg}$ . assuming a muzzle velocity of $100\:\mathrm{m/s}$ , the ball gains about $10^3\:\mathrm{kgm/s}$ of momentum . the earth 's momentum therefore changes by the same amount in the opposite direction , but since the earth 's mass is around $6\times 10^{24}\:\mathrm{kg}$ , this corresponds to a velocity change of $10^3/6\times 10^{24} \sim 10^{-22}\:\mathrm{m/s}$ , which is so small it is effectively unmeasurable . in fact , the above is a slight oversimplification . the earth is not completely solid , so what actually happens is that the ground near the cannon will start moving first , and then set other parts of the earth moving very shortly afterwards . the result is seismic waves , which echo around the planet until they eventually get dissipated . if the cannon is big enough these can be measured from very far away - potentially from the other side of the world . but once all the waves have died down , the end result is the same : the earth 's velocity has changed by a very small amount . similar considerations can be made regarding angular momentum about the earth 's centre , i.e. firing the cannon will also slightly change the earth 's rotation . of course , this tiny velocity change will be exactly cancelled out once the shot hits something and transfers its momentum back to the earth through the same process .
first of all , in lower dimensions ( 2+1 and 1+1 ) the gravity is much simpler . this is because in 3d curvature tensor is completely defined by ricci tensor ( and metric at a given point ) while in 2d curvature tensor is completely defined by scalar curvature . this means that there are no purely gravitational dynamical degrees of freedom , in particular no gravitational waves . general note : horizon ( which is the defining feature of black hole ) representing our inability to obtain information about events under it would always imply the entropy of corresponding solution . so , in all of black hole models there is some black hole thermodynamics . for hawking radiation one needs to include quantum effects into consideration and also radiative degrees of freedom ( if there are no gravitons or photons or any other '-ons ' than nothing would radiate ) . let us start with case of 3d ( that is 2+1 ) . the einstein equations in 2+1 spacetime without any matter fields would simply imply that spacetime is flat , that is ' constructed ' from pieces of minkowski spacetime . it may have nontrivial topology , so 2+1 gravity is a topological theory , but no black hole solutions exist . this model ( in mathematical sense ) is exactly solvable . to introduce non-trivial 2+1 solutions we can add matter or cosmological constant ( which could be considered the simplest form of matter ) . it turns out that the spacetimes with negative cosmological constant ( which would locally be composed of pieces of anti-de-sitter spacetimes ) do admit the black hole solution : btz black hole ( name after authors of original paper ) . this solution shares many of the characteristics of the kerr black hole : it has mass and angular momentum , it has an event horizon , an inner horizon , and an ergosphere ; it occurs as an endpoint of gravitational collapse ( for that , of course , we need to include matter beyond cosmological constant in the consideration ) ; and it has a nonvanishing hawking temperature and interesting thermodynamic properties ( see , for instance , paper by s . carlip ) . the hawking temperature of btz black hole $t\sim m^{1/2}$ which , in contrast to the ( 3+1 ) -dimensional case , goes to zero as $m$ decreases . additionally , the simplicity of the model allows quantum treatment of it including statistical computation of the entropy ( see references in paper by e . witten ) . there are many other variations of solutions in 2+1 gravity theories ( for instance by including dilaton and em fields , scalar fields etc . ) but all of them require negative cosmological constant . this is because dominant energy condition forbids the existence of a black holes in 2+1 dimensions ( see here ) . now to 1+1 dimensions . locally all gr models in 1+1d are flat . so to include nontrivial spacetime geometry we need to modify gravity . this can be done by including dilaton field . the resulting models often admit nontrivial geometries with black holes ( see paper by brown , hennaux , teitelboim , wiki page on cghs model , paper by witten on bh in gauged wzw model , and this review ) . these black hole solutions also admit nontrivial thermodynamics and hawking radiation . in particular the hawking temperature is proportional to mass , so as the black hole evaporates it becomes colder ( unlike 4d case where $t \sim m^{-1}$ ) . now to higher dimensional gravity . gravity itself is much richer than in lower dimensional cases , so analogues of all 4d black holes also exist in higher dimensions , as well as some new black hole-like solutions such as black strings and black p-branes . there are also multi-black hole configurations where multiple black holes are placed along the ring or line such that the total force on each of them is zero , resulting in equilibrium configuration . since many uniqueness theorems for black holes only work for 3+1 dimensions there are even solutions with nontrivial horizon topologies such as black rings . i suggest to look at the living review recommended by ben crowell or to this lectures by n . obers . the simplest black hole would be schwarzschild–tangherlini solution ( analogue of schwarzschild black hole ) which is vacuum solution to einstein field equations : here $\mu = r_s^{d-3} = \frac{16 \pi g m}{ ( d-2 ) \omega_{d-2}}$ is mass parameter . this gives us the relationship between mass and schwarzschild radius : $r_s \sim m^{1/ ( d-3 ) }$ . the entropy is given by bekenstein-hawking formula : $$ s = \frac {\cal a}{4g}=\frac 14 \left ( \frac{\omega_{d-2} r_s^{d-2}}{ g} \right ) . $$ temperature could be found from the first law $ ds = d m / t $: $$t = \frac{d-3}{4 \pi r_s} . $$ rotating solution ( generalization of kerr metric ) would be myers-perry metrics . note , that rotations in higher dimensions are more complex , so the angular momentum is represented by several parameters . also note , that many solutions with horizons elongated in one direction ( such as black strings or black rings ) turn out to be unstable via the gregory-laflamme instability , where the smooth ' tubular ' horizon evolve growing perturbations of certain wavelengths . so possibly black strings and black rings would tend to decay into droplets-like black hole along them ( the exact mechanics is yet unknown ) . but of course , the second law of thermodynamics would be observed , meaning that total area of the horizons would increase .
the average/expected value of a product is in general not the same as the product of expected values . ( the " mean value " function is linear though : a sum of mean values is equal to the mean value of the sum . ) the product of the sum and sum of product are related by the covariance : $cov ( x , y ) = &lt ; xy&gt ; - &lt ; x&gt ; &lt ; y&gt ; $ , as you stated yourself . i hope this helps . source : http://en.wikipedia.org/wiki/expected_value#non-multiplicativity
firstly , i do not think your convention for the derivative is consistent with how you lower the index on $\lambda^b$ . let us start with $$ \frac{\partial}{\partial \lambda^b} \lambda^a = \delta_b^a $$ if you lower the index on $\lambda^b$ you get $$ \lambda_a = \epsilon_{ac} \lambda^c $$ putting this together gives $$ \frac{\partial}{\partial \lambda^b} \lambda_a = \epsilon_{ac} \frac{\partial}{\partial \lambda^b} \lambda^c = \epsilon_{ab} = -\epsilon_{ba} $$ which has a different sign from what you have . furthermore , the spinor bracket you write is lorentz invariant if you transform both spinors at the same time . the actions of $j^a$ and $j^b$ on $\langle ab \rangle = \epsilon_{cd} a^c b^d$ is given by \begin{align} j_{ab}^a\ , \epsilon_{cd} a^c b^d and = \left ( a_a \frac{\partial}{\partial a^b} + a_b \frac{\partial}{\partial a^a} \right ) \ , \epsilon_{cd} a^c b^d \\ and = \epsilon_{cd} \left ( a_a \delta_b^c + a_b \delta_a^c \right ) b^d = + ( a_a b_b + b_b a_a ) \\ % j_{ab}^b\ , \epsilon_{cd} a^c b^d and = \left ( b_a \frac{\partial}{\partial b^b} + b_b \frac{\partial}{\partial b^a} \right ) \ , \epsilon_{cd} a^c b^d \\ and = \epsilon_{cd} a^c \left ( b_a \delta_b^d + b_b \delta_a^d \right ) = - ( a_a b_b + b_b a_a ) \end{align} so if you sum them the transformation of $a$ is cancelled by the transformation of $b$ .
quite a simple answer : scattering of light ( rayleigh scattering would be more precise here . . . ) an observer in ground sees the sky as blue due to scattering of light by air molecules present in the atmosphere . for an observer in space , the water bodies reflect the color of sky . . . the water bodies ( ocean , lakes , river ) appear blue ( 'cause water is quite colorless ) because of the way sunlight is selectively scattered as it goes through our atmosphere . taking raman effect into account , water absorbs more of the red light in sunlight . by this way , water also enhances the scattering of blue light in the surroundings . by rayleigh scattering law : ( it is more important here ) the amount of scattering is inversely proportional to the fourth power of its wavelength . due to the larger amount of $n_2$ and $o_2$ molecules ( 78% and 21% ) in the atmosphere , blue light which is having shorter wavelength is scattered to a greater extent . thus , the earth would not be blue if it does not have enough $o_2$ and $n_2$ molecules in its atmosphere . the scattering depends on the characteristics of gaseous molecules in atmosphere . . . this is applicable to other planets also . ( like mars appearing red , venus appears yellow , etc . )
this will be limited by our field of view ( fov ) . i could not find a better source , but wikipedia says the vertical range of the field of view in humans is typically around 135° and the horizontal range around 180° . so for the earth to be entirely within your field of view it will be limited by your vertical range . and by using a little bit of geometry you can find the height above the surface of the earth , $h$ , if you approximate it as a perfect sphere with radius $r$=6371.0 km : $$ h=r\left ( \frac{1}{\sin\frac{\theta}{2}}-1\right ) $$ which gives : $h\approx$ 524.9 km ( =326.2 miles ) . however if you would look trough a lens , which would increase your fov , this height could be significantly be decreased . the camera used to capture the sky diver will probably have had a higher fov . the minimum fov at a height of 39 miles ( =62.76km ) would have been 164° .
heat consists of random vibrations in a material . if a magnetic field connects two objects , then it creates a mechanical coupling between the two objects . such a coupling will couple vibrations , therefore heat will be conducted by the magnetic field . in practice , the effect will be very small , but given enough time , heat will be conducted by the magnetic field . the same applies to suspension by a combination electric and magnetic field . this is not entirely incompatible with some of the other ( wrong ) answers . for example , since the suspended object is not at absolute zero , it is not possible for the magnetic field suspending it to be entirely static . as an example , let 's consider a superconducting metal container of gas suspended in a magnetic field : now suppose that the gas is at some non-zero temperature . then there is a non-zero probability that the gas will concentrate onto one side : in order for this to happen , assuming that the center of mass of the container remained constant , the metal part of the container had to have shifted in the opposite direction : assuming that the levitation is stable , a motion of the superconducting container must be opposed by the magnetic field . thus the above motion is opposed by a force from the magnetic field . however , for every action there is an opposite and equal reaction ( newton 's 3rd law ) , therefore there will be a corresponding opposing force on the suspension : therefore thermal motion in the suspended container will induce thermal motion in the base , hence there will be conduction of heat .
by the eigenenergies you quote , i imagine you are dealing with an infinite potential well with walls at $x=0$ and $x=l$ . in this case , the wavefunction is zero outside of the well , so $$\phi_n ( x ) = \left\{\begin{array} and \sqrt{\frac{2}{l}}\sin\left ( \frac{n\pi}{l}x\right ) and \text{if }0&lt ; x&lt ; l , \text{ and}\\ \qquad \quad0 and \text{otherwise . }\end{array} \right . $$ the integral is therefore over $x\in [ 0 , l ] $ , and converges without a problem .
assuming you are in orbit around the sun ( presumably a highly elliptical orbit ) you will not feel any force due to gravity . in principle you might feel tidal forces , but for an object the size of a spaceship these are negligable even if you graze the surface of the sun . the most obvious problems are the heat from the sun and the radiation it emits . the radiation is a mixture of electromagnetic radiation and charged particles , both of which are not good for anything relying on it is dna remaining intact . it is difficult to do much about the heat because in space the only way you can cool is by radiation . what you had probably do is surround your spaceship with a mirrored shell and keep a layer of vacuum between the shell and your ship . even with very good mirroring the shell will heat up , but for a while at least it will keep the heat off your spaceship . the messenger probe in orbit round mercury uses a reflective shield , and contains internal refridgeration - i do not knw exactly how this works but presumably it uses a radiator on the side of the probe pointing away from the sun . there is not a lot you can do about the radiation except surround your spaceship with a thick layer of lead , and that much lead would be difficult to put into space . the solar probe plus is planned to get within 4 million miles of the sun 's surface , and this will be the closest we have managed to get any spacecraft . however the spp does not have any human passengers to worry about . i suspect radiation is the real problem for human passengers . even for a hypothetical manned mars mission the radiation dose the astronauts would receive is a worry , and the intensity of the radiation goes up as the inverse square of the distance .
according to the american meteor society , the sonic boom of an asteroid or meteor ( sometimes referred to as a ' fireball' ) is due to if a very bright fireball , usually greater than magnitude -8 , penetrates to the stratosphere , below an altitude of about 50 km ( 30 miles ) , and explodes as a bolide , there is a chance that sonic booms may be heard on the ground below . this is more likely if the bolide occurs at an altitude angle of about 45 degrees or so for the observer , and is less likely if the bolide occurs overhead ( although still possible ) or near the horizon . and from caltech 's coolcosmos page when an object travels faster than the speed of sound in earth 's atmosphere , a shock wave can be created that can be heard as a sonic boom . the reason for asteroids causing sonic booms in the lower atmosphere , is according to the article how the falling meteor packed a sonic punch ( klotz , 2013 ) is due to because the meteor is supersonic , the waves , which travel at the speed of sound , can’t get out of the way fast enough . the waves build up , compress and eventually become a single shock wave moving at the speed of sound . looking a bit further in to what a sonic boom ( using a jet as an example ) is and how it occurs is illustrated in the following diagram image source so , if a meteor , asteroid is going faster than the speed of sound for particular part of the atmosphere , then a sonic boom will occur . going back to the american meteor society 's description of the likely cause of a sonic boom , they stated that if a meteor comes in below an altitude of about 50 km ( 30 miles ) then a sonic boom is likely to occur , one of the reasons is that the speed of sound is slower , due to the temperature of the atmosphere at that height and lower . below is a graph showing the speed of sound plotted against temperature as a function of atmospheric elevation : image source .
friction does not depend on velocity ( unlike viscous drag ) . an object that is stationary on a table will continue to be stationary when you push it gently - because there is an opposing force of friction . so no , your understanding is wrong : friction is present even when the object is just starting to move . let me draw a diagram : that ought to clear it up . . .
it is pretty easy to calculate . for geostationary orbits , the orbital period $t$ should be equal to the rotational period of the earth $\omega_e$: $ \matrix { t and = and 2 \pi \sqrt{a^3/\mu} \\ \omega_e and = and 1\ \mathrm{stellar\ day} } $ $ \ \ t=\omega_e \rightarrow a = \sqrt [ 3 ] {\mu \cdot \frac{\mathrm{day}^2}{4\pi^2} } = \sqrt [ 3 ] {398600.44 \cdot \frac{86164.099^2}{4 \pi^2}} \approx 42164\ \mathrm{km} . $ note that this equals the semi-major axis of the orbit , which means that if you want the altitude , you will have to subtract earth 's equatorial radius : $ h = a - r_e = 42241 - 6378 = 35786 \ \mathrm{km} $
newtonian mechanics is not quite as predictable as it is made out to be , both in theory and in practice . in theory , it is an easy task to set up a newtonian system of particles that will eventually violate the lipschitz continuity condition . you can toss reversibility and predictability out the window when that happens . in practice , we can not know state perfectly , and that means the complicated dynamic systems you mentioned eventually become unknowable . they will eventually entire a region of strong chaos , and there is no telling what happens after that . with regard to entropy , consider an interstellar gas cloud . the jeans instability can make a portion of the cloud collapse to form a protostar and a protoplanetary disk . you will not see a protostar and protoplanetary disk undo themselves and reform that gas cloud because of entropy . that is yet another gravitational example , but there are plenty of non-gravitational examples . consider a semi-rigid body with three distinct principle moments of inertia rotating in space . eventually that body will end up spinning about the axis with the largest moment of inertia . this is an irreversible process , and it lies almost entirely within the realm of newtonian mechanics . the only non-newtonian aspect is that the body radiates heat generated by internal friction away into the universe .
the mssm has 2 complex doublets i.e. 8 real components in the higgs fields . four of them are electrically neutral ( real bosons , antiparticles to themselves ) , four of them ( i.e. . two particle-antiparticle pairs ) are electrically charged . one neutral real boson and two charged ones ( one charged pair ) get eaten by the gauge bosons because 3 generators are broken , going from $su ( 2 ) \times u ( 1 ) _y$ to $u ( 1 ) _{\rm em}$ . the electrically charged ones are cp-partners of their oppositely charged antiparticles , of course . we do not learn anything if we consider cp for charged particles . one combination of the positive and negative particle is always cp-even and the orthogonal combination is cp-odd . the four neutral ones are evenly split to two cp-even and two cp-odd ones , too . it is the cp-odd ones that are eaten by the gauge bosons . so one is left with the cp-even charged ones , three cp-even neutral bosons , and one cp-odd neutral higgs boson . why are cp-odd bosons eaten ? it is because the gauge bosons pick a minus sign under c , simply because they are linked to generators which are c-odd , too . ( antiparticles need to be transformed by the opposite gauge transformation phases . ) so under p , the gauge bosons are ordinary vectors , not axial vectors , but the c flips their sign , which is why they are " axial vectors under cp " . the goldstone bosons that are eaten are cp-odd . now , are they c-even , p-odd , or vice versa ? this is really a meaningless question because the spectrum of the electroweak theory ( the fermions ) is not p-symmetric . not even the free lagrangian ( not even when mixing is neglected ) . it is because there are left-handed doublets and right-handed lepton singlets , etc . so it makes no sense to try to assign p parity to fields - because no choice will lead to a p-invariant theory . if you remove fermions , you may say that the goldstone bosons are p-even , c-odd . they have the " normal sign " under p because the gauge bosons do , but they have the opposite one under c .
formally , the gauge-invariant observables do not depend on the choice of gauge-fixing condition ( such as , e.g. , lorenz gauge , coulomb gauge , axial gauge , temporal gauge , etc ) . similarly , the hamiltonian can formally be gauge-fixed in any gauge . however , it is my understanding that to avoid the gribov problem , an algebraic ( rather than a differential ) gauge-fixing condition is preferred . see also the footnote on p . 15 in s . weinberg , quantum theory of fields , vol 2 .
when you say " assume that the earth planet is a black hole " i am guessing you mean us to replace the earth with a black hole of the same size , and then see what happens to the light . if so you are quite correct that there is a distance within which light cannot escape and spirals into the black hole . this distance is known as the last stable orbit and for a non-rotating black hole it is at a radius of three times the event horizon radius ( for a rotating black hole things get complicated ! ) . it is possible for light to orbit the black hole at the last stable orbit , but this orbit is an unstable one in the sense that a small perturbation will make the light spiral inwrads or outwards and either be consumed or escape .
let 's back up for a second . before going into the complexities of non linear waves , let 's ask what a linear wave is . actually , let 's go even further back and ask " what do we mean when we say linear ? " " linear " comes from the study of things like vector spaces . we have objects ( call them vectors , or arrows , or whatever ) that can be both added together and scaled by a number , with the result being another object of the same type . any collection of objects that satisfies certain conditions ( which basically boil down to " addition and scalar multiplication behave as expected" ) can be considered a vector space . now let 's talk about waves . but to keep things simple , let 's just talk about the effect of some waves at a single point , where the effect can change in time . one wave might have a value $\psi_1 ( t ) = \sin ( \omega_1 t ) $ at this point . another might have a different frequency : $\psi_2 ( t ) = \sin ( \omega_2 t ) $ . suppose we scale the waves by factors of $a$ and $b$ , and suppose we have them both affect the point together . if the waves ' effects just scale and add in the sensible way , then the value of the combined wave at the point will be $$ \psi_{ ( a\otimes1 ) \oplus ( b\otimes2 ) } ( t ) \equiv a \sin ( \omega_1 t ) + b \sin ( \omega_2 t ) = a \psi_1 ( t ) + b \psi_2 ( t ) . $$ here i am using the symbol "$\otimes$" to mean " physically scaled by the preceding factor " and "$\oplus$" to mean " combined physically . " in this particular case , $\otimes$ and $\oplus$ reduced to sensible scalar multiplication of the the wave value and regular addition of the values of two waves . we call these " linear " waves . one of their characteristics is that you can think of the waves as noninteracting $\psi_2$ will add its effect to the total in the same way , regardless of how much amplitude $\psi_1$ has already contributed . but i did not have to have that structure . in some cases , driving a physical displacement with twice the force does not result in twice the displacement , and having two different driving forces work together does not result in a force that gives a displacement that is is the sum of the independent displacements . for instance , perhaps the rule is $$ \psi_{ ( a\otimes1 ) \oplus ( b\otimes2 ) } ( t ) \equiv \sqrt{a \sin ( \omega_1 t ) + b \sin ( \omega_2 t ) } \neq a \psi_1 ( t ) + b \psi_2 ( t ) . $$ this then would be a nonlinear wave . they are defined by having the definition of how disturbances scale ( $\otimes$ ) and combine ( $\oplus$ ) be incompatible with scalar multiplication and regular addition of the waves ' values . that is , our physical definitions of $\otimes$ and $\oplus$ did not yield the structure of a vector space - at least not in any obvious way . the physics question remaining then is whether or not this situation is ever actually realized . the above discussion defines nonlinear waves , but it does not prove any such things exist . as it turns out , though , many waves important to physics show nonlinear behavior if you push them far enough . the classic example in optics is when the amplitude of an electromagnetic wave is so great that electrons in nearby atoms ( thinking classically here ) are pushed and pulled quite far from the " sweet spot " distance they want to have from their nuclei . then the restoring force that pushes them back to that sweet spot is not simply directly proportional to their displacement , their motion is anharmonic , and the wave becomes nonlinear .
note that the finite transformation of : $$ w^a_\mu \to w^a_\mu + \frac{1}{g} \partial_\mu \theta^a + \epsilon^{abc} \theta^b w^c_\mu $$ is : $$ w^a_\mu t^a \to g w_\mu^a t^a g^{-1} + \frac{i}{g} \partial_\mu g \tag{1} $$ where : $$ g = \exp ( -i \theta^a t^a ) \ ; \ ; \ ; \text{and} \ ; \ ; \ ; [ t^a , t^b ] = i \epsilon^{abc} t^c $$ thus , the first term on the right-hand side of equation $ ( 1 ) $ transforms under the adjoint representation of the lie group . the second term does not transform under the adjoint representation , but it should be easy to verify that the transformed gauge field still takes values in the lie algebra ( hint : looking at infinitesimal transformations is the easiest method to verify this ) . in case you want more information on the adjoint representation of the lie group , it might be worth looking at this question .
bps objects are stable because they are the lightest objects with given values of certain conserved charges . so there exists no potential final state that would be lighter and that the bps state could decay into , by conserving the energy . the excess energy may be invested to the kinetic energy of the final energy but a deficit energy means that the decay is prohibited . as an analogy , note that the electron has to be stable because there exists no lighter $q=-e$ object than the electron ( and positron ) . bps objects are either those preserving some ( enough ) supersymmetry ; or objects saturating the bps bound $m=q$ , schematically speaking ( for branes , it is the tension equal to the charge density ; coefficients should be inserted everywhere ) . these conditions are equivalent because $$ \{q , q\} = m-z $$ schematically , for some conserved supercharge $q$ . so the expectation value of $\{q , q\}$ in the bps state is zero – because $q$ annihilates the state – but it is also equal to $m-z$ which means that the mass is equal to the charge . for non-bps states , we have the strict $m\gt z$ . here , $z$ is the conserved central charge .
it might be worth taking a look to the original text , galileo 's discourses on two new sciences . the reasoning you are looking for is on the third day , a translation of which may be found online . the relevant parts are labelled theorem i and theorem ii in the above-linked translation . to derive that distance in a uniformly accelerated motion ( e . g . free fall ) goes as time squared , galileo first argues that the time in which any space is traversed by a body starting from rest and uniformly accelerated is equal to the time in which that same space would be traversed by the same body moving at a uniform speed whose value is the mean of the highest speed and the speed just before acceleration began . this is argued on a graphical basis ( see above link ) . however , even though the pictures may look pretty similar to modern functional representations ( e . g . of velocity vs . time ) and arguments involve finding equal areas in different situations , the arguments never involve an actual calculation of an " area " with mixed units , which was not yet conceivable at the time ( e . g . $m/s \cdot s = m$ ) . in fact , the whole third day seems very convoluted precisely because the notion of velocity was not clearly numerical yet , since only commensurable ( same unit ) quantities could conceivably be operated with ( added , divided , . . . ) . proportions of non-commensurable quantities , however , could be compared ( today we had say they are dimensionless ) , as in if a moving object traverses two distances in equal intervals of time , these distances will bear to each other the same ratio as the speeds ( earlier in the third day )
the water gets colder the longer you run it ( in the uk at least ) because the water mains pipes buried in the ground are colder than the ones in your house , so sadly this is not evidence for any fundamental physical effect . in principle any fluid flowing in a pipe gets hotter because energy is dissipated in viscous flow . you could in principle calculate the energy dissipated using the pressure drop per length of pipe , which is described by the darcy-weisbach equation , but this would be a somewhat involved calculation for real pipes/taps and in any case it is not relevant to the core of your question . when you relate velocity to temperature you are presumably thinking of the maxwell-boltzmann distribution for the temperature dependance of the velocity profile in gases . the trouble is this distribution is arrived at by considering redistribution of energy between gas molecules due to collisions between them . if you simply add a constant velocity to every gas molecule you are not making any difference to the way the gas molecules collide with each other , because it is only their relative velocities that matter . although water is a liquid not a gas the same argument applies . it is the velocities of the water molecule relative to each other that determine the temperature . so just adding a constant velocity to every water molecule makes no difference .
the euler number ( often called the euler characteristic ) is given a in terms of nice integral formulas in the gauss-bonnet theorem , but it can be defined in other ways . the difference in the factors simply comes from the fact that the two-dimensional scalar curvature $r$ is twice the gaussian curvature $k$ ( see towards the end of the first paragraph here ) . since you are reading polchinski , you will probably have no problem with most proofs of the gauss-bonnet theorem ( which are all over the internet ) , but i think do carmo 's differential geoemetry has a rather nice , elementary proof .
entanglement is just a correlation in measured properties of the subsystems ( particles ) expressed in a quantum way . you may only determine whether properties are correlated ( or entangled ) in a given , " initial " state if you repeat some measurements of the system with the same initial state many times . if you only measure two spins , for example , once , you get some result , like up-up or up-down or down-up or down-down but none of the four possibilities is more or less entangled than others . all of them may occur in entangled initial states and all of them may occur in non-entangled initial states . entanglement only means " predicted properties of the two subsystems are correlated , moreover correlated in a way that is not captured by a simple classical model of correlation " . whether the predicted properties are correlated may be determined from the probability distribution ( s ) but to measure the probability distribution ( s ) , you have to repeat the experiment with the same initial state many times . more precisely , entangled states are those that can not be written as a tensor product of wave functions describing the separate subsystems . once at least one of the entangled variables is measured , the entanglement becomes meaningless because the value of the variable is suddenly known and we are only left with some general wave function for the other , previously entangled variable which remains unknown up to the second measurement ( this reduction of the dependence of the wave function is misleadingly referred to as the infamous " collapse" ) . and if there is only one variable , it can not be entangled . but nothing physical is changing about the variable that has not been measured yet . the overall probability distribution for various outcomes $y$ remains the same after the first measurement of the ( faraway ) variable $x$ is performed ( imagine it is a probabilistic distribution $\rho ( y ) =c\rho ( x_\text{just measured} , y ) $ that is left afterwords , $c$ is such that $\int dy\ , \rho ( y ) =1$ ) . so no information can be transmitted by the fact that the first measurement took place . quite generally , quantum mechanics does not need any genuine ( one that could transfer useful information ) faster-than-light communication to guarantee things such as correlations of measurements done with entangled states . and in relativistic , local theories – especially quantum field theories and string theory – one may prove completely generally that a superluminal transfer of information is not only unnecessary for quantum mechanics to work ; it is actually prohibited and impossible , by the basic laws of special relativity .
let $$ \eta_{\mu\nu}={\rm diag} ( +1 , -1 , -1 , -1 ) \qquad \bar\eta_{\mu\nu}={\rm diag} ( -1 , +1 , +1 , +1 ) $$ with corresponding lorentz force laws ( in units where mass equals charge ) $$ \ddot x^\mu=\eta_{\nu\lambda}f^{\mu\nu}\dot x^\lambda \qquad \ddot{\bar x}^\mu=\bar\eta_{\nu\lambda}\bar f^{\mu\nu}\dot{\bar x}^\lambda $$ as the trajectories $x^\mu , \bar x^\mu$ should agree ( and so will all its derivatives ) for all initial conditions , we can equate the terms $$ \tag{1} \eta_{\nu\lambda}f^{\mu\nu} = \bar\eta_{\nu\lambda}\bar f^{\mu\nu} $$ contracting with the inverse $\eta^{\lambda\sigma}$ of $\eta_{\nu\lambda}$ finally yields $$ f^{\mu\sigma} = -\bar f^{\mu\sigma} $$ as $$ \bar\eta_{\nu\lambda}\eta^{\lambda\sigma} = -\delta_\nu^{\sigma} $$ this means the signs of the components of the electromagnetic tensor $f^{\mu\nu}$ do indeed depend on the metric convention . this also applies to $f_{\mu\nu}$ , whereas the tensor of mixed rank $f^\mu{}_\nu$ is independant of this choice ( which is just ( 1 ) ) .
the cosmic background radiation was always with us , it is not reaching us now . it just became cooler and cooler as time went on . one has to understand that when we are talking big bang and general relativity we are talking of the universe starting from one ( x , y , z , t ) point and as time goes on , expanding . this means that all ( x , y , z ) points of our universe trace back to that one point singularity that went " bang " . envision the two dimensional surface of a balloon , as shown in the wiki link . at time=0 the balloon is one point , call it r=0 . as it expands the points on its surface start receding from each other , and all points on that surface were at r=0 at t=0 . their neighborhood expands as the balloon blows up , and this means the electromagnetic radiation that started in the earth 's neighborhood hot , cools due to the expansion and becomes the cosmic microwave background . that is what i mean it is not coming from anywhere , it just is .
assuming you are attempting to determine $\langle n|x|n\rangle$ , you are on the right track . here are some hints : determine what the ladder operators do the the energy eigenstates ; \begin{align} a|n\rangle = ? , \qquad a^\dagger |n\rangle = ? \end{align} write the position operator in terms of ladder operators as you have done . what are the inner products of the energy eigenstates : \begin{align} \langle m|n\rangle = ? \end{align} combine this all together to obtain the desired expressions for $\langle x\rangle = \langle n|x|n\rangle$ .
you can callculate the standart deviation as show in this link http://en.wikipedia.org/wiki/standard_deviation#generalizing_from_two_numbers the standart deviation $\sigma=\sqrt{\frac{\sum_{i=1}^n a_i^2}{n}-\left ( \frac{\sum_{i=1}^n a_i}{n}\right ) ^2}$ , where $a_i$ is the $i$-th number in your set and $n$ is the number of numbers you have in your set ( in your example $a_1=32.5$ , $a_2=32.0$ , $a_3=32.3$ so $n=3$ ) using the numbers from your question i got $\sigma \approx 0.20548$
things are not always in states of minimum energy . this is something that applies to equilibrium states . simple examples of this idea are certainly familiar to you already - a ball comes to rest at the bottom of a valley , not partway down the side . if we want to find the equilibrium state for a white dwarf , there must be no forces on it . if there are no forces , then small changes to the white dwarf do not change its energy , since the change in energy is force*distance . that means the energy should be a " stationary point " , where the energy curve is flat . ( in this case , we are looking at a curve that describes energy as a function of radius . ) we want a minimum so that the equilibrium will be stable . you can apply different reasoning to get to the same result . for example , the white dwarf is in a cold universe , so thermodynamics says it will give away energy as much as it can since this increases the total entropy . this only stops when the white dwarf is at a minimum energy , or at least gets down to a few degrees k .
the entropy article in wikipedia includes a paragraph on the statistical mechanics definition of entropy . it is useful for such questions as you pose . specifically , entropy is a logarithmic measure of the density of states : $$s = - k_b\sum_i p_i \ln p_i$$ where $k_b$ is the boltzmann constant , equal to 1.38065×10$^{−23}$ j k$^{−1}$ . the summation is over all the possible microstates of the system , and $p_i$ is the probability that the system is in the ith microstate . for most practical purposes , this can be taken as the fundamental definition of entropy since all other formulas for s can be mathematically derived from it , but not vice versa . ( in some rare and recondite situations , a generalization of this formula may be needed to account for quantum coherence effects , but in any situation where a classical notion of probability makes sense , the above equation accurately describes the entropy of the system . ) in what has been called the fundamental assumption of statistical thermodynamics or the fundamental postulate in statistical mechanics , the occupation of any microstate is assumed to be equally probable ( i.e. . $p_i=1/\omega$ since $\omega$ is the number of microstates ) ; this assumption is usually justified for an isolated system in equilibrium . [ 23 ] then the previous equation reduces to : $$s = k_b \ln \omega$$ this means that any changes in the number of microstates increases entropy . thus radiation does so , each new photon out of the sun increasing its number of microstates . thermal equilibrium means that the temperature is the same in the systems in thermal equilibrium and as @zephyr commented above the earth would have the temperature of the sun , if we were in thermal equilibrium with the sun .
this might help understand why the current also obeys a wave equation . the voltage is essentially the gradient of the electric field . so if you have an oscillating voltage , you are trying to set up an oscillating electric field in the wire . the current is related through ohm 's law $\mathbf{j} = \sigma \mathbf{e}$ . now , you cannot have an instantaneous change in \mathbf{e} all through the wire -- there is a time delay corresponding to the time taken for the em disturbance to propagate ( in fact , we should not be talking about voltage in the traditional sense any more , since this is not an electrostatics problem ) . so the " voltage " downstream on the conductor is different from that at the source . and after all , the electrons are driven by the changing electromagnetic field , so they will oscillate too . i am not sure if i am making sense , but in summary , the thought that voltage is constant throughout the conductor is a hang-over from electrostatics , and this need not be the case when we have changing electromagnetic fields .
my question is during the initial deceleration , is that simply applied opposite the current orbital velocity vector ? and when the satellite arrives at the new orbit , what direction is the correctional acceleration/deceleration made ? is it the difference of the desired tangential direction that it wants to go and the current direction it is going ? or simply opposite its current vector ? the two directions are the same . the impulses are done at the apoapsis and the periapsis of the transfer orbit , where the velocity is also tangential to the circular orbits ( the larger one at apoapsis and the smaller at periapsis ) . i understand that the point of the hohmann transfer orbits is to use as little velocity change ( and thus fuel ) as possible , but can an orbital transfer be faster or slower if you are willing to burn more fuel to affect the velocity change ? [ snip ] finally , if you had unlimited fuel , and time was more a consideration , would you even bother with this process versus something more " point and shoot , turn and burn " ? if you have enough delta-v and thrust , you can just " point and burn " . you will be doing a hyperbolic transfer orbit that , in the limit of very large velocities , will tend to a straight line ( $e \to \infty$ ) .
a 242ω resistor is in parallel with a 180ω resistor , and a 240ω resistor is in series with the combination . a current of 22ma flows through the 242ω resistor . the current through the 180ω resistor is __ma . approaching this step by step , note that you can calculate the voltage across the $242 \omega$ resistor since you are given the current through it . by ohm 's law : $$v_{r_{242}} = 22ma \cdot 242 \omega$$ now , you are also given that this resistor is in parallel with a $180\omega$ resistor so the voltage across this resistor must be identical to the voltage across the $242 \omega$ resistor . thus , and again by ohm 's law , you can calculate the current through the $180\omega$ resistor . two 24ω resistors are in parallel , and a 43ω resistor is in series with the combination . when 78v is applied to the three resistors , the voltage drop across the 24ω resistor is___volts . again , approaching this step by step , the 2 parallel connected $24\omega$ resistors can be combined into 1 equivalent resistor of resistance $$r_{eq} = \dfrac{24 \cdot 24}{24 + 24}$$ now you can use voltage division to find the voltage across the equivalent resistance : $$v_{r_{eq}} = 78v \dfrac{r_{eq}}{r_{eq} + 43}$$ can you take it from here ?
in principle , you can charge a conductor indefinitely . but remember that in order to cause a flow of charges from a body ( call it a ' source' ) to another ( the conductor in question ) , the potential of the former has to be lower than the potential of the latter . this potential difference causes a current to flow from the source to the conductor , resulting in a transfer of charges . all the charges that are already in the conductor will exert an electrostatic repulsion on the incoming ones , slowing them down and making it more and more difficult ( as more and more charge accumulates in the conductor ) to charge it further . as charge continues to flow out of the source and into the conductor , the potential difference decreases and the potentials of the two objects become more and more similar . at some point , the potential difference will reach 0 . the current will stop flowing when the charge in the source is equal to the charge in the conductor , which corresponds to the situation in which the electrostatic repulsion from the charges in the conductor is equal to the force attempting to put more charges in . here , the potential difference is 0 . if you want to force more charges to flow out of the source and into the conductor , therefore increasing its charge , you have two options : 1 ) you either increase the potential of the source , so as to create a non-zero potential difference and thus causing current to flow . 2 ) introduce a new force that pushes the charges out of the source and into the conductor : this force ( eg chemical force ) has the job of bringing a charge of the source against the electric field exerted by the charges of the conductor . the real limit to the charging of a conductor would be when there is no physical space available for the electrons . electrons are fermions and they cannot occupy the same position in space , so it will become harder and harder to squash them together . now , in the context of capacitance : imagine a circuit with a generator ( battery ) and a capacitor . dc current will flow only in half of the circuit ( left or right depending on the choice of charge carries , electrons or ions , that is conventional ) . let 's say protons carry the charge ( this is the convention for current , although physically it is the electrons that to the moving ) . protons leave the + terminal and accumulate on the closest plate of the capacitor . the plate is now the conductor , and the + terminal of the battery is the source . the plate has initially 0 charge and therefore 0 potential . the charges accumulating on the plate will exert an electric field that is going to oppose incoming protons . the potential of the + terminal decreases , the potential of the plate increases , the potential difference reaches 0 and current stops flowing : there are as many protons in the + terminal as there are on the plate . unless you increase the charge in the + terminal ( therefore increasing its potential ) or apply another force on the protons , no more current will flow . if you now disconnect the battery and close the circuit , you have one of the capacitor plate charged ( so at a non zero potential ) and the other one with no charge ( 0 potential ) . potential difference => current will flow until the charge on both plates is the same .
just to mention the basics for " other ways"' , you can hit your photon really hard with a fast moving electron or proton , i.e. inverse compton scattering . ics is very important in many astrophysical contexts . if you even reflect the photon off a moving mirror , you can slightly increase or decrease its wavelength
it can be justified using distribution theory .
i think you have an error in assumption 2 . if $n$ is the number of molecules , then the mass of the sample would be $n$ multiplied by the mass per molecule , not $n$ multiplied by the total mass of the sample . you are kind of " overcounting " mass . if you take $m$ to be the mass per molecule ( molecular mass ) , then i believe it works out .
the source in the einstein field equations is the stress-energy tensor , not the scalar mass-energy . adding rotation will affect multiple elements of the stress-energy tensor . you can sometimes get rough estimates of effects in gr by using $e=mc^2$ and pseudo-newtonian arguments , but sometimes these are way off . as an example where it is way off , two light rays propagating in parallel ( not antiparallel ) directions experience zero gravitational interaction . in the case of the distant field , i believe the answer to your conjecture is yes , in the sense that in any asymptotically flat spacetime , the distant field is newtonian , and its strength is what you would expect based on the bondi or adm mass of the ingredients that went in .
i have played the game , see my report : http://motls.blogspot.ca/2012/11/a-slower-speed-of-light-mit.html?m=1 and i join m . buettner . i am confident that all relativistic effects are incorporated . it includes the length contraction in the direction of motion , time dilation , but those basic things are rapidly changed by the fact that it really shows what you " see " and not what " is there " at a fixed value of your instantaneous coordinate $t'$ . so the effects that are " purely optical " and depend on the propagation of light and relativistic effects changing it include the relativistic doppler shift – things change the color immediately when you change the speed although the change of your location is negligible at the beginning – and the shrinking of transverse directions if you are moving forward ( or their expansion if you move backwards ) which makes object look " further " ( optically smaller ) if you are moving forward . because of this shrinking , you may effectively see " behind your head " . you also see things how they looked like some time ago . because of the transverse shrinking , you also see straight lines as curved ones if your speed is high enough . one should also verify that the streetcars moving in front of you from the left to the right are " rotated along a vertical axis " . i could not verify this effect but i see no reason to think that their simulation should do it incorrectly . good game . see also real time relativity and velocity raptor . you may get to those sources from my blog mentioned at the top . however , i am confident that the " general relativistic " comments are straw men . if the spacetime is flat , and in the absence of strong gravitational fields , it is , there is no reason why the proper simulation should consider general relativity . special relativity is enough , despite the fact that the child ( and the other stars of the game ) are accelerating . of course , acceleration " tears " solid objects because the proper lengths change asymmetrically etc . but if the material is flexible enough , the objects survive .
i believe this is just a restatement of the first newton 's law .
the maxwell equations are linear equations , which implies that a linear combination of solutions is a solution itself . that is , if you know the magnetic field $\vec b_1$ arising from a current $\vec j_1$ , and the magnetic field $\vec b_2$ due to a current $\vec j_2$ , you know that the magnetic field in the presence of both currents , i.e. the current $\vec j = \vec j_1 + \vec j_2$ , is just the sum of the magnetic fields , namely $\vec b = \vec b_1 + \vec b_2$ . this is a little harder to implement using your formula , as it only gives you the magnitude of the magnetic field and not its direction , but i am positive that you will be able to figure that out , using , for example , the right hand rule on both wires to determine the direction of the magnetic field and then add the relevant components together .
parameters of the surface in order to calculate the size of the sphere we need the equation for gravity field : $$ \nabla\vec{g} = -4\pi g \rho \qquad ( 1 ) $$ and the hydrostatic version of newton 's 2nd law : $$ \rho \vec{g} = \nabla p \qquad ( 2 ) $$ where $g$ is the gravitation constant , $\rho$ is the density of the " cover " , $p$ is the pressure inside the " cover " . in order to solve the first equation one can use gauss 's theorem . in spherical coordinates this will give $\vec{g} = \bigl ( -g ( r ) , 0 , 0\bigr ) $ . the second equation will give the distribution of pressure in the " cover " . on the internal surface it is equal to the pressure of the gas . on the external surface it is zero . stability the surface described by the equation ( 2 ) acts like it is made of water . my intuition says it is unstable . if we put a small piece from the external surface to the internal one it will have less gravitational potential energy . the change of the pressure will be negligible especially for small particles . this is known as rayleigh–taylor instability ( thanks to @mmc ) . in order to get a stable surface we need some solid material . in that case additional forces appear in equation ( 2 ) . then if we increase the pressure of the gas , the cover will expand but not fly away . it will be held by the elastic forces . in this case , though , the pressure should be so high that the balloon stability would be based on the strain not gravity . so this would be a usual balloon .
when expanded , $ ( 1/8 ) \phi^2 ( \phi-2 ) ^2$ contains quadratic , cubic , quartic ( 2nd , 3rd , 4th degree ) terms in $\phi$ . so it is a polynomial with these monomials . the final form of the potential places general coefficients in front of these terms . the quadratic term is universally written as $ ( 1/2 ) m^2\phi^2$ because it contributes the usual mass term $m^2\psi$ to the klein-gordon equation of motion . note that the potential has units of $m^{d+1}$ where $d+1$ is the spacetime dimension in your conventions because when integrated over $length^{d+1}$ spacetime , we should get a dimensionless action . it follows from the $m^{d+1}$ dimension of $m^2 \phi^2$ that $\phi$ has the dimension of $m^{d/2-1/2}$ . in the cubic term , $\phi^3$ therefore has dimension $m^{3d/2-3/2}$ and we have to multiply it by a coefficient with units $m^{-d/2+5/2}$ to obtain another $m^{d+1}$ term . this $m^{-d/2+5/2}$ coefficient is written as a product of the same power of $m$ , the mass from the quadratic term , and a $\lambda_3$ which is may be kept dimensionless . in the same way , the quartic term contains $\phi^4$ whose dimension is $m^{2d-2}$ but we need the dimension of the whole $v$ to be $m^{d+1}$ so we need to add $m^{d-3}$ , dimensionally speaking , which the formula does , and it adds a new dimensionless coefficient $\lambda_4$ to this term .
let 's assume mass of the person plus spacesuit to be $m_1$=100kg asteroid density : $\rho=$2g/cm$^3$ ( source ) that is 2 000kg/m$^3$ 15km/hour is a good common run . that is roughly v=4m/s the orbital height is negligible comparing to the radius , assume 0 over surface . linear to angular velocity ( 1 ) : $$ \omega = {v \over r } $$ centripetal force ( 2 ) : $$ f = m r \omega ^2 $$ gravity force ( 3 ) : $$ f= g \frac{m_1 m_2}{r^2} $$ volume of a sphere ( 4 ) : $$ v = \frac{4}{3}\pi r^3 $$ mass of a sphere ( 5 ) : $$ m_2 = v \rho = \frac{4}{3}\pi r^3 \rho $$ combining ( 1 ) , ( 2 ) , ( 3 ) , reducing : $$ { m_1 r v^2 \over r^2 } = g { m_1 * m_2 \over r^2 } $$ $$ r v^2 = g m_2 $$ combining with ( 5 ) $$ r v^2 = g \frac{4}{3}\pi r^3 \rho $$ $$ r^2 = \frac{v^2}{\rho g \frac{4}{3}\pi} $$ $$ r = v ( {\frac{4}{3}\pi g \rho} ) ^{-{1 \over 2}} $$ substituting values : $$ r = 4 ( {1.33333*3.14159* 6.67300*10^{-11} * 2000} ) ^{-{1 \over 2}} $$ that computes to roughly 5.3 kilometers more interestingly , the radius is directly proportional to the velocity , $$ r [ m ] = 1.337 [ s ] * v [ m/s ] = 371.51 [ h/1000 ] * v [ km/h ] = 597 [ m*h/mile ] * v [ mph ] $$ so , a good walk on a 2km radius asteroid will get you orbiting . something to fit your bill would be cruithne , a viable target for a space mission thanks to a very friendly orbit . note , while in rest on cruithne , the astronaut matching the m_1=100kg would be pulled down with force of 4.5n while not in motion . that is like weighing about 450g or 1lbs on earth .
what you have worked out so far is the left hand side of the answer . you have written that the sum of the accelerations is zero , but that is not correct because the system is being driven by the roller moving across the sinusoidal surface . parameterize in terms of time by writing $x=ut$ , then rewrite your $y_0 ( x ) $ equation as $y_0 ( t ) $ . now that you have the vertical position of the roller as a function of time , you can find its acceleration by taking the derivative twice with respect to time . the result should look comforting . this new acceleration term goes with the force that drives the system .
it is important to remember that quantum field theory is a theory about fields , not particles . i know you said shy away from equations , so i am just going to reference one part of one , and you can see this equation on any o'l web site , like wikipedia . take the dirac equation , here there is a quantity $\psi$ that shows up . and part of the history of this $\psi$ was what it meant . ultimately , it was determined to be a field : the fermion field . this is our fundamental understanding as of now about the world , that there are fields , and that interactions take place between fields , mediated by quantum excitations of these fields . in light of this , the wave function you talked about corresponding to the electron is not the fermionic field i mentioned above . the fermion field can be excited either to produce or destroy certain fermions like electrons and positrons . as far as how deep the oscillator analogy runs , i will just say this : how deep or how far it runs is debatable , but i do not think anyone will argue its fundamental role in developing qft . quantizing fields and placing field variables in terms of canonical field variables is pivotal for an understanding of qft , and before even getting to qft , a good understanding of the sho in quantum mechanics is indispensable . this is because the creation and annihilation of excitations in qft is analogous to the creation and annihilation of energy states in the non-relativistic quantum sho . i hope this helps .
the best calculation of hawking radiation is the one given on wikipedia , in the article for hawking radiation . this is the method implicitly described by unruh in his famous paper on accelerated observers in qft , and it is mathematically equivalent to the method hawking uses in papers of the 1977-78 period , which use periodicity of the analytically continued solution in imaginary time . hawking 's original calculation assumes that the emitted field quanta are non-interacting . the result is clearly true for interacting theories as well , but this is not completely clear in hawking 's original calculation . further , hawking 's 1976 method uses a pure-black hole solution ( not an eternal solution , no white hole ) , meaning that the horizon was created at a finite affine parameter in the past . the outgoing radiation , when traced back in time , is all coming from a sub-microscopic infinitesimal region right at the moment the horizon first formed . during the sojourn next to the even horizon , the peeling-off property of the horizon ( the fact that outgoing light rays peel away from stationary light rays right on the horizon ) lead to a big magnification of the tiny region where the black hole is first formed . any outgoing photon mode , extended back , comes from a place where its wavelength is trans-trans-trans planckian , this is right the moment of formation of the black hole horizon . the trans-planckian property led many people to be skeptical of hawking 's result . analytic continuation is impossible for general metrics , and it seems strange to use an analytic continuation to find a physical temperature . the calculation below is equivalent to the analytic continuation to imaginary time , but explains why this method works . unruh radiation when an observer is accelerating in minkowski space , the trajectory in space time is a hyperbola , which is the relativistic analog of a circle . the natural coordinates for an accelerating observer are the relativistic analog of polar coordinates . choosing the x coordinate to be the direction of the acceleration , these coordinates are given by $$ x = r\cosh ( \tau ) $$ $$ t= r\sinh ( \tau ) $$ the y , z coordinates transverse to the acceleration are unchanged , and the metric in these coordinates is the relativistic polar metric : $$ ds^2 = dr^2 - r^2 d\tau^2 + dy^2 + dz^2 $$ and it is important to compare this the standard euclidean polar metric $dr^2 + r^2 d\theta^2$ . it is obvious then that the time coordinate is like the angle , while the r coordinate is radial . a green 's function for a quantum field in the $x , t$ coordinates analytically continues to imaginary time , this is wick rotation , and it is the whole point of the feynman formalism . the natural hamiltonian for the $\tau$ coordinate analytically continues to a generator of rotations in the x-t plane , so that the $\tau$ coordinate is periodic with period $2\pi$ . this means that the accelerated observer measures a thermal green 's function for the field with a temperature that is diverging as $1\over r$ ( the $\tau$ period is $2\pi$ , but $\tau$ is dimensionless-- the physical local time coordinate is $r\tau$ , so that the physical energies of high frequency quanta ( those that can be localized ) are $1/r$ bigger ) . the divergence of the unruh temperature at the rindler acceleration horizon is obvious--- it is just the statement that a family of observers at different r in the $r , \tau$ coordinates have an acceleration that diverges as $1\r$ . the unruh radiation , when back-traced , also has a trans-planckian wavelength right at the acceleration horizon . the reason is just because any outgoing mode is redshifted an infinite amount from its value near the horizon , and the constant magnification of the near-horizon region is the analytic continuation of the constant rotation in imaginary time ( rotation is by cosines and sines , which continue to cosh 's and sinh 's , giving the magnifying at the acceleration horizon . the bogoliubov coefficients are given by these near-horizon blow-up factors . the bogoliubov formalism is not particularly enlightening . ) . since the unruh radiation is in flat space , there is no debate about it is correctness . you can model an accelerating detector in ordinary minkowski coordinates to see that the detector goes off as if it is immersed in thermal radiation . another important point about unruh radiation is that the only reason you see it all around you is because the radiation going out of the acceleration horizon free-falls back in , to hit you going the other way on its way back to the horizon . none of this thermal background can escape , because the rindler ( pseudo ) gravitational potential well is infinitely steep--- you need to do an infinite amount of energy to get away from an accelerating observer in the direction of acceleration . equivalence principle by the equivalence principle , a near-horizon observer by a black hole can not tell the difference between the black hole and nothing at all . the black hole horizon is identified with the rindler horizon for a nearby stationary observer , which must be accelerating quickly to keep from falling in . this is a powerful statement , because knowing that the black-hole metric is locally rindler ( using the coordinate $s=\sqrt{r-2m}$ ) means that one can continue into the black hole just by assuming that the horizon is not a special location . this continuation past the horizon is standard , it is how the black hole interior is derived from the exterior solution . it is sometimes called " analytic continuation " , but this is a misnomer , as it does not require analyticity to work . it just needs that the metric is asymptotically rindler , so that the spacetime is really still locally minkowksi space in different coordinates , there are no curvatures or singularities , and so continues into the interior . there are not many different stationary parametrizations of minkowski space , if you want the space to look stationary you need to use a rindler coordinate of some type . this means that any locally flat horizon is locally rindler . extremal black holes are not locally flat , and their near horizon limit is ads , not minkowksi , reflecting the diverging curvature at extremality , due to the pinching off of the interior . the local temperature seen by a near horizon observer diverges as $1/s$ , which is proportional to the radial distance to the horizon . this local temperature is equal to the local periodicity of the solution in imaginary time , because this is true in rindler space--- the $\tau$ periodicity translates into a $t$ periodicity near the horizon . but the black hole solution is stationary , which means that once you know the period in imaginary time near the horizon , if you extend the thermal radiation using the gravitational redshift factor , it stays in equilibrium . any outgoing radiation is redshifted to a lower temperature , while any incoming radiation is blueshifted to a higher temperature . the operator which moves $t$ to $t+a$ is not quite a hamiltonian , since it generates rotations near the horizon ( just like the rindler $\tau$ ) , so you find that for equilibrium , the thermal radiation is the unruh radiation plus a bath at a temperature that goes as the inverse redshift . for unruh radiation , the redshift factor , the square root of $g_{00}$ , is $r$ , it goes to infinity at $r=\infty$ . in the schwartzschild solution , the square root of $g_{00}$ is $\sqrt{1-{2m\over r}}$ , and it asymptotes to a finite limit at infinite r . the analytic continuation matching the unruh temperature gives the hawking temperature at infinity to be ( see wikipedia for the simple calculation , it is an exercise ) : $$t_h = { 1\over 8\pi m}$$ the argument from matching to the unruh limit shows why analytic continuation to imaginary time is correct--- the unruh periodicity continues to the long distance periodicity . the generator of time translations is a symmetry , and if it has a period at some position , imposing the condition that the period is constant gives a consistent background for a path integral , and this path integral is necessarily thermal , with asymptotic temperature given by hawking 's formula . physical emission the argument above only shows that the black hole is in equilibrium with a thermal gas whose temperature at infinity is $1\over 8\pi m$ . it also shows that the correct thermal ensemble for the exterior observer living with an interacting quantum field theory is the one from the path integral in an imaginary-time background which is periodic in t with period $8\pi m$ everwhere , with a background metric equal to the analytic continuation of the schwartschild metric . but what if there is no radiation at infinity ? then there is no infalling radiation , and the thermal emissions from the black hole are not compensated , so the black hole glows thermally . the actual radiation from a black hole is given by detailed balance--- the black hole is in equilibrium with a thermal gas at infinity at the hawking temperature , so the emission rate for a free field theory is related to the absorption by the condition that they balance out in thermal equilibrium , and , since the quanta are free , they are independent processes . this gives the graybody factors for black hole emission from a calculation of the absorption at the same wavenumber .
the fermion doubling is manifested through the existence of extra poles in the dirac propagator on the lattice . these poles cannot be made to disappear at the continuum limit . ( the number of doublers can be reduced by different discretizations but not eliminated at all , this is essentially the nielsen-ninomiya theorem ) . the reason for the fermion doubling lies in the existence of chiral anomaly . this anomaly exists in the continuum limit due to the chiral non-invariance of the path integral measure and not because of the non-invariance of lagrangian . in the lattice formulation of a chiral theory based on a discretization of the lagrangian , the anomaly is absent and the lattice formulation generates the extra species just to cancel this anomaly in the continuum limit . since the axial anomaly exists in nature $\pi^0 \rightarrow \gamma \gamma$ , this situation is unacceptable . the fermion doubling problem is an artifact of the realization of the theory by means of quarks where the axial anomaly is not present in the lagrangian but in the path integral measure . there are approaches of other types of discretizations such as by means of fuzzy spaces in noncommutative geometry , where the quarks are not the basic fields of the theory . in these approaches , the fermion doubling problems do not exist . update this is an update referring to marek 's first comment . for fermions , the axial anomaly can be recovored only by a non-trivial regularization of the path integral measure of the fermions . any finite dimensional approximation of this measure as a product of berezin-grassmann integrals does not produce the axial anomaly . this is the reason why the lattice regularization does not produce the axial anomaly and as a consequence the phenomenon of doubling occurs where the different species of doublers are of opposite chirality to cancel the anomaly . this is a property of fermion fields represented by grassmann variables . in effective field theories ( where the basic fields are pions ) such as sigma models , the axial anomaly is manifested through a wess-zumino-witten term in the lagrangian , but these theories are not even perturbatively renormalizable and i think this is the reason why they were not put on a lattice . one approach that i know of which solves the fermion doubling problem is the regularization by means of a fuzzy space approximation of the space-time manifold . the philosophy of this approach is explained in the introduction of mark 's rieffel 's article . the resolution of the fermion doubling using this approach is given nicely in badis ydri 's thesis . ( there is also more recent work on the subject by a . balachandran and b . ydri in the arxiv ) . the main idea is that the poisson algebra of functions over certain spaces ( such as the two sphere , and more generally coadjoint orbits of compacl lie groups ) can be approximated by finite dimensional matrices . these approximations are called fuzzy spaces . gauge fields and fernmions can be constructed on these fuzzy spaces , which have the correct continuum limit when the matrix dimensions become infinite . this formulation contains the axial anomaly inherently , thus it is free of the doubling problem . the only drawback that i can see of this approach is that it is applicable only to some special 4-dimensional manifolds such as $cp^2$ or $s^2 \times s^2$ , because the fuzzy manifold is required to be berezin-quantizable .
what is the net force ? newton : $f_{net}=ma=m\frac{dv}{dt}$ so the net work done to accelerate a particle from $v_0=0$ to final velocity $v_f$ is $w_{net}=\int f_{net}ds=\int m\frac{dv}{dt}ds=m \int_{v_0}^{v_f} v dv=\frac{1}{2}mv_f^2$ where in the last step i used $ds/dt=v$ and $m$ constant . if you had some crazy system where $m=m ( v ) $ , then that mass could be a function of velocity . maybe some effective mass due to interactions . . . so what you wrote is $not$ correct since ( as qmechanic said ) , the units do not match ; $mdv$ has units of $\text{kg ms}^{-1}$ , and units of work are $\text{j=kg m}^2s^{-2}$ .
the speed of light is entirely a local concept - it does not care if there are 10 atoms or 10 billion galaxies somewhere in the universe . obviously we can not go to distant galaxies to directly measure the speed of light , so in the absolutely strictest sense this is not directly empirically tested . however , the constancy of the speed of light is one of the most fundamental tenets of physics . in some sense , just about every observation we make in astronomy tests it , for if there were any variation it would manifest in all sorts of crazy ways in every single system we look at . the confusion seems to stem from the term " universal . " the word " universal " means " fundamental " or " unchanging in space and time " or " lies at the heart of our theoretical framework , permeating everything we do . " it does not mean " tied to the universe " or " depends on global properties of the universe . " along the same lines , a scented candle could be said to have an " earthy " scent , but this has nothing to do with it being located on earth the planet .
the total internal energy of a system is completely out of the question as an answer , of course . i would even go as far as saying that it is the quintessential and most important extensive variable of a system . therefore , most things that have to do with ' energies ' within thermodynamics will also be extensive variables . i am not sure of what is being expected from you as an answer , but as a tentative guess from a condensed matter bloke , without prior knowledge about the particular context in which this question was formulated , i would think of ' energy per something ' quantities which are characteristic signatures of the thermodynamic state of a physical system . that would be the case of the bond energy per atom in a condensate ( i.e. . , solid or liquid ) system or the mean thermal energy per degree of freedom which is $\frac{1}{2}k_bt$ from the equipartition theorem ( and an intensive quantity , as temperature is also intensive ) . other possible answers that come to mind would be the fermi energy of the gas of electrons in a neutral solid , or the characteristic gap energies of an insulating , semiconducting or superconducting material , although these are very solid state physics-centered answers .
your calculations are correct . the reason we do not feel it is because the 1 atmosphere of pressure will be applied to all surfaces of our body , including the soles of our feet . in fact the interior of our body is also pushing out against our skin with 1 atmosphere of pressure so there is no net force being applied in either direction at the skin 's surface . the only way you could feel this pressure would be if part of your body was in contact with a vacuum while the rest of your body remained in air at 1 atmosphere of pressure . imagine that you were standing on an opening to a vacuum chamber that was almost , but slightly less than the size of the soles of your feet . if there was a perfectly air tight fit between the vacuum opening and your feet , you would indeed feel that pressure as that much force on your feet just as you calculated . as a simpler experiment , just put your hand over the hose of a vacuum cleaner - that is only a very partial vacuum but the force you need to exert to remove your hand is due to the 1 atmosphere of pressure around us all pushing your hand onto the partial vacuum . in fact if you measured that force needed to pull your hand off of the vacuum cleaner hose , you could compute what the air pressure in the vacuum hose was .
" quasi-something " is " almost something " . in particular , quasicontinuum is a discrete set that contains so many entries ( typically energy eigenvalues ) with so small spacings that it looks like a continuum . for example , if the energy eigenvalues are $$e_n = n\epsilon$$ where $n$ is integer and $\epsilon$ is $0.0001$ times some natural energy difference , then $e_n$ is almost continuous and the set $\{e_n|n\in z\}$ is a quasi-continuum . in many physical questions , a quasi-continuum behaves just like a continuum .
you are confused by using $f=ma$ , since , as you walk up the stairs , you are not accelerating , because you walk with a constant velocity . on the sack , we find a net force $f=0$ . but , there is gravity acting , such that the force that you execute is just opposite to gravity : $f=mg$ . the work you put in is then $w=f\times d$ , and thus $w=mgh$ . surprise , surprise , this is also exactly the change in potential energy in the system , which you might have in your notes .
in most cases , this is related to an assumption of small displacements from equilibrium . assume that the system is described by a potential function $v ( s ) $ , where $s$ represents the coordinate ( s ) associated with the normal modes . let $s_0$ represent value of the coordinates the equilibrium state . taylor expanding the potential about this point yields $$ v ( s-s_0 ) \approx v ( s_0 ) + v' ( s_0 ) ( s-s_0 ) + ( 1/2 ) v'' ( s_0 ) ( s-s_0 ) ^2 + . . . $$ the key feature is that we know $v' ( s_0 ) =0$ , since that is the the definition of equilibrium . we can also ignore the first term since it is independent of the state of the system . thus the resulting form of the equation of motion of the form $$ 0 = \ddot{ s } + \omega^2 s^2 $$ with $\omega^2$ a function of $v'' ( s_0 ) $ and the masses/moments of inertia of the system . this equation has $\sin ( \omega t ) , cos ( \omega t ) $ as its solutions . thus , simple harmonic motion is a generic feature of small oscillations about any mechanical equilibrium .
i assume you are interested in using light sent from earth . you could use a laser which does not spread out so much as it travels and so less of it would be wasted . if you used some bulb radiating in one direction , you would use the inverse square law to get the power at a distance $x$ , the distance to jupiter . then multiply the power per unit area at that distance by the area of the circle formed by jupiter 's radius to get the reflected power . then use the square law again in the same way to find the power returned to earth . the power you will need will depend on the sensitivity of the photodiode , used to measure the light . the laser is better because the power per unit area goes down way slower with distance .
a positive cosmological constant leads to positive scalar curvature by definition . just trace over the einstein equation and you end up with $$ r = 4\lambda - 8\pi t $$ which is just $$ r = 4\lambda &gt ; 0 $$ in vacuum . the implicit , but more interesting questions are probably the following ones : why can we interpret the cosmological constant as dark energy ? modelling matter as a fluid in equilibrium , ie $$ t_{\mu\nu} = ( \rho + p ) u_\mu u_\nu + p g_{\mu\nu} $$ the einstein equation reads $$ r_{\mu\nu} - \frac 12 r g_{\mu\nu} = 8\pi ( \rho + p ) u_\mu u_\nu + ( 8\pi p - \lambda ) g_{\mu\nu} $$ now , if we want to fold the $\lambda$ term into the matter terms , we require $$ \rho_\lambda + p_\lambda = 0 \\ 8\pi p_\lambda = -\lambda $$ which is $$ \rho_\lambda = -p_\lambda = \frac\lambda{8\pi} $$ a positive energy density with negative pressure . take note that this pressure is not directly responsible for any acceleration or deceleration of the cosmological expansion : it is uniform across space and stays constant in time , and lacking a gradient , does not induce any forces . its effect is purely gravitational in nature - after all , this is just the cosmological constant in disguise . does positive spacetime curvature actually lead to parallel geodesics converging ? not necessarily due to the lorentzian signature of the metric . take 1+1 de sitter space , which can be realized as a hyperboloid in minkowski space and would look like this ( picture taken from wikimedia commons ) : we get geodesics from the intersections of planes through the origin of the ambient minkowski space with the hyperboloid , and time-like ones from those which are angled less than 45° towards the time axis . the vertical lines thus correspond to time-like geodesics and clearly do not converge . this is where slicing into space-like hypersurfaces comes in : in flrw cosmology , there is a preferred slicing where the galactic fluid is homogeneous . in de sitter space , there is no matter and thus no preferred slicing , but we can nevertheless use it to illustrate various features of the cosmological standard model . the horizontal circles , which we obtain by intersecting a parallel family of planes in the ambient space with the hyperboloid , correspond to a spatially closed universe . choosing appropriate coordinates yields the metric $$ ds^2 = -dt^2 + \alpha^2\cosh^2\left ( \frac t\alpha\right ) d\omega^2 $$ where $d\omega$ is the metric of the euclidean sphere and $\alpha=\sqrt{3/\lambda}$ . by tilting our planes , we can also create flat slicings with corresponding metric $$ ds^2 = -dt^2 + e^{2t/\alpha}dy^2 $$ and open slicings with metric $$ ds^2 = -dt^2 + \alpha^2\sinh^2\left ( \frac t\alpha\right ) dh^2 $$ where $dh$ is the metric of the euclidean hyperbolic space . while the light-like geodesics shown above - corresponding to particles at rest in case of the closed slicing - diverge , the spatial curvature will determine what happens to particles in parallel motion through space . however , this is not something that can be shown in our picture of a 1+1 spacetime . how does this result in an accelerated expansion of the universe ? looking at the spatial part of the metrics , all three slicings ultimately lead to an exponential expansion of space , which , in case of a de sitter universe , is just a matter of geometry . in the closed case however , accelerated expansion happens only after a decelerating collapse to some minimal size determined by the value of the cosmological constant . in friedmann models , as long as the cosmological constant dominates over the matter content , we will eventually approach de sitter geometry and thus also exponential expansion .
none , really . such junctions form in semiconductor crystals . those are remarkable materials . let 's look at electrons in solids first . many atoms have weakly bound valence electrons in outer orbits . these orbits would have specific energies if the atom existed in isolation . but when man atoms are packed together and their outer orbits overlap , the energy of these orbits shift slightly . electrons in those orbits now can have a band of possible energies . now those weakly bound electrons are good for carrying currents , but to hop through the material the overlapping orbits better not be completely filled . there would not be room for the moving electron . in metals , there is in fact a band which is about half filled . ideal - there is plenty of room for electrons to move , but also still enough electrons to move . semiconductors have a full and an empty band close together , and with some external help ( doping , electric fields , etc ) this can be used to switch from conducting to non-conducting . regardless , as the outer orbits overlap and join to form a band , the electrons in that band no longer belong to a single orbit and therefore a single atom . at the quantum level , the probability function of the electron is smeared out over the crystal . and thus it is not sensible to talk about the exact atoms ionized .
the value of $\rho_b$ is incorrect . check the formula you are using . use $\vec{n}\cdot\ ( \vec{d_{out}}-\vec{d_{in}} ) =\sigma_f$ $\vec{n}\cdot\ ( \vec{p_{out}}-\vec{p_{in}} ) =\sigma_b$ on the respective surfaces .
it is not a "$g ( 2 ) $ lattice " one has to compactify the m-theoretical dimensions upon ( after all , the $g_2$ lattice is 2-dimensional ) ; it is the $g_2$ holonomy manifolds . there are lots of different topologies of these seven-dimensional manifolds . they are analogous to the calabi-yau manifolds but do not allow one to use the machinery of complex numbers .
you can see from the q over x diagram that the p-doping is stronger than the n-doping . ( more aceptors per volume in the p-region than donators per volume in the n-region . ) the overall depletion zone is neutral ( e . g . , the diode is not electrically charged ) . therefore , the overall charge must be zero . the positive charge of the depleted n-region with less donators per volume must be the same as the negative charge of the depleted p-region with more acceptors per volume . therefore , the space charge region must penetrate the n-region deeper than the p-region .
when the block collides with the wedge , there will be a normal impulsive force from the wedge , in a direction perpendicular to the surface of the wedge . if you notice , this normal impulsive force has a component in the vertical direction . this is what provides the block with the vertical velocity . as for your second question , the magnitude of the velocity will not change . this is because work has not been done by any of the forces ( normal force and gravity ) during the time the block starts sliding on the wedge . the normal force does not do work because its direction is perpendicular to the displacement . gravity will not do work because there is no sufficient displacement in the vertical direction , in the small time it takes to just start sliding on the wedge . so the speed of the block at the base of the wedge will be the same as it was on the horizontal plane . of course , it will decrease as the block goes up on the wedge . note : this is assuming the block is particle like . if it has finite dimensions , there will be rotational motion when it reaches the wedge .
first , the speed of other galaxies is not too helpful . for example , the radial velocity of the andromeda galaxy relatively to us is 300 km/s , i.e. 0.1% of the speed of light only . moreover , internally , everything in that galaxy moves by pretty much the same speed and is confined to the vicinity of that galaxy which makes us pretty sure that no piece will reach us before andromeda galaxy will . more importantly , macroscopic systems in outer space are not moving with the same huge speeds near the speed of light as the cosmic rays essentially because of the second law of thermodynamics . when cosmic rays are accelerated to high speeds , we may treat them statistically and the high energy of these elementary particles may be assigned a high temperature . but the physical systems with many degrees of freedom prefer to evolve to the most likely , high-entropy configurations . that is why the excess energy ( e . g . in a supernova ) tends to divide between the elementary particles chaotically . in particular , the individual particles ' kinetic energy results from velocities that have a random direction . at these huge temperatures ( kinetic energies per particle ) , the atoms are unbound ( well above the ionization energy ) and macroscopic matter does not exist . so the likelihood that a large object will move towards the earth at a near-luminal velocity is as unlikely as the possibility that the numerous atoms in the large objects are assigned velocities with the exactly equal direction although the directions are being chosen from an isotropic , random distribution . after some time for " thermalization " ( interactions between atoms are allowed to change the system with the conservation laws ' being the only absolutely constraints ) , the greater object you consider , the less likely it is that all the atoms in that object will be doing the same thing . this is a form of the second law of thermodynamics . the previous paragraph prevents the creation of " coherent macroscopic cosmic rays " , macroscopic objects that would move in the same direction , from the thermal chaos of hot environments such as the supernova . but even if some astrophysical process managed to eject a chunk of matter at these high speeds , the previous paragraph will still guarantee that we will not receive it on earth . instead , the individual atom of that speedy object would still have some residual mutual velocities so the object would split into individual atoms and we would observe cosmic rays only once again . i could summarize the situation in this way : to shoot a large object by a huge speed from a very distant celestial body to the earth requires one to have the same and huge radial velocities of all atoms but almost vanishing relative transverse velocities . the likelihood of that goes to zero exponentially in any thermal environment . if one assumes that the source of the initial speed is not thermal , then one must accept that the projectiles will derive their speed from their broader environments – speeds of astrophysical bodies that already exist – and those are simply of order 0.1 percent of the speed of light as in the andromeda example or lower . these modest speeds boil down to the inhomogeneities during the structure formation and , ultimately , to inflation , or to the planetary speeds derivable for a star . whenever you want to locally violate the modesty of the speeds , e.g. by a gravitational collapse , you do not " shoot " any new particles before the collision and when the collision happens , the excess energy of the collision also inevitably creates a high temperature and we are back to the previous paragraph . so near-luminal macroscopic bodies are not observed . i would even go further and despite my being a believer that life in the universe is extremely rare , i would say that an object moving by a near-luminal speed would prove the existence of an advanced civilization . i should be a bit careful : the gravitational slingshot is a process that allows the speed to be enhanced even naturally . but even if the source of the speed were a gravitational slingshot and the speed would be really high , like 99.9999% of the speed of light , chances would be high that some intelligence was behind the optimization of the gravitational slingshot because it is extremely unlikely for such an outcome to occur naturally .
nic and approximist 's answers hit the main points , but it is worth adding an additional word on the reason the orbits lie roughly in the same plane : conservation of angular momentum . the solar system began as a large cloud of stuff , many times larger than its current size . it had some very slight initial angular momentum -- that is , it was , on average , rotating about a certain axis . ( why ? maybe just randomly ! all of the constituents were flying around , and if you add up those random motions , there will generically be some nonzero angular momentum . ) because angular momentum is conserved , as the cloud collapsed the rotation rate sped up ( the usual example being the figure skater who pulls in her arms as she spins , and speeds up accordingly ) . further collapse in the direction perpendicular to the plane of rotation does not change the angular momentum , but collapse in the other directions would change it . so the collapse turns the initial cloud , whatever its shape , into a pancake . the planets formed out of that pancake . by the way , you can see the signs of that initial angular momentum in other things too : not only are all of the planets orbiting in roughly the same plane , but so are most of their moons , and most of the planets ' rotations about their axes as well .
physically what is happening is this : when you touch the positively charged source to the conductor ( the metal sphere ) , electrons leave the conductor through the point of contact . this leaves the point of contact on the conductor with a large deficit of electrons , and thus the point has a positive charge density . the positive charge density produces an electric field in the conductor , which immediately pulls on remaining electrons in the conductor . the electrons remaining spread out until they have eliminated all of the electric fields in the conductor ( if there were remaining fields , the electrons would continue to rearrange ) . the electrons will now be ' more spread out ' than the protons ; the difference between the new electron surface density and the original tells you the distribution of ' excess positive charge ' on the surface . i hope this helps , let me know if you have an application in mind for this ; i often times find it helpful in thinking about problems to temporarily ignore the fact that in practice there is only one charge carrier ( the electron ) and just think about excess positive charge as positively charged particles spreading out .
when you see the history of the universe plotted against time , the time used is the comoving time i.e. the time measured by a clock that is at rest with respect to the universe around it . this is the time co-ordinate used in the flrw metric , which is a solution to the equations of gr that , as far as we can tell , gives a good description of the universe back to very early times . earlier than around the planck time after the big bang we expect the notion of time to become imprecise because it is not possible to measure times shorter than the planck time . without a working theory of quantum gravity it is not possible to comment further . however for all times later than the planck time we expect time to be a good co-ordinate and be well behaved . this allows physicists to calculate at what time the various stages in the evolution of the universe happened . response to comment let me attempt to phrase my answer more broadly . the first point is that in relativity ( both special and general ) you need to be careful talking about time . for example you have probably heard that time runs more slowly when you move at speeds near the speed of light . however there is a well defined standard time that cosmologists use for describing the history of the universe . we call this comoving time . so when you hear statements like " the universe is 13.7 billion years old " we mean it is 13.7 billion years old in comoving time . you do not need to know how comoving time is defined , just that it gives us a good timescale for describing the history of the universe back to the planck time . which brings us to . . . you have probably also heard of heisenberg 's uncertainty principle . again i will gloss over the details , but one side effect of the uncertainty principle is that it is impossible to measure times less than about 5 $\times$ 10$^{-44}$ seconds . i do not know of a simple way to explain this to someone who is not familiar with quantum mechanics , so i am afraid you will have to take this on faith . and this brings us back to hawking 's programme . as long as the times we are interested in are greater than 5 $\times$ 10$^{-44}$ seconds we can define the time using comoving time so we can assign reliable times to cosmological events like the electroweak transition . but for times close to 5 $\times$ 10$^{-44}$ seconds the whole notion of a " time when something happens " becomes meaningless because it is fundamentally impossible to measure times that short . i would guess this is what hawking means when he says time ceases to exist .
what exactly is meant by lorentz invariance ? a physical quantity that is unchanged by a lorentz transformation , i.e. , a coordinate system ( or frame ) independent quantity that is independent of the spacetime coordinates . another term used is lorentz scalar . often lorentz invariant quantities are prefixed with the word proper : proper time , proper distance , proper acceleration . sometimes , they are prefixed with the word invariant , e.g. , invariant mass . this is in contrast with lorentz covariance . an equation is said to lorentz covariant if the equation holds in all inertial reference frames , i.e. , if the equation is valid in one inertial frame , the lorentz transform of the equation is valid . other lorentz covariant objects are four-vectors ( and their duals ) and higher order four-tensors . update to address a comment : " a quantity that is independent of the spacetime coordinates . " that is not right . $\mathbf e \cdot \mathbf b$ is dependent on the spacetime coordinates , yet it is lorentz invariant . no , $\mathbf e \cdot \mathbf b$ is not coordinate dependent though it can vary from event to event in spacetime . i would like to address this comment so there will not be any confusion of the phrase " independent of spacetime coordinates " . to say that a quantity is independent of spacetime coordinates is not to say that it is independent of spacetime . consider a scalar field $\phi$ on spacetime . $\phi$ is a rule that assigns a number $\phi ( \mathcal p ) $ to each event $\mathcal p $ in spacetime . clearly , the number $\phi ( \mathcal p ) $ is coordinate independent ; $\phi ( \mathcal p ) $ does not depend on the coordinates we choose to assign to the event $\mathcal p$ however , coordinate independence does not mean that $\phi ( \mathcal a ) = \phi ( \mathcal b ) $ for two distinct events $\mathcal a$ and $\mathcal b$ . thus , while the numbers may change from event to event , no choice of coordinates can change the number associated with a particular event .
as i understand , the main question that you ask is : " does the fact that we get newton 's second law mean that the lagrangian formulation ( and therefore the principle of least action ) is correct ? " to my taste this question can not be asked of the physics as we know it . what the physics must do is to describe experimental phenomena in such terms as to be able to predict new ones . we all know that this can be done by making use of some maths and by assigning some measurable quantities to physical processes , such as velocity of a particle , its mass , charge and so on . if we say that a movement of a particle is described ( and predicted given the initial data ) by $f=m\ddot{x}$ , this does not mean that the particle actually calculates its own acceleration and moves according to what sir newton formulated . so the newton 's law is just a way to encode all the experimental data , past and future , in one nice formula . the same about other physical laws . now , we have a set of different laws : mechanics , electromagnetism , gr , etc . in a search of some principle that unifies all these laws we discover a prescription , that by doing very similar steps allows to reproduce all of them . since , these laws themselves are just prescriptions to predict phenomena we are just one step higher . however , we have something more universal , less model specific and more flexible . this is what we call " more fundamental " . so , the answer is : no , it is not a coincidence , this is an intentionally designed prescription that allows to reproduce newtons law from just one function $\mathcal{l}$ . why we like this prescription among others ? well , because there are basically no others . the principle of least action is the most fundamental ( read universal and flexible ) prescription to unify physical laws , that we in classical physics . in quantum physics we go further and come to the prescription of path integral . hence , what about the physical meaning . many years ago i used to read books by arnold about geometry of mechanical systems and it seems that there was some geometric sense of $\mathcal{l}$ , but i do not remember exactly . you may try to find yourself . however in general there is nothing particular in the form $\mathcal{l}=t-v$ of classical mechanics as in field theory this is already not so straighforward . my point is , that all physical laws including the least action principle are just prescriptions that allow us to predict phenomena . we can not ask if a prescription is correct or not : as long as it gives good predictions it is " correct " at that stage . in this sense the least action principle is correct as is any other law of physics .
the spacetime in general relativity does not contain " holes " in the sense of excized regions because of a physical argument--- if you can shoot a particle at the region , it should continue into the region . this is the reason that geodesic completeness is used instead of completeness in gr . the condition of geodesic completeness says that the manifold must not have places where geodesics stop for no reason . of course , the singularity theorems guarantee that geodesic completeness fails inside a black hole . but the failure in the case of time-like singularities is mild--- the singularity is only reachable by light rays . the closest thing to an excized region is a black hole . the interior is excized in the sense that it is disconnected causally from the exterior . you can remove the interior and simulate the exterior only ( classically ) and you do not expect to run into too many troubles . whether this is completely true in the quantum version is not clear to me . as for other topological quantities , you can put them in by hand , but it is not clear if they can appear dynamically . there is the topological censorship conjecture , which states that you will not be able to see a topological transition in classical general relativity . i do not know the status ( or even the precise statement ) of this conjecture .
this is really a chemistry question , but i do not think there is a chemistry se at the moment . anyhow , there may not be clearly defined secondary bonds . if you take silicone ( dimethicone ) it has a glass transition temperature about -125c . there are not any specific bonds involved : it is just due to the van der waals forces that you get in any solid . when you have polymers with polar groups , e.g. polymethylmethacrylate you can get hydrogen bonding between the polymer chains . these are more what you had describe as a " secondary bond " and are indeed broken by rising temperature . you generally find hydrogen bonded polymers have much higher glass transition temperatures than polymers that cannot form hydrogen bonds .
consider a conformal theory of fields $\phi$ defined on minkowski spacetime $m=\mathbb r^{3,1}$ and with target space $v$ . let $\rho_m$ denote a representation of the conformal group $g$ on $m$ and let $\rho_v$ denote a representation of $g$ on $v$ . then we can define an action $\rho_f$ of $g$ on fields $\phi$ as follows : $$ ( \rho_f ( g ) \phi ) ( \rho_m ( g ) x ) = \rho_v ( g ) \phi ( x ) $$ this is a more notationally descriptive version of di-francesco eq . 2.114 ; $$ \phi' ( x' ) = \mathcal f ( \phi ( x ) ) $$ in other words , the action of $g$ on fields has two parts , a spacetime part , and a target space ( aka internal ) part . now , suppose that we find that there is another representation $\rho_d$ of $g$ acting on fields ( by way of differential operators for example ) for which $$ \phi ( \rho_m ( g ) x ) = \rho_d ( g ) \phi ( x ) $$ then notice that the first equation above can be written as follows : $$ ( \rho_f ( g ) \phi ) ( x ) = \rho_d ( g^{-1} ) \rho_v ( g ) \phi ( x ) $$ how does this manifest itself in terms of signs and generators ? well , suppose that for any group representation $\rho$ of $g$ , we have a corresponding , linear representation $r$ of its lie algebra $\mathfrak g$ such that if we write an element $g\in g$ as the exponential of a lie algebra element $x$ ; $g = e^x$ , then $$ \rho ( g ) = e^{r ( x ) } $$ then we can write the transformation of the fields as $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-r_d ( x ) }e^{r_v ( x ) }\phi ( x ) $$ for any $x\in\mathfrak g$ . if these lie algebra representations commute ( as they would if , for example , $r_d$ is a representation via differential operators and $r_v$ is some spacetime-independent representation on the target space ) , then we can write $$ ( \rho_f ( g ) \phi ) ( x ) = e^{r_v ( x ) -r_d ( x ) }\phi ( x ) $$ now , suppose that we choose a basis $x_a$ for the lie algebra $\mathfrak g$ such that every element $x\in \mathfrak g$ can be written as $$ x = i\omega_ax_a $$ for some real numbers $\omega_a$ . then using linearity of the representations $r_d , r_v$ we have $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-i\omega_a ( r_d ( x_a ) - r_v ( x_a ) ) }\phi ( x ) $$ so if we use di-francesco 's notation $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-i\omega_ag_a}\phi ( x ) $$ then we have $$ g_a = r_d ( x_a ) - r_v ( x_a ) $$ to see that this leads to consistent signs etc . , let 's look at an example : example . translations consider a field whose target space transforms trivially under translations ; $$ ( \rho_f ( g ) \phi ) ( x+a ) = \phi ( x ) $$ then if $ia^\mu p_\mu$ is the lie algebra element that corresponds to translations $x\to x+a$ , and if we define $$ r_v ( p_\mu ) = 0 , \qquad r_d ( p_\mu ) = -i\partial_\mu $$ then we have $$ g_\mu = -i\partial_\mu $$ so that $$ e^{-ia^\mu g_\mu}\phi ( x ) = e^{-a^\mu\partial_\mu}\phi ( x ) = \phi ( x-a ) = ( \rho_f ( g ) \phi ) ( x ) $$ as desired .
there is no evidence that the global earthquake frequency has changed significantly over that last few centuries . because the physical mechanism responsible for earthquakes is motion along faults , we believe there have been earthquakes for as long as the earth has had a crust and lithosphere comparable to today . the oldest continental rocks dated , and ample geological evidence , indicate that plate tectonics has been ongoing for over 4 billion years . there may have been some variation in the rate of tectonic processes over time , but there is no evidence that plate tectonics is ' slowing down ' because the earth is cooling . you also should not assume that the temperature of the earth 's core has been decreasing over this time . the radioactive decay and gravitational differentiation of the core could provide heat source in the earth 's interior . prior to the formation of the crust , billions of years ago , the processes of the primordial earth would have been different than the processes responsible for earthquakes today .
this was something that confused me for awhile as well until i found this great set of notes : homepages . physik . uni-muenchen . de/~helling/classical_fields . pdf let me just briefly summarize what is in there . the free klein-gordon field satisfies the field equation $ ( \partial_{\mu} \partial^{\mu} +m^2 ) \phi ( x ) = 0$ the most general solution to this equation is $\phi ( t , \vec{x} ) = \int_{-\infty}^{\infty} \frac{d^3k}{ ( 2\pi ) ^3} \ ; \frac{1}{2e_{\vec{k}}} \left ( a ( \vec{k} ) e^{- i ( e_{\vec{k}} t -\vec{k} \cdot \vec{x} ) } + a^{*} ( \vec{k} ) e^{ i ( e_{\vec{k}} t- \vec{k} \cdot \vec{x} ) } \right ) $ where $\frac{a ( \vec{k} ) + a^{*} ( -\vec{k} ) }{2e_{\vec{k}}} = \int_{-\infty}^{\infty} d^3x \ ; \phi ( 0 , \vec{x} ) e^{-i \vec{k} \cdot \vec{x}} $ and $\frac{a ( \vec{k} ) - a^{*} ( -\vec{k} ) }{2i} = \int_{-\infty}^{\infty} d^3x \ ; \dot{\phi} ( 0 , \vec{x} ) e^{-i \vec{k} \cdot \vec{x}}$ introducing an interaction potential into the lagrangian results in the field equation $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi = -v&#39 ; ( \phi ) $ choosing a phi-4 theory $v ( \phi ) = \frac{g}{4} \phi^4$ this results in $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi = -g \phi^3$ introduce a green 's function for the operator $ ( \partial^{\mu} \partial_{\mu} + m^2 ) g ( x ) = -\delta ( x ) $ which is given by $g ( x ) = \int \frac{d^4k}{ ( 2\pi ) ^4} \ ; \frac{-e^{-i k \cdot x}}{-k^2 + m^2}$ now solve the full theory perturabtively by substituting $\phi ( x ) = \sum_{n} g^n \phi_{n} ( x ) $ into the differential equation and identifying powers of $g$ to get the following equations $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi_0 ( x ) = 0$ $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi_1 ( x ) = -\phi_0 ( x ) ^3$ $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi_2 ( x ) = -3 \phi_0 ( x ) ^2 \phi_1 ( x ) $ the first equation is just the free field equation which has the general solution above . the rest are then solved recursively using $\phi_0 ( x ) $ . so the solution for $\phi_1$ is $\phi_1 ( x ) = \int d^4y\ ; \phi_0 ( y ) ^3 \ , g ( x-y ) $ and so on . as is shown in the notes this perturbative expansion generates all no-loop feynman diagrams and this is the origin of the claim that the tree level diagrams are the classical contributions . . .
good question , and the answer is that $w ' &gt ; w$ if $f &gt ; mg$ . the reason for this is that if $f = mg$ then the net force is zero so the particle travels at a constant velocity . that means it is kinetic energy has not changed so the only change is the potential energy . however if $f &gt ; mg$ then the net force is positive and the particle is accelerating upwards . that means when it reaches the height $h$ both the potential energy , $mgh$ , and the kinetic energy , $0.5mv^2$ , have increased . the difference between $w$ and $w'$ is the extra kinetic energy of the particle .
consider the system in equilibrium , with the rod hanging straight down . imagine taking a marker and drawing a vertical diameter across the disk . if the rod were fixed to the disk 's center so that the disk could not rotate , then would this marker line still be vertical mid swing ? what does this tell you about the rotation of the disk around its central axis relative to its starting position ? if the disk were not rigidly attached as in the former case , would there now be any torque on the disk around its central axis ? what would the marker line look like mid swing in this case ?
the result is straight forward . as landau and lifshitz explain in p . 41 , when a body disintegrates into two pieces of masses $m_1$ and $m_2$ respectively , their momenta must be equal in magnitude and oppositely directed by the law of conservation of momentum . so , let each body have momentum $p_0$ . then , $ ( 16.1 ) $ and $ ( 16.2 ) $ say that the difference in the internal energies equals the kinetic energy of the reduced mass . $e_i - e_{1i} - e_{2i} = \frac{p_0 ^2}{2m}$ where $\frac{1}{m} = \frac{1}{m_1} + \frac{1}{m_2}$ let us now see what happens when the body splits into more than two parts . of course , we cannot say anything about the magnitudes and directions of the momentum of each body . but , we shall try to obtain some information about maximum possible kinetic energy and so on . so , now we have one part with mass $m_1$ that we are interested in . if we sum the momenta of the remaining parts , we can conclude that the total momentum of the remaining parts is equal in magnitude and directed oppositely to the momentum of $m_1$ just like the previous case . let it be $p_0$ . hence , we can now use the same equations for masses $m_1$ and $m-m_1$ $\frac{p_0 ^2}{2} ( \frac{1}{m_1} + \frac{1}{m-m_1} ) = e_i - e_{1i} - e_{i}^{'}$ which on simplification gives $\frac{p_0 ^2}{2m_1} \frac{m}{m - m_1} = e_i - e_{1i} - e_{i}^{'}$ and the result follows .
just use the free-fall equation . the time spent in falling from a height $ h $ verifies this : $ h - \frac{g}{2} t^2 = 0 $ so you get : $ g = \frac{2h}{t^2} $ note that you have to determine the height $ h $ . how ? , maybe you can estimate it thinking that the game 's character is about 1.80 m . the $ g $ you are obtaining here is the acceleration of the gravity . in order to get the " universal " gravity constant for the game 's universe , $ g $ , you will need to know , or at least estimate , the mass of the planet and the radius of the planet . . . or the ratio $\frac{m}{r^2}$
the angular size of the object can be calculated by basic trigonometry : $\theta=2\cdot \arctan ( r/d ) $ , where $r$ is the radius of the object you are viewing , and $d$ is the distance between you and the object ( $\theta$ is the angle ) . the average ( volumetric ) radius of saturn is 58,232 km . the distance between titan and saturn is 1,221,830 km . plugging the numbers in gives an angular size of 5.46° . doing the same for our moon gives you 0.52° . dividing one by the other gives you a factor of $\sim 10.5$ difference . &nbsp ; note 1: when you do this math with a calculator , verify you get the correct results for the moon from earth before you go on to something else . you may encounter issues where the results of your arctan ( ) function will be given in radians , not degrees . if the math gives you a weird result , multiply by $180/\pi \approx 57.3$ . note 2: saturn would not actually be visible from the surface of titan due to the thick atmosphere of the moon . also , tidal locking has nothing to do with this problem other than if saturn may be visible from an arbitrary location on titan ( if you could see through its atmosphere ) .
using the distance between the sun and the earth , at least for distances within the solar system , just gives a better feel for the scales involved . you can not really imagine a distance of , say , 1000000000 kilometers -- or at least i can not . ( i deliberately did not include commas in that number , to illustrate the point . ) but using a concrete physical distance creates a kind of mental anchor , and makes the relative scale easier to visualize . tell me that neptune is about 4.5 billion kilometers from the sun , and i think " wow , that is a really big number " . tell me that it is about 30 aus from the sun , and that is something i can fit into a mental image . one au is still unimaginably long , but the ratio of 30 aus to 1 au is easy . on the other hand , if you want to do physical calculations ( say , calculating the orbit of some body under the influence of various gravitational fields ) , then it makes more sense to use metric units ( meters , kilometers ) . the universal gravitational constant G is expressed in units of m 3 ·kg -1 ·s -2 ; it could be expressed with an au as the length unit , but i have never heard if it being done that way . basically , au is used to express distances for a human audience ; meters and kilometers are used for calculations . update : ghoppe comments : actually , ephemerides have been often calculated in astronomical units and not in si units because neither g nor the mass of the sun can be measured to high accuracy in si units , but the value of their product is known very precisely due to kepler 's third law . the value of au depends on the product .
there are at least two issues : @lagerbaer 's comment points out that that you have to convert the bin numbers into time intervals . the time for your experiment is already fixed , so if you use more bins , then each bin indicates a shorter time interval . the final bin in your third plot means a time twice as much as the final bin in your second plot , and three times as much as the final bin in your first plot . so when you fit your data for the half-life , you are actually getting the half-life in terms of bins . correcting this might be as simple as multiplying by the width of each bin to convert the half-life from bins to $\mu\mathrm{s}$ . to properly fit your data , you have to account for the uncertainty in your measurements . each of your bins is a count or a count rate , and will have some uncertainty in that count or count rate . the decay of a particle follows poisson statistics , where the uncertainty in the number of counts increases with the number of counts . but it increases more slowly , so the relative uncertainty goes down as your measured count rate goes up . including the uncertainties in your analysis re-weights your bins according to their relative uncertainty . by leaving that out , you are inadvertently over-weighting the later bins . over-weighting the later bins like that will give you a larger value for the half-life . one way to look at this is to look at bin 15 in your third plot . for an exponential decay , it should be less than bin 14 , and greater than bin 16 . but it is less than bin 16 . that is a reflection of the counting statistics . the weighting issue in point 2 is a subtle effect ; i do not think it would double the half-life you get from your fit . the bin size in point 1 is much larger , and could account for an error of that size . you should correct that first . and depending on the level of the course , you might not even be expected to do the full uncertainty analysis .
i do not know the answer for the case with two horizons ( non-extremal case ) , but when there is only one ( that is the extremal case : q=m in suitable units and common notation for charge and mass ) , here is how it works . let us first note that in the extremal case , the timelike coordinate outside the black hole remains timelike inside it . this is a major difference with the non-extremal case and with the schwarschild black hole . let us look at what happen to probes and observers in this universe . as you mention , an observer that is at rest in the rn coordinate system and outside the horizon will never see a falling object ( say a massive probe , that is a body with a small mass compared to the black hole mass ) cross the horizon . on the other hand , it takes a finite amount of proper time for the falling body to cross the horizon . the probe , however , will not meet the ( time-like ) singularity . it will automatically , at some finite distance of the singularity , " bounce " back and " return " towards the horizon . this is not in contradiction with causality as the radial coordinate is still space-like in this region . the probe will then again cross the horizon ( but this time from inside to outside ! ) , and one can show that all this is achieved in finite proper time . we thus arrive at the puzzle : where is the probe now ? we know it is outside the horizon . but on the other hand we know it cannot be in the same patch it was before falling into the black hole , as for static observers in this region the probe nerver crossed the horizon . the solution to this puzzle is simply that the probe has arrived a new space-time patch corresponding to the outside of the horizon . ( this is somewhat similar to the schwarshild black hole , where you see that there are light rays that seem to come " from " the black hole , but since it is impossible to escape from it , the light rays must come from a new patch that correspond to the inside of the black hole , that is then called a white hole . ) repeating the above reasoning , we see that consistency of the different viewpoints in the extremal rn universe requires that it contains an infinite number of patches . i do not know the discussion for the non-extremal case , but i hope that at least some aspects of your question are now clearer . cheers
the only mistake i see is " the length of the spring was in turn reduced by a/4" . it should read : increased by a/4 . in that sense you are right . but for the force of the spring you should still use the displacement of a/4 . after all , the spring could have length l not equal to the gap a . that one is of your own device .
the sound that reaches your ear is just air pressure fluctuating over time . you can use a transducer of some sort to convert the value of air pressure to some other form - for example : to the depth of a groove being cut into a helical track on a layer of wax on a rotating drum to the depth of a groove being cut into a spiral track on a circular disc of metal from which other plastic disks are pressed . to a strength of magnetisation of a magnetic layer on a plastic tape being wound onto a spool to a series of numbers representing the pressure at regular tiny intervals of time . the idea that the variations in pressure over time are due to , or consist of , a collection of frequencies is just a mathematically equivalent description but it does not represent extra information , it is just a different way of describing the same information . here 's some diagrams from a synthesizer manual above are three very different sounds with apparently the same frequency ( say 440 hz ) above is shown how you can add sine waves of two frequencies to produce a more complex waveform above is shown how you can continue adding sine waves of differing frequencies to construct an arbitrary waveform ( a sawtooth ) . the sawtooth waverform can be recorded directly as depths in a groove on a record . but you could " record " the same thing as a set of numbers that represent the frequencies of a dozen sine waves you could add together to produce a single pressure wave that varies over time in the same way . see fast fourier transform
as stated , $\mathbf{n}$ is a unit vector and $n_x$ , $n_y$ and $n_z$ are its cartesian components . $\mathbf{n}$ is just a vector pointing in an arbitrarily direction with magnitude 1 . taking $\mathbf{n} \cdot \mathbf{\sigma}$ , we have \begin{equation} \mathbf{n} \cdot \mathbf{\sigma} = n_x\sigma_x + n_y \sigma_y + n_z \sigma_z \\ = n_x \left ( \begin{array}{cc} 0 and 1\\1 and 0\end{array}\right ) + n_y \left ( \begin{array}{cc} 0 and -i\\i and 0\end{array}\right ) + n_z \left ( \begin{array}{cc} 1 and 0\\0 and -1\end{array}\right ) \end{equation}
light passing through a moving medium undergoes a shift due to the difference in frames between the two media . this problem is quite simple to solve in the frame of the river . in this frame the light is moving at an angle and the river is still . the air is moving relative to the river but since air has an index of refraction of $1$ , its movement does not have any effect on the behaviour of light . then you can use the ordinary snell 's law and finally boost back to the original frame . the only subtlety here is that we are in some sense using both the particle and wave viewpoints of light since we will discuss momenta as well as snell 's law , however i can not see an issue with doing so . i denote the lab frame without a prime and the river frame with a prime . the initial light momenta was , \begin{equation} p _i = e \left ( 1,1,0,0 \right ) ^t \end{equation} boosting into the river frame we have , \begin{equation} p ' _i = e\left ( \gamma , 1 , - \beta \gamma , 0 \right ) ^t \end{equation} therefore the angle of incidence is \begin{align} and \tan \theta _i ' = \beta/ \sqrt{ 1 - \beta ^2 } \\ \rightarrow and \sin \theta _i ' = \beta \end{align} now using snell 's law we have , \begin{equation} \sin \theta _f '= \frac{ \beta }{ r} \end{equation} where $ r $ is the ratio of indices of refraction , $ n _f / n _i $ . therefore the momenta of light in the water in the frame of the water is , \begin{equation} p _f ' = e \gamma \left ( 1 , \sqrt{ 1 - \beta ^2 / r } , \beta / r , 0 \right ) ^t \end{equation} boosting back to the lab frame we have , \begin{equation} p _f = e\left ( \gamma + \beta ^2 \gamma /r , \sqrt{ 1 - \beta ^2 / r ^2 } , \beta \gamma + \beta \gamma / r , 0 \right ) ^t \end{equation} to find out how the light will behave once it exits the river we note that the angle of incidence at the second interface is the same as the angle of refraction in the water ( still in the river frame ) . we have , \begin{equation} \sin \theta _{ \mbox{out }} '= r \sin \theta _f ' = \beta \end{equation} this is the same as the initial angle in the water frame . after boosting back to the lab frame we should get back the same perpendicular light arrow . in total the journey of the light should take the form , $\hspace{1cm}$ where the shade of red is proportional to the speed of the river , the lightest being $0.1c$ and the darkest being $0$ . as we would expect in the limit that $\beta\rightarrow 0 $ the refraction effect goes to zero and we note that the effect is only significant for huge river speeds .
this vector potential can be written in every point on the plane except the origin as : $$ a_x = -\frac{\partial \psi}{\partial y}$$ $$ a_y = \frac{\partial \psi}{\partial x} $$ with $$\psi = \frac{1}{2}\mathrm{log} ( x^2+y^2 ) $$ $a$ is not exact , because $\psi$ is singular at the origin . but this means that the magnetic field is zero at every point except the origin . at the origin itself , the magnetic field must be infinite , because the flux through an arbitrary small loop is nonvanishing : $$\phi = \int a = \int_0^{2\pi} d\phi = 2 \pi$$ such a magnetic field can be generated by a infinite solenoid whose radius is shrunk to zero while keeping the flux constant . in the hydrodynamical terminology , the function $\psi$ is called the stream function , it satisfies the laplace 's equation ( harmonic function ) except at the origin . this specific stream function describes a vortex ( the vector potential describes the velocity field of the vortex ) . the streamlines of this velocity field are circles around the origin and its magnitude is inversely proportional to the radius . in order to see that the stream function is harmonic except at the singularity , and generalize the construction to the case of the sphere , we can use complex coordinates on the plane : $$ z = x+iy$$ in this representation we have : $$\psi = \frac{1}{2}\mathrm{log} ( \bar{z}z ) $$ applying the laplace operator , we get $$\nabla^2 \psi=\partial_{\bar{z}} \partial_z \psi = \delta^2_l ( z ) $$ where we have used $$ \frac{\partial}{\partial \bar{z}}\frac{1}{z} = \delta^{ ( 2 ) }_l ( z ) $$ is the complex coordinate on the plane . $\delta^{ ( 2 ) }_l$ is the two dimensional dirac delta function with respect to the lebesgue measure . i.e. , $$\int_{\mathbb{c}} f ( z ) \delta^{ ( 2 ) }_l ( z-z_0 ) \mathrm{dre} ( z ) \mathrm{dim} ( z ) = f ( z_0 ) $$ the vector potential in the complex representation has the form : $$ a_z = \frac{1}{i}\frac{\partial \psi}{\partial \bar{z}}$$ $$ a_{\bar{z}}= -\frac{1}{i}\frac{\partial \psi}{\partial z} $$ explicitly : $$a = \frac{1}{2i}\frac{zd\bar{z}-\bar{z}dz}{\bar{z}z}$$ this fact describes another physical interpretation of this vector potential as follows : in two dimensions a function satisfying the laplace 's equation ( harmonic function ) ( except at the point singularities ) qualifies to be a stream function whose anti-symmetrized gradient ( which is the vector potential in our problem ) describes the velocity field of a vortex . please observe that that this velocity field is invariant under rotations about the origi n and its magnitude is inversely proportional to the distance from the origin . in this interpretation , the line integral of the vector potential is the vorticity . we can change the location of the singularity ( flux line ) to any other point in the plane , say $ ( x_0 , y_0 ) $ $$a = \frac{ ( x-x_0 ) dy - ( y-y_0 ) dx}{ ( x-x_0 ) ^2+ ( y-y_0 ) ^2} = \frac{1}{2i}\frac{ ( z-z_0 ) d\bar{z}- ( \bar{z}- \bar{z_0} ) dz}{ ( \bar{z}- \bar{z_0} ) ( z-z_0 ) } $$ in this case it is not hard to verify that this vector potential can be derived from the stream function : $$\psi = \frac{1}{2}\mathrm{log} ( ( \bar{z}-\bar{z}_0 ) ( z-z_0 ) ) \equiv \mathrm{log} ( |z-z_0| ) $$ we can add several stream functions centered at various points on the plane with different vorticities to obtain a general solution representing fluxes at these points : $$\psi = \sum_k \gamma_k \mathrm{log} ( |z-z_k| ) $$ the constant $\gamma_k $ expresses the fluxes around the $k$-th center ( or the vorticity in the hydrodynamical terminology ) . one can easily verify that the single centered vector potential ( and also the corresponding stream function ) are invariants under the metric preserving automorphisms of the plane consisting of translations and rotations : ( which can be compactly written in the complex notations as : ) $$z \rightarrow e^{i\alpha} z + v$$ $$z_0 \rightarrow e^{i\alpha} z_0 + v$$ from the expression of the single centered stream function , one observes that the denominator is the geodesic distance on the plane , thus a candidate generalization to the sphere ( $s^2$ ) would be the replacement of this by geodesic distance on the sphere : $$|z-z_0|^2 \rightarrow \frac{|z-z_0|^2}{ ( 1+\bar{z}z ) ( 1+\bar{z}_0z_0 ) }$$ where $z$ is the stereographic projection coordinate on the sphere : $$ z = \mathrm{tan}{\frac{\theta}{2}}e^{i \phi}$$ ( $\theta$ and $\phi$ are the spherical surface coordinates ) . thus the candidate solution on the sphere is : $$\psi= \mathrm{log} ( |z-z_0| - \frac{1}{2} \mathrm{log} ( 1+\bar{z}z ) - \frac{1}{2} \mathrm{log} ( 1+\bar{z}_0z_0 ) ) $$ this solution is invariant under the metric preserving automorphisms of the sphere : $$ z\rightarrow \frac{\alpha z + \beta}{-\bar{\beta} z +\bar{\alpha} }$$ , with $|\alpha|^2+|\beta|^2 = 1$ the matrix : $$\begin{pmatrix} \alpha and \beta\\ -\bar{\beta} and \bar{\alpha} and \end{pmatrix} \in su ( 2 ) $$ which is the automorphism group of the round metric thus the candidate vector potential corresponding to this solution is obtained by applying the gradient operator in the sphere 's curvlinear coordinates : $$ a_z = \frac{1}{i} ( 1+\bar{z}z ) ^2 \frac{\partial \psi}{\partial \bar{z}}$$ $$ a_{\bar{z}}= -\frac{1}{i} ( 1+\bar{z}z ) ^2 \frac{\partial \psi}{\partial z} $$ explicitly $$a = \frac{1}{2i} ( 1+\bar{z}z ) ( 1+\bar{z}_0z_0 ) \frac{ ( z-z_0 ) ( 1+\bar{z}_0 z ) d\bar{z}- ( \bar{z}- \bar{z_0} ) ( 1+\bar{z}_0 z ) dz}{ ( \bar{z}- \bar{z_0} ) ( z-z_0 ) }$$ the laplacian on the sphere is given by : $$\nabla^2 = ( 1+\bar{z}z ) ^2 \frac{\partial}{\partial z}\frac{\partial}{\partial\bar{z}}$$ applying the laplacian operator to the candidate stream function , we obtain : $$\nabla^2 \psi= ( 1+\bar{z}z ) ^2 \delta^{ ( 2 ) }_l ( z-z_0 ) +1 = \delta^{ ( 2 ) }_s ( z-z_0 ) +1$$ where $ \delta^{ ( 2 ) }_s$ is the dirac delta function corresponding to the spherical measure : $$\int_{\mathbb{s^2}} f ( z ) \delta^{ ( 2 ) }_s ( z-z_0 ) \frac{ \mathrm{dre} ( z ) \mathrm{dim} ( z ) }{ ( 1+\bar{z}z ) ^2}= f ( z_0 ) $$ the additional constant term in the laplacian constitutes a problem because it means that this stream function is not harmonic outside the singularities . the solution to this problem on the sphere is to add up several solutions with a vanishing total flux ( vorticity ) . $$\sum_k \gamma_k = 0$$ in this case the constant contributions from all centers will cancel .
you are correct to observe that there is an often unstated assumption in the standard setup of this problem . when given this problem you are supposed to assume that the off-major-axis components of angular velocity make a contribution to l which is negligible compared to the on-axis angular velocity . obviously this is a good assumption if the top is rotating fast enough , but it is not exactly true . if you start with the initial condition , that the top 's angular velocity is completely aligned with its major axis at the time of release , you should find that the top 's major axis does not really rotate uniformly in a circle but rather there is a very small sinusoidal variation about the uniform circular motion .
let us consider the velocities of the ball at some interim height $h$ on the ball 's way up and down . the absolute value of the velocity is higher on the ball 's way up , as the potential energy of the ball is the same and the total energy is less on its way down due to air resistance . as this applies to any height $h$ , the conclusion is that the time the ball goes up $t_1$ is less than the time the ball goes down $t_2$ .
i disagree with the premise of your question that energy is a " simply mathematical object " . indeed , the fact that it can have density and flux is one the reasons why it ought to be considered a real , tangible thing instead of just a bookkeeping device . while energy in on all its forms is a property of something else , being a property does not reduce a physical thing to being simply a mathematical object . as dmckee said , the idea is that when you have a field ( any field , not just the em field ) you can meaningfully discuss the energy at a particular point . if a field can store energy , then it makes sense that different sized " chunks " of the field will store different amounts of energy . think , for example , about electromagnetic waves that can deliver power to a receiver . a big box that contains many wave periods will obviously have more energy inside it than a small box that contains very few wave periods , since the wave delivers a certain amount of energy per period . there is nothing special about waves vs . other field configurations , so the same reasoning applies to any form of the electromagnetic , or any other , field . well , if it is meaningful to talk about the total field energy contained in some box then its meaningful to compute the average density of energy by dividing out the box volume . if i take the limit of smaller and smaller boxes around a point , this average density approaches the density at that point . we can go through the same sort of reasoning with momentum density ( which is closely related to the energy flux ) . the fact that , as you observed , energy appears to flow like a fluid is because conservation of energy is actually a much stronger statement than you are imagining . you are used to talking about energy conservation in global terms : the change in the energy of a system is equal to the work done on the system by its environment minus the work done on the environment by the system . in field terms , this means that if you draw any sized box , then the change in the total energy in the box equals the total flux going through the sides of the box . by making the box arbitrarily small , we get ( using the divergence theorem ) a local statement of conservation of energy : if $\rho ( \vec{x} , t ) $ is energy density and $j ( \vec{x} , t ) $ is the energy flux then $\frac{\partial\rho ( \vec{x} , t ) }{\partial t} + \nabla \cdot j ( \vec{x} , t ) = 0$ . this is a very powerful statement and is more fundamental than the global statement . for instance , in general relativity local conservation of energy still holds but , because of the complications introduced by curvature , the global statement fails .
2012-07-11 addendum based on excellent inputs , in particular from @annav , my answer is now " no , even a direct worst-case hit by the oh-my-god particle would not kill you , even by radiation , because there is insufficient distance and angle to generate a fatal radiation cone . thanks all , and be sure to look at the earlier answer that @dmckee pointed out . ** original answer** ( my answer seems to differ from the earlier ones that @dmckee aptly pointed out , so i will go ahead and risk posting it . my main difference is that i suspect that a head on collision with a large nucleus could produce a wide enough horizontal-splatter radiation cone to produce a fatal event . ) since the 1991 oh-my-god particle was most likely a proton and had the kinetic energy of a fast baseball , i am going out on a limb and saying yes , you could be killed by a single particle . this harvard physics site suggests an approximate energy transfer of about 0.2% in transfers with heavy nuclei , which as i discuss below may be enough to do you in with that kind of particle . but it would be the ensuing radiation event and cone that would do you in , not the kinetic energy of the particle . the main issue is that your head does not have anything in it remotely solid enough to stop or even slow down a particle with that much momentum . so , like a locomotive passing through a cloud of fog , it is going to zip through pretty much as if your head is not there . the question , then , is to ask not what the fog will do to the locomotive , but what the locomotive will do to the fog -- that fog being your head . even a single solid , exactly head-on collision with a nice fat iron nucleus just as an ultra cosmic ray proton enters your head would probably not be a pretty event in terms of the resulting secondary radiation shower . i am guessing ( nothing more , i have not tried to calculate anything ) that outward splattering of a nice little quark plasma , one created as the iron nucleus vaporizes during the transition event , could produce a sufficiently wide cone of particle-zoo ejecta to irradiate a fatal percentage of your brain . rapid heating of your brain would not be a problem , however , since 1/500 of the approximately 50 joules of kinetic energy would work out to be only about 0.1 j of heat energy tops . by comparison a standard firecracker releases about 500 j of energy . and what are the real odds on such a dead-center strike on a large nucleus near the surface of your brain , assuming you were an astronaut unprotected by out atmosphere ? low almost beyond belief . look at the date on the oh-my-god particle : 1991 . we have not seen one quite that feisty since . as @dmckee aptly notes , ordinary cosmic rays or their secondary outputs hit us all the time , and astronauts watch direct collision buzz through their retinas without much harm .
let me guess : you take the spring as it is and hang your objects , right ? then measure the displacement . try to do the following : hang any arbitrary object so that the string will stretch a bit from its initial state . then add you 100g and 200g objects to the initial mass and measure the difference in spring 's length . i will be surprised if you will not get good results . explanation : there are other forces involved when the spring is in its initial condition ( as in the picture ) . when you initially stretch it a bit , you neutralize these forces and the only force left is hooke 's one .
you apply a ( net ) force ( i.e. . push it ) . recall that the generalized version of newton 's 1st law is that force is proportional to the rate of change in momentum : $$ \vec{f} = \frac{\mathrm{d} \vec{p}}{\mathrm{d}t} \ , , $$ or in the language of impulse ( $j$ ) $$ \vec{j} = \delta\vec{p} = \langle \vec{f} \rangle \delta t \ , , $$ with $\langle \rangle$ meaning the time-average of the enclosed quantity . written this way the law is completely valid in special relativity as well as in newtonian mechanics , which is nice because much of particle physics occurs are relativistic relative velocities .
initially , you have 6 equations given by ( 16 ) and ( 17 ) . now you insert both the expressions for the components of the metric and the power series expansion and determine its coefficients in such a way that the equations are satisfied order by order .
let 's distill your question down to its conceptual essence : given an lc circuit , the capacitance $c$ , the maximum voltage $v_m$ and the maximum current $i_m$ , can you find the inductance $l$ . that is really all there is to it since , once you find $l$ , you know the frequency and the period . ( by the way , in an lc circuit there is only one independent voltage and one independent current ) . the answer is yes , you can find $l$ . since you know the maximum voltage , you know the maximum energy stored by the capacitor and thus , you know the maximum energy stored by the inductor ( there is no energy lost in an lc circuit ) . thus , you have the relation : $\dfrac{cv^2_m}{2} = \dfrac{l i^2_m}{2}$ you can solve for l with this since you are given the other values and the rest follows .
lehner and pretorius have recently given some persuasive numerical evidence that there are generic violations of cosmic censorship which arise in the time evolution of black strings in 5d gravity , aka the gregory-laflamme instability http://arxiv.org/pdf/1006.5960 . the failure of cosmic censorship certainly does not imply the downfall of causality . classically it means we can see a singularity without it being hidden behind a horizon . this seems to me like a very good thing . we all believe that there is some uv completion of gravity , although different camps have different views on what this is . this theory will cut off the singularity through some combination of classical modifications to einstein gravity and quantum effects . being able to see such a direct effect of the uv completion would be a wonderful thing , so i think we should all hope that there are also generic violations of cosmic censorship in four dimensions , although as far as i know the jury is still out on this .
first order of business is to find where the heck on earth you are . first , $\omega = 360 \sin ( \phi ) /day$ , where $\omega$ is 216.528 degrees ; $\phi$ is the latitude of your position . north of the equator is positive , south negative . this gives you a band to follow around the earth horizontally , positions where could possibly be . you can further narrow your position down because a foucault pendulum can be used to find the acceleration of gravity at its position . once you figure this out , you can go to nasa websites and check out when this location has its next or last total solar eclipse .
you have not really asked a precise question , or given an example , but i think i know what you are getting at . you have misunderstood what it means for the two states to ' have different symmetry ' . suppose , as you say , that $g$ is some operator representing a symmetry of the system . this means that $g$ is unitary , and $ [ g , h_0 ] = [ g , v ] = 0$ ( we could also consider $ [ g , v ] \neq 0$ , but i do not think this is what you need ) . since $g$ is unitary and commutes with $h_0$ , the ground state of $h_0$ will also be an eigenstate of $g$ ( here we assume non-degeneracy of the ground state ) : $g|\phi_0\rangle = \lambda|\phi_0\rangle$ for some complex number $\lambda$ . the same argument applies for $h$ , so $g|\psi_0\rangle = \lambda'|\psi_0\rangle$ for some ( possibly different ) complex number $\lambda'$ . now consider $\langle\psi_0|g|\phi_0\rangle$ . we can let $g$ act either ' forwards ' on $|\phi_0\rangle$ , or ' backwards ' on $\langle\psi_0|$ ( exercise : show that $\langle\psi_0|g = \lambda'\langle\psi_0|$ as a consequence of unitarity of $g$ ) , to get $$ \lambda\langle\psi_0|\phi_0\rangle = \lambda'\langle\psi_0|\phi_0\rangle ~ , $$ and therefore $$ ( \lambda-\lambda' ) \langle\psi_0|\phi_0\rangle = 0 ~ . $$ so if $\lambda ' \neq \lambda$ , we find $\langle\psi_0|\phi_0\rangle = 0$ . example : a good example would be a particle moving in one dimension , with $h_0 = \frac{p^2}{2m} + \lambda x^4$ , and $v = -\mu^2 x^2$ , where $\lambda$ and $\mu$ are real constants . there is a symmetry $g : x\to -x$ ; the ground state of $h_0$ is even under this symmetry , whereas the ground state of $h = h_0 + v$ is odd . in symbols , $g|\phi_0\rangle = |\phi_0\rangle~ , ~~ g|\psi_0\rangle = -|\psi_0\rangle$ .
they are variants , different kinds of quantum field theory , but they are not mutually exclusive . the different adjectives you mention separate quantum field theory to " pieces " in different ways . the different sorts of variants you mention are being used and studied by different people , the classification has different purposes , the degree of usefulness and validity is different for the different adjectives , and so on . conformal quantum field theory is a special subset of quantum field theories that differ by dynamics ( the equations that govern the evolution in time ) , namely by the laws ' respect for the conformal symmetry ( essentially scaling : only the angles and/or length ratios , and not the absolute length of things , can be directly measured ) . conformal field theories have local degrees of freedom and the forces are always long-range forces , which never decrease at infinity faster than a power law . they are omnipresent in both classification of quantum field theories - almost every quantum field theory becomes scale-invariant at long distances - and in the structure of string theory - conformal field theories control the behavior of the world sheets of strings ( here , the cft is meant to contain two-dimensional gravity but the latter carries no local degrees of freedom so it does not locally affect the dynamics ) as well as boundary physics in the holographic ads/cft correspondence ( here , cfts on a boundary of an anti de sitter spacetime are physically equivalent to a gravitational qft/string theory defined in the bulk of the anti de sitter space ) . conformal field theories are the most important class among those you mentioned for the practicing physicists who ultimately want to talk about the empirical data but these theories are still very special ; generic field theories they study ( e . g . the standard model ) are not conformal . topological quantum field theory is one that contains no excitations that may propagate " in the bulk " of the spacetime so it is not appropriate to describe any waves we know in the real world . the characteristic quantity describing a spacetime configuration - the action - remains unchanged under any continuous changes of the fields and shapes . so only the qualitative , topological differences between the configurations matter . topological quantum field theory ( like chern-simons theory ) is studied by the very mathematically oriented people and it is useful to classify knots in knot theory and other " combinatorial " things . they are the main reason behind edward witten 's fields medal etc . axiomatic or algebraic ( and mostly also " constructive " ) quantum field theory is not a subset of different " dynamical equations " . instead , it is another approach to define any quantum field theory via axioms etc . that is why it is a passion of mathematicians or extremely mathematically formally oriented physicists and one must add that according to almost all practicing particle physicists , they are obsolete and failed ( related ) approaches which really can not describe those quantum field theories that have become important . in particular , aqfts of both types start with naive assumptions about the short-distance behavior of theories and are not really compatible with renormalization and all the lessons physics has taught us about these things . constructive qfts are mainly tools to understand the relativistic invariance of a quantum field theory by a specific method . then there are many special quantum field theories , like the extremely important class of gauge theories etc . they have some dynamics including gauge fields : that is a classification according to the content . qfts are often classified according to various symmetries ( or their absence ) which also constrain their dynamical laws : supersymmetric qfts , gravitational qfts based on general relativity , theories of supergravity which are qfts that combine general relativity and supersymmetry , chiral qfts which are left-right-asymmetric , relativistic qfts ( almost all qfts that are being talked about in particle physics ) , lattice gauge theory ( gauge theory where the spacetime is replaced by a discrete grid ) , and many others . gauge theories may also be divided according to the fate of the gauge field to confining gauge theories , spontaneously broken qfts , unbroken phases , and others . string field theory is a qft with infinitely many fields which is designed to be physically equivalent to perturbative string theory in the same spacetime but it only works smoothly for open strings and only in the research of tachyon condensation , it has led to results that were not quite obtained by other general methods of string theory . we also talk about effective quantum field theories which is an approach to interpret many ( almost all ) quantum field theories as an approximate theory to describe all phenomena at some distance scale ( and all longer ones ) ; one remains agnostic about the laws governing the short-distance physics . that is a different classification , one according to the interpretation . effective field theories do not have to be predictive or consistent up to arbitrarily high energies ; they may have a " cutoff energy " above which they break down . it does not make much sense to spend too much time by learning dictionary definitions ; one must actually learn some quantum field theory and then the relevance or irrelevance and meaning and mutual relationships between the " variants " become more clear . at any rate , it is not true that the classification into adjectives is as trivial as the list of colors , red , green , blue . the different adjectives look at the framework of quantum field theory from very different directions - symmetries that particular quantum field theories ( defined with particular equations ) respect ; number of local excitations ; ability to extend the theory to arbitrary length scales ; ways to define ( all of ) them using a rigorous mathematical framework , and others .
the important thing is the cross-sectional area of the horizon , and this is independent of lorentz transformation , since the $y$ and $z$ coordinates are not changed . additionally , you can calculate that light will be captured by the horizon with non-zero cross section , and the geodesics ultra-relativistic particles will asymptote to the geodesics of null particles .
first , observe that although the non-relativistic lagrangian is not invariant . it changes by a total derivative , thus the equations of motions remain invariant . the reason of the difference between the lorentzian and the galilean cases is that the group action of the lorentz group on the classical variables ( positions and momenta ) is a by means of a true representation , while in the case of the galilean group the representation is projective . in the language of geometric quantization , $exp ( i \frac{s}{\hbar} ) $ , where $s$ is the action is a section in $l \otimes \bar{l}$ , where $l$ is the prequantization line bundle and $\bar{l}$ its dual . in other words , the action needs not be a scalar , only an exprssion of the form : $\bar{\psi} ( t_2 ) exp ( i \frac{s ( t_1 , t_2 ) }{\hbar} ) \psi ( t_1 ) $ , where $\psi ( t ) $ is the wavefunction at time $t$ and $s ( t_1 , t_2 ) $ is the classical action between $t_1$ and $t_2$ . the reason that the representation in the galilean case is projective is related to the nontriviality of the cohomology group $h^2 ( g , u ( 1 ) ) $ in the galilean case in contrast to the lorentz case . i have given a more detailed answer on a very similar subject in my answer to anirbit : poincare group vs galilean group and in the comments therein .
you are always subjected to a low background of ionizing radiation from a number of natural and artificial sources , which include cosmic rays , trace amounts of radioactive nuclei in the air and in food , and indeed from the ground . a good place to read up on this is the corresponding wikipedia article . the radiation from the core , however , has no chance of making it to the surface . gamma radiation is typically stopped by a few to maybe 20 or 30 cm of rock or soil . ( there is also the danger that material used to shield against gamma radiation becomes radioactive itself , but of course this is hardly an issue with the earth 's core . ) alpha and beta rays are even easier to stop . trace amounts of radioactive nuclei , though , can be present in the soil and buildings around you and will then expose you to a small amount of radiation . it is important to note that this is natural and nothing to worry about . the most significant contribution to background radiation , away from zones like hiroshima , chernobyl or fukushima , is the trace amount of radon in the air you breathe .
nothing , your book did it wrong .
well , there is the " easy java simulations " ( ejs ) . it is open source and very intuitive . you do not need to know java to use it . i do not know , though , if it is good for big systems , but since it is written in java it should handle it quite well . it can be downloaded from here and you can also check some examples in that website . well for more complex things , mathematica .
rocket fuels initially , followed by a series of gravitational assists ( slingshots ) : http://en.wikipedia.org/wiki/gravity_assist the linked article mentions voyager 1 mission as an example .
if you mix d$_2$o and h$_2$o you quickly get dho due to the grotthuss mechanism . i assume this is what you mean by heavy water being contaminated on exposure to humid air . obviously it would not be contaminated by exposure to dry air because there is no hydrogen present in dry air . the hydrogen atoms in polyethylene are not mobile and will not react with d$_2$o . however polyethylene is more porous than you might think and will eventually let water through .
transmission gratings are less sensitive to polarization and alignment , but cannot transmit at higher wavelengths ( ie typically ~2000nm ) .
question 1 a laser gives light with least amount of angular spread . as far as my knowledge goes . though a directed pointer can be made out of any light source . question 2 the air molecules will scatter off some light , but this may not be noticeable unless you use extremely sensitive detection apparatus . i dont have any quantitative estimates of how sensitive and so on . to me it seems absolutely essential that one need to place this " material " such as smoke , or dust particle in the path of the light to measure it . there is a simple reason for this . in vacuum , if you are interested in determining if light passes through a certain region or not , you have to place something there , or the existence of light in that region cannot be detected . the very placing of this material , will however disturb the behaviour of the light beam .
the thing here is that when a particle decays ( nb here i talk about point-like decays , i will address decaying compound systems later ) , the products were not " in " the original particle in the first place . that is $$a \to b + c + d$$ does not imply that $a$ was made up of $b$ and $c$ and $d$ . it implies the combination of two facts that $a$ and $b+c+d$ have compatible quantum numbers . that $m_a$ is sufficiently larger than $m_b + m_c + m_d$ to allow for the products to escape from each other . in other words a neutron is a particle distinct from the collection of a proton , an electron and a electron anti-neutrino , but those of the neutron 's quantum numbers that are respected by the charged-current weak interaction are the same as the collection of lighter particles and the neutrons mass is high enough to produce them . compound systems with tree-level decays now consider beta decay of a non-trivial nucleus . let 's take tritium for the sake keeping the writing simple . the fact that the system is bound means $$m_t &lt ; m_p + 2m_n \ , . $$ but it is still true that $$m_{^3\mathrm{he}^{+2}} + e^- + \bar{\nu}_e &lt ; m_t\ , $$ and the quantum number of the resulting system are compatible with those of a triton alpha-decay alpha decay is a little different . there is no tree-level transformation there , instead there are multiple masses to think about . i am going to use $m$ to label the mother isotope , $d$ for the daughter isotope and $\alpha$ for the alpha particle . here we have a hierarchy of masses . the mother isotope is bound , so we know that $$ m_m &lt ; z_m m_p + ( a_m - z_m ) m_n \ , . $$ ( here $z_m$ and $a_m$ are the charge number and mass number of the mother isotope . ) but , because the decay is energetically allowed we also know that $$m_d + m_\alpha &lt ; m_m \ , , $$ which is to say that the combined system of the daughter and an alpha is more bound than the mother isotope .
typically you would attempt to measure rather than calculate the effect . perhaps by having a second , calibrated device with a long probe that provides an independent measurement of the temperature . you do this in situ if possible or in some reasonable test stand ( which might be as simple as disposable cooler filled with you working fluid ) . the only real alternative is to read the data sheet ; either for the whole device ( if is a off-the-shelf instrument ) , or for the particular chip ( if it is something that you manufactured to spec ) . as a desperate fall-back position you might be able to find a rule-of-thumb for devices in the same class , but those are unlikely to be centrally tabulated . ask around is my best suggestion .
from a fluid dynamics standpoint , as a body moves through a fluid , a small region of fluid is dragged along with it . this is what forms the boundary layer . in the near-body region , odor will be dragged along with the body . likewise , behind a moving person is a turbulent wake and a low pressure region . the low pressure reason will " suck " the odor along with the body , and the turbulence will mix the odor into the air which will also help distribute it . turns out there is an experiment , in this paper , that looks at the effect of a stationary body and a moving body ( as in human body ) in a room with stratified contaminants . the principles discussed therein are along the lines of your question .
the thin lens formula carries over with a few approximations directly to wave optics . this formula , together with an appropriate analysis of diffraction , will let you get a first approximation to the behaviour of many systems . leftaroundabout 's answer gives the overall picture . a good way to translate this into a wave picture is with the following assumptions : the paraxial approximation : i.e. all fields are a superposition of plane waves that are propagating at small angles to the optical axis ; gaussian scalar fields . these are fields that , on a transverse plane , vary like $\exp\left ( - ( ( x-x_0 ) ^2+ ( y-y_0 ) ^2 ) \left ( \frac{1}{2\ , \sigma^2} + \frac{i\ , k , \kappa}{2}\right ) \right ) $ where $\sigma$ is the spotsize and $\kappa$ the wavefront curvature . the " algorithms " are as follows . you begin with the helmholtz equation in a homogeneous medium $ ( \nabla^2 + k^2 ) \psi = 0$ . if the field comprises only plane waves in the positive $z$ direction then we can represent the diffraction of any scalar field on any transverse ( of the form $z=c$ ) plane by : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}\qquad ( 1 ) $$ in words : take the fourier transform of the scalar field over a transverse plane to express it as a superposition of scalar plane waves $\psi_{k_x , k_y} ( x , y , 0 ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) $ with superposition weights $\psi ( k_x , k_y ) $ ; note that plane waves propagating in the $+z$ direction fulfilling the helmholtz equation vary as $\psi_{k_x , k_y} ( x , y , z ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) $ ; propagate each such plane wave from the $z=0$ plane to the general $z$ plane using the plane wave solution noted in step 2 ; inverse fourier transform the propagated waves to reassemble the field at the general $z$ plane . now we make the paraxial approximation to the propagation relationship in step 2 above , i.e. we assume that the plane waves are not skewed at too steep angles relative to the $z$ axis so that $k_x^2+k_y^2 \ll k^2$ . then our two propagation equations above become the fresnel propagation integral : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \frac{k_z^2+k_y^2}{2\ , k} z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}\qquad ( 2 ) $$ now witness that a beam $\psi ( x , y , 0 ) $ with gaussian dependence on $x$ and $y$ becomes , under the fresnel diffraction integral , $\psi ( x , y , z ) $ with gaussian dependence on $x$ and $y$ . the fourier transform of a gaussian is a gaussian , a fact which does not change when we multiply by the $\exp\left ( i \frac{k_z^2+k_y^2}{2\ , k} z\right ) $ kernel in the fresnel diffraction integral , and of course a gaussian is recovered by the inverse fourier transform . furthermore , thin lenses can be thought of simply as phase masks , i.e. a field whose transverse variation is $\psi ( x , y ) $ passing through them is transformed by : $$\psi ( x , y ) \to \exp\left ( -i\ , k\ , \frac{ ( x-x_0 ) ^2+ ( y-y_0 ) ^2}{2\ , f}\right ) \psi ( x , y ) \qquad ( 3 ) $$ so the gaussian form is preserved by all the diffraction and lensing operations . best of all , for gaussian beams the diffraction integrals split up into a product of separate $x$ and $y$ dependences and these separate dependences are also operated on independently by the phase mask ( 3 ) , so propagation analysis can be done as a product of two decoupled one-transverse dimensional diffraction problems . therefore the propagation through a system comprising homogeneous , diffractive mediums and thin lenses can be done wholly in closed form expressions ( use mathematica though ! ) and you end up with something slightly more general than the thin lens formula that becomes the thin lens formula when the axial distances involved become longer than the rayleigh diffraction length ( of the order of wavelengths ) . this method assumes perfectly unaberrated waves . however , witness that any lens whose surface sag ( height ) near its vertex can be described by an analytic function of distance $r$ from the optical axis ( axis of symmetry ) will impart a phase mask well described by the above in the paraxial limit , i.e. by limiting the numerical aperture ( maximum skew angle ) in any field such that the support of any field in the transverse plane $z=0$ is small enough . as long as the width of the support needed to validate the analysis above stays big compared with a wavelength , the analysis above will be valid in the paraxial limit . this means it works in the paraxial limit for all lenses of practical curvatures in the neighbourhood of the optical axis .
this is easily explained if you use the method of images . the man sees a mirror image of himself which has an apparent location reflected about the plane of the mirror , i.e. equal distance from the mirror but in the opposite direction . hence the apparent distance from man to his image is always twice the distance from the man to the mirror , hence the 50% scale .
your second method is correct . to compare , say , the magnetic field with what you find in jackson , you really need to realize that there is an assumption that you have unit basis vectors there , and that the cross product is actually a hodge dual ( which will invoke factors of the square root of the determinant of the metric ) . these will make direct comparisons a bit tricky when going from one notation to the other . of course , ultimately , both methods will work . the second method is far more error proof and is also coordinate ( and the only metric dependent step is the lowering of the index of $a^{\mu}$ ) independent . ( note that what i"m saying above is that it does not matter if you replace the ordinary derivatives with covariant derivatives , since : $$\nabla_{a}v_{b} = \partial_{a}v_{b} - \gamma_{ab}{}^{c}v_{c}$$ , which means that $$\nabla_{a}v_{b} - \nabla_{b}v_{a} = \partial_{a}v_{b} - \partial_{b}v_{a} - \gamma_{ab}{}^{c}v_{c} + \gamma_{ba}{}^{c}v_{c} = \partial_{a}v_{b} - \partial_{b}v_{a}$$ , so they really are the same thing . )
the crucial fact about these idealized circuits and electric potential differences that leads to the assertion you want to justify is wires are modeled as perfect conductors ( ohmic resistors with negligible resistance ) for which there is zero potential difference between any two points . ( this was edited from " perfect conductors are equipotentials . " ) if we assume that the wires are perfect conductors ( zero resistance ) , then ohm 's law immediately gives the result above . having this fact in hand , we first notice that we have the following mathematical identity ( which is basically the " loop rule" ) : $$ v_a + ( v_d-v_a ) + ( v_c - v_d ) + ( v_b-v_c ) + ( v_a - v_b ) = v_a $$ which we can rewrite as $$ \delta v_{da} + \delta v_{cd} + \delta v_{bc} + \delta v_{ab} = 0 $$ now using the fact above , we note that the potentials at any two points connected solely by a wire are the same , so that $$ \delta v_{da} = 0 , \qquad \delta v_{bc} = 0 $$ which gives $$ \delta v_{cd} + \delta v_{ab} = 0 $$ and therefore since $$ \delta v_{ab} =- \delta v_{ba} $$ we get the desired result $$ \delta v_{cd} = \delta v_{ba} $$
this problem has four separate regions of operation impact : the pen applies an impulse over a short period of time at $b$ and the coin pivots about $a$ as the center of gravity starts to lift off . initial speed is given from the impulse magnitude and the geometry . pivot : the coin pivots about the contact location as the friction force is enough to keep the coin from sliding along the contact . contact forces are found from the pivot constraint . sliding : as the friction is overcome by the motion of the coin it starts to slide along the contact . the reaction forces are found from the friction relationship and the force components along the coin and perpendicular to the coin . flight : the coin flies through the air without contacting the laptop anymore . now you can use the projectile equations to track its position .
yes it is . the total momentum vector of a system does not change at all ( constant length and direction ) , so the projection of it on a line ( or any function you apply to it ) will not change . projection is a linear operator , so that if you project each particle 's momentum on a line and then sum , you get the same result as summing first ( to get the total momentum ) then projecting .
yes . from clausius theorem the following inequality can be deduced : $$\delta q \le tds$$ where the equality holds in the reversible case . so , a reversible adiabatic process is necessarily isentropic , but irreversible adiabatic processes are not so . to put it in another way , in an irreversible process , according to the above inequality , either entropy changes , or heat must be somehow removed from the system to make it possible to have zero change in entropy . so an irreversible isentropic process can not be adiabatic .
this was previously a comment to space_cadet 's answer but became long ( down-vote was not me though ) . i do not understand space_cadet 's talk about unstable orbits . recall that two-body system with coulomb interaction has an additional $so ( 3 ) $ symmetry and has a conserved laplace-runge-lenz vector which preserves the eccentricity . because interactions between planets themselves are pretty negligible one needs to look for explanation elsewhere . namely , in the initial conditions of the solar system . one can imagine slowly rotating big ball of dust . this would collapse to the sun in the center a disk ( because of preservation of angular momentum ) with circular orbits and proto-planets would form , collecting the dust on their orbits . initially those planets were quite close and there were interesting scattering processes happening . the last part of the puzzle is mystery though . if there were still large amount of dust present in the solar system it would damp the orbits to the point of becoming more circular than they are today . the most popular explanation seems to be that the damping of the eccentricity was mediated by smaller bodies ( like asteroids ) . read more in " final stages of planet formation " - peter goldreich , yoram lithwick , re'em sari .
i currently do not have access to the paper , but this should meet your needs : vortex ring interactions ( by wakelin-riley ) http://qjmam.oxfordjournals.org/content/49/2/287.abstract here is my interpretation : consider two rings moving co-axially in the positive x-direction with their rings in the y-planes ( ring a in front of ring b ) . the particles in each circle of the ( fattened ) ring circulate and force spreading of the air in front of it ( see http://en.wikipedia.org/wiki/file:vortex_ring.gif ) . then 1 ) a 's circulations will force b to spread and enlarge ( visually look at the vectors in the vicinity of a ) . then a moves through and this repeats , because b 's circulation will now spread a . 2 ) here ring a is moving in positive x-direction and b in negative x-direction . their respective circulations will stretch each other and slow them down until they meet at rest and fade . here are more direct papers : a note on the leapfroggeing between coaxial vortex rings at low reynolds numbers ( by lim ) http://pof.aip.org/resource/1/phfle6/v9/i1/p239_s1 interaction of two vortex rings moving along a common axis of symmetry ( by oshima ) http://jpsj.ipap.jp/link?jpsj/38/1159/
the paper does not go into details about what interpretations would be disproved by their results . there is a good reason for this : there are no interpretations that would be disproved by their results . they are disproving a straw-man . here is the central result proved by the paper , phrased in a less obscure way : " if a system is in the state $|+_z\rangle$ then it is definitely not in some other different state $|+_x\rangle$ or whatever . " if this seems obvious and uncontroversial , it is ! admittedly , in the conclusions section , they claim they are saying things that are not obvious . . . but they are wrong . let 's start from the beginning . they define the debate by saying there are two quantum states , $|\phi_0\rangle$ and $|\phi_1\rangle$ . there is one procedure to prepare $|\phi_0\rangle$ and a different procedure to prepare $|\phi_1\rangle$ . they say there are two schools of thought . the first school of thought ( the correct one ) is that " the quantum state is a physical property of the system " , so that " the quantum state is uniquely determined by [ the physical situation ] " . that is the one that they will prove is correct . they say the alternative ( the incorrect one ) is that " the quantum state is statistical in nature " , by which they mean " a full specification of [ the physical situation ] need not determine the quantum state uniquely " . let 's say you have a spin-1/2 system , in state $|+_z\rangle$ . then . . . hold on a minute ! i just committed myself to the first school of thought ! i said the system was really in a certain quantum state ! in fact , everyone doing quantum mechanics is always in the first school of thought , because we say that a system has a quantum state and we do calculations on how the state evolves , etc . what would be the second school of thought ? you would say , " i went through a procedure which supposedly prepares the system into state $|+_z\rangle$ . but really the system does not just have one unique state . it has some probability somehow associated with it . this exact same procedure might well have prepared the state $|+_x\rangle$ or whatever . real physicists have a way to deal with this possibility : mixed states , and the density matrix formalism . if you try to prepare a pure state but do not do a very good job , then you get a mixed state , for example the mixed state which has a 70% chance of being $|+_z\rangle$ and a 30% chance of being $|+_x\rangle$ . so again , as i said at the start , they have proven the obvious fact : " if a system is in the state $|+_z\rangle$ then it is definitely not in some other different state $|+_x\rangle$ or whatever . " with such an obvious and uncontroversial premise , how do they purport to conclude anything that is not totally obvious ? let 's go to the conclusions section . they conclude that the " quantum process of instantaneous wave function collapse [ is different from ] the ( entirely nonmysterious ) classical procedure of updating a probability distribution when new information is acquired . " indeed , if a spin is in the state $|+_z\rangle$ , then acquiring new information will never put it in the state $|+_x\rangle$ . you have to actually do something to the system to change a spin from $|+_z\rangle$ to $|+_x\rangle$ ! ! for example , you could measure it , apply a magnetic field , etc . let 's take a more interesting example , an epr pair in the state $ ( |++\rangle+|--\rangle ) /\sqrt{2}$ . after preparing the state , it really truly is in this specific quantum state . if we carefully manipulate it while it is isolated , we can coherently change it into other states , etc . now we separate the pair . someone who wants to describe the first spin as completely as possible , but has no access to the second spin , would take a partial trace like usual to get a density matrix . he then gets an email that the second spin is in state + . he modifies his density matrix to the pure state + . you will notice that their example does not show that this so-called collapse violates any laws of quantum mechanics . their disproof is specific to pure states , and would not work in this mixed state example . therefore , they cannot conclude that " the quantum collapse must correspond to a real physical process " in the epr case . one more example : a spin in state $|+_z\rangle$ , and you measure it in the x-direction . the schrodinger equation , interpreted with decoherence theory , says that the wavefunction of the universe will coherently evolve into a superposition of ( macroscopic measurement of + ) and ( macroscopic measurement of - ) . in the paper , they say this in a different way : " each macroscopically different component has a direct counterpart in reality " . this is just saying the same thing , but sounds more profound . i should hope that anyone who understands decoherence theory will agree that both of the macroscopic measurements are part of the universe 's wavefunction , and that the universe really does have a unitarily-evolving wavefunction even if we cannot see most of it . we rarely care , however , about the wavefunction of the universe ; we care only about the branch of the wavefunction that we find ourselves in . and in that branch it is quite reasonable for us to collapse our wavefunctions and to say that the other branches are " not reality " ( in a more narrow sense ) . -- update -- i tried to reread the paper in the most charitable way that i can . now , i think i was a bit too harsh above . here is what the paper proves : central claim : say you have a hidden-variables theory , so when you " prepare a pure state $|\psi\rangle$" , you actually prepare the state $\{|\psi\rangle , a\}$ , where a is the hidden variable which randomly varies each time you prepare the state . it is impossible to have $\{|\psi_0\rangle , a\}=\{|\psi_1\rangle , a&#39 ; \}$ , if $|\psi_0\rangle\neq|\psi_1\rangle$ . in other words , the hidden-variable-ensembles of different pure states do not overlap . they are disproving a straw-man because there is no interpretation of quantum mechanics that asserts that the hidden-variable-ensembles of different pure states must overlap with each other . even hidden-variable theories do not assert this . there is a so-called " statistical interpretation " in the literature ( advocated by l . ballentine ) , which also does not assert this . so this is a straw-man , because nobody ever argued that hidden-variable ensembles of different states ought to overlap . but , it is not a manifestly ridiculous straw-man . at least , i can not think of any much simpler way to prove that claim . ( admittedly , i do not waste my time thinking about hidden-variables theories . ) i can imagine that someone who was constructing a new nonlocal-hidden-variables quantum theory might like to know that the hidden-variable-ensembles should not overlap .
if you do not want to use statistichal mechanics , you can view it as a completely mathematical thing . when you write the differential form $\delta q$ , you are not speaking of an exact differential , i.e. it is not really the differential of any function of the thermodynamical state . temperature is , in this case , called the integrating factor , which means that $\delta q/t$ is an exact form , in particular , it is $ds$ , the differential of entropy . this is a way to let entropy come out . on the other hand , much more physical explanations can be given . the first uses obviously stat mech , but without making calculations , i can just tell you that $s$ turns out to be very closely connected with the number of possible microscopical states a thermodynamical ( thus macroscopical ) state can admit . finally , a reason is that the quantity $\int \delta q/t$ is never negative in normal thermodynamical transformations , that is , it allows a simple formulation of the second principle . i hope this is what you were looking for .
i would likely to further dmckee 's answer by answering the op 's follow-up question : can you please explain why considering secondary sources as in huygen 's principle is justified , i.e. , why do we get correct results by assuming secondary sources when there are actually no sources other than the original source ? as feynman explains in case of electromagnetic waves , it is because the diffracted wave is equivalent to the superposition of electric fields of a hypothetical plug containing several independent sources . huygen 's principle is actually a fairly fundamental property of solutions of the helmholtz equation $ ( \nabla^2 + k^2 ) \psi = 0$ or the d'alembert wave equation $ ( c^2\nabla^2 - \partial_t^2 ) \psi = 0$ . for these equations the green 's function is a spherical wave diverging from the source . all " physically reasonable " solutions ( given reasonable physical assumptions such as the sommerfeld radiation condition ) in freespace regions away from the sources can be built up by linear superposition from a system of these sources outside the region under consideration . already this is sounding like huygen 's principle , but one can go further and , with this prototypical solution and the linear superposition principle together with gauss 's divergence theorem , show that waves can be approximately thought of as arising from a distributed set of these " building block " spherical sources spread over the wavefront : this result leads to the kirchoff diffraction integral , thence to various statements of huygens 's principle . this treatment is worked through in detail §8.3 and §8.4 of born and wolf , " principles of optics " or in hecht , " optics " , which i do not have before me at the moment .
the magnitude of the work done by friction in linear motion is equal to the work done by the torque of friction only if the wheel is smooth rolling . in smooth rolling , we have $$v_{cm}=\omega r$$ ( this is equivalent to the fact that the point of contact is at rest ) or equivalently : $$dx=d\theta r$$ now the work done by friction in linear motion : $$dw_f=-f_f \cdot dx$$ and the work done by the torque of friction is : $$dw_f=\tau \cdot d \theta=f_frd \theta=f_fr \frac{dx}{r}=f_fdx$$ . so the the " two works " are equal in magnitude and no energy is dissipated .
the " wave " part of the wave-particle duality for particles such as electrons and protons ( as opposed to em radiation ) is called their wavefunction . it does not have any classical analogue and any attempt at understanding it using classical intuition can only be a crude analogy . however , if you are happy with the concept of a probability wave then it is exactly that . why is not this a problem with the uncertainty principle , then ? well , there is a corresponding uncertainty principle between the wavelength of any wave ( or more precisely its wavenumber $k=2\pi/\lambda$ ) and its position in space . the wavelength of a wave is only precisely definable , to arbitrary precision , if you have an infinite wave ; otherwise , you can only measure a finite number of periods and divide , and that will yield an imprecise measurement of $\lambda$ . ( even worse , the amplitude will taper out near the edges , so it will be hard to tell where each peak or trough is . ) to have a better-defined wavelength , then you need a bigger wavepacket , but this means that the position of the wavepacket in space , which only makes sense to a precision of the wavepacket size , has bigger uncertainty . this trade-off game can be expressed as $$\delta k\delta x \gtrsim1 , $$ and can be made precise using fourier analysis of waves .
all these dilemmas with different speed of light as observed by different observers are not real . they never occur in reality . they are only imaginary and can never be tested experimentally . observers in two different inertial frames cannot see/measure the same ray of light . this is physically impossible , for in order for you to see the light it must come exactly to your eye ( measuring device ) . light " exists " only for the one to whom it comes directly . if light is " passing you by " , you cannot see it . therefore , two different observers will never actually measure different speeds of the same light . if one will measure it , the other will not be able to . if i understand your set-up correctly , the person being in the other spaceship ( not the one where the tank is located ) will see only the light sent to him from the tank through vacuum separating the two spaceships . therefore , as the speed of light in vacuum is $c$ , than that is exactly what he will measure . ( and yet , the person in the spaceship with the tank in it will never see light sent to the other rocket , so he will not be able to make any direct comparisons . )
this is simply the sum of the gravitational potential energy over all the points that make up the body . each point has a mass $\rho dv$ , meaning the mass density times the infinitesimal volume element , and this is multiplied by g and h , because the potential energy of a point at height h is $mgh$ . if you are asking why the potential energy is $mgh$ for a point , this can be argued using reversible elevators attached by pullies . if you want to raise a mass m by a certain amount h , you can do this by putting an equal mass on the other side of a pulley-elevator and lowering the mass by an equal amount . this process is easy--- you do not have to do work--- because the masses balance on the two sides . so if there is a conserved energy , it must be a quantity which is unchanged when you lower a mass by a given amount , so long as you raise the same amount of mass somewhere else by the same amount . since you can cut up big masses into small pieces , you can raise a big mass of mass 2m by h units of height by lowering two masses of size m each by h units . in all these processes , the sum of the mass times the height is conserved . when you have different gravitational forces , like deep in the interior of the earth , you can compensate by using a bigger mass . so it is mg which is the right unit to balance , the force , not the mass itself . this argument establishes that the potential energy is proportional to mgh , and that there is no numerical constant , that the potential energy is just equal to mgh , is just a best convention for defining the unit of energy from the unit of force . this argument is presented in detail in feynman 's lectures on physics vol 1 , in an early chapter . it is essentially due to archimedes , and it is also discussed in this answer to a related question : why does kinetic energy increase quadratically , not linearly , with velocity ?
what i am missing in your question , is the dimension of $\lambda$ . you can see from your second formula , that the dimension of $\lambda$ is $\text{m}^{-2}$ ( because of the laplacian ) . further , to add a bit more information , normally we would write your eigenvalue problem as $$\delta v = - k^2 v . $$ we use $k^2$ here mostly because it will make the answer look nicer ( no square roots ) . we then call $k$ the wavenumber
very briefly . the line of reasoning is the following : the acceleration $a^{\mu}$ is gr is rather formal construction , it is the covariant derivative of the speed $u^{\mu}$ with respect to some natural parameter $\lambda$ which parameterize a trajectory . for massive particles you can choose this parameter to be proper time $d\tau$ thus $a^{\mu}=du^{\mu}/d\tau$ , although it is not possible for massless particles , for which $d\tau=0$ . therefore your question is related to the following one : what is $du^{\mu}$ ? it turns out that the simplest ( and only ) way to construct the covariant differential of a vector field is to compare two infinitesimally separated vectors in the same point ( it is essential ) , e.g. , the vector $u^{\mu}\left ( x^{\alpha}+dx^{\alpha}\right ) $ and the vector $u^{\mu}\left ( x^{\alpha }\right ) $ which should be subject to a parallel translation to the point $x^{\alpha}+dx^{\alpha}$ . after the parallel translation we obtain a new infinitisimally close vector $u^{\mu\prime}=u^{\mu}\left ( x^{\alpha}\right ) +\delta u^{\mu}$ , thus $$ du^{\mu}=u^{\mu}\left ( x^{\alpha}+dx^{\alpha}\right ) -u^{\mu\prime}=du^{\mu }-\delta u^{\mu} , $$ where $du^{\mu}=u^{\mu}\left ( x^{\alpha}+dx^{\alpha}\right ) -u^{\mu}\left ( x^{\alpha}\right ) $ is the ordinary differential . therefore , the small addition $\delta u^{\mu}$ is the result of parallel translation . there are two obvious properties of $\delta u^{\mu}$: it should be linear in $u^{\mu}$ and should vanish with $dx^{\mu}\rightarrow0$ . therefore one can represent $\delta u^{\mu}$ as follows : $$ \delta u^{\mu}=-\gamma_{\alpha\beta}^{\mu}u^{\alpha}dx^{\beta} , $$ where $\gamma_{\alpha\beta}^{\mu}$ is the set of some matrices usually referred as “connection coefficients” or “christoffel symbols” . using $\gamma$ , one can generalize the covariant differential $d$ to any tensor quantities . although , there are no additional mathematical requirements on $\gamma$ , there are physical ones in gr — the equivalence principle requires that $\gamma$ should be symmetric $\gamma_{\alpha\beta}^{\mu}=\gamma_{\beta\alpha}^{\mu}$ and $dg_{\alpha\beta}=0$ . the last condition results in $$ \partial_{\mu}g_{\alpha\beta}=-\left ( \gamma_{\mu , \beta\alpha}+\gamma _{\beta , \mu\alpha}\right ) , $$ where $\gamma_{\mu , \beta\alpha}=g_{\mu\rho}\gamma_{\beta\alpha}^{\rho}$ . using the condition that $\gamma$ is symmetric one can find : $$ \gamma_{\beta\alpha}^{\rho}=\frac{1}{2}g^{\rho\sigma}\left ( \partial_{\beta }g_{\sigma\alpha}+\partial_{\alpha}g_{\sigma\beta}-\partial_{\sigma} g_{\alpha\beta}\right ) . $$ let 's now consider a parameterized trajectory $x^{\mu}\left ( \lambda\right ) $ , the contravariant vector called speed is $u^{\mu}=dx^{\mu}\left ( \lambda\right ) /d\lambda$ , therefore the contravariant acceleration takes the form : $$ a^{\mu}=\frac{du^{\mu}}{d\lambda}=\frac{du^{\mu}}{d\lambda}-\frac{\delta u^{\mu}}{d\lambda}=\frac{du^{\mu}}{d\lambda}+\gamma_{\alpha\beta}^{\mu }u^{\alpha}u^{\beta} . $$ if we choose $d\lambda=d\tau$ then in the locally-inertal frame ( $\gamma=0$ ) for the trajectory $x^{\mu}\left ( \lambda\right ) $ the acceleration $a^{\mu }$ coincides with the ordinary one $\left ( 0 , \mathbf{a}\right ) $ . and vice versa , $du^{\mu}=0$ means that $u^{\mu}$ is constant in the locally-internal frame although it does imply that it is constant in any other frame , in fact $du^{\mu}=\delta u^{\mu}$ implies that a free-fall trajectory is actually a parallel translation in gr , which ( for an external observer ) looks like the action of gravitational forces .
given some distribution or density $\rho ( x ) , $ a moment is the ' expectation value ' of some power of $x \in \mathbb{r}$ . to be precise , the $n$-th moment $m_n$ is given by $$m_n = \int_{\mathbb{r}} x^n \rho ( x ) \mathrm{d}x . $$ in the mechanics case , $\rho ( x ) $ is simply the mass density . you can extend this to vectors in $\mathbb{r}^d$ in a straightforward way ; for example , for the moment of inertia you replace $x^2$ by $\mathbf{x}^2 = x_1^2 + \ldots x_d^2$ to obtain $$i = m_2 = \int_{\mathbb{r}^d} \mathbf{x}^2 \rho ( \mathbf{x} ) \mathrm{d}^dx$$ which should match the definition given in your mechanics textbook . for the first moment of mass , you need to distinguish different directions . as you indicate , you can choose your coordinates such that $$\int_{\mathbb{r}^d} x_i \ , \rho ( \mathbf{x} ) \mathrm{d}^d x = 0$$ where $i$ runs over the coordinates . in three dimensions , you have $x_1 = x , x_2=y$ and $x_3=z . $
one of the biggest failure of theoretical condensed matter and/or material sciences is that up to now , nobody has ever been able to predict what compounds will be a good superconductors . of course , since we do not really understand high-tc superconductivity , we cannot predict which ceramic will or will not be a nice superconductor . but even in the case of more standard superconductors described by bcs or more refined theories ( like eliashberg 's theory ) , the predictive power of theoretical approaches is close to zero . to summarize , all superconductors are found experimentally , and then theorists try to explain why this particular alloy/compound has these properties .
any object , whether it be magnetized or not ( and in particular whether it be ferromagnetic or para/diamagnetic ) , will only experience magnetic forces when it is placed in an external magnetic field . if you are thinking of a long , straight conductor , then the magnetic field it produces will increase as you get near it ( though it does decrease inside the conductor ) . that said , there are some configurations of currents that produce a magnetic field " outside " and but none " outside " . two examples are a toroidal solenoid , in which the field is confined to the solenoid interior and is zero in the donut hole , and two long cylindrical , coaxial solenoids carrying the same current in opposite directions , for which the field is confined to the middle region and cancels out in the inner region . a coaxial cable is an example of the latter . if you are thinking about an extended distribution of currents such as a wire of nonzero thickness , then yes , the field is zero at the centre and it is small nearby . however , you can hardly place a metallic object inside a wire .
have a look at http://en.wikipedia.org/wiki/sunlight as this gives the irradience of sunlight as a function of wavelength . the spectrum is very similar to a black body temperature of 5,600k . i was not sure from your question if you were asking about the units used for irradience . the units used in the wikipedia article ( and in the astm page it references ) are power per square metre per nanometre of bandwidth . for example a value of 1.5 at 400nm means that the total power of all the light between 400nm and 401nm is 1.5w/m$^2$ . to model photosynthesis you need to multiply the spectrum by the absorption spectrum of chlorophyll then integrate across all wavelengths to get the total power per square metre .
as can be seen in this pie chart taken from the wikipeia article on " universe " , the significant parts of the universe are , in descending order , dark energy , dark matter , gas , stars , and the ghostly subatomic particles called neutrinos . that is the most laymanish terms that can be used , because nobody knows what the first two actually are .
the top quark is so massive relative it is partners that it decays on time scales faster than the hadronization time scale . look at the tables again . . . you do not find any top quark mesons . instead the top--uniquely among the quarks--has decay modes . ( pdf link )
looks like i have to answer this question :- ) let me first answer the math question : every zero-energy eigenstate is of the form of a symmetric polynomial times the laughlin wave function . to be concrete , let us consider an $n$ boson system , with delta-potential interaction $v=g\sum \delta ( z_i-z_j ) $ where $z_i$ is a complex number describing the position of the $i^{th}$ boson . the zero energy state $\psi ( z_1 , . . . , z_n ) $ satisfies $\psi ( z_1 , . . . , z_n ) =p ( z_1 , . . . , z_n ) exp ( -\sum_i |z_i|^2/4 ) $ where $p$ is a symmetric polynomial that satisfy $\int \prod_i d^2 z_i \ \psi ( z_1 , . . . , z_n ) ^\dagger v \psi ( z_1 , . . . , z_n ) =0$ . now it is clear that all the zero energy state are given by symmetric polynomial that satisfy $p ( z_1 , . . . , z_n ) =0$ if any pair of bosons coincide $z_i=z_j$ . for symmetric polynomial this implies that $p ( z_1 , . . . , z_n ) \sim ( z_i-z_j ) ^2$ when $z_i$ is near $z_j$ . the laughline wave function $p_0=\prod_{i&lt ; j} ( z_i-z_j ) ^2$ is one of the symmetric polynomials that satisfies the above condition and is a zero energy state . since any other zero-energy symmetric polynomial must satisfy $p ( z_1 , . . . , z_n ) \sim ( z_i-z_j ) ^2$ , $p/p_0=p_{sym}$ has no poles and is a well defined symmetric polynomial . so every zero-energy eigenstate $p$ is of the form of a symmetric polynomial $p_{sym}$ times the laughlin wave function $p_0$ . more discussions can be found in the first part of arxiv:1203.3268 . however , a physically more relevant math question is : every energy eigenstate below a certain finite energy gap $\delta$ is of the form of a symmetric polynomial times the laughlin wave function for any number $n$ of particles . ( here $\delta$ does not depend on $n$ . ) we only have numerical evidences that the above statement is true , but no proof .
in fact it depends on the type of glass . depicted is solar transmittance through glas . for greenhouses special heat absorbing glas may be used . the diagram shows a dip of transmittance at $\approx 1200\ , $nm . lower transmittance is because absorption ( as in the name of the glas ) and as well in reflection ( see fresnel equations ) . your reasoning is right : the glas of a greenhouse may reflect infrared light . the most common types have a transmission band up to the nir ( near infrared ) regime . e.g. crown glas ( $350\ , \text{nm}&lt ; \lambda &lt ; 2\ , \mu$m ) and fused quartz ( $200\ , \text{nm}&lt ; \lambda &lt ; 3\ , \mu$m ) .
db is basically a ratio measurement in logarithmic form . the sound intensity level $l_1$ is found by applying the following formula to two intensities such as $i_1$ and $i_0 $ . $$ l_\mathrm{i}=10\ , \log_{10}\left ( \frac{i_1}{i_0}\right ) \ \mathrm{db} \ , $$ i.e. , $i_1$ is $l_1~db$ higher than $i_0$ . in your question , $l_1 = 2.00 . $ we also know that intensity is proportional to the inverse of distance squared : $$\frac{i_1}{i_0}= ( \frac{r_1}{r_0} ) ^{-2}$$ where $r_1$ is the nearer distance ( thus higher intensity ) . we are also given that $r_1 = r_0+1$ with this you should be able to solve for everything .
formulas on that page are about as simple as you will find and should meet your needs . if you are confused by something specific , just ask ! for shortening the period of night , you should go by the distance ( in degrees , say ) the sun is below the horizion , rather than any fixed time . that is the setting of the " zenith " variable . you might like http://en.wikipedia.org/wiki/twilight for background , then the values " official " , " civil " , " nautical " , and " astronomical " on the page you referenced should make sense . without even knowing the use for your sensor , i would suggest starting with the nautical twilight value , that is set the variable zenith to 102 . ( nautical twilight is pretty dark ! ) upd : the math on the page looks good . i tried coding it and it gave the right answer ( for my location today anyway ) to within about a minute .
( 1 ) as for the ( a ) the total force of the ground/hinge ( e . g . thrust or normal force + friction ) is generally neither vertical nor horizontal . edit : you can obtain the force of the ground/hinge by calculating the force of the rope first , and then add all three forces together to get zero . as for the ( b ) you have three forces acting to beam , force of the ground , gravitational force and force of the rope . since problem suggests " considering equilibrium " , torques of these three forces must equal zero . ( 2 ) force at point b is simply the force of rod bd to rod ac ( and vice versa ) . effectively , you have three forces acting on rod ac . note also that the force of the rod bd is along its direction ( because it is limited by two joints at its ends and there is no force in between ) .
i believe you have a mistake in your formula as the self-inductance of a coil is given by $$l\approx\mu_0 \frac{n^2 a}{\ell} ; $$ here $n$ is the number of windings , $a$ is area of the cross-section , and $\ell$ is the length of the coil . your task is to maximize $l$ with the constraint that the length of the copper wire is $w$ . assuming that the solenoid is a cylinder , the cross-section read $a=\pi r^2$ with $r$ the radius of the cylinder . a solenoid with $n$ windings needs a wire of length $w= 2\pi rn$ . thus , $$ l \approx \mu_0 \frac{w^2}{\ell} . $$ we see that the inductance of the solenoid decreases with increasing length ( keeping the total length of the wire fixed ) . thus , we obtain the largest self-inductance having the smallest length which is a single loop with $n=1$ . for a single loop the formula given above is not correct ( as it assumes $\ell \gg \sqrt{a}$ ) and thus we have $$l\approx \mu_0 r \ln ( r/r ) \approx \mu_0 \frac{w}{2\pi} \ln ( w/r ) $$ with $r$ the radius of the wire .
use wheatstone bridge : http://en.wikipedia.org/wiki/wheatstone_bridge this is classical way of measuring resistivities with no error due to imperfectness of measurement instruments ( e . g . galvanometer ) .
the first formula ( scaling argument ) $$ \frac{u^2}{l} \sim \nu \frac{u}{\delta^2} , \tag{*} $$ comes directly from equations for boundary layer equations ( and not specifically for blasius boundary layer ) . we have continuity equation : $$\dfrac{\partial u}{\partial x}+\dfrac{\partial v}{\partial y}=0 $$ and x-component of momentum equation : $$u\dfrac{\partial u}{\partial x}+v\dfrac{\partial u}{\partial y}=-\dfrac{1}{\rho}\dfrac{d p}{dx}+{\nu}\dfrac{\partial^2 u}{\partial y^2} . $$ the general assumptions behind this equations is that along the x-axis quantities vary slower than along y-axis . let us denote $l$ the characteristic length scale along the x- axis , $\delta$ the scale along the y-axis and $u$ is the characteristic velocity of the fluid . ( $l$ could be , for instance , the length of the body and $\delta$ the boundary layer thickness . ) we thus have $l \gg \delta$ and $u \sim u$ . various derivatives could be estimated by $$\dfrac{\partial }{\partial x} \sim \frac{1}{l} , \qquad \dfrac{\partial }{\partial y} \sim \frac{1}{\delta} . $$ now applying this approximations to continuity equation we get for instance the scale of y-component of the velocity : $$v \sim \frac \delta l u . $$ now for the x-momentum equation both terms of the left side and the term with pressure on the right have all the following estimate : $$ u\dfrac{\partial u}{\partial x}\sim v\dfrac{\partial u}{\partial y} \sim \dfrac{1}{\rho}\dfrac{d p}{dx} \sim \frac{u^2}l , $$ ( the term with pressure could be rewritten using bernoulli 's equation as $u_\infty \dfrac{ d u_\infty}{dx}$ which would lead to the same estimate ) . the single remaining term of x-momentum equation will have the following approximation : $$ {\nu}\dfrac{\partial^2 u}{\partial y^2} \sim \nu \frac {u}{\delta^2} . $$ combining this estimates we get $ ( * ) $ . note , that i wrote it using the $\sim$ and not $\approx$ as in wiki page . now we further assume that the problem at hand is the blasius boundary layer , that is semi-infinite plate in uniform flow parallel to it . in this problem there is no intrinsic length scale ( because the plate is infinite ) , so role of scale along the x-axis would be played by the current value of the x-coordinate $l = l ( x ) = x$ , and the traversal scale $\delta$ also has to be dependent on $x$ . then we simply find from $ ( * ) $ $$\delta ( x ) \sim \sqrt{\frac{x \nu}{u}} , $$ which is yours second equation .
even considering the same fabric with different colours it will depend a lot on optical properties of the dye which we cannot tell only by the colour that we can see . i would need to know " how much the silver paint would reflect and in what spectral range " and also " how is the absorption coefficient as a function of the wavelength ( from near-uv , visible to near-infrared ) " . i believe that this is the most important point and not the colour that we perceive with our eyes . nevertheless , keeping things simple , i think that the reflective coating being outside or inside would make the same amount of radiation that passes through . the only difference is that putting the reflective layer inside you would be increasing the path length of the radiation , thus increasing the absorption , and therefore the temperature edit : in order to clarify my idea i added this picture . dashed layer is reflective and the the grey layer is the normal dark layer . i would like to divide this discussion in two : " radiation protection " and " heat protection " regarding the radiation protection , i believe that it depends on how the the dark layer absorbs light in all spectral range of the sun light , i.e. absorption coefficient and thickness of the material+dye . it also depends on how the reflective layer reflects the light , i.e. reflectance . i believe that the radiation protection does not depend on the arrangement , so both schemes protect from radiation as good . ( i am not considering far infrared radiation ) regarding heat protection , the thought is the same , except we should consider that in the first case the length of the radiation in the dark layer is longer , thus absorbing more radiation thus heating more . also , the protection from the heat will depend on the heat conductivity of the layers and surface morphology which will affect how well the layer will cool down . i understand that having a reflective coating outside an umbrella would be unpleasant for other people . but it would be the best choice for heat protection . however , regarding the emergency heat blankets , i do not know why they also have a reflective layer on the inside .
accelerated charged particles emit electromagnetic radiation . in this case , where the acceleration is caused by a magnetic field and is perpendicular to the velocity , the radiation is called cyclotron radiation . since the magnetic field does not work on ( electrically ) charged particles , the radius of the charged particle should reduce , as it is energy ( and so it is speed ) reduces due to the radiation .
gauss ' law and coulomb 's law are equivalent - meaning that they are one and the same thing . either one of them can be derived from the other . the rigorous derivations can be found in any of the electrodynamics textbooks , for eg . , jackson . for eg . , consider a point charge q . as per coulomb 's law , the electric field produced by it is given by $$\vec{e} = \frac{kq}{r^2}\hat{r}$$ , where $k = \frac{1}{4 \pi \epsilon_0}$ . now , consider a sphere of radius $r$ centred on charge q . so , for the surface $s$ of this sphere you have : $$ \int_{s}\vec{e} . \vec{ds} = \int_{s}\frac{kq}{r^2}ds = \frac{kq}{r^2}\int_{s}ds = \frac{kq}{r^2} ( 4\pi r^2 ) = 4\pi k q = \frac{q}{\epsilon_0}$$ , which is gauss ' law . note that if the $r^2$ in the expression for the surface area of the sphere in the numerator did not exactly cancel out the $r^2$ in the denominator of coulomb 's law , the surface intergral would actually depend on $r$ . hence you would not have the result that the surface integral is independent of the area of the surface , which is what is implied by gauss ' law . though this result has been derived for a sphere , it can be derived for any arbitrary shape and size of the surface , you can refer to jackson for eg . , for the rigorous derivation . note that by performing these steps in reverse , you can also derive coulomb 's law from gauss ' law , thus demonstrating that they are equivalent .
multispecies spinor qed is described by the following lagrangian : \begin{equation} {\cal l} = \sum_i\bar{\psi} _i i \left ( \partial _\mu + i e _i a _\mu \right ) \gamma ^\mu \psi _i - m \bar{\psi} _i \psi _i - \frac{1}{4} f _{ \mu \nu } f ^{ \mu \nu } \end{equation} if $ e _i $ is the same for every particle then we have a flavor symmetry under , $ \psi _i \rightarrow e ^{ i t _a \theta ^a } \psi _i $ . if it is not then we do not . with this alone there is no apriori reason to believe such a symmetry exists . in the standard model we see a lot of similar flavor symmetries . we do not currently have any good justification for these symmetries , and since we do not know of any reason for their existence , there is a lot of reserach that is done to try to explain them .
wrong . you are neglecting the viscosity of the water . friction from the inner wall of the sphere will move the water , which will in turn move the marbles and they will likely rotate around the sphere , though at a speed slower than the rotation of the sphere . if your sphere was on earth , then the only reason the marbles stay at the bottom is gravity pulling them down . if you shake the sphere ( even assuming that the walls do not touch the marbles ) , then they will move in this situation too due to the force of the water flowing around the sphere .
i have confirmed with zurek , he told me that it was wrong ( at least the period-wise ) and it has been pointed out many times by other people including animesh dutta .
the repelling is another way of saying that owing to the strength of the hydrogen bonding between water molecules , the water molecules are better off with themselves alone as compared to with non-interacting non-polar molecules within . a substance dissolves only in a solvent , where the solvent-solute interaction is as strong ( or stronger ) than the solvent-solvent interaction and therefore the solvent finds it better ( energetically and thermodynamically favourable ) to allow the solute molecules to dissolve , i.e. take up spaces between the molecules . but if the solute-solvent interaction is poor , ( as in the case of non-polar/hydrophobic molecules ) , the solvent finds it better to be among itself and not allow the hydrophobic molecules to take up spaces between the molecules , which is equivalent to having repelled the non-polar substances , i.e. their mixing with water is energetically opposed . this is also the reason why polar substances do not dissolve in non-polar solvent . there are hydrophobic surfaces which depend on the surface energy or the contact angle of water on that surface , but the same argument cannot be extended to molecules where there is no surface to account for the surface energy .
the question formulation ( v2 ) seems to mix the notions of invariant and covariant , which essentially is also the main point of user1504 's answer ( v1 ) . let 's say we have a group $g$ . the group $g$ could e.g. be a finite group or a lie group . when we say that a theory is invariant under $g$ , it normally implies at least two things . the group $g$ acts on the theory . this in particular means that there is a well-defined given prescription on how the constituents of the theory change under the action of the group . often in physics ( but not always ) , it happens that the group action is linearly realized , i.e. the fields , the matrix elements , and other objects form linear representations of the group $g$ . at this stage , the representations could be reducible or irreducible , finite dimensional or infinite dimensional . the corresponding object then behaves covariantly ( not necessarily invariantly ) under the action of the group . if a representation is completely reducible , we can decompose it in irreps . in case of an off-shell formulation : the action $s$ is off-shell invariant under the group $g$ . or phrased equivalently , the action $s$ form a trivial representation of the group . in case of an on-shell formulation : the equations of motion behave covariantly under the group $g$ . a refinement . ron maimon makes in a comment an important point that if there is a hierarchy of say two theories , and if the group action is a priori only defined in the smaller theory , then the group $g$ does not necessarily have to have a well-defined action on the larger theory . for instance , small theory = on-shell formulation ; large theory = off-shell formulation . small theory = minimal $s$-matrix formulation ; large theory = an underlying field theoretic formulation . small theory = formulation on gauge-invariant physical subspace/submanifold/phase space ; large theory = formulation on brst-extended space/manifold/phase space .
the oscillatory part is nothing but thomas-fermi approximation or more riguresly , this is a version ( someone should correct me if i am wrong ) weyl 's formula regrading on how to obtain the wkb from the trace formula : you can read the 2 papers by berry and tabor on how they derived a trace formula ( like that of gutzwiller ) but to the case of integrable systems . from the derivation there you can see how the ebk pop up . . .
the mass of a quantum of a field is defined from the second derivative of the potential term $$ m^2 = \left . \frac{\partial^2 v ( \phi ) }{\partial \phi^2} \right|_{\phi=0} $$ and similar for fields with spin ( fields that are not scalar fields ) . the general form of the potential – or the whole lagrangian – is always more complicated but only this leading term determines how non-interacting wave packets and the particles of the field propagate . the higher-order terms only affect the interactions . so the potential may be expanded as $$ v = v_0 + 0\phi +\frac{m^2}{2}\phi^2+ o ( \phi^3 ) $$ here , the constant term does not matter except for causing curvature in general relativity ( the cosmological constant ) . the linear term may be set to zero by redefining the field $\phi$ additively so that its vev is $\phi=0$ , the quadratic term is the first important nontrivial term , and the higher-order terms do not affect small " waves " i.e. masses of the particles at all because the equations of motion are of the form $$\box \phi =m^2 \phi + o ( \phi^2 ) $$ and the higher-order terms may be neglected for a small $\phi$ . the quantization of the field from which the particles ' masses may be extracted effectively deals with the infinitesimal values of $\phi$ , too . to calculate the leading non-trivial ( non-constant and nonzero ) term in the potential , it is enough to linearize the dependence of all other things on the fields at the relevant point . so there is no inaccuracy introduced whatsoever .
i guess lubos motl 's comment really refers to the terminology used in my post . if i try to insist on what i meant by " fermionic string " , the string formed by $s=s_{rns}-s_p$ , the massless free dirac action $s=\iint\limits_{s} i\hbar\gamma^\mu\partial_\mu\psi \mbox{ d}^2\xi$ , then i guess it would simply mean that the theory is inconsistent . the only way i can see that this is so , is that the " fermionic string " again is an inconsistent string theory . i think i get why this is so . if there are no fields $x^\mu$ in the action , then the string worldsheet can not get embedded into spacetime at all ( ! ) . this theory would then not exist . so the answer boils down to " the ( purely ) fermionic string is not studied because it is not even a consistent theory , since the string worldsheet would not be embedded into spacetime . "
we can consider the following model : a tube of constant temperature $t_e$ of lenght l , radius $r$ where water is flowing uniformly at a speed $v$ ( that you can obtain from your flow $p$ ) . a " slice " of water travels an interval $dx$ in a duration $dt = \frac{dx}{v}$ . the tube will contribute to the " heating " of the water by $\frac{dq}{dt} = ( t-t_e ) k 2 \pi r dx$ where $k$ is the conductivity and where we use a very simple model ( in particular for the radius , we do not distinguish external and internal radii ) . during this interval the temperature $t ( x ) $ of the water will vary by $dt = -\frac{dq}{c \rho dv}$ where $c$ is the heat capacity at constant pressure of water , and where $dv = 2 \pi r dx$ . replacing we have $\frac{dt}{t-t_e}=-\frac{k}{\rho c v} dx$ whose solution , if the temperature in the tank ( ie x = 0 ) is $t_t$ : $t ( x ) = ( t_t - t_e ) e^{ ( -\alpha x ) }+t_e$ where $\alpha = \frac{k}{\rho c v}$ . depending on the lenght of the tube you have the temperature at the tap .
i think the dominant effect might actually be the fact that the salt you add might not be at boiling temperature . but this is just based on the fact that the boiling-point elevation due to salt in water is actually quite low for typical amounts of salt used in cooking , say . i am not too familiar with the second effect you mention though .
since all the balls are accelerating together , this problem is equivalent , by the non-relativistic equivalence principle , to the problem of balls moving without gravity , or on a horizontal surface , which are free to sort themselves out according to the same force law . this reduced problem is interesting and widely studied . depending on the force law , you can get either a 1-d integrable model , or a 1-d statistical equilibrium , and both have a massive literature .
you cannot have a total vorticity with periodic boundary conditions , since if you take a path around all of your vortices , it will have a non-zero circulation . but you have periodic bc , so you can continuously deform that path to a point , and a point has zero circulation . mirror images are not quite the same as in electrostatics . we want periodic boundary conditions . to get periodic boundary conditions you imagine tiling the plane with your system . this will trivially be periodic and so you can just take a tile , and work with that . so you want the mirror image of a vortex to be a vortex . ( in electrostatics you usually use mirror images to enforce not periodic boundary conditions , but to enforce constant voltage . that is why you flip the sign of the mirror charge . here we want periodic b.c. if you flipped the sign of the vortices i believe you would get *anti*periodic boundary conditions , but do not quote me on that . ) this evolving in imaginary time is presumably an " annealing " type of operation . you are free to run the gp equation on any initial condition you want . however , to cleanly see the interaction of the vortices , we want to the vortices to be in their ground state . otherwise when we turn on the time evolution they will get rid of their excess energy by shedding waves and other junk . one way to get to the ground state is to evolve you equation in " imaginary time " . your usual time evolution is $\exp ( i\hat{h}t ) $ . if plug in $t = i\tau$ you get $\exp ( -\hat{h}\tau ) $ . applying this to a state exponentially suppresses the higher-energy components , so you get rid of the high energy stuff . this is related to finite-temperature ( just replace $\tau$ with $\beta$ and you have the partition function ) , but for your purposes you can just consider it a convenient mathematical trick . note that since you are annealing anyway , the specific details of what state you start might not be so important , since you will end in the same place anyway ( hopefully ) . finally , they are a little thin on the details , so if you plan to use this work , you should just send the authors an email asking for details .
at $r=0$ we should have $−v_0 r ( r ) =er ( r ) $ , which implies $e=−v_0$ . ( is this allowed ? ) nope , not allowed . in any case , that ode is not as bad as it looks . change variables from $r ( r ) $ to $u ( r ) $ [ which was defined in the problem as $u ( r ) = r r ( r ) $ ] , and it will wind up looking very familiar . you will come up with a second-order ode that has two linearly independent solutions , $$u_1 ( r ) = \cdots , \quad u_2 ( r ) = \cdots$$ then you can get the two independent solutions to the original schrodinger equation as $r_i ( r ) = \frac{u_i ( r ) }{r}$ . the final physical solution for $r ( r ) $ needs to be well-defined at the origin . ( in fact you realized this in the second part , but you do not need to worry about it there because $r &gt ; r_0$ does not include the origin ; but you do need to worry about it here ) so you need to pick a linear combination of $r_1 ( r ) $ and $r_2 ( r ) $ that is not infinite at $r = 0$ . hint : what needs to be the numerical value of $u ( 0 ) $ ? the other part , with $r &gt ; r_0$ , is extremely similar . again , remember that there are two linearly independent solutions . you only found one of them . also , the solution you found does not blow up if $k$ is negative - but are you sure that $k$ is negative ? what do you know about the value of $e$ ? ( specifically , what does it mean for the particle to be in a bound state ? ) you do not need to care about what the solution to the second part does at the origin , because the region you are solving the equation in does not include $r = 0$ . but it does include $r \to \infty$ , so you will need to pick a linear combination of the solutions that stays finite in that limit .
i claim that if the transformation between frames is homogeneous and differentiable , then it is affine ( homogeneity is not strictly speaking sufficient for linearity since the full transformation between frames is actually a poincare transformation which is affine , not linear ) for a mathematically precise proof , we need a mathematical definition of homogeneity . to arrive at such a definition , we note that the basic idea is that we can pick our origin wherever we choose , and it will not " affect the measurement results of different observers . " in particular , this applies to measurements of the differences between the coordinates of two events . let 's put this in mathematical terms . let $l:\mathbb r^4\to\mathbb r^4$ be a transformation . we say that $l$ is homogeneous provided \begin{align} l ( x+\epsilon ) - l ( y+\epsilon ) = l ( x ) - l ( y ) \end{align} for all $\epsilon\in\mathbb r^4$ and for all $x , y\in\mathbb r^4$ . we can now precisely state and prove the desired result . note that i also assume that the transformation is differentiable . i have not thought very hard about if or how one can weaken and/or motivate this assumption . proposition . if $l$ is homogeneous and differentiable , then $l$ is affine . proof . the definition of homogeneity implies that , \begin{align} l ( x+\epsilon ) -l ( x ) = l ( y+\epsilon ) - l ( y ) \tag{1} \end{align} for all $\epsilon , x , y$ . now we note that the derivative $l' ( x ) $ of $l$ at a point $x$ is a linear operator on $\mathbb r^4$ that satisfies \begin{align} l ( x+\epsilon ) - l ( x ) = l' ( x ) \cdot \epsilon +o ( |\epsilon| ) \end{align} and plugging this into $ ( 1 ) $ gives \begin{align} ( l' ( x ) -l' ( y ) ) \cdot\epsilon = o ( |\epsilon| ) \end{align} for all $\epsilon , x , y$ , where $|\cdot|$ is the euclidean norm . now simply choose $\epsilon = |\epsilon|e_j$ with $|\epsilon|\neq 0$ where $e_0 , \dots e_3$ are the standard , ordered basis elements on $\mathbb r^4$ , multiply both sides on the left by $ ( e_i ) ^t$ where $^t$ means transpose , divide both sides by $|\epsilon|$ , and take the limit $|\epsilon|\to 0$ to show that all matrix elements of $l' ( x ) -l' ( y ) $ are zero . if follows immediately that \begin{align} l' ( x ) = l' ( y ) \end{align} in other words , the derivative of $l$ is constant . it follows pretty much immediately that $l$ is affine , namely that there exists a linear operator $\lambda$ on $\mathbb r^4$ , and a vector $a\in\mathbb r^4$ such that \begin{align} l ( x ) = \lambda x + a \end{align} for all $x\in\mathbb r^4$ . $\blacksquare$
there are many , many algorithms and pieces of software to do this . in addition to molecular dynamics , there are also methods based on statistical simulations in quantum monte carlo , and density functional theory as implemented in programs like quantum espresso . it is a simple and worthwhile exercise to program these things yourself - if you wish to study the oscillatory behavior of a molecule subject to some arbitrary external potential , you can do this quite readily using basic programming and visualization tools provided you establish the proper functions and equations to describe your system . i will note that these algorithms all have explicit ranges of validity and underlying assumptions , and one must very carefully understand the limitations before interpreting the results . in many cases , the accuracy and precision of the algorithms will be questionable , because assumptions at some level have to be made to reduce the system size since not even the most powerful supercomputer can handle a calculation with anything approaching a macroscopic number of particles . nevertheless , they can provide some sense of the starting point and can give insight into trends . edit to add : see giant list of software applications for quantum chemistry
in the equations as you have written them , the constant of proportionality is an outward-pointing vector for the electric field and an inward-pointing vector for the gravitational field . or in other words , if you take the radial component only : it is a positive constant for the electric force and a negative constant for the gravitational force . the details : gauss 's law for electrostatics actually says $$\iint\vec{e}\cdot\mathrm{d}\vec{a} = \frac{q_\text{enc}}{\epsilon_0}$$ and for newtonian gravity , you can write $$\iint\vec{g}\cdot\mathrm{d}\vec{a} = -4\pi g m_\text{enc}$$ for a spherically symmetric surface and mass/charge distribution , letting $\hat{n}$ represent the outward-pointing normal vector at each point on the surface , these simplify to $$\vec{e} = \frac{q_\text{enc}}{4\pi\epsilon_0 r^2}\hat{n}$$ and $$\vec{g} = -\frac{gm_\text{enc}}{r^2}\hat{n}$$ note that the constant of proportionality in the first case is $\hat{n}/4\pi\epsilon_0 r^2$ , which points outward , and in the second case is $-g\hat{n}/r^2$ , which points inwards .
just because the maximum speed is $6\pi\text{ cm/s}$ does not mean that $6\pi = 6\pi \cos ( 3\pi t ) $ . keep in mind that speed is the absolute value of velocity $x&#39 ; $ .
$\tau_{xy}$ and $\tau_{zy}$ act in the same direction but they act on different faces . this diagram should clear it up for you :
just realize that you can form ordinary dirac spinors from 2-spinors by using charge conjugation , $i\sigma_2\eta^*$ , that gives a right- handed field that can fit in the right-handed slot ( forming a 4 component majorana field ) $$ \psi_1=\left ( \begin{array}{c}\eta \\ i\sigma_2\eta^*\end{array}\right ) $$ and analogous for $\psi_2$ in terms of $\chi$ . then you just look at the ' mass terms ' $\bar\psi_1 \psi_2$ to get your term ( well in fact you need to insert also a $p_l$ ) . i think the textbook by ramond shows this kind of things . actually , if you add also the hermitian conjugate to your expression , you can even fit all in a single dirac spinor $$ \psi=\left ( \begin{array}{c}\eta \\ i\sigma_2\chi^*\end{array}\right ) $$ and look at $\bar{\psi}\psi$ .
the book i am reading defines the position of the com of a two-particle system to be $x_{com}= \large\frac{m_1x_1+m_2x_2}{m_1+m_2}$ i am sorry if this seems like a trivial question , but could someone explain to me the interpretation of this definition ? perhaps even why they defined it to be this way . it is a weighted average of the position of the particles where the weighting is the mass . to see this , consider the two-particle , discrete case where the masses are the same , then it reduces to $x_{com} = \frac{x_1 + x_2}{2}$ which is clearly an average position . it is useful because it turns out that you can often ( but not always ) factor out com motion from motion relative to the com and simplify your like . physicists like to simplify their own lives . what does the author mean by , " the ' particles ' then become differential mass elements $dm$ ? that is just the usual continuum limit . you imagine breaking a continuous distribution into little boxes and treating them with the discrete equation then letter the size of the boxes get arbitrarily small .
migdal 's book recommended in comments is good . if you cannot find it , some migdal 's problems can be found in the standard textbook l.d. landau and e.m. lifshitz , quantum mechanics , non-relativistic theory , §41 . transitions under a perturbation acting for a finite time . there are five problems considered in the end of the section : a uniform electric field is suddenly applied to a charged oscillator in the ground state . determine the probabilities of transitions of the oscillator to excited states under the action of this perturbation . the nucleus of an atom in the normal state receives an impulse which gives it a velocity $v$ ; the duration $\tau$ of the impulse is assumed short in comparison both with the electron periods and with $a/v$ , where $a$ is the dimension of the atom . determine the probability of excitation of the atom under the influence of such a " jolt " ( a . b . migdal 1939 ) . determine the total probability of excitation and ionization of an atom of hydrogen which receives a sudden " jolt " ( see problem 2 ) . determine the probability that an electron will leave the $k$-shell of an atom with large atomic number $z$ when the nucleus undergoes $\beta$-decay . the velocity of the $\beta$-particle is assumed large in comparison with that of the $k$-electron ( a . b . migdal and e . l . feinberg 1941 ) . determine the probability of emergence of an electron from the $k$-shell of an atom with large $z$ in $\alpha$-decay of the nucleus . the velocity of the $\alpha$-particle is small compared with that of the $k$-electron , but the time which it takes to leave the nucleus is small in comparison with the time of revolution of the electron ( a . b . migdal 1941 , j . s . levinger 1953 ) . another application , which is very close connected to the problem 4 , is the so called molecular/atomic effects in tritium beta decay . one can measure the electron neutrino mass by studying the $\beta$-electron energy spectrum in the process $\ , ^{3}h\to \ , ^{3}he^{+}+e^{-}+\bar{\nu}_{e}$ near the end-point , where the electron energy is very close to 18.6 kev . the electron is very fast so it can hardly influence all environment around . thus the main effect is the sudden change of the charge of nucleus . there were a lot of studies of possible molecular excitations due to such sudden perturbation . google finds a good thesis about this subject : natasha doss , calculated final state probability distributions for $t_{2}$ $\beta$-decay measurements .
the behavior of the non-axial rays is illustrated on the picture below . rays ( red ) falling in direction determined by the vector cd ( in circle ) reflect from the surface of a parabola ( blue ) , forming an intersection at point j ( red dot ) . the intersection point is obviously out-of original focus ( yellow dot a ) . tracking the direction vector shows the tracks of an intersection point ( red and gray dots ) , which form a mustache-like pattern originating from a . more interesting is the following picture - it shows that parallel rays do not even focus in a single point at all ! one pair of rays intersect at j ( red ) , while other pair intersect at n ( green ) . green and red tracks are different , so rays do not focus . they are smeared along a ( probably linear ) path consisting j-n .
i am not sure what you mean by medium here , but i believe i can still provide an answer to your question . a fuel-thruster works by pushing the fuels reactants back and thus pushing the thruster forward . a human cannot swim in the vacuum because there arms do not push anything back . in water , a human can swim by pushing water back and thus pushing the human forward . by your use of words , i guess you can consider the " fuel " in this case as a " medium " for the reactive force . in answer to your first question , which i interpret as " does a force require both something that caused the force and something that receives the force " , the answer is yes . however , we can write quantities , such as the potential , that only depend on a source .
it is easy to check if your calculations are right from this ( rather crude ) drawing : there you see that the size of an object is related to the size of the image according to $$h_o=h_i\frac{d}{f}$$ where $h_o$ and $h_i$ are the sizes of the object and image . $f$ is not a focal distance , but the distance from the pinhole to the sensor or film . so , if you have a 10cm object at 10m from the camera , and you want that to be 0.5mm ( 50px width ) , you need to place the sensor 0.5mm*10m/10cm=5cm behind the pinhole . if you do that , the angle your camera will see is $\tan ( \alpha ) =h_\mathrm{ccd}/ ( 2f ) $ . the field of view is defined as twice this angle ( $\mathrm{fov}=2\alpha$ ) . with the above numbers , the vertical fov is 4.1º and the horizontal fov is 5.5º .
the simple answer is that the sun 's gravity produces the same acceleration on both the earth and the moon . the sun is pulling both of them along , but they are falling together . you may imagine two skydiver jumping out of a plane at the same time ( and we had better ignore air resistance ) . they are subjected to gravitational forces from the earth that vastly larger than the forces between them , but that does not rip them away from each other because they both experience the same acceleration .
according to wigner , the wave function of a quantum particle can be multivalued , i.e. , can acquire a nontrivial phase around a closed loop . a phase is nontrivial when it cannot be removed using a gauge transformation by $e^{i \alpha ( \theta ) }$ , with a true function $\alpha$ , i.e. , $\alpha ( 2\pi ) = \alpha ( 0 ) $ . the wave functions having this property are sections of nontrivial line bundles over the configuration manifold . the reason that a wave function is not required to be a true function is because its overall phase and magnitude are nonphysical ( if one defines quantum expectations as : ) $&lt ; x&gt ; = \frac{\int \psi \hat{x} \psi}{\int \psi \psi}$ such wave functions arise when the configuration manifold is not simply connected with a nontrivial cohomology group $\mathcal{h}^{1} ( m , \mathbb{r} ) $ ( this is the case of the circle ) . in this case , there will exist vector potentials on the manifold which are not the gradients of a true function on the manifold . $a \ne d\alpha ( \theta ) $ . with , $\alpha ( 2\pi ) =\alpha ( 0 ) $ . however , there is no need for the flux to be quantized as the wave function needs not be a true function on the configuration manifold . on the contrary , if the flux had been quantized , then no aharonov-bohm effect would not have observed . a quantization condition occurs when $\mathcal{h}^{2} ( m , \mathbb{r} ) $ ( the dirac quantization condition ) , but this is the case of a particle moving on a sphere rather than on a circle . however , this is not the case in superconductivity . the difference between the two situations lies in the fact that the " macroscopic wave function " of a superconductor is not a " wave function " . i.e. , it is not the coordinate representation of a state vector in a hilbert space . it is a quantum field describing goldstone bosons ( cooper pair ) of the superconducting phase ( usually called an order parameter ) . the modulus of the macroscopic wave function $|\psi ( \theta ) |^2$ describes the number density operator of the goldstone bosons . its two point functions describe the ( long range ) correlations . this quantum field couples minimally to electromagnetism , and this is the reason why its equation of motion is similar to the schrodinger equation of a particle coupled to electromagnetism . but the main difference this field is a true scalar field and not a section of a line bundle . this gives us the reason why the phase it acquires in a full loop should vanish because otherwise for example , its correlation functions would depend on how many times the circle was wrapped .
no . i can take a ball and swing it back and forth periodically with my hand . the motion is periodic , but the situation is not conservative - my body generates a lot of heat . a simple mathematical example is a forced , damped harmonic oscillator . it has a steady-state periodic solution that dissipates energy . if you want to know whether a force field is conservative , take its curl . time-independent force fields ( force is a function of position but not time ) are conservative iff their curl is zero .
the path integral in quantum mechanics computes the evolution kernel , which is the matrix element of the evolution operator : $\mathrm{exp} ( ih t ) $ , ( $h$ is the hamiltonian ) , between two position eigenstates . the path integral expresses the evolution kernel as a sum over paths : $u ( x , t , x_0 ) = \int_{x ( 0 ) =x_0}^{x ( t ) =x} \mathrm{exp} ( \frac{is}{\hbar} ) \mathcal{d}x$ . where $u$ is the evolution kernel , $s$ is the classical action . on the other hand , the evolution operator has an expansion as a sum over the energy eigenstates : $u ( x , t , x_0 ) =\sum_n \mathrm{exp} ( \frac{-ie_n t}{\hbar} ) \psi_n ( x ) \psi_n^{*} ( x_0 ) $ where , $\psi_n ( x ) $ are the energy eigenstates . from this expression , it is clear that the evolution kernel has a discrete spectrum , whenever the energies are quantized . in other words , in the case of quantized values of the energy , the fourier transform of the evolution kernel : $\hat{u} ( x , \omega , x_0 ) \equiv \int_{-\infty}^{\infty} u ( x , t , x_0 ) \mathrm{exp} ( i\omega t ) dt$ will be a sum of dirac delta functions centered at the frequencies : $\omega_n = \frac{e_n}{\hbar}$ please observe that the weight of each dirac delta function is just the projection operator on the corresponding discrete eigenstate .
in quantum field theory and its extensions including string theory , the electric charge is a generator of a $u ( 1 ) $ symmetry which should be promoted to a local symmetry i.e. gauge symmetry . in string theory , the $u ( 1 ) $ symmetry and the gauge field often appear as parts of the low-energy effective action . this could be enough to answer the question : we reduce the problem to the same problem in the approximate theory - quantum field theory . except that we do not have to end at this point . string theory produces many geometric pictures how to " imagine " or " visualize " the electric charge . those " visualizations " are often dual to each other : it means that even though these ways to present the charges superficially look totally different , one may actually demonstrate that their physical implications are totally equivalent and indistinguishable . kaluza-klein theory the oldest picture embedded in string theory goes back to 1919 and a discovery by theodor kaluza , later refined by brilliant physicist oskar klein . five-dimensional general relativity , with the new dimension compactified on a circle , produces $u ( 1 ) $ electromagnetism aside from the four-dimensional general relativity . the mixed components of the metric , $g_{\mu 5}$ , may be interpreted as the gauge field $a_\mu$ in the large dimensions . the isometry rotating the circle ( compact fifth dimension ) at each point is interpreted as the $u ( 1 ) $ gauge symmetry . and charged particles are particles that carry a momentum in the new , fifth direction . by quantum mechanics , the momentum has to be quantized ( for the wave function to be single-valued ) , $p=q/r$ , where $r$ is the radius of the circle ( $2\pi r$ is the circumference ) and $q$ is an integer that may be identified with the electric charge . a particle with the opposite charge is simply a particle that moves in the opposite direction along the hidden circular dimension . this works not only for strings but even for point-like particles in higher-dimensional spacetimes . windings string theory offers a special , more intrinsically stringy origin of the charges , too . closed strings may wrap around a non-contractible loop in spacetime - such as the circle from the kaluza-klein theory . they obey boundary conditions on the string : $$ x^5 ( \sigma+\pi ) = x^5 ( \sigma ) +2 \pi r w . $$ those $w$ times wound strings would not exist in a theory without strings . the winding number $w$ - how many times the string is wrapped around the circle - is interpreted as another type of charge . $b_{\mu 5}$ , a component of an antisymmetric tensor field , is interpreted as a new gauge field $a_\mu$ for this $u ( 1 ) $ symmetry . the oppositely charged particles are strings wrapped in the opposite direction ; to be distinct , the closed strings have to be oriented ( carry an arrow ) . this winding number origin of the charge is equivalent to the kaluza-klein origin by an equivalence we call t-duality . the gauge groups in the heterotic string theory combine the kaluza-klein-like charges and the winding-like charges and promote them to large non-abelian groups such as $so ( 32 ) $ or $e_8\times e_8$ . generalizations of wound strings exist for higher-dimensional branes : the total " wrapping number " of some membranes or branes around non-contractible cycles in spacetime ( homology ) are also manifesting themselves as electric charges . many non-perturbative dualities exist . some cycles on which the branes may be wrapped may be shrunk to zero size but they still exist : in those cases , the charged objects are localized in space ( the gauge field only lives on a singularity which may be extended just like a brane ) . that is the case of the ade singularities . in all cases , oppositely oriented branes correspond to oppositely charged particles . note that the orientation may be defined for the " whole world volume " so the reversal of the spatial orientation may be mimicked or compensated by the reversal of the world volume in the temporal dimension . open strings and d-branes when open strings are allowed , they can carry charges ( historically known as " chan-paton factors" ) at the end points - these are the points stuck on the d-branes . so the end points behave as quarks : if the string is oriented and carries an arrow from the " beginning " to the " end " , the beginning may be called a quark and the end may be called an antiquark . in this setup , the charges are most analogous to those of point-like particles . the world line of the quark and antiquark ( going backwards in time ) is nothing else than the boundary of the open world sheet as embedded in the spacetime . even this seemingly point-like origin of charges may be dual - exactly equivalent - the purely stringy ways to produce the charges .
there are several reasons for using the hamiltonian formalism : 1 ) statistical physics . the standard thermal states weight pure states according to $$\text{prob} ( \text{state} ) \propto e^{-h ( \text{state} ) /k_bt}$$ so you need to understand hamiltonians to do stat mech in real generality . 2 ) geometrical prettiness . hamilton 's equations say that flowing in time is equivalent to flowing along a vector field on phase space . this gives a nice geometrical picture for how time evolution works in such systems . people use this framework a lot in dynamical systems , where they study questions like ' is the time evolution chaotic ? ' . 3 ) generalization to quantum physics . the basic formalism of quantum mechanics ( states and observables ) is an obvious generalization of the hamiltonian formalism . it is less obvious how it is connected to the lagrangian formalism , and way less obvious how it is connected to the newtonian formalism . [ edit to answer javier 's comment ] this might be too brief , but the basic story goes as follows : in hamiltonian mechanics , observables are elements of a commutative algebra which carries a poisson bracket $\{ , \}$ . the algebra of observables has a distinguished element , the hamiltonian , which defines the time evolution via $d\mathcal{o}/dt = \{h , \mathcal{o}\}$ . thermal states are simply linear functions on this algebra . ( the observables are realized as functions on the phase space , and the bracket comes from the symplectic structure there . but the algebra of obserbables is what really matters : you can recover the phase space from the algebra of functions . ) on the other hand , in quantum physics , we have an algebra of observables which is not commutative . but it still has a bracket $\{ , \} = \frac{i}{\hbar} [ , ] $ ( the commutator ) , and it still gets its time evolution from a distinguished element $h$ , via $d\mathcal{o}/dt = \{h , \mathcal{o}\}$ . likewise , thermal states are still linear functionals on the algebra .
we have to differentiate between radiant flux and luminous flux . the former refers to power carried by all emitted photons ( watts ) while the latter refers to brightness as perceived by the human eye ( lumens ) . with that out of the way it is clear that by using filters to change the perceived color is equivalent to wasting all the luminous flux associated with the unwanted wavelengths as filters work by absorption . so if there was a way to produce only the desired wavelengths so that filters are unnecessary then for the same amount of power in , there is more useful power out . the problem is that filament based lighting produce a continuous spectrum of wavelengths and must be filtered to produce specific colors . fluorescents and leds actually produce at least 3 individual colors from phosphors/dopants respectively . this is why the latter 2 technologies tend to be more efficient--they do not have to produce true whites , do not produce significant ir , and yet their light is still perceived as white . of course , if we obtain single phosphor fluorescents or single die led , these will always be more energy efficient than to use 3 phosphors only to filter 2 out . additionally , luminous flux pertains to the biology of human eyes , which have been estimated to be most sensitive to 555nm ( green ) and taper off at red and blue . this means a pure red lamp producing 5w radiant flux of 600nm light does not appear as bright as 5w radiant flux of 555nm light .
the quantity you are describing ( $s ( \omega ) $ ) is called the power spectral density . i can not say if your interpretation of the power spectral density in this case is mistaken or not , because i have not encountered it myself . but in context of a stationary physical process , the power spectral density describes how the total power in the system is distributed over various frequencies . here power is taken to mean the square of the signal , i.e. , if the signal is $f ( t ) $ , then the total power is given as $$ p = \frac{1}{t} \int_{0}^{t} |f ( t ) |^2 dt$$ the function that you have described is actually the fourier transform of the autocorrelation function , a result given by the weiner-khinchin theorem . if your $x ( t ) $ is a stochastic variable , then $s ( \omega ) $ is the spectral power distribution for that variable/process .
the major problem with ultrasound as a mechanism of purification is that it does not break molecules . heat at least denatures proteins and breaks hydrogen bonds , but ultrasound is of a just smaller order of magnitude of energy at the atomic scale , which can be of the order of the adhesive forces holding the liquid together , but not of stronger molecular bonds . but i think you can do it a different way : use a sound waves intensity gradient to move the biological impurities in the water to a part of the container , by having them walk down an effective potential gradient , like optical tweezers move molecules . this requires only that the density/stiffness of the molecules be different from water , so that the sound energy at a given mode is different inside the molecules than in the water . if it is greater , the molecule will move to the regions of greater intensity . if it is less , it will move towards the regions of smaller intensity . by arranging the sound wave to have an intensity gradient , you can make all the molecules segregate towards/away from the microphone , leaving water in the middle with only ionic or small molecule impurities , which are not affected by the sound . you can flush the sides away , and repeat to make a purer water . this might work for getting rid of prions , which are not disinfected by boiling . this article is the only thing i found that might be relevant , but it is paywalled : http://www.annualreviews.org/doi/pdf/10.1146/annurev.bb.20.060191.001541 i think this might be a very useful idea .
following brown , maclay , " vacuum stress between conducting plates : an image solution " ( doi:10.1103/physrev . 184.1272 ) consider two ideally conducting parallel conducting plates separated by distance $a$ along the $z$-axis . simple symmetry argument allows us to obtain the possible structure of vacuum stress-energy tensor $\langle t_{\mu\nu}\rangle_0$ . the plates remain invariant under rotation in $xy$-plane as well as under boosts along any direction in $xy$-plane . therefore tensor $\langle t_{\mu\nu}\rangle_0$ must have the corresponding $so ( 2,1 ) $ symmetry . in addition it must be traceless . the only possible structure that satisfies these requirements is $$\langle t_{\mu\nu}\rangle_0 = \left ( \frac14 g_{\mu\nu} - \hat{z}_\mu\hat{z}_\nu\right ) \cdot f ( z ) , $$ where $\hat{z}_\mu$ is space-like unit vector along the $z$-axis and $f ( z ) $ is unknown yet function . furthermore , conservation of energy-momentum means that $f ( z ) $ must be constant , except at the plates , so this function takes two different values $c_1$ and $c_0$ outside and inside the plates ( reflection symmetry means that values of $f ( z ) $ on both sides outside must be equal ) . considering limits $a\to 0$ and $a \to \infty$ ( which both correspond to a single mirror situation ) we must conclude that $c_1 = 0$ . the remaining unknown constant $c_0$ must be computed through some sort of regularization . brown , maclay use explicit green funciton construction , one can use $\zeta$-function regularization or simply compute total energy as $e= \sum_i \hbar \omega_i /2 $ . the result is ( inside the plates ) : $$ \langle t_{\mu\nu}\rangle_0 = \mathrm{diag} ( -1,1,1 , -3 ) \frac{\hbar c}{a^4}\frac{\pi^2}{720} , $$ and zero outside . so here it is : the negative energy . . . and also pressure causing casimir force . this result should be the stress-energy tensor standing in the rhs of einstein equations : $$ r_{\mu\nu} - \frac12 g_{\mu\nu} r = \frac{8\pi g }{c^4} \langle t_{\mu\nu}\rangle_0 . $$ so that is how you calculate geometry : by solving einstein equations with stress-energy tensor including $\langle t_{\mu\nu}\rangle_0 $ ( and matter , which will be present ) .
when you have a matrix $\phi = \begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix}$ , with one column and two rows , and its transpose matrix $\phi^t = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}$ , with one row and two columns , the product of the two matrix $\phi^t \phi$ is a matrix $p$ with one column and one row : $p =\phi^t \phi = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}\begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix} = ( \phi_1^2+\phi_2^2 ) $ because this matrix $p$ has one row and one column , it may considered as a scalar . for your particular problem , you have : $ ( \vec{\nabla}\phi ) ^t . ( \vec{\nabla}\phi ) = \sum\limits_{i=1}^n ( {\partial_i}\phi ) ^t ( {\partial_i}\phi ) = \sum\limits_{i=1}^n ( ( \partial_i \phi_1 ) ^2 + ( \partial_i \phi_2 ) ^2 ) $ the first equality comes from the definition of the inner product and the gradient , and the second equality comes from the definition of the transpose operation $t$ , and the manipulation of these matrices , as seen at the beginning of the answer .
i have since found this pdf from cvi melles griot giving a temperature coefficient of 0.016 nm/°c at 400 nm , increasing to 0.027 nm/°c at 820 nm . this will vary between coating types but it is enough to get started .
not with currently known physics . sending a signal to yourself would require sending a faster than light signal to something that could retransmit it to you and that was itself moving almost at the speed of light . this works because of the way that we cut spacetime into $e^3$ slices . the transmitter moving at close to the speed of light would have its slices tilted relative to ours in such a manner that a superluminal particle could move into its future while moving into our past . the book " it is about time : understanding einstein 's relativity " by n . david mermin has the clearest exposition of this that i have seen . light , by definition , moves at the speed of light . so light could not be used . we would require not only particles with superluminal velocities but also the ability to control those particals to send a signal with them .
the wikipedia article on the system says $\alpha$ cen a has a luminosity of $l = 1.5~\mathrm{l}_\odot$ , and that the a-b system has a period of $80$ years . at a distance of $d = 11~\mathrm{au}$ ( which is not mentioned in the wiki , so i am trusting the op has a good source for this ) , the power per unit area received at the location of b from a is $$ x = \frac{l}{l_\odot} \left ( \frac{d}{1~\mathrm{au}}\right ) ^{-2} = 0.012 $$ times that which the earth receives from the sun , which is certainly not much . now , let 's assume the dark side of the planet is indeed cold . the $80$-year period means the thermal equilibration timescale is much shorter than the timescale of variation in power received . 1 if we just consider the case when a is in opposition , the ratio of light-intercepting cross-sectional area to heat-emitting blackbody area will be $1$ , not $1/4$ as it is for a spinning planet . throw in a little stefan-boltzmann law , and you find the planet 's temperature to be $$ ( 4x ) ^{1/4} t_\text{earth} = 120~\mathrm{k} , $$ where $t_\text{earth} = 254~\mathrm{k}$ is the non-greenhouse average temperature of the earth . this is a rough calculation of course , but it shows that there is no significant heating , even under the best circumstances , from $\alpha$ cen a . this makes sense , since the distance from a to the planet is further than from the sun to saturn . thus : no , there will not be liquid water due to this effect . a quick check of the phase diagram of water assures us it is not liquid at any pressure at $-150^\circ\mathrm{c}$ . just as there is very little heat , there is very little lighting . however , the amount is not vanishingly small . as this blog points out , you can read a book on pluto just using the sun 's light , so this planet would not be completely dark to our eyes with their remarkable dynamic range . i will interpret " calendar " to mean " progression of seasons , " in which case . . . i suppose there might be some changes as certain gasses sublimate , similar to how pluto or comets start outgassing when they approach the sun . the more interesting " seasonal " variation will be spatial , not temporal . there could be a very thin strip of nice temperatures near the terminator , 2 though i would not be surprised if this moved around too much for there to be a permanent region conducive to liquid water . 1 if you think the planet might have more thermal inertia than this , consider how quickly the earth cools off as the seasons change . 2 this seems like it would make for a good science fiction setting .
the field inside the sphere will not be zero if it is hollow and there is a point charge in the hollowed out part . the field will be zero in the conductor , because the field is always zero in a conductor in electrostatics . what you might be refering to is that the field will be zero inside the hollow sphere if it is charged , because the charges will distribute symmetrically over the sphere .
joule dissipation ( equivalently , ohmic heating ) is a statistical process . it does not occur at the microscopic scale , and a single travelling electron in an electrical field is certainly microscopic . resistivity depends on particle collisions , which turns translational kinetic energy into thermal kinetic energy . ohm 's law relates the current $\vec{j}$ in a medium to the electric field $\vec{e}$ via the conductivity $\sigma$ , $$ \vec{j}=\sigma \vec{e} $$ another way of looking at this is that dissipation $p$ is given by $$p=\frac{j^2}{\sigma}$$ written this way , we have an expression very similar to that for ohmic heating in a resistor , namely that $$p=i^2 r$$ since the conductivity is the inverse of the resistivity , this is an intuitive result . the advantage of joule 's expression is that it allows for the determination of joule heating at a particular point in space , rather than over the entire resistor ( whatever that may be ) . this is useful in plasma physics , for example , where it may be the case that joule heating is localized , rather than uniform throughout the plasma . nonetheless , it is predicated on the medium being strongly collisional , and still must refer to a macroscopic , rather than microscopic effect .
it looks like p and s have been a little bit dirty at this point . go back to equation 4.29 . . . you really need to take the limit $t\to \infty ( 1-i\epsilon ) $ of this expression . physically what is happening is that the true vacuum $|\omega\rangle$ is not the perturbative vacuum $|0\rangle$ . you are extracting the true vacuum contribution by evolving to large imaginary times . equality only holds in the limit . your first term is an increasing phase divided by $t$ so it goes like a constant , but the second term is a $t$ independent constant divided by $t$ so it disappears .
it depends on what you mean by completely . ab initio calculations of ferromagnetism are routinely done for a wide range of systems . googling for ab initio calculation of ferromagnetism or some similar terms will find you lots and lots of examples . so in this sense the answer to your question is yes . however in many materials whether the system is ferromagnetic or not depends on a delicate balance between the electron magnetic interactions and the exchange interaction , and when the system is finely balanced it would require an extremely accurate calculation to get the correct answer . for example iron ( which is the archetypical ferromagnet ) exists in three slightly different crystal structures called ferritic , martensitic and austenitic . the ferritic and martensitic are ferromagnetic but the austenitic form is not . so the same element can switch between a ferromagnet and paramagnet just by changing it is crystal structure . i am not sure whether the theoreticians can honestly say they can predict the correct behaviour for these borderline cases .
usually " quantum liquid " refers to the ground state of a hamiltonian that do not break translation symmetry of the hamiltonian . ( in a sense , " quantum gas " = " quantum liquid " . ) " quantum spin liquid " refers to the ground state of a spin hamiltonian that do not break spin-rotation and translation symmetries of the hamiltonian .
no , elements of $spin ( n ) $ do not obey the clifford algebra . instead , it is the gamma matrices that obey it . and no , the commutator of the $spin ( n ) $ lie algebra is not the commutators of the elements of the group but elements of the lie algebra . now positively . the spinor representation is the representation on which the generators $j_{ij}$ ( the basis of the lie algebra ) act , $s\mapsto j_{ij}\cdot s$ . the elements of the $spin ( n ) $ group may be obtained by exponentiation : $$ g = \exp ( \sum_{i , j} i\omega_{ij}j_{ij} ) $$ where $j_{ij}$ is the basis of the lie algebra . while in the vector representation , $j_{ij}$ is given by a nearly vanishing $n\times n$ matrix with entries $\pm i$ on the $i , j$ and $j , i$ position , respectively , the matrices $j_{ij}$ have a completely different form in the spinor representation of $spin ( n ) $ . they may be written as $$ j_{ij} = \frac{\gamma_i \gamma_j - \gamma_j \gamma_i}{4} $$ where $\gamma_i$ are gamma matrices that do obey the clifford algebra $$ \gamma_i \gamma_j + \gamma_j \gamma_i = 2\delta_{ij}\cdot {\bf 1}$$ so the matrices obeying this algebra may be combined to bilinear expressions , the antisymmetric tensor $j_{ij}$ with two indices , and these $j_{ij}$ obey the $spin ( n ) $ lie algebra , and as with every lie algebra , the elements of the lie groups may be obtained by exponentiating combinations of the lie algebra matrices . ( equivalently , the lie algebra is the tangent space of the lie group manifold in the vicinity of the unit element of the group . )
i happen to have attended the atw ( aerospace thematic workshop ) twice in 3 years , so i can give you some first insight . i do not know precisely when plasma flow control was first experimented on , however this field gathers a quickly increasing amount of researchers around the world . the reason for that is that , before , people could use continuous dc plasmas ( not practical at atmospheric pressure because of filamentary structure of the discharge , and strong thermal instabilities that always lead to an arc , that is a power-consuming and ineffective discharge ) or at best dbds ( dielectric barrier discharges : you have two electrodes , and between them , an insulator . when you switch the voltage on , a discharge will " creep " on top of the dielectric , and ionize the air , without transiting to arc because current cannot pass ) . but today , you have nanosecond high voltage discharges : high voltage rise time allow for high currents , and nanosecond limitation of the discharge allow for avoiding the transition to arc . this way , you deposit a lot of energy in the flow , and you chose which way the energy will be deposited : thermal energy with very fast heating ( fast gas heating . . . actually the topic of my phd thesis ! ) , vibrational energy , or slow gas heating ( as in arc discharges ) . and with very fast energy deposition into thermal energy , you get shock waves , which even in hypersonic flows , can do something to the high-enthalpy flow , while keeping the energy requirement low enough to be embarked on a flying vehicle . and this is a great step taken forward , that used to be really impossible in the past ! one application of plasma actuation is to mitigate the noise produced by reflected shocks in supersonic aircraft air inlets , and some very recent results show you can indeed tamper with instabilities that generate noise , and damp them . but as you asked for an honest answer , i would say this research still needs at least 5-10 years before anything practical can be proposed for industrial purposes . . . and possibly more . i hope this could help you get more insight into this field !
the question ( v1 ) deals with the axiomatic qft formulation of the real klein-gordon field $\phi$ in minkowski spacetime $m$ with a vacuum state $\omega_0$ . let $o\subseteq m$ be a fixed non-empty bounded open subset . i would say that the error in op 's proof is the sentence if we take $c$ in the commutant of ${\mathfrak f} ( o ) $ , i.e. $c\in{\mathfrak f} ( o ) &#39 ; $ , then we can show that $\omega_0$ is an eigenvector of $c$ . my counterexample goes roughly like this . since $o\subseteq m$ is bounded , we can choose another non-empty open subset $u$ in $m$ that is causally disconnected from $o$ , i.e. such that every pair of points $x\in o$ and $y\in u$ are space-like separated . consider next a test-function $f\in s ( m ) $ with support ${\mathrm supp} ( f ) \subseteq u$ in such a way that the one-particle state $\phi [ f ] \omega_0$ does not vanish . from one of the wightman axioms , we know that $c:=\phi [ f ] $ commutes with the operators in ${\mathfrak f} ( o ) $ . however , a one-particle state $\phi [ f ] \omega_0$ cannot be proportional to the vacuum state $\omega_0$ . as a consequence , ${\mathfrak f} ( o ) $ is not irreducible .
you just use the lens formula : $$ \frac{1}{u} + \frac{1}{v} = \frac{1}{f} $$ do the calculation for the first lens to find the position of the image , then do the calculation for the second lens taking $u$ to the distance of the first image from the second lens . make $u$ negative if the image is on the far side of the second lens .
you are right about your understanding of these terms . this terminology appears in extensions of the randall-sundrum type brane world models . the original model contains a single compact extra dimension bounded by two branes and is known as a hard wall model with the " hard wall " referring to the hard cutoff of space by the ir brane . with such a geometry it is found that the kaluza klein ( kk ) masses of particles that live in the bulk scale as $m_n^2 \sim n^2$ ( like the energy levels of a particle in a box ) . attempts were made to use rs type setups to be dual to qcd in order to calculate meson masses etc . this is known as ads/qcd . however the meson mass spectrum is what is called a regge spectrum i.e. $m_n^2 \sim n$ and so the rs type model needed to be adapted . this paper first introduced the idea of a soft wall to solve this problem . one of the branes in the hard wall model is removed and a dilaton field $\phi$ is introduced which dynamically cuts off the space-time $$s= \int d^5x \ , \sqrt{g}\ , e^{-\phi}\mathcal{l} . $$ the profile of the dilaton in the extra dimension then determines the kk spectrum of bulk fields and for a quadratic dilaton profile ( $\phi ( z ) \sim z^2$ ) a regge spectrum is produced . the removal of one of the branes ( hard spacetime cutoff ) and replacement by a smooth dynamical cutoff coming from the dilaton coined the name " soft wall " . following this idea , people decided to model electroweak physics with such a geometry ( see e.g. here ) . all the standard model fields , including the higgs must now propagate in the bulk . the new setup offered unique phenomenology and is far less constrained by electroweak precision observables and fcncs which cause severe tensions in the original rs . note that since the dilaton field is not normally given a kintic term in such models , it is not a true dynamical field and one may simply consider the effect as being a different form of metric than rs . so essentially the difference between hard wall and soft wall is just a different geometry of the extra dimension which produces different phenomenology .
the two formulae are dissimilar so the difference is mathematically evident . if you are asking : why are we calling the nuclear force a strong force , the wiki article has the answer in a nuttshell : , calling it a residual strong force : the residual strong force is thus a minor residuum of the strong force which binds quarks together into protons and neutrons . this same force is much weaker between neutrons and protons , because it is mostly neutralized within them , in the same way that electromagnetic forces between neutral atoms ( van der waals forces ) are much weaker than the electromagnetic forces that hold the atoms internally together . qualitatively : the proton and neutron as bound colorless states of three quarks each , to first order are neutral in the strong force , in the same way that a molecule is neutral to the electromagnetic force . first order , because at higher orders there are moments/distortions of the collective many body potential that extend outside the neutral region and create the wan der waals electromagnetic forces for the molecules and the nuclear force for the nuclei .
the flight actually took 25 minutes , the video you linked to is edited down , and the later stages of it look pretty turbulent . i would guess it was the shaking that eventually broke off the chair leg , possibly due to metal fatigue . note that the leg broke off after the balloon had burst and while the chair was falling back to earth . as anna says , the low temperature may well have been a factor . the ductile-brittle transition for mild steel is around -50c so at -60c it would be a lot more brittle than at room temperature . from the way the leg broke off i would guess it was the weld that failed rather than the steel itself .
if anyone is interesed , the answer to my question is $$i\mathcal{m}=\frac{-ie^4\epsilon_{\eta} ( k ) \gamma^{\eta}g_{\eta \delta}\gamma^{\delta}\epsilon_{\delta} ( k' ) g_{\delta \beta}\gamma^{\beta}\epsilon^*_{\beta} ( k'' ) g_{\alpha \beta}e^*_{\alpha} ( k''' ) \gamma^{\alpha}g_{\alpha \eta}}{ ( q^2+i\epsilon ) ^4}$$
yes , the field is infinite , but it is only log divergent near the plate , so that it is hard to see the divergence numerically . you can see this easily by solving the problem of a uniformly charged infinite plate , which is a 2d problem . here the charges are uniform along the negative real axis , where the 2d space is imagined to be the complex plane . this problem can be understood as follows : the 2d electrostatic field of a point charge at the origin , written as a map from c to c can be written in complex form as : $$ e_x + i e_y = \frac{z} { 2\pi |z|^2 }= \frac{1}{ 2\pi \bar{z}}$$ it points radially outwards . this is a pure antiholomorphic function , except at the origin . it is more familiar to deal with holomorphic functions , so conjugate it ! $$ e_x - ie_y = e ( z ) = {1\over 2\pi z} $$ now you want to superpose all the charges on the negative z axis . this is a simple integral : $$ \int_{-\infty}^0 e ( z-a ) da = - {1\over 2\pi} \log ( z ) $$ where i threw away an infinite additive constant ( you should think of this as calculating the potential difference between the point z=1 and any other point ) . this is the function with a given fixed cut discontinuity on the negative real axis . so at the point $r , \theta$ , the electric field is $$ e_x = - {\rho\over 2\pi} \log ( r ) $$ $$ e_y = {\rho \theta\over 2\pi} $$ where i have restored the $\rho$ . the part in the y-direction is finite , as your intuition says--- the discontinuity is equal to the charge density ( this charge density is the cut discontinuity of the electric field analytic function , which is a way of making it obvious that the electric field goes as the log--- the log function as a constant cut discontinuity ) . the divergent part is in the x direction , and it is only invisible in the bulk disk because when you get close to the surface , you have cancellations from the left and from the right that wash it out . so the answer is yes , the e field is log divergent , but only the component in the plane of the disk pointing out . the solution of the disk asymptotes to the plane solution in the near disk limit .
as described on your link , we are trying to compute the probability that we will transition between two fully described states . so , by $i$ and $j$ they mean a complete specification of all of the attributes for each of the agents . now , the probability that we will transition is determined by the rules of the simulation . in each time step we ( 1 ) choose a random agent ( $k$ ) . ( 2 ) choose a random neighbor ( $r$ ) , ( 3 ) decide whether they interact , with probability $n_{kr}/f$ ( their cultural similarity ) , and ( 4 ) if they interact , have $k$ adopt one of $r$ 's features that differ ( of which there are $f - n_{kr}$ options ) at random and adopts its trait . so , assuming we are looking at a particular complete specification $i$ and $j$ that differ only in a particular agent , $k$ having changed the trait of one of his features , the only way we could have gotten from $i$ to $j$ in one step is if we ( 1 ) were lucky enough to choose the $k$ guy out of all of the guys on the lattice . this happens with probability $1/l^2$ . ( 2 ) we choose a neighbor that had the correct trait for the feature of interest . here we do not necessarily know how many such neighbors there are , but let 's denote the number of neighbors we could steal the train in question from $h_{ij}$ and use $h_{ij}$ to represent the set of neighbors , and $n$ to denote the total number of neighbors . ( 3 ) even if we select one of the correct neighbors , they have to interact , which only happens with probability $ n_{kr} / f $ . ( 4 ) even if they interact , we need to choose the right feature to copy , since this is a choice of the correct one out of $ f- n_{kr}$ options , we will make this choice with probability $1/ ( f - n_{kr} ) $ . so , schematically we have $$ \begin{align*} p_{\text{state}_i , \text{state}_j} and = p ( \text{ choosing agent } k ) \times p ( \text{ choosing valid neighbor} ) \\ and \quad \times p ( \text{they interact} ) \times p ( \text{choosing the right feature} ) \\ and = \left ( \frac{1}{l^2} \right ) \left ( \frac{1}{n} \sum_{r \in h_{ij}} \right ) \left ( \frac{n_{kr}}{f} \right ) \left ( \frac{1}{f - n_{kr}} \right ) \\ and = \frac{1}{l^2 n} \sum_{r\in h_{ij}} \frac{ n_{kr}}{f} \frac{ 1 }{ f - n_{kr} } \end{align*} $$ which is as the page reports , though i think they may have goofed , because in order to have a transition , we need to select one of the neighbors that have the shared trait , and this will depend on the number of neighbors $n$ , so i think they accidentally have $h_{ij}$ instead of $n$ in their transition probabilities .
as chris and leftaroundabout said , these are really independent things . the $1/r^2$ law comes from the fact that as the energy in the sound wave travels away from its source , it gets spread out over a larger and larger sphere . since the total energy must remain the same ( assuming no dissipation ) , the amount of energy in each unit of area on the sphere must decrease . the rate it must decrease is $1/r^2$ . so this law is really a result of conservation of energy . the other law says what happens if there is dissipation . then as the energy travels through the medium ( say , air ) , then some of the energy in the sound will be lost to heat . the way it works is that after a sound wave has traveled a distance $l_d$ , the amplitude of the wave decreases by a factor of $e$ . putting these two together , we expect intensity $i$ as a function of distance to the source $r$ to have the form $i ( r ) = i_0 r_0^2 \frac{e^{-2r/l_d}}{r^2} . $ now why did not your teachers tell you about this . well if you calculate $l_d$ according to the formula on wikipedia , you get about six miles . so four sounds that you ordinarilly hear , it is not an important effect . now as leftaroundabout said , the attenuation law was derived assuming a plane wave , but i am fairly certain that you will get the functional form i said for the spherical wave . the decay length might be slightly different , but it will just be an $o ( 1 ) $ correction .
i am not an expert on analysing classical pulsators , but the thing to remember is that , given a good signal-to-noise ratio , the fourier transform will still have it is strongest amplitude at the correct period . you will just see secondary peaks at other frequencies that are integral multiples of the main frequency . i.e. overtones . plus , in many systems you can see the pulsation periods quite clearly by eye . e.g. the curves here . the first thing i would add is that this does break down a bit when the star oscillates at more than one frequency . in that case , the distribution of peaks gets more complicated , although you can still try to work out what the two primary frequencies are , and which of the other peaks are beats , overtones , or combinations of beats and overtones . as you can guess , this is not a simple process . in some systems , it is a real headache ! the second thing is that , nowadays , we can model the pulsations , and these models are able to reasonably reproduce the basic shape , which we can then fit . i do not know the details of the modelling but i have seen convincing results presented at conferences . ( a quick search did not turn up details but i will have a closer look when i have more time . ) the third thing is that , indeed , there are many other methods for trying to find the main frequency . this is a review from a conference i attended a few months ago and this is the cited paper that compares a variety of methods .
in nmr , the strong magnets set the frequency of the nuclear resonance , using the constant magnetic field . typically the resonance radio waves are around the mhz frequencies whereas wi-fi is around 2.5ghz . when the frequencies are different , they do not disrupt each other 's signals . electro-magnets would not interfere , as it is a constant field produced rather than an elecromagnetic wave , and even then it would have to be at the same frequency .
a colleague in astronomy had a student a few years ago who did a calculation about the possibility of primordial black holes , created in the big bang . if the size of these was just right , they could be evaporating into nothing due to hawking radiation right " now " ( scare quotes because this would necessarily include distant black holes that evaporated many years ago , whose light is just reaching us now ) . the last burst of hawking radiation for these would look basically like a faint gamma-ray burst , in which case it ought to be directly detectable . i am not sure of the current status of this-- their preliminary result was , if i remember correctly , that you might be able to test this by measuring the probability distribution for gamma-ray bursts of the appropriate size and duration , but we did not have any telescopes capable of picking them up at the time . i am not sure if that is changed or not . anyway , that would give you a direct way to detect black holes of a certain size , though they would not be around after the detection , so it might not really fit the spirit of the question . . .
the only safe way for beginners to answer questions in special relativity is to sit down with a large sheet of paper and work through the lorentz tranformations : $$\begin{align} t ' and = \gamma ( t - \frac{vx}{c^2} ) \\ x ' and = \gamma ( x - vt ) \end{align}$$ let 's be absolutely clear what the tranformations tell us . if we use a coordinate system $ ( t , x ) $ to label spacetime points , and another observer moving at constant velocity $v$ relative to us uses another coordinate system $ ( t ' , x' ) $ , the transformations convert our labels $ ( t , x ) $ to the other observer 's labels $ ( t ' x' ) $ . so to answer your question we take the two spacetime points labelling the ends of the train and apply the transformations . this tells us where those two points are in the moving observer 's coordinates . in our frame at $t = 0$ the middle of train is at $ ( 0 , 0 ) $ , so the front of the train is at $ ( 0 , d/2 ) $ and the rear of the train is at $ ( 0 , -d/2 ) $ ( i have called the length of the train $d$ to avoid confusion with the $x$ coordinate ) : to find the potition of the front of the train in the primed frame we just feed $t = 0$ and $x = d/2$ into the lorentz transformations : $$\begin{align} t ' and = \gamma ( - \frac{vd}{2c^2} ) \\ x ' and = \gamma \frac{d}{2} \end{align}$$ so in the moving frame the lightning strike at front of the train is at $ ( -\gamma\tfrac{vd}{2c^2} , \gamma\tfrac{d}{2} ) $ . i will not go through the details , but same calculation puts the lightning strike at the end of the train is at $ ( \gamma\tfrac{vd}{2c^2} , -\gamma\tfrac{d}{2} ) $ . so the answer is that the observer on the train sees the lightning strike the front of the train at $t ' = -\gamma\tfrac{vd}{2c^2}$ and the rear of the train at $t ' = \gamma\tfrac{vd}{2c^2}$ . the time between the lightning strikes is $\gamma\tfrac{vd}{c^2}$ . quick footnote rereading my answer it is just occurred to me that i have called the length of the train $d$ in the rest frame of the track . the length of the train for the observers on it will be greater - you can use the lorentz transformations to calculate this too . length of the train re graviton 's comment , the easiest way to calculate the length of the train in the train 's rest frame is to work backwards . let 's call the length of the train in its rest frame $\ell$ , and we will choose our zero time so that the rear of the train is at $ ( 0 , 0 ) $ and the front is at $ ( 0 , \ell ) $ . to transform from the train frame to the track frame we just use the lorentz transformations as before , but in this case the velocity is $-v$ because if the train is moving at $v$ wrt to the track then the track is moving at $-v$ wrt the train . when we do the transformation $ ( 0 , 0 ) $ just goes to $ ( 0 , 0 ) $ so we just need to work out where $ ( 0 , \ell ) $ is in the track frame . plugging in $t = 0$ and $x = \ell$ we find the point in the track frame is : $$\begin{align} t and = \gamma ( t ' - \frac{ ( -v ) x'}{c^2} ) = \gamma\frac{v\ell}{c^2} \\ x and = \gamma ( x ' - ( -v ) t ) = \gamma\ell \end{align}$$ so in the track frame the front of the train is at $ ( \gamma\tfrac{v\ell}{c^2} , \gamma\ell ) $ . but we do not want to know where the front of the train is at time $t = \gamma\tfrac{v\ell}{c^2}$ , we want to know where is was at $t = 0$ . so we take our value for $x$ at time $\gamma\frac{v\ell}{c^2}$ and subtract off the distance moved in time $\gamma\frac{v\ell}{c^2}$ , which is just the time multiplied by the velocity . this gives us the value for $x_0$: $$ x_0 = \gamma\ell - \gamma\frac{v\ell}{c^2} v $$ the rest is just algebra . we write the expression out in full to get : $$\begin{align} x_0 and = \ell \left ( \frac{1 - \frac{v^2}{c^2}} {\sqrt{1 - \frac{v^2}{c^2}}} \right ) \\ and = \ell \sqrt{1 - \frac{v^2}{c^2}} \\ and = \frac{\ell}{\gamma} \end{align}$$ and since the rear of the train is at $x = 0$ at time zero and the front of the train is at $x = x_0$ at time zero the length of the train is just $x_0$ so : $$ d = \frac{\ell}{\gamma} $$ at all speeds $&gt ; 0$ the value of $\gamma &gt ; 1$ , so the length of the train as observed from the track is less than the length of the train in its rest frame i.e. the train is shortened . this is the lorentz contraction .
lithium , beryllium and boron are not produced in ( normal ) stellar nucleosynthesis - instead , three atoms of helium fuse to form carbon ( the triple-alpha process - two helium nuclei fall apart again almost instantly ) . but the necessary conditions only arise late in the lifetime of a star , when it has stopped burning hydrogen to helium and instead burns helium to heavier elements . in massive stars there is a catalytic cycle with carbon , nitrogen and oxygen called the cno cycle , during main sequence evolution - the equilibrium state is very nitrogen rich ( which is why massive stars are usually nitrogen overabundent and carbon and oxygen depleted ) , but in the latter states of the stellar lifetime the balance shifts as helium burning forms carbon - visible in the spectra of carbon-type wolf-rayets - while adding a helium nucleus to carbon gives you oxygen . nitrogen then dips and becomes somewhat underabundent . side branches of the cno cycle are responsible for some other elements , while successively adding helium nuclei to oxygen gives things like silicon , magnesium , calcium ( alpha-process elements ) and iron . the heavy elements all require neutron capture in red ( super ) giants .
let 's look to your own statements . first , time derivative after transformations is not equal to an " old " derivative : for $\mathbf r ' = \mathbf r - \mathbf u t = \mathbf r - \mathbf u t ' \rightarrow \mathbf r = \mathbf r ' + \mathbf u t'$ $$ \partial_{t'} = ( \partial_{t'}\mathbf r ) \partial_{\mathbf r} + ( \partial_{t'}t ) \partial_{\mathbf t} = ( \mathbf u \cdot \nabla ) + \partial_{t} , \quad ( \mathbf u \cdot \nabla ) = u^{i}\partial_{x_{i}} . $$ so , with $\nabla ' = \nabla$ , " bianchi " equations transforms to $$ ( \nabla \cdot \mathbf b' ) = 0 , \quad [ \nabla \times \mathbf e ' ] + \frac{1}{c}\partial_{t}\mathbf b ' + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b ' = 0 . \qquad ( . 1 ) $$ second , the form of $\mathbf {e}' ( \mathbf r ' , t' ) , \mathbf b ' ( \mathbf r ' , t' ) $ is not equal to $\mathbf e ( \mathbf r , t ) , \mathbf b ( \mathbf r , t ) $ . let 's use the lorentz force expression , $$ \mathbf f = q\mathbf e + \frac{q}{c} [ \mathbf v \times \mathbf b ] . $$ it does not depend on acceleration , so the statement that $\mathbf f ' = \mathbf f$ under galilean transformation is true . it means that $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v ' \times \mathbf b' ] . $$ by using galilean transformation for speed , $\mathbf v ' = \mathbf v - \mathbf u$ , this equation can be rewritten as $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v \times \mathbf b ' ] - \frac{1}{c} [ \mathbf u \times \mathbf b' ] , \qquad ( . 2 ) $$ so the statement that $\mathbf e = \mathbf e ' , \quad \mathbf b = \mathbf b '$ is not correct . so you need to find expressions $\mathbf e ' $ and $\mathbf b'$ via $\mathbf e $ , $\mathbf b$ . by rewriting $ ( . 2 ) $ , $$ \mathbf e + \frac{1}{c} [ \mathbf v \times ( \mathbf b - \mathbf b ' ) ] = \mathbf e ' - \frac{1}{c} [ \mathbf u \times \mathbf b ' ] , $$ in a reason of arbitrary $\mathbf u $ you can get the solution : $$ \mathbf b ' = \mathbf b , \quad \mathbf e ' = \mathbf e + \frac{1}{c} [ \mathbf u \times \mathbf b ] . $$ by substitution these equations to $ ( . 1 ) $ you will get $$ ( \nabla \cdot \mathbf b ) = 0 , \quad [ \nabla \times \mathbf e ] + \frac{1}{c} [ \nabla \times [ \mathbf u \times \mathbf b ] ] + \frac{1}{c}\partial_{t}\mathbf b + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b = [ \nabla \times \mathbf e ] + \frac{1}{c}\partial_{t}\mathbf b = 0 , $$ because for $\mathbf u = const$ $$ [ \nabla \times [ \mathbf u \times \mathbf b ] ] = \mathbf u ( \nabla \cdot \mathbf b ) - ( \mathbf u \cdot \nabla ) \mathbf b = - ( \mathbf u \cdot \nabla ) \mathbf b . $$ so the first pair of maxwell 's equations is clearly invariant under galilean transformations . let 's look to the other pair of maxwell 's equations : $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e = 0 , \quad ( \nabla \cdot \mathbf e ) = 0 . \qquad ( . 3 ) $$ by using an expressions which were derived above , you can rewrite $ ( . 3 ) $ as $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e ' - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e ' = $$ $$ = [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e - \frac{1}{c^{2}}\partial_{t} [ \mathbf u \times \mathbf b ] - \frac{1}{c^{2}} ( \mathbf u \cdot \nabla ) [ \mathbf u \times \mathbf b ] = 0 , $$ $$ ( \nabla \cdot \mathbf e ) + \frac{1}{c} ( \nabla \cdot [ \mathbf u \times \mathbf b ] ) = ( \nabla \cdot \mathbf e ) -\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) = 0 . $$ the requirement of galilean invariance of second equation leads to te state that $\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) $ , which is not true in the general case . analogically reasoning can be used for the first equation . so the second pair of maxwell 's equations is not invariant under galilean transformations .
the velocity calculation was distance* ( change in angle ) . however , this does not take into account the changing time-delay of light : we see it sped-up because the time delay is decreasing , like a tv recording where you are fast-forwarding as you gradually catch up with real time . fortunately , all we need to do to calculate the real speed is to account for the time delay , no weird relativity is necessary . suppose a far away object is approaching at $0.8c$ and has a transverse velocity of $0.25c$ ( total velocity of $0.84c$ ) . it emits a burst of light ( in our frame ) at time $t$ and $t+5$ seconds . it is $d$ light seconds from us at time $t$ and $d-4$ at $t+5$ . accounting for the time delay of light , we see flashes at time $t+d$ and $t+5+ ( d-4 ) = t+d+1$ ; we see them only $1$ second apart . in those $5$ seconds , it moved transversely a distance of $5\times0.25 = 1.25$ light seconds . since it appeared to move $1.25$ light seconds in $1$ second , we see apparent superluminal motion .
with quantum mechanics , you have to ask your questions very , very carefully . is it possible to have a sun-sized star in your pocket ? it depends on what you mean by " in " . do all of the atoms of the star need to be entirely in your pocket , or is it sufficient that some part of each atom 's wave function be inside your pocket ? it is possible to have any number of atoms counted as being in your pocket and they could just appear there . a nonzero probability of that could be calculated , but perhaps not contemplated . there is a small chance that a dropped coin will come to rest on its edge - a rare event that can be calculated and contemplated . there is a small chance that one atom will disappear from one side of the room and reappear and the other side . that can be calculated . the odds of a sun 's worth of atoms disappearing from nearby stars into your pocket in a second is nonzero and calculable , but probably not contemplatable . that said , if you said " is it possible there is a sun-sized star in my pocket right now ? " and a sun 's worth of particles just found themselves located in your pocket as you uttered " now " , your friends could not respond : " yes , there is very very tiny possibility this can be true ! " because they would be sucked into the black hole that just came into existence before they could speak those words . the way you ask the question will lead to different answers .
note : the blockquotes only apply with a gradual increase in acceleration starting from $0\ \text m\text s^{-2}$ . the trajectory that it makes depends almost entirely on the coefficient of friction between the two surfaces . this is because the ' net ' normal force will become less and less decreasing friction until the force : $\vec f = mg\sin\theta$ is larger than the static friction force : $\vec f = \vec n\mu$ , after which the block starts sliding down . now , the block will leave the ground if the acceleration , $\vec a_x$ , is large enough . using simple trigonometry it is found that the object leaves the ground when : $$\frac{\vec a_x}{\sin\theta} &gt ; \vec g\cos\theta$$ since the maxima of $\sin x *\cos x$ is $0.5$ , the object will instantly leave the incline . ( because $\sin\theta$ and $\cos\theta$ are positive in the first quadrant and the accelerations are of the same magnitude ) . if the static friction is large , the trajectory will look like the block jumps to a lower level . ( this is where your last diagram is completely wrong , because the net force acts down - the only force acting on it then is $mg$ ) . if on the other hand , the coefficient of friction is low , the lock will slide before it finally releases , and thus make a much longer jump . friction affects how much horizontal velocity it gains ; i.e. low friction will cause the block to ' move away ' from the incline much faster , because its velocity compared to that of the incline ( which is also accelerating ) is a lot lower . note that the block will always catch up with the incline eventually , because you defined $|\vec a|$ to be equal to $|\vec g|$ , and $$\vec g&gt ; \vec a\sin\theta$$ so essentially what happens is that every time the block touches the incline it leaves again . also do not speak about the " inertial force" ; simply call it friction . the only forces initially acting on the block are the : normal force , $\vec n=m\vec g\cos\theta$ ; the weight , $\vec w=m\vec g$ ; the force along the incline due to its weight , $\vec f=m\vec g\sin\theta$ ; and lastly friction , which is equal the previous force until that is greater than $\mu \vec n$ .
the problem is you have the wrong relations between $\{n_\mathrm{h} , n_\mathrm{he}\}$ and $\{n_\mathrm{p} , n_\mathrm{n}\}$ . every hydrogen contains 1 proton , and every helium contains 2 , so $n_\mathrm{p} = n_\mathrm{h} + 2 n_\mathrm{he}$ . the neutrons are only contributed to by helium in the accounting : $n_\mathrm{n} = 2 n_\mathrm{he}$ . inverting these relations yields \begin{align} n_\mathrm{h} and = n_\mathrm{p} - n_\mathrm{n} \\ n_\mathrm{he} and = \frac{1}{2} n_\mathrm{n} . \end{align} it looks like you got the direction wrong , in the sense that there should be fewer helium nuclei than protons or neutrons ( the 2 's are on the wrong side ) . also , " hydrogen " means ${}^1\mathrm{h}$ not ${}^2\mathrm{d}$ unless otherwise stated .
the $q_x$ and $q_y$ transitions are electronic excitations in the conjugated $\pi$ orbitals of the bchl a molecule . they involve two different sets of conjugated bonds . the $q_x$ involves a shorter chain of conjugated bonds so it occurs at a higher energy/frequency . i could not find a really good diagram to show which bonds are involved in the in the $q_x$ and which in the $q_y$ excitations , but figure 1 in this paper has a reasonable illustration .
( i am copy-pasting from @mitchellporter 's comments into this community wiki . ) off the mark . in qm you have states and you have " observables " and the states give the probabilities for the observables . the states ( wavefunctions ) evolve deterministically , the probabilities only pertain to how they manifest in observables . a state can imply a 100% probability for a particular outcome , then it is an " eigenstate " of that observable . . . ' t hooft considers wavefunctions which , as they evolve , are in an eigenstate , then , after a certain period of time , another eigenstate , and which keep passing through eigenstates at a fixed rate . these " moments when the wavefunction is in an eigenstate " correspond to the discrete timesteps of the cellular automaton .
i have found out the answer . when you do a measurement ( one measurement ) you have many uncertainty sources . but if you want to have the combined you do not add like 1 + 1 , because that would give you the 2 uncertainty but that does not have to be the case . the true value of the measurement may lie in between of the uncertainty regions but it may lie on the left or on the right of the range . as we do not know where exactly is the measurement we have to treat that situation like a rectangular distribution . we have $a=x-$$\triangle$$x$ and $b=x+$$\triangle$$x$ , where $\triangle$$x$ is the uncertainty . so we see that the standard deviation of this distribution is exactly our uncertainty divided by $\sqrt3$ . without that our uncertainty would be bigger than it needs to be . and so the final uncertainty of the single measurement is then : $u ( x ) =$$ ( \triangle$$x ) /\sqrt3$
as ravachol quoted , the material on the surface of the moon is mainly the result of billions of years of micrometeorite impacts onto larger rocks on the lunar surface . we call this material " regolith " as opposed to " soil " because the latter term is used in geology to indicate a more biological/organic origin . " regolith " should technically be used to describe the surface material on all surfaces except earth , but even planetary scientists will slip from time-to-time . the process of creating regolith is usually referred to as " gardening " because the surface material is hit by extraplanetary/lunar material , it will be broken up and tend to move a little , and then the material underneath it can be hit . this in general creates a process of slow overturn where the upper several meters will be broken up regolith before you hit more competent rock . however , the depth of the regolith can change significantly over the surface of the moon . it is also not just created by the micrometeorite impacts , but also larger impacts . a 1-km-diameter crater will produce a layer of ejecta that , after about 1 billion years , will become generally indistinguishable from the surrounding regolith except for maybe a slight topographic difference ( ramp as you approach the crater rim ) . these larger impacts are rarer , but they do create significantly more future regolith material than the micrometeorite bombardment in one go . and yes , to a lesser extent , cosmic rays and solar wind will help generate regolith , but they are very minor compared with actual meteoritic material . also , you can have tidal forces and some seismic activity ( mostly from tidal forces ) that will add to this , but those are also very minor .
for single particles , s orbitals have angular momentum quantum number $\ell = 0$ , and p orbitals have $\ell = 1$ . the magnetic quantum number $m$ runs from $-\ell$ to $+\ell$ in integer steps , so s orbitals can only have one value of $m=0$ ( hence a singlet state ) , and p orbitals can have $m=-1,0,1$ ( hence a triplet of states ) . is this what you are asking ? edit what i think your instructor meant is along the following lines . for a two-electron state , the combined wavefunction of the two electron system needs to be antisymmetric under exchange of electrons . the total wavefunction is the tensor product of the spin and spatial parts , so if one is symmetric , the other needs to be antisymmetric . note that the singlet $|\downarrow \uparrow\rangle - |\uparrow \downarrow\rangle$ , with $s=0$ , is antisymmetric under particle exchange , so the wavefunction of the electrons needs to be symmetric . so we can arrange $\psi ( x_1 , x_2 ) $ for the spatial part of the wavefunction to be symmetric . the theory of addition of angular momentum tells us that states with even angular momentum are symmetric under exchange , so in this case $\ell = \ell_1 + \ell_2 = 0 , 2 , \dots$ is allowed . for the s state ( $\ell = 0$ ) , we can just write $\psi ( x_1 , x_2 ) = \psi_s ( x_1 ) \psi_s ( x_2 ) $ where $\psi_s$ is a single-particle wave-function with $\ell_{1,2} = 0$ . for the triplet states , with $s=1$ , each spin state is symmetric under particle exchange , so we need to arrange a spatial wavefunction that is antisymmetric . allowable spatial wavefunctions have $\ell = 1 , 3 , \dots$ . the simplest is $\ell=1$ which is a symmetrized linear combination of $\psi_s$ and $\psi_p$ . the exact form of the wavefunction depends on $m_\ell$ . note though that we are doing all of this from the point of view of the spins . so that while the $s$ state has both particles individually in an $s$ state , the two-particle $p$ state has individual particles in a superposition of $s$ and $p$ states .
among the base units of the international system , the kilogram is the only one whose name and symbol , for historical reasons , include a prefix . names and symbols for decimal multiples and submultiples of the unit of mass are formed by attaching prefix names to the unit name " gram " , and prefix symbols to the unit symbol " g " ( cipm 1967 , recommendation 2 ) . bipm the reason why " kilogram " is the name of a base unit of the si is an artefact of history . louis xvi charged a group of savants to develop a new system of measurement . their work laid the foundation for the " decimal metric system " , which has evolved into the modern si . the original idea of the king 's commission ( which included such notables as lavoisier ) was to create a unit of mass that would be known as the " grave " . by definition it would be the mass of a litre of water at the ice point ( i.e. . essentially 1 kg ) . the definition was to be embodied in an artefact mass standard . after the revolution , the new republican government took over the idea of the metric system but made some significant changes . for example , since many mass measurements of the time concerned masses much smaller than the kilogram , they decided that the unit of mass should be the " gramme " . however , since a one-gramme standard would have been difficult to use as well as to establish , they also decided that the new definition should be embodied in a one-kilogramme artefact . this artefact became known as the " kilogram of the archives " . by 1875 the unit of mass had been redefined as the " kilogram " , embodied by a new artefact whose mass was essentially the same as the kilogram of the archives . the decision of the republican government may have been politically motivated ; after all , these were the same people who condemned lavoisier to the guillotine . in any case , we are now stuck with the infelicity of a base unit whose name has a " prefix " . bipm the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
let 's start from the main question Do electrons move when only one end/pole of the battery is connected ?  they do ! let 's say that you attach a wire to the positive terminal of the battery and this terminal is at " conventional " +5v . imagine a lot of positively charged particles accumulated there , now electrons would move as near them as possible , creating the same +5v potential throughout the wire . a very similar effect takes place in the charging of a capacitor by dc source , in that case too the circuit is never completed but we know that charges flow because that is how the capacitor charges . i could only understand the other question as Does attaching conductors to battery or other sources of electrical energy ionize them?  it would be wrong to say that something like this happens , because when atoms get ionized they have quite a bit of freedom of movement for example $\text{na}^+$ and $\text{cl}^-$ in water are ionized and move freely similarly ions in hot plasma move about freely . while in conductors the concept of free electrons is accepted because electrons rather than completely leaving the atom , get into contact with several atoms and even though they leave a charged specie behind , that specie does not have freedom of movement and hence cannot be called ionized .
so , buckling is the bifurcation of static equilibrium . and thus : more technically , consider the continuous dynamical system described by the ode $\dot x=f ( x , \lambda ) \quad &gt ; f:\mathbb{r}^n\times\mathbb{r}\rightarrow\mathbb{r}^n . $ a local bifurcation occurs at $ ( x0 , λ0 ) $ if the jacobian matrix $\textrm{d}f_{x_0 , \lambda_0}$ has an eigenvalue with zero real part . this also happens to coincide with astm e-9 standard , section 3.2.1 that says : bucklig -- ( 3 ) a local instability , either elastic or inelastic , over a small portion of the gage length in plain english , the above mathematical formula implies : at time t , when constantly increasing a load to a column , the position of the column has more than one solution ( e . g . the column bends right or left ) . before time t , there is only one solution for the position .
this is a very late answer . there is the book ellipsoidal figures of equilibrium by the god himself in this field s . chandrasekhar . chapter 3 is devoted fully to understanding the gravitational potentials of ellipsoids . theorems 3 and 9 are what you are looking for . you also asked can they be derived in terms of ellipsoidal harmonics ? chandrasekhar does not derive the equations in terms of ellipsoidal harmonics . in fact , he states that very early on in the introduction ( section 16 ) . instead he employs spherical polar coordinates and proceeds by establishing a series of lemmas on the moments of the mass distribution . this amounts to considering integrals of the form $$i ( u ) = a_1 a_2 a_3 \int_u^{\infty} \frac{du}{\delta} ; \qquad a_i ( u ) = a_1 a_2 a_3 \int_u^{\infty} \frac{du}{\delta ( a_i^2 + u ) }$$ where $\delta^2= ( a_1^2+u ) ( a_2^2+u ) ( a_3^2+u ) $ and $a_i$ are the semi-major axes of the ellipsoid . then come the two theorems you need theorem 3 : at a point $x_i$ interior to the ellipsoid , the potential is $$\phi = \pi g \rho \big [ i ( 0 ) - \sum_{i=1}^3 a_i ( 0 ) x_i^2 \big ] $$ theorem 9 : at a point $x_i$ exterior to the ellipsoid , the potential is $$\phi = \pi g \rho \big [ i ( \lambda ) - \sum_{i=1}^3 a_i ( \lambda ) x_i^2 \big ] $$ where $\lambda$ is the positive root of $$\sum_{i=1}^3 \frac{x_i^2}{a_i^2 + \lambda} = 1$$
think about it physically . how much gas is in the tube ? there are three sources : the gas coming in at $x=0$ which was already in solution . the gas being carried away by the fluid at the other end , $x=l$ diffusion across the walls of the tube . these three terms are represented in the equation $$ \frac{d}{dt} \left ( a \int_{0}^{l} u ( x , t ) dt \right ) = v ( 0 ) au ( 0 , t ) - v ( l ) au ( l , t ) + p \int_{0}^{l} q ( x , t ) dx $$ the first term is the velocity of the liquid , $v$ , times the cross section , $a$ , giving the total volume of liquid coming in at $x=0$ . we multiply this by the concentration of gas in the liquid at $x=0$ to get the velocity that gas enters at $x=0$ . the second term goes in much the same way , but we have a negative sign because it is the gas which is leaving at the other end . you say you understand that the third term as diffusion through the walls , which is correct .
this link : http://alienryderflex.com/polarizer/ has an excellent explanation ; much better than anything i could write here . essentially , it says that this occurs because the 45 degree filter outputs a projection of the vertical rays at 45 degrees . this , in turn , has a horizontal component , which the final filter projects in its output .
is it true that gravitational lensing only occurs for objects made of plasma ? most of space contains a plasma , or at least some ionisation , at some density or other , making the claim hard to dispute at that level . also , gases and/or plasmas will tend to exist with higher densities in deeper gravity wells . what is the merit of claim that bending could be caused by plasma and not by gravity ? in general it is possible for light paths to be changed by interaction with matter ( a glass lens being an obvious example ) . in principle similar distortion could happen with a rarefied plasma over large distances . a density variation at the right scale , and with a simple shape would be required . i expect however that the claim in the link is just a conjecture/challenge , and not mathematically or scientifically robust ( although i do not personally have the knowledge and skills to assess that ) . i suspect that the lensing seen in an einstein ring would be very hard to duplicate using realistic values for plasma density near the lensing object . to me , there are plenty of warning signs that this is a crackpot claim from the link . unfortunately the world is full of them . often such people cannot be corrected by scientific evidence , in fact they will self-promote that there is " controversy " when engaged in conversation , or complain of being " suppressed by the establishment " when ignored . i am not saying those thing apply to the author ( i have not checked other publications ) , but i would be wary of the " a nasa physicist " label - it is likely correct on some technical label ( he worked for nasa and has a qualification in physics ) but is not particularly meaningful . it does not help the lay audience ( nb i include myself here ) when real scientific controversies and useful scepticism can follow a similar pattern . popular proponents of badly-thought-through or outlier theories often manage to pick up on the form of this and produce " science-y " work , making it harder to filter out good from bad . update : i have struggled through a few pages of http://www.extinctionshift.com and stand by this - the work appears to be random picking of equations plus handwaving arguments about how they connect . nothing is properly analysed . does this mean general relativity is invalid ? no , even if somehow the claim had merit , there is plenty of evidence for gr outside of gravitational lensing . for example : time-dilation effects of general relativity can be measured directly by atomic clocks . they have got so accurate that we can actually measure the time dilation of a few metres height difference at ground level . the gps system needs to account for this effect . however it is well known that general relativity and the standard model of quantum mechanics , although our best current models in physics , cannot be the last word . they are not compatible , and where they overlap ( typically when considering cosmology or black holes ) there is a need for a combined or replacement theory . that does not make the theories " wrong " any more than newtonian physics was " wrong " , but it does mean we can expect some extensions or a deeper theory to emerge at some point .
yes , alternating current will radiate electromagnetic waves . for example , in telecommunication , the transmitter itself generates a radio frequency alternating current , which is applied to the antenna . when excited by this alternating current , the antenna radiates radio waves . the usual " pick up " that we get from electric currents in the circuits in the walls of a building have a frequency of about one hundred cycles per second . if we increase the frequency to $500$ or $1000$ kilocycles ( $1$ kilocycles = $1000$ cycles ) per second , we are " on the air " , for this is the frequency range which is used for radio broadcasts . ( of course it has nothing to do with the air ! we can have radio broadcasts without any air . ) $^{*}$ $*$extracted from the feynman 's lectures on physics-volume one .
per kittel 's " elementary statistical physics": $$dq=du_a+mdh$$ . where $du_a$ does not include the magnetic field energy as part of the system . an alternative , equivalent , formulation is $$dq=du_b-hdm$$ where $du_b$ does include the field energy : $u_a=u_b-\mathbf{h \cdot m}$ . with $dq=tds$ , i think you are there . . .
i will try to give a simple answer without going too deep into physics details . localization of light is the confinement of a light wave so that it is very intense in one particular location . usually it vanishes rapidly as you get farther away from that location . ( note that by " light wave " i am not talking about the traveling plane waves that you would use to describe a light beam ; this is more of a stationary oscillation mode . ) this crops up in the context of plasmonic nanoparticles because surface plasmons ' wavelengths are smaller than normal light waves of the same frequency , allowing surface plasmons to be confined into even tinier spaces . there are two major advantages that lead to applications . one is that confining the light into a small space allows easy manipulation of it , for example by tailoring the shape of your nanoparticle . the other is that confining the light into a small space also squeezes all the energy it carries into that small space . this is called field enhancement . you can then do processes that require a lot of electric field strength ( e . g . nonlinear things ) without needing a huge amount of light to achieve that field strength . ( i adapted some of this answer from the introduction to my doctoral dissertation . )
yes , that is ok ! you have stumbled upon one of the basic strange phenomena of relativity .
the kinetic theory of gases does in fact say that molecules in a gas move very rapidly ( although some move quite slow , and others even faster ) . however , there is another crucial component to the theory . the idea of mean free path . here , a molecule is moving very fast but does not get very far before hitting another molecule . this is why things like odors travel relatively slow and why gases do not " mix instantaneously . " a given molecule may only move 1e-6 meters before bouncing off of another molecule , and the direction it bounces is random . this slows down the progress of mixing . additionally , in your example of opening the rear window of a truck , it is important to look at the macroscopic effects of the flow , which kinetic theory does not really explain . the truck cabin generates a wake with a recirculation zone behind it . this means that the air immediately behind the window is relatively stagnant , which is also why the air in the cabin does not leave as fast as it could .
all you need to do is set up a multipole expansion of the gravitational waveform . you will find that the monopole moment is proportional to the time derivative of the mass of the stress-energy tensor , and the dipole moment is proportional to the second time derivative of the momentum from the stress-energy tensor , both of which are conserved . thus , the first nonzero moment comes from the quadrupole moment . this is worked out in great detail in mtw .
one of the best recent references is the 2008 rmp article by nayak et al . non-abelian anyons and topological quantum computation a somewhat less technical reference is an anyon primer by sumathi rao . there are many others but these two are good for someone starting out .
the thermal radiation associated with some object is typically described in terms of the " black-body " spectrum for a given temperature , given by the planck formula . this formula is based on an idealization of an object that absorbs all frequencies of radiation equally , but it works fairly well provided that the object whose thermal spectrum you are interested in studying does not have any transitions with resonant frequencies in the range of interest . as the typical energy scale of atomic and molecular transitions is somewhere around an ev , while the characteristic energy scale for " room temperature " is in the neighborhood of 1/40 ev , this generally is not all that bad an assumption-- if you look in the vicinity of the peak of the blackbody spectrum for an object at room temperature , you generally find that the spectrum looks very much like a black-body spectrum . how does this arise from the interaction between light of whatever frequency and a gas of atoms or molecules having discrete internal states ? the thing to remember is that internal states of atoms and molecules are not the only degree of freedom available to the systems-- there is also the center-of-mass motion of the atoms themselves , or the collective motion of groups of atoms . the central idea involved with thermal radiation is that if you take a gas of atoms and confine it to a region of space containing some radiation field with some characteristic temperature , the atoms and the radiation will eventually come to some equilibrium in which the kinetic energy distribution of the atoms and the frequency spectrum of the radiation will have the same characteristic temperature . ( the internal state distribution of the atoms will also have the same temperature , but if you are talking about room-temperature systems , there is too little thermal energy to make much difference in the thermal state distribution , so we will ignore that . ) this will come about through interactions between the atoms and the light , and most of these interactions will be non-resonant in nature . in terms of microscopic quantum processes , you would think of these as being raman scattering events , where some of the photon energy goes into changing the motional state of the atom-- if you have cold atoms and hot photons , you will get more scattering events that increase the atom 's kinetic energy than ones that decrease it , so the average atomic ke will increase , and the average photon energy will decrease . ( or , in more fully quantum terms , the population of atoms will be moved up to higher-energy quantum states within the box , while the population of higher-energy photon modes will decrease . ) for thermal radiation in the room temperature regime , of course , the transitions in question are so far off-resonance that a raman scattering for any individual atom with any particular photon will be phenomenally unlikely . atoms are plentiful , though , and photons are even cheaper , so the total number of interactions for the sample as a whole can be quite large , and can bring both the atomic gas and the thermal radiation bath to equilibrium in time . i have never seen a full qft treatment of the subject , but that does not mean much . the basic idea of the equilibration of atoms with thermal radiation comes from einstein in 1917 , and there was a really good physics today article ( pdf ) by dan kleppner a few years back , talking about just how much is in those papers .
he is comparing $\sqrt{1-v\over 1+v}$ to the classical doppler shift $ ( 1-v ) $ ( where v is the velocity divided by c , since i use units where c=1 ) . the formula you give $1-v\over 1+v$ does not have a classical interpretation , and einstein reduces to doppler 's at slow speeds .
indeed , without assuming it from first principles as in bogoliubov formulation , the invariance property of $s$ operator you mention holds when the interaction lagrangian does not include derivatives of fields like in qed . that is a consequence of dyson 's expansion in interaction picture . when , as said above , the interaction lagrangian does not include derivatives of fields , one has : $${\cal h}_i = -{\cal l}_i$$ so that $$s = \sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) \:d^4x_1\cdots d^4x_n\: . $$ it is worth noticing that $\hat{\cal l}_i ( x ) $ includes only free field operators as we are dealing with the so-called interaction picture , so everything is explicitely known , commutation relations of field operators in particular . since the lagrangian functions are scalars , we have : $$u_\lambda\hat{\cal l}_i ( x ) u^\dagger_\lambda = \hat{\cal l}_i ( \lambda^{-1} x ) \qquad ( 1 ) $$ moreover , in view of free fields commutation relations one also has:$$ [ \hat{\cal l}_i ( x ) , \hat{\cal l}_i ( y ) ] =0 \qquad ( 2 ) $$ if $x$ and $y$ are spacelike separated . the fact that $s$ is invariant under the action of orthochronous lorentz group is quite obvious $$u_\lambda su_\lambda^\dagger = \sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int u_\lambda t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] u_\lambda^\dagger \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ u_\lambda \hat{\cal l}_i ( x_1 ) u_\lambda^\dagger\cdots u_\lambda \hat{\cal l}_i ( x_n ) u_\lambda^\dagger ] \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ \hat{\cal l}_i ( \lambda^{-1} x_1 ) \cdots \hat{\cal l}_i ( \lambda^{-1}x_n ) ] \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] \:d^4x_1\cdots d^4x_n$$ due to the lorentz invariance of the measure $d^4x$ . the identity : $$u_\lambda t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] u_\lambda^\dagger= t [ u_\lambda \hat{\cal l}_i ( x_1 ) u_\lambda^\dagger\cdots u_\lambda \hat{\cal l}_i ( x_n ) u_\lambda^\dagger ] $$ is consequence of the definition of $t$-ordinator , ( 1 ) , and ( 2 ) for spacelike separated arguments , considering all cases concerning the time order of $x_1 , \ldots , x_n$ and the fact that $\lambda$ does not change the temporal order for causally related arguments as it belongs to the orthochronous subgroup . for more complicated theories the result is not obvious and it could be false in its elementary formulation based on canonical quantization , excluding the case of gauge theories , where it can be proved separately . regarding weinberg 's statement about lorentz covariance of the $s$ matrix and lorentz invariance of $s$ operator , if i understood well the definition , i think that it works like this . let us start from the full ( interacting ) theory . there are vectors $\psi^\pm_{\{p_i\}}$ describing states which , at late time ( respectively $t\to +\infty$ and $t\to -\infty$ ) evolve like free particle states with momenta $\{p_i\}$ . the correspondingly associated free states , always evolving in accordance witht he free theory , are indicated by $\phi_{\{p_i\}}$ . the $s$-matrix is the matrix of elements : $$\langle \psi^+_{\{q_i\}}|\psi^-_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: . \qquad ( 3 ) $$ in the rhs the $s$ operator takes place . in view of it , the scattering process is completely described in terms of free states . to say that the $s$ matrix is lorentz covariant should mean ( as far as i understand ) : $$\langle \psi^+_{\{\lambda q_i\}}|\psi^-_{\{\lambda p_i\}}\rangle = \langle \psi^+_{\{q_i\}}|\psi^-_{\{p_i\}}\rangle\quad \forall \lambda \in o ( 3,1 ) \uparrow\: , \forall \{\lambda q_i\}\: , \{\lambda p_i\}\: . $$ from ( 3 ) , it immediately entails : $$ \langle \phi_{\{\lambda q_i\}}|s\phi_{\{\lambda p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: . \qquad ( 4 ) $$ if $u_\lambda$ is the unitary representation of $o ( 3,1 ) \uparrow$ on free states , so that $\phi_{\{\lambda p_i\}}= u_\lambda \phi_{\{p_i\}}$ , we therefore have : $$\langle u_\lambda \phi_{\{ q_i\}}|s u_\lambda\phi_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: , $$ that is $$\langle \phi_{\{ q_i\}}|u^\dagger_\lambda s u_\lambda\phi_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: , $$ and so : $$\langle \phi_{\{ q_i\}}| \left ( u^\dagger_\lambda s u_\lambda - s\right ) \phi_{\{p_i\}}\rangle = 0\: , $$ since the set of vectors $\phi_{\{p_i\}}$ forms a basis of the hilbert space ( of the free theory ) in view of the asymptotic completeness hypotheses , we conclude that : $$u^\dagger_\lambda s u_\lambda - s=0$$ i.e. $$s=u_\lambda s u^\dagger_\lambda\: , \quad \forall \lambda \in o ( 3,1 ) \uparrow\: . $$ in other words if the $s$ matrix is lorentz covariant , then the $s$ operator is lorentz invariant .
one may always choose any coordinates on a spacetime manifold or any other manifold , for that matter . that is not only a simple mathematical insight but also a cornerstone of the general theory of relativity . in fact , gr starts with the postulate that all ( non-singular etc . ) coordinate systems are as good as any other coordinate systems and the basic laws of physics should have the same ( and equally simple ) form in all of them . this democracy is known as the " general covariance " and the fact that gr respects this principle is why it is called the general theory of relativity . in cosmology , we may want one of the coordinates to be a cosmic time $t$ whose constant value may specify the same moment in the whole universe – the same time since the big bang , as reflected e.g. in the local temperature which is a function of $t$ . when we add the assumption that $t$ , one of the coordinates is a cosmic time , we are adding some extra information about the space . anti de sitter space or de sitter space or other spacetimes may admit various slicings to " cosmic time " , even with different signs of the spatial curvature of the slices . what allows us to create an embedding and invoke arbitrary coordinates on it within it is ' larger ' minkowski space ? this question seems to implicitly assume that the larger minkowski space into which you embedded the anti de sitter space plays a physical role . but it does not play any role . it is nothing more than one of the many ways to visualize the anti de sitter space and its shape ( although arguably a particularly simple one ) . it is not a " real " space . the spacetime or the universe is everything there is ; so if the spacetime is an anti de sitter space , it means that there is no " larger space outside it " . and coordinate systems on anti de sitter space are not " obliged " to resemble the coordinates $x_0 , x_1 , \dots$ on your larger minkowski space . in fact , the physically natural and useful coordinate systems almost never do .
since you have a magnetic field along the $z$-axis , you can choose the associated potential vector to be purely radial and $\rho$-dependent ( the $\mathbf{a}$ field turns around your wire , and decays as $r^{-1}$ if i remember my electromagnetism . that is , $\mathbf{a}=a_{\varphi}\left ( \rho\right ) \boldsymbol{\varphi}$ only . almost all your partial differential will be killed .
[ another comment to answer transplant ] it seems like you are asking about a classical analog to the superselection sectors of quantum mechanics . one situation where this occurs in classical mechanics is when considering particle motion of a manifold with non-trivial topology - i.e. with holes and handles . in such cases there can be more than one extremal path from a to b . an example is an arcade pin-ball machine , if you are familiar with those . also , when you talk about optics it is important to keep in mind that there are two different regimes , those of wave optics and geometric optics . in the second case one has well-defined " trajectories " and you can find extremal trajectories . not so in the first case .
my question is . why we need $\phi$ ? why just not speaking about probabilities itself , i.e. probability measures $\mathsf \phi$ on $\mathbb r$ such that $\phi ( a ) $ is the probability of particle 's position in $a\subset \mathbb r$ . one of the principles of quantum mechanics is that position and momentum are not independent but do not determine one another . this can be made precise in various ways , e.g. , $ [ \hat{x} , \hat{p} ] = i\hbar$ means the position-space wavefunction is related to the momentum-space wavefunction through a fourier transform , which instantiates the de broglie relations . the phase of a particle 's position-space wavefunction is not irrelevant--as a consequence of the schrödinger equation , it gives information about momentum , related to the gradient of the phase . in quantum mechanics and path integrals , feynman and hibbs expressed the view that quantum mechanics represents a replacement of probability theory itself . that is probably a good way of thinking about it : you are still dealing the " probabilities [ themselves ] " , but they behave in a blatantly non-classical way--for the wavefunction representation , they add like complex numbers . you do not actually ' need ' a wavefunction , but it encapsulates the probabilities of all possible measurements in straightforwardly ( assuming a pure state ) . as an alternative , wigner quasiprobability distributions work in an analogue of a classical phase space , but they can assign negative probabilities ( for incompatible measurements , fortunately ) . thus , you could work with distributions that add in a classical manner if you wanted to , at the price of departing from standard probability theory in yet another way . but whatever you do , you are not going to fit quantum mechanics in a classical probability theory . it is clear from the double-slit experiment that something very wonky is going on , and bell 's inequalities can prove this in general . wavefunctions are not your only choice , but classical probabilities do not work . moreover , why we speaking about observables as operators . why not just speaking about random variables on $\mathbb r$ . ( i know that observable is not a arbitrary operator ; but may be the restrictions can be expressed in terms of random variables ? ) again , you could talk about random variables , as wigner distributions do this , but not for individual observables ( that would not describe the state fully ) , but being quasiprobabilities , they do not fit into standard measure theory . as for why observables are operators , it is because they form an algebra that can be represented via operators on a hilbert space . perhaps surprisingly , there is nothing quantum-mechanical in that statement . let 's back up a bit . the classical phase space describes the state as a combination of positions an momenta , $ ( q ; p ) $ . observables are continuous real-valued functions over the phase space , e.g. , position $q$ , momentum $p$ , kinetic energy $t ( q ; p ) = p^2/2m$ , etc . , with the time evolution of any observable $f$ being given by the poisson bracket with the hamiltonian : $$\frac{\mathrm{d}f}{\mathrm{d}t} = \frac{\partial f}{\partial t} + \{f , \mathcal{h}\}\text{ . }$$ these observables can be added , multiplied , and so forth , and in general form a poisson algebra with the poisson bracket as the lie bracket . even in classical mechanics , observables form an algebra , a commutative one . by the gel'fand-naĭmark theorem , a somewhat general class of algebras ( $c^\star$ ) can be represented as the algebra of bounded linear operators on some complex hilbert space . ( although since in most situations the interesting observables in qm are not bounded , how this is generalized to include unbounded operators is somewhat hairy ; some of textbooks , e.g. , griffith 's , just put a blurb in a footnote about rigged hilbert spaces , a decision i am going to emulate here . ) many expressions in quantum mechanics via integrals with observables and states , as i understand , just express expectation , variation and standard deviation of such random variables w.r.t. probability that define the states none of these things are inherently quantum-mechanical . as implied by the previous section , even classical mechanics can be formulated as having the physical state as a vector on a complex hilbert space , observables as ( some ) linear operators with measurements outcomes being the eigenvalues . there state can be represented as a complex wavefunction $\psi ( x , p ; t ) $ with measurement collapsing it to the eigenspace corresponding to the result , and it evolves in by an analogue schrödinger equation ( the time-evolution given by the liouvillian operator sintead ) . this has been done in detail by koopmnan and von neumann . for example , heisenberg 's inequality just inequality for product of standard deviations of position and momentum . now that is very quantum-mechanical . the heisenberg uncertainty principle is a direct consequence of non-commutativity of observables : $$\sigma_a\sigma_b \geq \frac{1}{2}\left|\langle [ \hat{a} , \hat{b} ] \rangle\right|\text{ , }$$ where $ [ \cdot , \cdot ] $ is the commutator .
the majority opinion is that einstein was wrong . however , i see some problems with the standard quantum mechanics ( sqm ) approach that you outlined . sqm contains two major parts : unitary evolution ( described , e.g. , by the dirac equation ) and the measurement theory ( e . g . , collapse , or the projection postulate , which , loosely speaking , states that , after measurement of some observable , the system stays in an eigenstate of that observable with the relevant eigenvalue ) . both of these parts are used in your reasoning : on the one hand , unitary evolution ensures that the particles have zero total spin all the time , and the projection postulate ensures that , if particle a was measured to have spin up , it is in the relevant eigenstate . so far so good . the problem is that the two parts of sqm are mutually contradictory ( the notorious measurement problem in quantum mechanics - http://plato.stanford.edu/entries/qt-measurement/ ) . for example , unitary evolution cannot introduce irreversibility or turn a superposition of two states into their mixture , whereas the projection postulate does just that . so if you include the instrument ( and the observer , if you wish ) into the system containing particle a and then run unitary evolution to describe the measurement process , you will get a superposition of states with different spin projections and , strictly speaking , will not get a definite outcome . actually , your reasoning is close to that used in the proof of the bell theorem to prove that the bell inequalities can be violated in quantum mechanics . however , such proof contains mutually contradictory assumptions . on the other hand , there have been no loophole-free experiments demonstrating violations of the bell inequalities , so there are both theoretical and experimental difficulties with your reasoning , although it is widely used . therefore , i tend to think that your question has not been resolved yet , as its resolution may demand either resolution of the measurement problem in quantum mechanics or loophole-free bell experiments .
moment of inertia the definition of ( mass ) moment of inertia of a point mass is $$ i=r^2m $$ however in the real world you do not encounter point masses , but objects with non-zero volume ( finite density ) . and leads to an integral to determine moment of inertia $$ i=\int_m{r^2dm}=\int_v{\rho ( r ) r^2dv}=\int_x{\int_y{\int_z{\rho ( x , y , z ) ( x^2+y^2+z^2 ) dz}dy}dx} $$ the solutions of this integral of a few bodies , with constant non-zero density within geometric volume and zero density outside of it , can be found here . for example the moment of inertia of thin rod rotating around its center of mass is equal to $i=\frac{ml^2}{12}$ and for a solid cylinder $i=\frac{ml^2}{2}$ . experimental setup in your experimental a string is on one end connected to the hanging mass , lead over the pulley and then wounded around a drum ( the other end is also connected to the drum ) . this drum is of the object from which you would like to determine its moment of inertia and it is assumed that it can rotate freely ( without slip ) around its axis . according to your documentation you measure how far the pulley has rotated , i will call this angle $\theta$ , and its first and second time derivative $\omega=\dot{\theta}$ and $\alpha=\ddot{\theta}$ . the displacement of the hanging mass is related to the angular displacement of the pulley and its radius , $r_p$ , assuming that the string does not slip , so $$ s=r_p\theta $$ where $s$ is the vertical downward displacement of the hanging mass . this displacement is equal to the amount of string unrolled from the drum ( assuming that the string is not elastic ) , which means that the angular displacement of the object from which you would like to determine the moment of inertia , i will call this $\theta_i$ , can be calculated from this the other way around using the radius of the drum $r_d$ $$ s=r_p\theta=r_d\theta_i\rightarrow\theta_i=\frac{s}{r_d}=\frac{r_p}{r_d}\theta $$ this linear correlation also applies to the $\omega$ and $\alpha$ . the only force applied on this system ( which can perform work ) is gravity on the hanging mass . using all this you can derive the equation of motion ( possibly using free body diagrams and tension in the string ) .
every term contains one $\lambda$ in the superscript and one in the subscript , so you sum over those . the only indices which do not appear in both superscript and subscript in the same term are $\mu$ and $\nu$ . example : $$\gamma_{\lambda\sigma}^\lambda\gamma_{\mu\nu}^\sigma = \gamma_{00}^0\gamma_{\mu\nu}^0 + \gamma_{01}^0\gamma_{\mu\nu}^1 + \cdots + \gamma_{10}^1\gamma_{\mu\nu}^0 + \cdots$$
an electric field is said to be static if it does not change with time , i.e. the the charges that produced that field are stationary . this does not imply any constriction on its spatial dependence . in particular , no spherical symmetry is implicit in the definition of electrostatic field , and that field may not depend only on $r$ , as your example shows . this is common when you consider examples of field produced by more than one point charge , e.g. an electric dipole : $$\mathbf{e} ( \mathbf{r} ) ={3\mathbf{p}\cdot\hat{\mathbf{r}}\over 4\pi\varepsilon_0 r^3}\hat{\mathbf{r}}-{\mathbf{p}\over 4\pi\varepsilon_0 r^3}$$ depends on $\mathbf{r}$ and $\mathbf{p}$ .
it is multiplication . when you turn the wheel at the beginning by $x$ degrees , the axle at the end will turn by $x/100$ degrees . this comparison of lengths/angles of movement directly translates to the mechanical advantage . the work is a constant , so because of $w=x_1 f_1=x_2 f_2=x_1/100 f_2=x_1/100\cdot 100 f_1$ , it follows that $f_2=100 f_1$ , which is exactly the mechanical advantage .
ondřej 's answer is partially correct . in reality , you actually do have to worry about how far your eye is from the " eye lens " as the secondary lens system is called in a telescope . this is due to the need to match or over fill the eye 's entrance pupil with the telescopes exit pupil . if you do not do this , you will see a small circle where the telescope is actually working , and it can limit your field of view . you can also think about the telescope in the following way : the objective lens ( the first lens or lens system ) is forming an aerial image ( a really tiny one ) near the shared focal point . the eye lens ( the second lens or lens system ) is magnifying that image , in exactly the same way that you would use a simply magnifier to observe beetles or something . so , to select an eye lens first you want the most magnification which you can achieve without distortion or blurriness . you can do this by taking your eye lens system and looking at normal objects as if it were a magnifying glass . the calculation for the magnification of the eye lens is : $$ m_{\text{eye}} = \frac{250mm}{f_{\text{eye}}} $$ where $f_{\text{eye}}$ is the focal length of the lens ( or lens system ) and $250mm$ is the near point of the average relaxed human eye . the telescope magnification is then given by $$ m_{\text{telescope}} = -\frac{f_{\text{objective}}}{f_{\text{eye}}} $$ the second thing that you have to worry about is the exit pupil matching as i stated above . a galilean telescope will always have a narrower field of view and a little circle that you can see . for a reflective or keplerian telescope , you can add field lenses to increase the field of view . this is basically what huygens or ramsden eyepieces do . since you want your eye 's field of view to be maximized , you can do this by making sure that your exit pupil is located a reasonable distance outside of the telescope system . you will have to do calculations to find this distance , i suggest buying grievenkamp 's field guide to geometrical optics for a complete description of first order optics and a decent account of aberrations .
the weak force " looks " different because in the first ( and still most important ) reincarnation we have encountered it - namely beta-decay ( including the decay of the neutron ) - the force seems to be a contact interaction : it has an extremely short range , essentially zero . however , any phenomenon that differs from the indefinite existence of an object that moves in the same direction by the same speed forever requires a force to be explained . the force required for the beta-decay is the weak nuclear force . while the decay seems to " directly " transform a neutron into a proton , electron , and antineutrion , a closer investigation of the force that began in the 1960s has demonstrated that this force is actually analogous to other forces , including electromagnetism , because its range is finite ( nonzero ) . it is only limited because it is mediated by the w and z bosons which are , unlike photons , massive . so the force does not get " too far " . however , in our modern description of the forces , electromagnetism and the weak force have to be described by a unified " electroweak " theory and they mix with one another . at distances much shorter than the range of the w/z bosons , the electromagnetic and weak forces become equally strong and , in some proper sense , indistinguishable .
there are two immediate problems with this idea . first , the acceleration due to the dark energy appears to be the same in all directions . in general relativity ( and newtonian gravity for that matter ) the influence of distant matter can only cause a tidal acceleration that expands matter in some directions and compresses it in others . a uniform expansion has to be due to something present locally . second , gravitational influences travel at the same speed as light , which means we can see the most distant matter that could gravitationally influence any of the other matter we can see . what we actually see ( in the cosmic microwave background ) is remarkable uniformity in all directions , to within about 0.01% . so there is no density fluctuation close enough to have an effect . dark energy is not as strange as you think ; it is actually quite natural in quantum field theory , and the biggest mystery is why the dark energy is so small , not that it exists . and modern big bang cosmology starts with inflation , not with a singularity .
in the normal usage , real and virtual are not properties of feynman diagrams themselves , but of the particles depicted in them . the particles corresponding to external lines ( attached to at most one vertex only ) are real , the others ( attached to two vertices ) are virtual . a feynman diagram may be considered as a repetitive part of a bigger diagram . this requires that the external lines of the diagram are kept off-shell , so that the corresponding integrals depend on off-shell momenta ( rather than only on-shell momenta , which would suffice for s-matrix elements ) . their computation is more complex as one has to account for a bigger parameter space . the use of off-shell feynman diagrams in this sense is that they can be used in resummation techniques as building blocks of infinite families of on-shell feynman diagrams . indeed , such a recursive usage necessitates treating the external lines as virtual . as with virtual particles , such off-shell feynman diagrams have no measurable counterpart but are just intermediate expressions in the resummation calculation . however , in papers such as http://www.sciencedirect.com/science/article/pii/0550321379901160 , a virtual feynman graph is simply an ordinary feynman graph involving a loop , in contrast to a real feynman graph = lowest order tree graph . see the explanation after eqs . ( 42 ) and ( 81 ) . this paper is quoted in http://arxiv.org/abs/1012.0507 after eq . ( 3 ) for details about virtual diagrams . in http://arxiv.org/abs/1112.1061 , this terminology applies to the graphs cut at the lines in fig . 1 , corresponding to a factorization . http://arxiv.org/abs/hep-ph/9701284 also uses this terminology ; see the titles of sections 3.1 and 3.2 , and the corresponding figures .
it is because when $$v ( x , y , z ) = v_x ( x ) + v_y ( y ) + v_z ( z ) , $$ ( i guess that your extra identity $v ( x , y , z ) =v ( z ) $ is a mistake ) , we also have $$ h = h_x + h_y + h_z$$ because $h = ( \vec p ) ^2 / 2m + v ( x , y , z ) $ and $ ( \vec p ) ^2 = p_x^2+p_y^2+p_z^2$ decomposes to three pieces as well . one may also see that the terms such as $h_x\equiv p_x^2/2m+v_x ( x ) $ commute with each other , $$ [ h_x , h_y ] =0 $$ and similarly for the $xz$ and $yz$ pairs . that is because the commutators are only nonzero if we consider positions and momenta in the same direction ( $x$ , $y$ , or $z$ ) . at the end , we want to look for the eigenstates of the hamiltonian $$ h|\psi\rangle = e |\psi \rangle$$ and because we have $h = h_x+h_y+h_z$ , a hamiltonian composed of three commuting pieces , we may simultaneously diagonalize them i.e. look for the common eigenstates of $h_x , h_y , h_z$ , and therefore also $h$ . so given the separation condition for the potential , we may also assume $$ h_x |\psi\rangle = e_x |\psi\rangle $$ and similarly for the $y , z$ components . however , the equation above is just a 1-dimensional problem that implies that $|\psi\rangle$ must depend on $x$ as a one-dimensional quantum mechanical energy eigenstate wave function , $$ \psi ( x ) = c\cdot \psi_n ( x ) $$ which is an eigenstate of $h_x$ . this has to hold but the normalization factor is undetermined . we usually say that it is a constant but this statement only means that it is independent of $x$ . in reality , it may depend on all observables that are not $x$ such as $y , z$ . so a more accurate implication of the $h_x$ eigenstate equation is $$ \psi ( x , y , z ) = c_x ( y , z ) \cdot \psi_{n_x} ( x ) . $$ in a similar way , we may show that $$ \psi ( x , y , z ) = c_y ( x , z ) \cdot \psi_{n_y} ( y ) $$ and $$ \psi ( x , y , z ) = c_z ( x , y ) \cdot \psi_{n_z} ( z ) $$ and by combining these three formulae , we see that the whole function must factorize to a product of functions of $x$ and $y$ and $z$ separately . if you need a rigorous proof of the last simple step , take e.g. the complex logarithms of the three forms for $\psi$ above and compare e.g. the first pair : $$\ln\psi = \ln c_x ( y , z ) +\ln\psi_{n_x} ( x ) = \ln c_y ( x , z ) +\ln \psi_{n_y} ( y ) $$ take e.g. the partial derivative of the last equation with respect to $y$: $$ \frac{\partial \ln c_x ( y , z ) }{\partial y} = \frac{\partial \ln\psi_{n_y} ( y ) }{ \partial y }$$ the other two ( 1+1 ) terms are zero because they did not depend on $y$ . the right hand side above only depends on $y$ , so the same must be true for the left hand side . i am going to make a simple conclusion but to make it really transparent , let 's differentiate the latter equation over $z$ , too . the $\psi_{n_y}$ term disappears as well so we have $$\frac{\partial^2 \ln c_x ( y , z ) }{\partial y\ , \partial z} = 0$$ it means that $\ln c_x ( y , z ) $ must have the form $k_x ( y ) +l_x ( z ) $ , and $e^{k_x ( y ) }e^{l_x ( z ) }$ must be the remaining factors in the wave function . we say that the wave function in the product form is a " tensor product " of the three independent one-dimensional wave functions and more " operationally " , as another user mentioned , the method described above is the method of " separation of variables " .
the color charges are paired ( color with anticolor ) , but there is no gauge invariant meaning to the identification of the color ( rgb ) . and due to qm , the quark states are a superposition over all the colors ( and antiquarks over the anticolors ) . the wikipedia page is pretty clear : http://en.wikipedia.org/wiki/color_charge . as it notes , you also can not distinguish a color from a certain superposition of the two non-complementary anticolors ( e . g . , r from a combination of anti-g + anti-b ) .
if you know the rotational kinetic energy then cut the power and time how long it takes for the motor to come to a stop . divide the kinetic energy by this time and you have the power dissipation as friction . the difference between this and the input power is then the power dissipated though non-frictional mechanisms . whether this is related to efficiency is debatable as the motor is not doing any useful work so the efficiency is 0% . all your doing is splitting up the energy dissipation into frictional and non-frictional parts . it is still an interesting experiment though .
i can think of two reasons why we need a lower bound , one statistical , one inuititive . first , the intuitive : the annihiliation/creation operators represent adding/removing particles ( or excitations , or whatever ) . the vacuum state as lowest energy state represents obviously the empty state from which no further particles can be removed . it is also clear ( in the case of bosons ) that i can just keep adding particles and nothing stops me from doing that , so an upper bound of energy would mean that there is a state $|\text{max}\rangle$ on which the creation operator acts as $a^\dagger|\text{max}\rangle = 0$ . this is , from an intuitive point of view , obviously nonsense . why should adding a particle destroy all the others ? where has all the energy gone ? demanding a general upper bound for energies is thus a bad idea . [ i want to stress again that this is only heuristic . it is not meant to translate to a rigourous argument directly . ] now , the statistical . we know that the time evolution drives systems towards the state with the lowest ( free ) energy . if the hamiltonian is not bounded below , there is no global minimum of the energy , and thus there are only metastable states . but since there is no ground state , such a system would be able to drop to ever lower energy levels . in consequence , a system not bounded below would be able to radiate energy infinitely . that is obviously nonsense , no physical system can hold an infinite amount of accessible energy . so though the physics can describe such a system , its time evolution behaviour is not something we observe in the real world . both arguments do not prohibit that there are systems with upper bounds . the easiest example are pure spin systems : take a number of particles with spin an put them in a ( constant ) magnetic field . now the lowest energy state is when all spins are flipped one way , and the highest energy state is when all spins are flipped the other way . you cannot add further energy to this system , it is naturally bounded below and above .
if you have a smart phone with a flash , you probably have a strobe feature at your disposal . you can put a drinking straw into it and record the sound it makes , and measure the frequency in sound-editing software . imagine the fun explaining this to paramedics . similarly , a laser-pointer , a mirror , and a photo-diode cleverly hooked up to your computer 's sound card can be coaxed into doing science . finally , a laser pointer , two mirrors and knowledge of the speed of light in your room can be used to to determine the rotational speed of the fan , provided your room is big enough , and the fan is moving fast enough . if this is going to work , it will probably be the least accurate but most fun .
you are right , in this case , scalar means lorentz invariant field . but it is not invariant under the transformations of su ( 2 ) xu ( 1 ) of the electroweak model . and it is a scalar under the su ( 3 ) of qcd . so the four real components of the higgs are indeed invariant under space-time transformations . physicists are usually not very clear in these distinctions , and you have to guess under which transformation the field is a scalar .
some students learning physics for the first time mistakenly think that objects that are accelerating have force . force is not a property possessed by an object , but rather something you do to an object that results in the object accelerating ( changing its speed ) , given by the equation f = ma . that is , forces cause acceleration , not the other way around . this means that if you observe an object accelerating , then it implies a force is acting on the object to cause such an acceleration . in this case , as the object strikes the hand , your hand applies a force to the ball causing it to slow down ( decelerate ) , and the ball applies an equal and opposite force to your hand causing it to accelerate ever so slightly ( newton 's third law ) , which is detected by your sensory neurons .
$e^{i\theta} + e^{-i\theta}$ is just $2\cos \theta$ . the superposed wavefunction is $$\psi ( x , t ) = 2n\cos ( ax ) e^{i ( f ( x ) + \omega t ) }$$ then $$\psi^*\psi = 4n^2\cos^2 ( ax ) $$ the average height is $2n^2$ if $x_0a = n\pi/2$ , in which case $n = \frac{1}{2}\sqrt{1/x_0}$ . otherwise you can do this integral .
the statement that bosonic strings came first and fermionic strings came later is not exactly correct as history . fermionic strings came almost simultaneously , when ramond discovered the two dimensional super-conformal algebra in 1971 . ramond style string theories did not have space-time supersymmetry ( or rather , they did , but the gso projection which was required to extract the physical spectrum was not discovered until 1976 , and the proof that this projection actually leaves a sensible theory did not come until the green-schwarz formulation was developed in the early 1980s ) . the neveu schwarz paper analyzes bosonic oscillations of a fermionic string , and was motivated by exploring all consistent bootstraps to find something that would work for the light mesons . the problems at the time was that a bosonic tachyon was interpreted as the experimentally known instability of the pion vacuum , so it was considered essential for a good theory . the neveu-schwarz sector , without the gso projection , contains such a tachyon . now we know that this means that the theory is sick , but back then , it was considered a good sign . the ramond fermions were then interpreted as bare baryons , to be dressed with the pion condensation , and this interpretation is also wrong , since baryons have a three-quark symmetry structure . the neveu-schwarz sector was interpreted as mesons , but they also had a tachyon ( which is gso odd and vanishes ) , and nothing looks like the qcd spectrum , not with the crude tools available then . the inconsistency of ramond-neveu-schwarz strings was expressed most simply by edward witten in the early eighties : the closed string sector of a fermionic string contains massless spin-3/2 particles , so it must couple to some space-time supercurrent in order to make sense . the graviton and spin 3/2 gravitinos must make a sensible supergravity theory . the development of supergravity was initiated to a large degree by string theory , since scherk immediately began to investigate supergravities after gso . he probably understood even then that the low-energy limit of superstrings would have to be some sort of supergravity . so it is most fair to say that the development of superstrings and of bosonic strings went hand in hand , but the full perturbation series for the bosonic string was completed earlier , while a full perturbation theory of the fermionic string had to wait until the early eighties .
you have the right idea . let 's use index notation for convenience . we have \begin{align} p^i ( t ) = \int_{\mathbb r^3} d^3x \ , x^i\rho ( t , \mathbf x ) \end{align} and as you note , we have the continuity equation ; \begin{align} \partial_jj^j = -\frac{\partial\rho}{\partial t} \end{align} so we get \begin{align} \dot p^i = -\int_{\mathbb r^3} d^3 x \ , x^i\partial_j j^j \end{align} where i am using the summation convention for the dot product . this is the point to which you have basically gotten . the trick is now to essentialy perform an integration by parts . in other words , we use the product rule for differentiation to note that \begin{align} \partial_j ( x^i j^j ) and = ( \partial_j x^i ) j^j + x^i \partial_jj^j \\ and = \delta^i_jj^j +x^i\partial_jj^j \\ and = j^i + x^i\partial_jj^j \end{align} and plugging this into the above expression for $\dot p^i$ gives \begin{align} \dot p^i = -\int_{\mathbb r^3} d^3 x \partial_j ( x^ij^j ) + \int_{\mathbb r^3} d^3x j^i \end{align} the first term is the volume integral of the divergence of $x^ij^j$ . since we are integrating over all space , this turns into a boundary term at infinity which vanishes for any finite ( or sufficiently rapidly decaying ) charge density . the result you want then follows \begin{align} \dot p^i = \int_{\mathbb r^3} d^3 x j^i \end{align}
for the first question , no there can be no such general rule . the reason is the same why there is no intuitive way to understand the difference between your right and left shoe -- they are just reflections of each other but neither one is more fundamental ( assuming you are not a pirate ) . in other words , all the difference ( that really matters ) comes from the given theory . if the theory is $p$-symmetric ( such as electromagnetism ) you have no way to distinguish between these ( that is why you have not heard anyone talking about right- or left-handed electrons in the classical physics courses ; there were just electrons ) . on the other hand , if the theory violates $p$-symmetry you have a chance since one type of particles will interact differently than the other . e.g. weak interactions violate the $p$-symmetry so that only left-handed electrons and left-handed neutrinos can form a weak isospin doublet ( and so can right-handed antiparticles ) . i will leave the experimental part of the question to someone more educated on these matters . but e.g. for neutrinos you might be able to measure helicity ( which is as good as chirality since neutrinos are almost massless ) which is a projection of spin on to the momentum . you know the momentum direction ( since you know where the collision happened and where you registered the particle ) and i suppose there are standard methods for measuring spin projection to arbitrary direction ( but i admit complete ignorance on these experimental matters ) .
the azimuthal momentum $$p_{\phi}~:=~\frac{\partial l}{\partial \dot{\phi}}$$ is the ( polar ) $z$-component of the angular momentum $l_z$ of the point mass $m$ relative to the heliocentric reference frame . it is a constant of motion because the azimuthal angle $\phi$ is a cyclic coordinate .
when you snap your fingers there are multiple sound waves , but the speed of sound is so fast you can not distinguish individual waves . the frequency of sound waves is around 100hz to 10khz so each wave completes one oscillation in 0.01 to 0.0001 seconds . what you are hearing when you snap your fingers is the envelope i.e. the overall amplitude of the sound waves . when you throw a stone into still water you get an expanding ring of waves moving out , so at the centre it is still then as you move outwards you pass through the waves and beyond them the water is still again . hearing the finger snap is the same as being struck by the expanding ring of ripples .
of course it has something to do with the liquid water entering the gas phase just above the cup of tea , but how does that give the bag of tea a directed motion to one side ? nope . the teabag is dangled by a string . remember that the string is made of wound up threads : now , the threads stay wound up because they fit well and they have a knack of being permanently deformed if held in the same orientation for too long ( for example , if you loop a thread and press it for a while , the loop stays ) . wetting them undoes this permanent deformation . when you dip it in tea , however , the liquid is absorbed by the threads , and they expand . there is no longer enough room for them to stay tightly wound . besides this , they lose the permanent deformation , so they have the motive and the means to unwind -- which is what happens .
in order to determine the finite sct from its infinitesimal version , we need to solve for the integral curves of the special conformal killing field $x$ defined by \begin{align} x ( x ) = 2 ( b\cdot x ) x - x^2 b . \end{align} i explain why this is equivalent to " integrating " the infinitesimal transformation below . this means we need to solve the differential equation $x ( x ( t ) ) = \dot x ( t ) $ for the function $x$ . explicitly , this differential equation is \begin{align} \dot x = 2 ( b\cdot x ) x - x^2 b . \end{align} this can be done with a trick , namely a certain change of variables . define \begin{align} y = \frac{x}{x^2} , \end{align} then the resulting differential equation satisfied by $y$ becomes simple ; \begin{align} \dot y = -b . \end{align} i urge you to perform the algebra yourself to confirm this . it is kind of magic that it works if you ask me , and the change of variables is precisely an inversion , so i think there is something deeper going on here , but i am not sure what it is . the solution to this equation is simply $y = y_0 -tb$ , so we find that the original function $x$ satisifes \begin{align} \frac{x}{x^2} = \frac{x_0}{x_0^2} - tb . \end{align} in other words , we have turned a monstrous nonlinear system of odes into a simple algebraic equation . in fact , one can show that the algebraic eqution $x/x^2 = a$ has the solution $x = a/a^2$ , from which it follows that the solution to our original problem is \begin{align} x ( t ) = \frac{x_0 - x_0^2 ( tb ) }{1-2x_0\cdot ( tb ) + x_0^2 ( tb ) ^2} , \end{align} as desired , since this is precisely the form of the " finite " sct . note that these only are local integral curves ; the solution hits a singularity when $t$ is such that the denominator vanishes . why solve for integral curves ? if you are wondering what your original question has anything to do with solving the integral curves of the special conformal vector field i wrote down , then read on . it helps to start with the concept of a flow . transforming points via flows . let a point $x\in\mathbb r^d$ be given , then for each $b\in\mathbb r^d$ , we assume , at least in a neighborhood of that point , that there is a $\epsilon$-parameter family of transformations $\phi_b ( \epsilon ) :\mathbb r^d \to \mathbb r^d$ with $\epsilon\in [ 0 , \bar\epsilon ) $ such that $\phi_b ( \epsilon ) ( x ) $ tells you what an sct corresponding to the vector $b$ does to the point $x$ is you ``flow " in $\epsilon$ . at $\epsilon = 0$ , this flow just maps the point to itself ; \begin{align} \phi_b ( 0 ) ( x ) = x , \end{align} namely it starts at the identity . for $\epsilon &gt ; 0$ , the flow translates the point along a curve in $\mathbb r^d$ . if you change $b$ , then this corresponds to moving way from $x$ in a different initial direction under the flow . infinitesimal generator of a flow . when we talk of an infinitesimal generator of such a transformation , we are talking about the term that generates the linear approximation for the flow in the parameter $\epsilon$ . in other words , we expand \begin{align} \phi_b ( \epsilon ) ( x ) = x + \epsilon g_b ( x ) + o ( \epsilon^2 ) , \end{align} and the vector field $g_b$ is called the infinitesimal generator of the flow . what you have pointed out in your question is that we know this infinitesimal generator ; \begin{align} g_b ( x ) = 2 ( b\cdot x ) x - x^2 b , \end{align} and we now want to reconstruct the whole flow simply by knowing this information corresponding to its linear approximation at every point . this is equivalent to solving some first order ordinary differential equations , which is why people often say we want to " integrate " the infinitesimal transformation to determine the finite one ; integration is a perhaps somewhat archaic way of solving the corresponding differential equation . finding the whole flow given its generator . ok , so what differential equation do we solve ? well , note that the vector field $g_b$ is tangent to the curves generated by the flow by its very construction ( we took a derivative with respect to $\epsilon$ with is the " velocity " of the curve generated by the flow ) , so the differential equation we want to solve is \begin{align} \dot x ( \epsilon ) = g_b ( x ( \epsilon ) ) , \end{align} and we want to solve for $x ( \epsilon ) $ . the solutions to this differential equation are referred to as integral curves of the vector field $g_b$ . acknowledgements . i did not figure out the first part of this answer completely on my own . the idea for making the substitution $y=x/x^2$ , which is really the crux of everything , came from here http://www.physicsforums.com/showthread.php?t=518316 , namely from user bill_k . the idea for how to solve the algebraic equation $x/x^2 = a$ came from math . se user @hanslundmark after i posted essentially your question in mathy language on math . se here . i should another math . se user @kirill solved for the integral curves in a totally different way in his answer to the question i posted . addendum . how does one get $\dot y = -b$ from the change of variable $y=x/x^2$ as claimed in the first section ? well , let 's calculate : \begin{align} \dot y and = \frac{x^2\dot x - x ( 2x\cdot \dot x ) }{ ( x^2 ) ^2} \\ and = \frac{x^2 ( 2 ( b\cdot x ) x - x^2 b ) - x ( 2x\cdot ( 2 ( b\cdot x ) x - x^2 b ) ) }{ ( x^2 ) ^2} \\ and = \frac{2x^2 ( b\cdot x ) x - ( x^2 ) ^2b - 4x^2 ( b\cdot x ) x + 2x^2 ( b\cdot x ) x}{ ( x^2 ) ^2} \\ and = -\frac{ ( x^2 ) ^2b}{ ( x^2 ) ^2} \\ and = -b \end{align} magic !
you are working a finite number of impurities and expecting an answer in the thermodynamic limit . this should be clear if you do your calculation at a fixed density of impurities , and resum the infinite series , taking the leading term at each power of the impurity density . this will give you a correction to the self-energy . alternatively , as you suggest , you could do the born approximation for a single impurity . note that to get anything non-trivial you need to work to second order in the scattering potential . you would then need to multiply this by the density of impurities , ( or maybe something like the mean free time between scatterings , do not know off the top of my head ) to get the appropriate correction to the self energy . also note that the second born approximation knows about only a fragment of the behavior we expect from an electron in a disordered system . it know only that the momentum becomes randomized by repeated scattering . it does not know anything about even simple things like diffusion . to get that you need to look at other diagrams in the perturbation theory .
the problem comes with the cgs units of charge . one of them is the statcoulomb with $$1 \mathrm{c} \leftrightarrow 2997924580 \ , \mathrm{statc} \approx 3 \times 10^9 \mathrm{statc}$$ it is this factor of $3 \times 10^9$ which seems to be the difference between your two calculations , and needs to be taken into account in the cgs calculation . your $\frac{e}{m_ec}$ may be about $1.76\times10^{7}$ but its units are not $s^{-1}g^{-1}$ .
in the original formula we have that $$\omega_1t-k_1x = a$$say , and $$\omega_2t-k_2x = b$$say , by hypothesis for a specific point at time $t$ and position $x$ . this is a point of constant phase ( for the $a$ wave and $b$ wave respectively . ) to determine the velocity of a ( sine or cosine ) wave from first principles one wants to know the velocity of that point : how far does a point of constant phase move in time t ? this gives the answer of the phase velocity $$ v_p=\frac{\omega}{k} $$ so the component waves are moving with phase velocities : $v_{p1}$ and $v_{p2}$ respectively . using the above values for $a$ and $b$ the middle derivation is an application of the trigonometric identity : $$ sin a + sin b = 2 sin ( 1/2 ( a+b ) ) \cdot cos ( 1/2 ( a-b ) ) . $$ this gives your expression for $s = s ( x , t ) $ . so how can one talk about a point of constant phase here to obtain the group velocity as it is a product of sin and cos ? well the trick indeed is to recognise the two separate wave components and treat these ( for now ) as two separate waves and calculate their ( phase ) velocity - ie the rate of movement of points of constant phase in each " wave " . for the sine wave ie the envelope we would get $\frac{\overline{\omega}}{\overline{k}}$ . now for the cosine wave . the short answer is that we are looking for its phase velocity also , namely $\frac{\delta{\omega}}{\delta k}$ . however what your professor has done here , is to calculate from first principles the velocity of that cosine wave . that is to ask for the definition of a point of constant phase , viz : $\frac{\delta{\omega}}{2}t - \frac{\delta k}{2} x = const$ and then to determine the velocity ( by differentiation , etc ) of this point , again resulting in $$ v_g=\frac{\delta\omega}{\delta k} $$
assuming cooling is mainly by convection , the cooling will be described by newton 's law of cooling . this states that the rate of temperature change is proportional to the temperature difference , and result is that the difference between the temperature of your object and the room decays exponentially : $$ t_{room} - t_{object} = ( t_{room} - t_{0} ) e^{-kt} $$ where $t_{0}$ is the initial temperature of your object and $k$ is some constant . you can take the temperature of the object as a function of time and then fit the expression above , but the problem is that this type of fit gives rather large errors in the final temperature $t_{room}$ unless you measure for long enough that you have almost reached the final temperature . you may also find newton 's law of cooling breaks down when the temperature difference is very small , because there is no longer effective convection .
your mistake is to multiply the equation $$\sum_a \int dr \left [ ( e_a + t_\text{nuc} + v_\text{nuc-nuc} - \mathcal{e} ) \chi_a ( r ) |e_a ( r ) \rangle |r\rangle\right ] = 0$$ on the left by $\langle e_a ( r ) |$ . the left-hand side is a vector in $\mathcal{s}_\text{mol}$ , and while you can project it down to $\mathcal{s}_\text{nuc}$ by multiplying it on the left with a bra from $\mathcal{s}_\text{el}$ , the variables $r$ and $a$ are mute in the sense that they do not have a value outside the integral / summation . thus , you should multiply on the left by the ( slightly but importantly different ) bra $\langle e_{a'} ( r' ) |$: $$\langle e_{a'} ( r' ) |\sum_a \int dr \left [ ( e_a + t_\text{nuc} + v_\text{nuc-nuc} - \mathcal{e} ) \chi_a ( r ) |e_a ( r ) \rangle |r\rangle\right ] = 0 . $$ equivalently , $$ ( e_a + t_\text{nuc} + v_\text{nuc-nuc} - \mathcal{e} ) \sum_a \int dr \left [ \langle e_{a'} ( r' ) |e_a ( r ) \rangle \right ] \chi_a ( r ) |r\rangle= 0 . $$ ( note that here the dependence of $e_a$ on $r$ is eliminated by making it the operator $e_a=\int dr e_a ( r ) |r\rangle\langle r|$ on $\mathcal{s}_\text{nuc}$ . ) now , if the vectors $|e_{a} ( r ) \rangle$ and $|e_{a'} ( r' ) \rangle$ are orthogonal , i.e. if $$\langle e_{a'} ( r' ) |e_a ( r ) \rangle=\delta_{aa'} , $$ then you can eliminate both the summation and the integral to arrive at the born-oppenheimer nuclear tise . however , there is not really any reason for this to happen , since the two are electronic eigenstates for different hamiltonians . it is these off-diagonal terms that must be neglected in the born-oppenheimer approximation ; assuming they are zero is equivalent to postulating the tensor-product structure of the total eigenstates which is at the heart of the boa . in practice , of course , these are close to zero since the molecule does not stretch or bend that much and the electronic eigenstates ' structure is not altered too much . the point is then to remember that these are approximately zero .
there is hardly a book covering all physics , but for particular subjects there is some . for example : jammer : the conceptual development of quantum mechanics . whittaker : a history of the theories of aether and electricity .
faddeev has implicitly dropped a total 4-divergence term $d_{\mu} ( a_0 f^{0\mu} ) $ in the lagrangian density ${\cal l}$ . this does not affect the equations of motion , i.e. , maxwell 's equations .
your textbook is giving quantum mechanics too little credit--- qm predicts the entire probability distribution for any answer to any measurement , not just the mean and standard deviation . there are lots of probability distributions with the same mean and standard deviation , and there are fine probability distributions with infinite mean and infinite standard deviation . putting that aside , your randon number generator example is exactly what people mean by hidden variables , and for local hidden variables , these are ruled out by bell 's theorem . for nonlocal hidden variables , you can not rule them out because there is a theory , called bohmian mechanics , which reproduces quantum mechanics exactly from these , but this theory is enormous and contrived . the issue is debated ad-nauseum here , you can see the threads where ' t hooft has contributed for more debates where the issues concern the state of the art for hidden variables theories .
this is more of a psychology question . after you start seeing things , you notice that a certain side of your vision is the part that will see your finger if you touch your forehead , and the other side will see your finger if you touch your lips . we designate the first as " up " and the second as " down " . the brain just gets a bunch of signals . " up " and " down " are artificial tags we attach , where " up " is the side of our forehead and " down " is the side of our lips . besides , if one was to wear glasses that inverted vision , the mind would indeed slowly adjust .
you can think about some critical phenomena is in terms of analytic continuation and fisher zeros . as you probably know , the taylor series expansion an analytic function can only converge within a disk that does not contain singularities . however , you can find taylor expansions by ' working around ' the singularity by means of analytic continuation . fisher ( and others ) realized that the boundary between two phases is separated by a line of zeros . even if you know a thermodynamic function exactly in one part of the phase diagram , you cannot analytic continuate into another . see fig . 1 . i mention this because it sounds similar to a paper and talk i recently heard by anatoli polkovnikov , who was asking similar questions in regard to a dynamic phase transition . if that does not help , other signs to look for are : critical slowing : it takes longer and longer for dynamics to converge . this applies to simulations as well , which is a good hint ! a point where correlations are algebraic , not exponential . scaling of dynamical phenomena , such as quenches or ramps . this is another one where simulations might help out .
i supposed you are in a context of bound states , with normalized eigenfunctions $\psi_n ( x , t ) = \phi_n ( x ) e ^{ie_nt}$ . of course , if you calculate $\langle x ( t ) \rangle_{\psi_n} = \int dx \bar \psi_n x \psi_n$ , you will find a position expectation value which does not depend on time . now , this is not the general case , if you take a linear combination of the $\psi_n$ : $\psi = \sum a_n \psi_n$ , and calculate $\langle x ( t ) \rangle_{\psi} = \int dx \bar \psi x \psi$ , you will find a positition expectation value which depends on time .
in sqcd , you can get " gluino hadrons " , mesons and baryons in which the gluino is one of the constituent fermions . ( also a gluinoball , a glueball with some gluinos mixed in . ) so you could have a " gluino string " , but in the opposite sense to what you want : it is two gluinos , at the ends of a gluon string . another possibility would be a topological defect in a gluino condensate . maybe you could get this in n=2 sqcd , with monopoles at the ends . but what you are looking for is a " fermionic meson " , two quarks connected by the " gluino string " . well , you can define this as an operator , a fermionic wilson line . the only place i can find such an entity playing a role is in holographic approximations to qcd like the sakai-sugimoto model . in holographic qcd , you typically have color branes and flavor branes . a gluon is a string between color branes , a quark is a string between a color brane and a flavor brane , a meson is a string between flavor branes , and a baryon is a brane connected by strings to flavor branes ( these strings are the valence quarks ) . these strings and branes all live in ads space , and qcd lives on the boundary . also , these are usually supersymmetric models - the exact , nonsupersymmetric holographic dual to qcd has yet to be determined . in superstring theory , you do have fermionic strings , and they are extended in space ; the fermi statistics come from the fermionic fields on the worldsheet . and the meson strings in holographic qcd do have mesino superpartners . see section 3.3.2 here . they are discussing mesino operators in the worldvolume theory of a flavor brane . one is a quark-squark composite , but the other is a quark and an antiquark connected by a gluino ( the adjoint fermion ) . so we had to go into anti de sitter space to find it , but there it is , the fabled fermionic gluino string . :- )
rather than looking at one orbit of io , consider observing io and jupiter for around 200 days , starting when the earth is exactly between the sun and jupiter , and ending when the earth is opposite jupiter , with the sun in between . in the 200 days , io will make around 110 orbits of jupiter . but , importantly , the light from that last orbit of io will need to travel an extra distance equal to the diameter of the earth 's orbit around the sun , making it arrive about 1000 seconds later than expected . this would add about 9 second to the average observed orbital period for io over that 200 days . and in the next 200 days , as the earth caught up to jupiter again , the average observed period of io would be about 9 second less than expected . the earth 's orbital velocity is about 30 km/sec . during one orbit of io around jupiter , anout 1.8 days , the earth-io distance could at most increase by $\frac{30000\times86400\times 1.8}{c}$ light-seconds ; adding 15 seconds to io 's apparent period ; still an observable amount . . .
in general , the elasticity of a collision is dependent on the properties of the colliding objects . in a perfectly elastic collision , no kinetic energy is dissipated , which means the collision creates no heat , no sound , etc . in a perfectly inelastic collision , the maximum possible amount of kinetic energy is dissipated as heat , sound , etc . this corresponds to the two particles sticking together after the collision . in real life , most collisions are neither perfectly elastic nor perfectly inelastic , but rather somewhere in the middle . ( one major exception to this is gas molecule collisions , which are perfectly elastic . ) some objects collide nearly perfectly elastically , such as billiard balls or steel ball bearings , while others collide nearly perfectly inelastically , such as balls of putty or mud . without knowing the specific properties of the colliding objects ( such as their elasticity , etc . ) , it is impossible to predict how elastic a collision will be .
there is one and only way to cancel something : add its negative to itself . however , there is an alternative to cancellation for shielding a region from external electromagnetic fields . generally speaking , methods of isolating a region from external electromagnetic fields ( em shielding ) can be divided into two categories , passive and active . a passive shield prevents the external field from reaching the isolated internal region . whatever the field is outside , the field is zero inside . this is convenient if the strength of the external field is variable or unknown . faraday cages ( shields made from a mesh of conducting material ) are examples of passive shields against static ( and non-static ) electric fields . alternatively , if you know the value of the external field from which you want to isolate a region , you can generate an equal and opposite field to the external field to actively " cancel it out " . the active alternative to a faraday cage for blocking electrostatic fields is a capacitor , whose geometry is precisely shaped so that the electric between the two charged plates exactly cancels the external field in the region of interest . the magnetostatic analog for active shielding is field cancellation using solenoids with the appropriate geometry . the passive alternative for magnetostatically shielding a region ( analog to the faraday cage ) is an enclosing surface made of material ( metal alloys ) with high magnetic permeability . they do not exactly block the external magnetic field per se in the same way a faraday cage blocks an external electric field , but rather draw the field into themselves , providing a path for the magnetic field lines around shielded cavity . however , the effectiveness of passive magnetostatic shielding diminishes for very weak fields . from a practical standpoint , this usually makes active shielding by electromagnets ( solenoids , helmholtz coils , etc . ) the more useful of the two options .
i do not just mean reactions that require heat to proceed , storing surplus energy in chemical bonds . i wonder about strongly endothermic reactions that suck heat out of environment a reaction that requires heat to proceed , a reaction that sucks heat out of the environment , and an endothermic reaction are all the same . these are all just descriptions of reactions that occur because entropy ( s ) increases , despite that fact that enthalpy ( h ) also increases . $g = h -ts$ a reaction is favorable if gibbs free energy decreases . the reaction can still be favorable , despite enthalpy ( h ) increasing , if entropy ( s ) increases enough . you take some substance a ( e . g . ammonium nitrate ) , and some substance b ( e . g . water ) . . . i wonder what happens on the microscopic scale you have to separate molecules/ions/atoms of a from others of a . this could involve breaking apart ionic interactions in a crystal , or intermolecular interactions for example . this takes energy . you have to seperate molecules/ions/atoms of b from others of b . for a solvent , this would involve breaking apart intermolecular interactions such as dipole-dipole interactions . this takes energy . new interactions between a and b form . this releases energy . if the energy released in step 3 is less than the energy required for steps 1 and 2 , the process is endothermic .
making your own eyewear for direct viewing of the sun is probably not too practical . you essentially need a filter that blocks well over 99% of the incoming light . you can use #14 or #16 welder 's goggles for this , or obtain specialized filters designed for this purpose . these filters are a glass substrate with a metal deposition layer on them that is designed to transmit only a small fraction of the light . as an alternative to direct viewing , it is much easier to make a system for projection viewing of the sun . in this case , you would create a projected image of the sun on a surface , and observe the image . you can look up " solar pinhole viewer " or check out these instructions for details on how to put together a simple solar viewing system with stuff you may have around the house ( cardboard , paper and aluminum foil ) : http://www.exploratorium.edu/eclipse/how.html for more on observing the sun , see this article : http://www.skyandtelescope.com/observing/objects/sun/viewing_the_sun_safely.html
maybe that was just window frost ( http://www.its.caltech.edu/~atomic/snowcrystals/frost/frost.htm - " forms when a pane of glass is exposed to below-freezing temperatures on the outside and moist air on the inside" ) ?
here is the answer that i gathered from months of looking at these boundary conditions : ( 1 ) and ( 2 ) would mean that the slope is zero and the bending moment / curvature at the ends is zero . ( 1 ) and ( 3 ) mean that the slope is zero and the shear stress at the end is zero .
you are right . however a shorter answer is the following : $v$ and thus the whole lagrangian is invariant under rotations around the axis $z$ . noether theorem immediately implies that the $z$ component of the angular momentum is conserved .
i think the analogy with the em field is a good one with some provisos e.g. gravitational waves require quadrapolar or higher oscillation ( gravitational dipoles do not radiate ) and gravitational waves are self interacting . the electric potential round a point charge does not need continual em waves to sustain it , and likewise the curvature round the earth does not need continual gravity waves to sustain it . for for both gravity and electromagnetism waves are only required to propagate changes in the fields , not static fields . planets , or any form of mass/energy , can not spring in and out of existance any more than electric charges can spring in and out of existance , so it is not useful to use this as an analogy . however if you take any randomly shaped electric field you can decompose it as a fourier series to get the combination of em waves needed to ( momentarily ) create it . i guess the same would be true of gravitational waves , at least in the weak field approximation .
you already received several answers . however the fundamental physical reason is elementary : in classical , quantum and relativistic physics the physical laws describing an isolated physical system in an inertial reference frame are the same ( are invariant ) if you rotate ( with an element of $so ( 3 ) $ ) the system ( there are many other symmetries depending on the theory , but the point is that rotations are symmetries ) . this is a fundamental postulate of all physics , valid at small ( noncosmological ) scales at least . so , for instance a curve describing the evolution of the system in the space of states remains a curve describing ( another ) evolution of the system if we apply the same rotation to the former curve at every time . there are many other examples . you understand that , unless the space of states be the physical space isomorphic to $\mathbb r^3$ , the action of $so ( 3 ) $ on the states cannot be implemented directly using the matrices $r\in so ( 3 ) $ . instead you should faithfully " represent " the elements group $so ( 3 ) $ in terms of natural transformations of the space of states . in quantum mechanics , for many theoretical reasons ( already mentioned wigner and kadison theorems ) these natural transformations are given by unitary ( and anti unitary ) operators , as theoretical consequence of the fact that the most elementary expected preservation property of these symmetries is that of probabilty transitions of pairs of pure states . ( actually one should use projective unitary representations , but i do not think it is the case to enter into the details here . ) another intersting fact of $so ( 3 ) $ ( actually $su ( 2 ) $ ) representations is that , due to the compactness of the group , all possible unitary representations are constructed out of the irreducible ones via the standard procedure of direct sum ( this is not a trivial fact , because usually one should deal with the much more complicated tool of the direct integral ) .
for your second problem , the propagator can be written with its indices as $$ ( s_f ) _{\alpha\beta} ( x-y ) = \langle t\{\psi_{\alpha} ( x ) \bar\psi_{\beta} ( y ) \} \rangle $$ then we have $$ \langle t\{\bar\psi ( x ) \gamma\psi ( y ) \} \rangle = \gamma_{\alpha\beta} \langle t\{\bar\psi_{\alpha} ( x ) \psi_{\beta} ( y ) \} \rangle = -\gamma_{\alpha\beta} ( s_f ) _{\beta\alpha} ( x-y ) = -\mathrm{tr} [ \gamma s_f ( x-y ) ] $$
your proposed method would work as long as you only pass light through linear optical components that do not change the light 's degree of polarisaion or overall power , in which case you would be using the jones calculus in disguise : you can keep the polarised and depolarised components separate . but the method will not work in general . however , you can still use the jones matrices to represent optical components , but you apply them in a new way . partial polarisation is a very hard thing to describe classically - it is almost the same ( and as hard ) as the classical discussion of partial coherence and one needs to have a thorough grasp of random processes to discuss it fully . born and wolf give a whole chapter to these concepts . but it is highly elegantly described in the quantum picture : partially polarised light is a statistical mixture of pure quantum states . i discuss both approaches in my answer here . so now , you should first read up on the density matrix ( see wikipedia article of this name ) . the name " matrix " is a little misleading , because it is really a " state " ( albeit a mixed one ) written down as a $n\times n$ matrix ( where $n$ is the dimensionality of the quantum states you are dealing with ) and not a " transformation " or " operator " on states , as the name " matrix " would imply . it is written as a matrix because this is the most convenient way to get statistics out of it : the $n^{th}$ moment of a measurement by an observable $\hat{a}$ is computed as ${\rm tr} ( \rho \hat{a}^n ) $ where $\rho$ is the density matrix representing the mixed state . so if the light state is a classical statistical mixture of polarisation states with $2\times 1$ jones vectors $\vec{x}_1 , \vec{x}_2 , \cdots$ with the classical probabilities of each state being $p_1 , p_2 , \cdots$ , then the density matrix is : $$\rho = \sum\limits_j p_j \vec{x}_j \vec{x}_j^\dagger$$ ( note the order : $\vec{x}_j \vec{x}_j^\dagger$ is a $2\times2$ projection matrix ) . such matrices are readily seen to be hermitian ( i.e. $\rho = \rho^\dagger$ ) so our $2\times 2$ mixed quantum light state is now represented as a general $2\times2$ hermitian ( i.e. $h = h^\dagger$ ) matrix : $$\rho = \sum\limits_{j=0}^3 s_j \sigma_j$$ where $\sigma_0 = {\rm id}$ is the $2\times 2$ identity matrix and $\sigma_j$ are the pauli spin matrices . the co-efficients $s_j$ are nothing but the stokes vector . any $2\times2$ hermitian matrix can be written like this . now , if the light passes through a lossless component , so that its jones matrix $u$ is unitary $u u^\dagger = u^\dagger u = {\rm id}$ , then the density matrix becomes : $$\rho^\prime = u \rho u^\dagger = s_0 {\rm id} + \sum\limits_{j=1}^3 s_j u \sigma_j u^\dagger$$ and the length of the " polarised " part of the light $ ( s_1 , s_2 , s_3 ) $ does not change . $s_0$ on the one hand and $ ( s_1 , s_2 , s_3 ) $ on the other stay separate and do not mix . the unitary matrix scrambles the $ ( s_1 , s_2 , s_3 ) $ but leaves their sum of squares constant and indeed , if we look only at the $ ( s_1 , s_2 , s_3 ) $ we are witnessing the group $su ( 2 ) $ of unitary jones matrices acting on the three dimensional lie algebra $i\sigma_1 , \sigma_2 , \sigma_3 ) $ of $su ( 2 ) $ through the adjoint representation $so ( 3 ) $ of $su ( 2 ) $ - in everyday language we are seeing rotations of the poincaré sphere . however , if our optical component is not lossless , then the transformation $u$ is simply a general $2\times2$ hermitian matrix and the $s_0$ and $ ( s_1 , s_2 , s_3 ) $ are mixed in a more general linear transformation . you can , if you like , still use your jones matrices , but you must use them not acting on a state , but acting on the density matrix : i.e. instead of your pure state $x$ transforming like $x\mapsto u x$ , your density matrix transforms by a so called spinor map $\rho\mapsto u \rho u^\dagger$ . another way of doing this is simply to note that in the map $\rho\mapsto u \rho u^\dagger$ , the four parameters $ ( s_0 , s_1 , s_2 , s_3 ) $ defining the density matrix undergo linear transformations . so instead of spinor maps , we can use a $4\times4$ matrix to represent a general optical component . this of course is the mueller matrix . for an optical component with general , nonunitary jones matrix $u$ , the corresponding elements of the mueller matrix $m$ are : $$m_{j\ , k} = {\rm tr}\left ( \sigma_j^\dagger u \sigma_k u^\dagger\right ) $$ the mueller matrix acts on vectors in the linear space of $2\times2$ hermitian matrices thought of as a vector space over $\mathbb{r}$ . this space comes with an inner product for finding components of " vectors " the killing form $\left&lt ; a , b\right&gt ; = {\rm tr} ( a^\dagger b ) = {\rm tr} ( a b ) $ , which is how i wrote the expression above dowm . the stokes vector is simply the density matrix living in this space but written as a $4\times 1$ real valued column and the mueller matrix implements the linear spinor map on the rewritten density matrix . more generally , the mueller calculus is simply another way of calculating the transformations wrought on a density matrix for any finite dimensional quantum system by various operations , which can include unitary operators or wigner-friend kind conversion of pure states to mixed ones . every $n$ dimensional quantum system implies an $n^2 \times n^2$ dimensional mueller calculus when the density matrices are written as columns . here the " basis vectors " are the matrices $\left|\left . x_j\right . \right&gt ; \left&lt ; \left . x_k\right . \right|$ where $x_j$ are the base quantum pure states . the $n^2 \times n^2$ mueller matrix operates on the vector of co-efficients $\rho_{j , k}$ in the density matrix $\sum\limits_j\sum\limits_k \rho_{j , k}\left|\left . x_j\right . \right&gt ; \left&lt ; \left . x_k\right . \right|$ . footnote : as trimok has pointed out ( thanks trimok ) the standard numbering of the pauli matrices gives a reordering of the op 's stokes parameters : . . . with the op conventions , you have the correspondence $s_1 \to s_z , s_2 \to s_x , s_3 \to s_y$ with $\rho = s_0\sigma_0 + s_x \sigma_x +s_y \sigma_y +s_z\sigma_z$
i worked this up the chain to martin bojowald , and he sent me this response : the short answer is : both . the number of edges and vertices as well as their properties ( the geometrical excitation level ) change . details have not been analyzed much because the numbers and properties of edges and vertices can be seen only when a full , inhomogeneous graph is used . however , all the explicit models of cosmological expansion in loop quantum gravity rely on a complete reduction to homogeneity , which is so radical that it smears out all edges and vertices and their properties into just one quantum number ( or maybe three in anisotropic models ) . but we do know that the full hamiltonian generates changes of both the numbers and properties of edges and vertices . the only reference i can think of is http://arxiv.org/abs/0705.4398 ; see especially fig . 1 . basically , lqc models are framed in terms of large-scale properties of quantum geometry , so nobody knows what happens to the spin networks in very much detail .
magnetic fields are generated by moving charged particles , and exert forces on moving charged particles . the magnetic field generated by a moving charged particle can be calculated using biot-savart 's law . since a current is a steady stream of charged particles , the magnetic field created by a current-carrying wire can be calculated integrating over the length of the wire . the exact formula in classical electromagnetism is : $$\mathbf{b} =\frac{\mu_0 q \mathbf{v}}{4\pi} \times \frac{\mathbf{r}}{r^2}$$ you want to pay close attention to the two vector quantities , v and r , the velocity of the charged particle and the position in space from it , and to the fact that they are combined with a cross product , $\times$ . the resulting magnetic field , b , is therefore perpendicular to both . so a single electron moving in a straight line basically generates a magnetic field that goes around in circles around the path it is moving along . this picture may help . then you have the force acting on a moving particle when there is a magnetic field . this is known as lorentz 's force , and the equation describing it is : $$\mathbf{f} = q\ \mathbf{v} \times \mathbf{b}$$ you again have a cross product , this time involving the magnetic field , and again v , although this is a different v than before , not the velocity of the moving charged particle generating the field , but of the moving charged particle the field is acting on . because of the cross product , a particle will not have any force acting upon it if it is moving parallel to the magnetic field . that is exactly why there is no force on the closed loop in your picture : because it lays along the direction of the magnetic field . you could combine both equations into a single one , and so the force on particle 2 by particle 1 , $\mathbf{f_{21}}$ , if $\mathbf{r_{21}}$ is the position vector of particle 2 from particle 1 , would then be $$\mathbf{f_{21}} =\frac{\mu_0 q_1 q_2}{4\pi\ r^2}\mathbf{v_2} \times \mathbf{v_1} \times \mathbf{r_{21}}$$
the circuit likely was closed by his body , or by a grounding wire he was holding . at least that is one way the demonstration has been done . i assume the other end was in contact with the generator . another way is to suspend the tube so it is not in electrical contact with anything , and swing it around , so that you observe a momentary discharge when the tube is orthogonal to the field . in this case the discharge shuts itself off because there is no closed circuit path as you observe . yes . think of v as like the height of a hill for a positive charge . e wants to push the charge down the hill , in the direction of lower v . hence the minus sign .
the main difference is that the radar produces a collimated one-directional beam , going out in one particular direction , while the radio station broadcasts in all directions . the radar does this in order to scan the sky . when it sees a reflection , it knows the direction from the direction it was transmitting . in the 1940s , you would do this with a rotating parabolic reflector , which redirected the outgoing radar waves in one direction . but the modern approach is to do it without moving parts , by using multiple antennas and adjusting the phase difference between them to have constructive interference in the direction you want to scan . then you do not need mechanical motors .
i would like to add a bit of mathematical detail the ( correct ) statements by djbunk . let a scalar function $f$ be given ( let 's not restrict ourselves to the electric potential ) . for any unit vector $\mathbf n$ , we can define the directional derivative $d_\mathbf{n}$ of the function $f$ in the direction $\mathbf n$ as follows : $$ d_\mathbf{n}f ( \mathbf x ) = \mathbf n\cdot\nabla f ( x ) . $$ the directional derivative gives the rate of change of the scalar function $f$ in the direction of the unit vector $\mathbf n$ . notice that $$ \mathbf n\cdot \nabla f ( \mathbf x ) = |\nabla f ( \mathbf x ) |\cos\theta $$ where $\theta$ is the angle between $\mathbf n$ and $\nabla f ( \mathbf x ) $ , so the directional derivative is maximized when $\theta = 0$ , and is minimized when $\theta = -\pi$ . in other words ; the the rate of change of a scalar function $f$ at a point $\mathbf x$ is positive and greatest in magnitude in the direction of the gradient of $f$ at $\mathbf x$ . this confirms bjbunk 's statements .
a very natural and interesting question . if the paper stays nearly flat , we may use the linearized approximation . i will neglect the gravitational potential energy because it is probably much smaller than the bending energy ( note that the shape of the bent paper is almost the same when the desk is vertical as when it is horizontal ) . the paper does not want to bend much , so the energy contains a term that punishes the second derivatives of $y$ ( the curvature ) $$e = k \int_0^l {\rm d}x\ , ( y&#39 ; &#39 ; ) ^2$$ where $y&#39 ; \equiv dy/dx$ , and so on . we want to keep the length of the paper fixed - it is prescribed by the way how you attach it at the end points . in the linear approximation , the length is a linear function of $$d = \int_0^l {\rm d} x\ , ( y&#39 ; ) ^2 $$ which may be seen , if you need it , by a taylor expansion of the exact expression for the length of the graph , $\int ( 1+y^{\prime 2} ) ^{1/2}{\rm d}x$ . add it with a lagrange multiplier , requiring $$\delta ( e+\lambda d ) = 0 . $$ i think that the resulting equations say $$ y&#39 ; &#39 ; &#39 ; &#39 ; = \frac{\lambda}{k} y&#39 ; &#39 ; . $$ note that in the euler-lagrange equations , all the primes " clump " . if you write $y=y&#39 ; &#39 ; $ , it says that the second derivative of $y$ is proportional to $y$ itself . the physically relevant solution is $$ y = y_0 \cos ( 2\pi x n / l ) $$ where only $n=1$ is possible if there is a table underneath the paper . consequently , $$ y = y_0 [ 1-\cos ( 2\pi x / l ) ] , $$ too . i added the term $1$ as an integration constant ( while the other is zero ) to guarantee that $y=0$ and $y&#39 ; =0$ at the boundaries . so what you get in practice is one wave of a cosine , from one minimum to the next one . a more detailed discussion would be needed to eliminate the other cosine-like function with the same frequency , the sine , as well as linear functions , and to discuss whether the exponentially increasing/decreasing solutions may ever be relevant . well , it would not be too difficult . a more general solution for the same ( cosine-like ) sign of $\lambda$ would be $$ y = y_0 [ 1-\cos ( \phi_0+ 2\pi x n / l ) ] + ax^2+b , $$ and the four conditions $y=0$ , $y&#39 ; =0$ at $x=0 , l$ as well as the fixed length of the paper would imply $n=1$ , $\phi_0=0$ , $a=0$ , $b=0$ , as well as the right normalization $y_0$ . as suggested above , the conditions would have other solutions where $n=2,3,4\dots$ but these solutions would not satisfy $y\geq 0$ for $0\leq x \leq l$ , so the paper could not arrange itself above the table . if there were a hole in the table , these higher harmonics ( several periods of the wave containing ) solutions would be possible - but i guess that they would be unstable . i am not sure whether i would be able to solve it analytically if the angle $y&#39 ; $ were not infinitesimal . it seems clear that the exact solution for significant angles is not just a cosine : the bent paper tends to resemble a balloon - for which $y ( x ) $ is not even single-valued - when there is too much redundant paper in the middle . on the other hand , if you sharply bend the paper at $x=0 , l$ so that $y&#39 ; $ may be arbitrary at these two points , the paper in between will be bent as an arc of a circle - see another answer below - and this result is accurate even if $y&#39 ; $ is of order one . note that in the linearized approximation , the arc gives $y$ being a quadratic function of $x$ so that $y&#39 ; &#39 ; =0$ still solves the fourth-order equation above .
first , note that the bohr model of the atom has some shortcomings . physicists have developed more accurate models since this one . but let 's ignore them for this . from the wording of your question i believe you are curious about the nature of energy when the electron is in a uniform circular orbit . that is , no energy transitions . ( in modern language , you are restricting yourself to a single atomic energy level . ) is this correct ? if so , read on . if you apply the classical idea of a force on an object moving in a uniform circular orbit , you will find that the corresponding force does not do any work . this is because the force and velocity are perpendicular . ( if this sounds unfamiliar , look up the dot-product definition of work . ) no work , no energy change . so i think your premise is not correct : the force by the proton on the electron does not " give " energy .
john norton at pitt relates the story quite nicely . in einstein 's own words : after ten years of reflection such a principle resulted from a paradox upon which i had already hit at the age of sixteen : if i pursue a beam of light with a velocity c ( velocity of light in a vacuum ) , i should observe such a beam of light as a spatially oscillatory electromagnetic field at rest . however , there seems to be no such thing . . . on the basis of experience . . . . from the very beginning it appeared to me intuitively clear that , judged from the standpoint of such an observer , everything would have to happen according to the same laws as for an observer who , relative to the earth , was at rest . for how , otherwise , should the first observer know , that is , be able to determine , that he is in a state of fast uniform motion ? in other words , assuming both ( 1 ) that all motion is relative and ( 2 ) that it is possible for an observer to travel at $c$ leads to an impossibility : ( 3 ) that there is a reference frame in which a beam of light is just a " spatially oscillatory electromagnetic field at rest " i.e. , a motionless electromagnetic wave . since he judged ( 3 ) to be impossible , then either ( 1 ) or ( 2 ) or both must be wrong . his insight was that ( 2 ) is wrong , that the speed of light is not attainable .
yes and no . as muphrid wrote in math . se , 2d problems in " real life " are really 3d problems where one dimension has been ignored because the system has translational symmetry along it . thus a single point charge in 2d corresponds to an infinitely long line charge in three dimensions - which has indeed got a force scaling as $1/r$ . this is a very useful physical system and its understanding is aided greatly by the use of complex analysis . however , it has the very real drawback that it is not a real physical system : infinitely long rods are hard to come by in laboratory cupboards . as such , all 2d problems can only ever be approximations to a 3d system where the variations along the third dimension are over length scales much greater than the other two , and that is not an unreasonable thing to ask . the same holds for 2d fluid dynamics simulations : if an airplane wing is long , it is reasonable to model it as a 2d flow over its cross section , particularly if it will get us a good understanding of the physics . while the 2d model can never be a very exact representation of actual systems - except perhaps for very restricted wind tunnel experiments - the physical insight we get from it can very often be transported to far more complex situations .
i think i worked it out myself . what i found is that trying to arrive at the hartree-fock hamiltonian via this mean-feald procedure is not really that appropriate . instead , one says : given a two-particle operator $\hat o = c_1^\dagger c_2^\dagger c_3 c_4$ , how can i find a single-particle operator o^{mf}$ that is a good approximation ? if we have a complete set of single-particle basis states that include states $1$ to $4$ of the two-particle operator , we would then look at the basis elements of $\hat o$ and $o^{mf}$ and try to find the best possible agreement . now , the two-particle operator can do three different things : 1 . leave a state as is . this is the case if either $1 = 3$ and $2 = 4$ or $1 = 4$ and $2 = 3$ . 2 . move one particle from a previously occupied state into a previously unoccupied state , e.g. if $1 = 3$ but $2 \not = 4$ or $1 = 4$ and $2 \not= 3$ . 3 . move two particles into new states , i.e. if $1$ and $2$ are different from $3$ and $4$ . with our single particle operator , we can only hope to somehow reproduce the behavior of case 1 and 2 , but not case 3 . so , given states of the type $|\psi_0\rangle$ that can be written as $\prod_i c_{n_i}^\dagger |0\rangle$ , we look at all matrix elements of $\hat o$ between $|\psi_0\rangle$ and states of the type $c_i^\dagger c_j |\psi_0\rangle$ , $i \not= j$ , that arise from $|\psi_0\rangle$ by exchanging two particles . it is a bit tedious to work these matrix elements out in detail , but one can then indeed confirm that by chosing $$\hat o^{mf} = \langle c_1^\dagger c_4\rangle c_2^\dagger c_3 + \langle c_2^\dagger c_3\rangle c_1^\dagger c_4 - \langle c_1^\dagger c_3\rangle c_2^\dagger c_4 - \langle c_2^\dagger c_4\rangle c_1^\dagger c_3$$ the matrix elements for the two-particle operator $\hat o$ and the mean-field operator $\hat o^{mf}$ will turn out to be the same ( for the states of the type introduced above ) .
there is always a bit of elasticity in any system , even copper pipes . if you attached the pump to a balloon then closed the valve you would not be surprised to see the pressure maintained in the ballon because obviously the rubber skin of the balloon has been stretched and it is exerting a pressure on the air . your pipework is much , much , less elastic than a ballon but nevertheless when you pressurise the water in the pipes there is a bit of give , and that keeps the pressure up even after you have closed the valve . however because the amount of stretching is so small losing even a small amount of water will bring the pressure back down to zero . that is why it is so easy to spot leaks .
you only need to average over initial conditions if your initial state is mixed . since your initial state is pure this is not necessary . for each trajectory $i$ , at each point in time $t$ , you will have some specific realisation of the stochastic wavefunction represented by the ket $\lvert\psi_i ( t ) \rangle$ . if you perform $n$ trajectories in total , the density matrix at time $t$ is given by $$\rho ( t ) = \frac{1}{n}\sum_{i=1}^n \frac{\lvert\psi_i ( t ) \rangle\langle\psi_i ( t ) \rvert}{\langle\psi_i ( t ) \rvert\psi_i ( t ) \rangle} , $$ where i have included an explicit normalisation factor in the denominator .
for small velocities and displacements around a circular kepler orbit , the equations of motion are the hill-clohessy-wiltshire equations . they can be exactly solved to give ( the above switches your convention for y and z . ) your approach of finding the forces in a rotating frame works ; there is a derivation here . the motion is in general a sort of looping that is unstable . a pure-z initial velocity or a pure-y initial velocity or displacement leads to periodic solutions ( using your notation ) , but any other initial displacement or velocity leads to run-away . ( however , the run-away is linear in time , not exponential . ) i made some plots of the trajectories here .
when you initially set the eigenvalue at the top rung to $\hbar l$ , you do not need to assume that $l$ is an integer , you can think of it as any multiplicative constant . clearly there is no loss of generality there . the beautiful aspect of the ladder operator approach is that you can use it to prove that $l$ must be a non-negative integer or half-integer . this argument is presented clearly in griffiths , at least in the second edition ( perhaps you are using the first edition ? ) . using the ladder operators $l_+$ and $l_-$ , and the conditions that there must be a top rung and a bottom rung for the ladder of eigevnalues , you automatically find that the eigenvalues of $l_z$ are $m \hbar$ , where $m$ . . . goes from $-l$ to $+l$ in $n$ integer steps . in particular , it follows that $l = -l + n$ , and hence $l = n/2$ , so $l$ must be an integer or a half-integer . so , the nature of $l$ is discovered as a conclusion - there is no initial assumption .
so is it correct to say that magnetic field are ultimately caused by currents ? no , think of a magnetic field as a field that permeates all of space and time , existing independent of anything else . ( however an empty field does not do anything so changes in $\vec b$ field are what matter . ) the ( change in ) magnetic field can be created by currents but also by other stuff too . using the gauss 's law : $$ \nabla \cdot \vec b = 0 $$ we can see that the magnetic field does not have any divergences i.e. no sources or sinks ( mono-poles ) . you can imagine it as an incompressible fluid ( like a tank of water , the water can move but you can not create high density water or vacuum volumes ) . so we have established there are no divergences however there can be curls in the field , i.e. the field can flow , but since there are no divergences these flows must be closed loops . the equation you describe , ampère 's circuital law ( with maxwell 's correction ) , is given by : $$ \nabla \times \vec b = \mu \vec j + \epsilon \mu \frac {\partial \vec e}{\partial t} $$ this states that there are two ways to create a curl in the $\vec b$ field . the first is with a current density i.e. movement of charge which necessarily requires electric charges . so yes ( curls in ) magnetic fields can be created by electric charges . however there is a second part which states that curls in magnetic fields can also be created by $\vec e$ fields changing in time . this usually requires a charge particle but it can also be caused by a changing magnetic field . this is how light propagates , changing $\vec b$ creates a changing $\vec e$ field which creates a changing $\vec b$ etc . if magnetic mono-poles exist then they can be used to create the first changing $\vec e$ field . which eliminates the need for charges ( well they are magnetic charges ) .
what you are looking for is usually called a window , not a filter . you need to check different manufacturers to choose a material that fits your requirements , including not only light transmission but also the pressure differential , chemical inertness , temperature range , etc . checking ars ' website , calcium fluoride ( caf2 ) might be what you are looking for .
not necessarily . consider this function as an example : $$\psi ( x ) = \frac{c\sin x^2}{\sqrt{x^2 + 1}}$$ this function is square-integrable and asymptotes to zero as $x\to\pm\infty$ , but its derivative goes to $2\cos x^2$ in the same limit . in quantum mechanics , we often assume that real systems are represented by wavefunctions which have no interesting features once you get far enough away from the origin . in practice , this means that the function and all its derivatives have " compact support:" $$\lim_{x\to\pm\infty}\frac{\partial^n\psi}{\partial x^n} = 0\ \forall\ n\in\mathbb{z}_{0 , +}$$ this mathematical statement corresponds to the physical assumption that you can ignore anything happening far enough away from your experiment . there are situations where it is useful to drop this assumption , though . for example , if you are analyzing a crystal lattice , it makes the calculations easier to assume the lattice extends infinitely far in all directions , and in that case you would use a wavefunction which is periodic all the way out to infinity . of course , such wavefunctions usually have nonzero values in addition to nonzero derivatives at large $|x|$ . i do not know of an example offhand which would use a wavefunction which asymptotes to zero but whose derivatives do not , though it would not surprise me at all to learn of one .
using gradshteyn and ryzhik ( seventh edition ) 3.876 ( 1 ) $$\int_0^\infty \frac{\sin{ ( p \sqrt{x^2+a^2} ) }}{\sqrt{x^2+a^2}} \cos ( b~x ) dx=\frac{\pi}{2} j_0 ( a\sqrt{p^2-b^2} ) ~~ [ 0&lt ; b&lt ; p ] \\ = 0~~ [ 0&lt ; p&lt ; b ] $$ differential of both sides with respect to $b$ will give the integral you want to calculate .
this calculation agrees with experimentally measured spectral lines , but why would we expect it to be true , even if we accept that the electron moves according to the schrodinger equation ? after all , there is no particular reason for an electron to be in an eigenstate . good question ! the function $\psi$ does not need to be hamiltonian eigenfunction . whatever the initial $\psi$ and whatever the method used to find future $\psi ( t ) $ , the time-dependent schroedinger equation $$ \partial_t \psi = \frac{1}{i\hbar}\hat{h}\psi $$ implies that the atom will radiate em waves with spectrum sharply peaked at the frequencies given by the famous formula $$ \omega_{mn} = \frac{e_m-e_n}{\hbar} , $$ where $e_m$ are eigenvalues of the hamiltonian $\hat{h}$ of the atom . here is why . the radiation frequency is given by the frequency of oscillation of the expected average electric moment of the atom $$ \boldsymbol{\mu} ( t ) = \int\psi^* ( \mathbf r , t ) q\mathbf r\psi ( \mathbf r , t ) d^3\mathbf r $$ the time evolution of $\psi ( \mathbf r , t ) $ is determined by the hamiltonian $\hat{h}$ . the most simple way to find approximate value of $\boldsymbol{\mu} ( t ) $ is to expand $\psi$ into eigenfunctions of $\hat{h}$ which depend on time as $e^{-i\frac{e_n t}{\hbar}}$ . there will be many terms . some are products of an eigenfunction with itself and contribution of these vanishes . some are products of two different eigenfunctions . these latter terms depend on time as $e^{-i\frac{e_n-e_m}{\hbar}}$ and make $\boldsymbol{\mu}$ oscillate at the frequency $ ( e_m-e_n ) /\hbar$ . schroedinger explained the ritz combination principle this way , without any quantum jumps or discrete allowed states ; $\psi$ changes continuously in time . imperfection of this theory is that the function oscillates indefinitely and is not damped down ; in other words , this theory does not account for spontaneous emission .
the attraction is that between a charge and an induced dipole . if the charged object is a sphere , the field is $qe\over 4\pi r^2$ , in units where $\epsilon_0=1$ . this field is what induces the dipole and causes the attraction . once you get a dipole moment in the dust , the dust dipole is attracted to regions of stronger field with a force that goes like the derivative of the e field . the total dipole is neutral , so you get net opposite positive and negative forces which only fail to cancel out to the extent that the e field is stronger in the regions of induced positive charge . the magnitude of the force is ( in the nearly perfect small-dust approximation ) $ d \cdot \nabla e$ , the dipole moment can be thought of as defined by this equation ( although that is not the definition--- the dipole moment vector d is the sum of qx over all the little infinitesimal regions in the dust , where q is the charge of the region , which sums to zero over the dust particle , and x is the position ) . so if the dipole moment is of fixed size , the force is $2qd\over r^3$ . but the dipole itself is proportional to e , with a coefficient that i will call " p " for polarizability , so you get a force which is $$ {2 q \over 4\pi r^3} { pq\over 4\pi r^2} = {2pq^2\over 16\pi^2 r^5}$$ and this is the force between a sphere and a dipole ( in units where $k={1\over 4\pi\epsilon_0} = 1$ ) . you see it falls off as $1/r^5$ , two powers of r for the induced dipole strength , proportional to the e field , and another three powers from the force , which is as the gradient of e . to calculate p requires knowing something about the material , namely it is dielectric polarizability constant for dc electric fields . this is radically different for different materials , depending on whether the molecules are polar themselves , and how mobile they are , whether it is liquid or solid . in general , you can figure this out from the extent to which the electric field in the interior of the dust is reduced from the exterior , assuming a bulk block of dust material . this is the dielectric constant $\epsilon$ of the dust . for a solid block of dust in a constant electric field e perpendicular to the plane surface of the dust-block , the electric field in the interior of the dust block is $e\over \epsilon$ where $\epsilon$ for dc fields is always bigger than 1 . for a metal , the field in the interior is zero , and this corresponds to infinite $\epsilon$ . i will calculate the polarization constant in an order of magnitude for metal dust , but for the largely nonpolar typical carbon-chain dust material , the actual answer will range from 1%-10% of the metal answer . for the metal , there is a nice useful trick for getting the induced dipole moment magnitude , which is the method of images/conformal maps . a dipole at the origin is conformally equivalent by an inversion around a sphere of certain radius to a constant electric field at infinity , so if you place a dipole at the origin , and the equal electric field inverted , the result makes the sphere have the same potential . the electric potential of a dipole is by differentiating the electric potential of a point charge : $$ \phi = {dz \over 4\pi r^3}$$ the potential of constant z-direction electric field is $$ \phi = ez$$ adding the two , the surface at potential zero ( only a zero potential stays an equipotential under inversion ) is a sphere : $$ ez - {dz\over 4\pi r^3} = 0$$ this is solved by a sphere ( away from the z=0 plane which is an accidental equipotential by symmetry ) $$ d = 4\pi e r^3 $$ and so the induced dipole moment is proportional to the cube of the sphere size . the coefficient p is $4\pi a^3$ for a metal spherical dust of radius a . so the answer for an ideal resistive metal dipole ( perfectly screening object , zero response time to adjust to a new field , these are good assumptions for this situation ) is $$ f = {q^2 a^3 \over 4\pi \epsilon_0 r^5} $$ where i restored $\epsilon_0$ in the final answer . for a non-metal resistive dipole , the induced moment is less by the facor $ ( 1-{1\over \epsilon} ) $ . for hydrocarbons , the dielectric constant is about 2 , so you get 1/2 the force . note than when r is of order a , the maximum force goes up as the object gets small .
the quadratic case is much more involved , but for a simple linear term describing drag the height of a projectile shot vertically is given by $m \ddot{y} + \frac{g}{v_{t}}\dot{y}+g=0$ for graviational acceleration $g$ , teminal velocity $v_t$ and mass $m$ . subject to the boundary conditions $y ( t=0 ) $ and $\dot{y} ( t=0 ) =v_0$ this differential equation has the solution $y = \frac{m v_t}{g} ( v_0 + v_t ) ( 1-\exp ( {-\frac{g t}{m v_t}} ) ) -v_t t$ the particle reaches the apex at $\dot{y}=0$ , which after solving for $t$ is given by $t_{max}= \frac{m v_t}{g} \log ( 1+\frac{v_0}{v_t} ) $ thus we find at time $t=2 t_{max}$ that $y ( t=2 t_{max} ) = \frac{m v}{g} [ v_0 ( v_0+2 v_t ) -2 v_t ( v_0+v_t ) \log ( 1+\frac{v_0}{v_t} ) ] $ if we find that at this time that $y$ is negative , we conclude the particle fell to ground from the apex quicker than it climbed to it . in term of the ratio $r=\frac{v_0}{v_t}$ this condition is $\log ( 1+r ) &gt ; \frac{r ( r+2 ) }{2 ( r+1 ) }$ this is never satisfied and so we conclude that your intuition was correct . in fact these two quantities reach equality only for $r=0$ which is the pathological case of no motion . i hope this walkthrough is able to offer you insight on how to tackle the more general problem .
wolfram 's early work on cellular automata ( cas ) has been useful in some didactical ways . the 1d cas defined by wolfram can be seen as minimalistic models for systems with many degrees of freedom and a thermodynamic limit . insofar these cas are based on a mixing discrete local dynamics , deterministic chaos results . apart from these didactical achievements , wolfram 's work on cas has not resulted in anything tangible . this statement can be extended to a much broader group of cas , and even holds for lattice gas automata ( lgas ) , dedicated cas for hydrodynamic simulations . lgas have never delivered on their initial promise of providing a method to simulate turbulence . a derivative system ( lattice boltzmann - not a ca - has some applications in flow simulation ) . it is against this background that nks was released with much fanfare . not surprisingly , reception by the scientific community has been negative . the book contains no new results ( the result that the ' rule 110 ca ' is turing complete was proven years earlier by wolfram 's research assistant matthew cook ) , and has had zero impact on other fields of physics . i recently saw a pile of nks copies for sale for less than $ 10 in my local half price books store .
this is somewhat controversial issue . but let me present the reasons , as far as i understood , why people like sir penrose thinks so . their arguments are roughly as follows : 1 ) the basic microscopic laws of physics are perfectly time symmetric . they are not biased in any time direction past or future . 2 ) second law follows from the fact that that given an initial condition of a system which is not in the most probable state will tend to go towards the most probable state by the same microscopic laws . since number of disordered states are much higher the system will become more and more disordered with time . accordingly its entropy will increase until a maximum value when the system comes to the thermal equilibrium . 3 ) since the microscopic laws are time symmetric the same argument can be made towards the past time direction as well . given an an initial condition of a system which is not in the most probable state should go towards more disordered ( high entropy ) states towards past as well . that is what the mathematics of the laws tells us . 4 ) this is against our experience . either all the parts of the universe we are observing ( including our memories of past ) has just undergone a huge fluctuation right now to give the impression that there was a more ordered past ( which is crazy ) or the system was already even more ordered ( low entropy ) and more special in the past . but that means even more huge a fluctuation . this reasoning will lead us to conclude that at the moment of big bang the universe was extra ordinarily ordered and most special . it should be so special that it requires explanation . critics often point out that prediction and retrodiction is not the same thing forgetting that when one talks about the very " arrow of time " no one can say with justification which is prediction and which is retrodiction . other than that it is also questionable whether second law can be applied this way to the whole universe or not .
the remaining 5% could in a normal , air-operated concentrator be `anything else ' that was present in the air . looking at this example and explanation , anything that ends up in the outlet stream is what was not absorbed . then the question remains : does absorption occur equally for all air components ( except $o_2$ of course ) or does it differ ? i can give you some numbers ( see also the presentation i referenced above ) , but the absorption depends strongly on the type of zeolite . for example this $aga$ zeolite has a 1.63:1 $ar$ selectivity and a 5:1 $n_2$ selectivity with respect to $o_2$ . whereas the $liagx$ zeolite has only 1.1:1 $ar$ selectivity . in this article they mention that they can get 95% $o_2$ with the remaining 5% being almost completely $ar$ . however , they use a contaminant free inlet mixture of $o_2$ , $n_2$ and $ar$ . the most interesting for you is probably this article . they study how $h_2o$ and $co_2$ affect the operation of a zeolite oxygen concentrator . what they show is that there is not going to be any $co_2$ in the outlet stream , but instead $co_2$ adsorbes so strongly on the zeolite that it will degrade its overall efficiency . in summary , to answer your question , species like $co_2$ and $no_2$ will deactivate your zeolite , but this is due to excessive adsorption so this means that the outlet flow will only contain $o_2$ and $ar$ and sometimes a small amount of $n_2$ .
as lurscher mentioned in a comment , you are using the wrong units for magnetic susceptibility . $\chi$ is actually a dimensionless number that is related to the magnetic permeability of a material relative to that of a vacuum . i think you were mixing it up with the molar magnetic susceptibility , which is $\chi_\text{mol} = \mathcal{m}\chi/\rho$ , where $\mathcal{m}$ is the molar mass of the substance ( units of $\mathrm{kg/mol}$ ) and $\rho$ is the density ( units of $\mathrm{kg/m^3}$ ) . $\chi_\text{mol}$ is the thing with units of $\mathrm{m^3/mol}$ , but it is $\chi$ that actually appears in the magnetic levitation formula . with that cleared up , let 's look at the equation . the left side , naturally , has units of $\mathrm{t^2/m}$ . if you include the magnetic constant on the right side , as wikipedia ( correctly ) does , you have $$\biggl [ \mu_0\frac{ \rho g }{\chi}\biggr ] = [ \mu_0 ] \frac{ [ \rho ] [ g ] }{ [ \chi ] } = \biggl ( \frac{\mathrm{t\ , m}}{\mathrm{a}}\biggr ) \frac{\mathrm{ ( kg/m^3 ) ( m/s^2 ) }}{1} = \frac{\mathrm{t\ , kg}}{\mathrm{m\ , s^2\ , a}}$$ here i am using the notation where putting brackets around a quantity designates the units of that quantity . for example , the units of the magnetic constant are $\mathrm{t\ , m/a}$ , so $ [ \mu_0 ] = \mathrm{t\ , m/a}$ . now you can equate the units of the two sides of the equation : $$\frac{\mathrm{t^2}}{\mathrm{m}} = \frac{\mathrm{t\ , kg}}{\mathrm{m\ , s^2\ , a}}$$ which simplifies to $$\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$$ so if this equivalence is correct , then it shows that the original equation is dimensionally consistent . and if you look on the wikipedia page for the tesla , it does indeed give $\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$ as one of the definitions of that unit . alternatively , you could check it using a formula involving magnetic field and current , such as $\vec{f} = i\mathrm{d}\vec{l}\times\vec{b}$ . the units of this are $\mathrm{n = a\ , m\times t}$ , and since $\mathrm{n} = \mathrm{kg\ , m/s^2}$ , you can set $\mathrm{kg\ , m/s^2 = a\ , m\ , t}$ and find exactly that $\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$ . this is a useful trick that keeps you from having to memorize the definitions of all the si ( or other ) units .
the reflector focuses the light from the bulb which is a point source to a straight line .
there is no significance in the choice between upper- and lower-case $\psi$ ( or $\psi$ ) to denote a system 's wavefunction . the two are used interchangeably and it is the author 's discretion to use either symbol . ( on the other hand , of course , one should not use the two symbols interchangeably within the same text ; if both are used they would refer to different objects . ) there is also little to say , in general , about whether the position $x$ is included as an argument to the wavefunction or not . some authors may , for example , choose to denote by $\psi ( t ) \in\mathcal h$ the hilbert space state vector , which in the position representation is given by the function $x\mapsto\psi ( x , t ) $ , but this is relatively rare . in general this is a case-by-case matter , but most of the modern literature uses dirac notation , in which the state vector $|\psi\rangle\in\mathcal h$ and its wavefunction $\langle x|\psi\rangle=\psi ( x , t ) \in\mathbb c$ are distinct objects , shown emphatically distinct by the notation . finally , these uses of notation are usually a source of confusion when the wavefunction is split into spatial and temporal parts , such as when setting up a separation of variables scheme for solving the tdse via the tise . in such cases , one postulates that the wavefunction $\psi ( x , t ) $ of a particle takes the form $$\psi ( x , t ) =\psi ( x ) e^{-i e t/\hbar} , $$ in which the temporal and spatial dependences are separated ; here if $\psi$ satisfies the tise then $\psi$ will satisfy the tdse . in such situations it is important to keep in mind that $\psi$ , whilst a convenient calculational tool , does not represent the state of the system : $\psi$ does . beware of notation in such situations , and always check what definition each symbol has been given in the particular text you are reading .
i am on record of having the opinion that there is no real argument against us being a simulation in a general sense , however we frequently find people jumping to quick into the simulation pool and stating there new what-ever-it-is proves the universe is a simulation . the example given above sounds like one of them . first off , quantum error correcting code ( qecc ) are mathematical approaches to allow for stable transfer of quantum information by correcting for decoherence effects . if some version of qecc is apparent in any formulation of quantum mechanics , it is interesting but probably not very meaningful in proving we exists in some sort of emulation . second , just because it shows up in one theory , unless that particular version is shown to have the ability to predict physical effects then it is hard to make the claim about its relevance . whether these things are testable is a matter of debate . however , there are people who are proposing to look for " glitches " in the universe . some would hypothesize that if we lived in a simulation based on lattice quantum chromodynamics ( lqcd ) we should be able to find places where the lattice work becomes apparent . this is clearly far-fetched but who am i to judge ? for the world of warcraft , although i do not play that game , the first evidence of a simulation is along the same lines as the theory to test for lqcd latticework . the pixelation of the characters would be the first indication of potential emulation . the universe as we know it has a continuous spacetime ( versus discrete ) , so any sign of blockiness is a good indicator . generally , anything that is an inconsistency with basic laws of physics ( e . g . perpetual motion , decreasing entropy , etc ) would be the first indicator something was wrong . now , in wow one can assume that they might operate under a slightly different set of physics than our real world . so ultimately inconsistencies in some portion of the world relative to the laws of physics would be a flag . something you should look into is the equivalence principle . in a nutshell it is a statement that the laws of physics should be the same regardless of you location in spacetime . it is very critical to our notion of the world around us , but a similar rule should be applicable wow , and significant inconsistencies would be cause for exploration .
if you think of the two parallel glass sides as canceling each other out you are pretty close to it . the first impact ( low to high indices ) does in fact disperse frequencies if the light is coming in at an angle , but the exits ( high to low ) mostly cancels that effect . there actually can be some small residual effects leading to small colored fringes . the prism works better because the oppositely angled sides enhance rather than cancel the dispersion effects . here are the results of some further analysis and experimentation . part of the answer for why color effects are so hard to find when light passes through flat glass plates appears to be in the eye of the beholder . . . literally ! here 's the scoop : angled light entering a flat plate should at first fan out its angles by color while within the plate . by symmetry , however , those slightly fanned out rays of colored light return to their original paths when they reach the second surface an re-emerge . so , the new rays will show essentially no difference in direction from their paths in the original beam , but will no be spaced very slightly apart from each other in rainbow order . for a typical plate of glass this separation would seldom be more than a millimeter , and for most glasses would be a lot less than that . now picture a point of light on one side of the glass and a human eye on the other side . arrange both so that the line between then is at a sharp angle to the surface of the glass . let 's look at the ray going from the point to the center of you pupil . your eye focuses that parallel white light as a single point on the retina , as expected . but when the angled glass plate is inserted , the same ray of light gets spread out along a tiny distance , usually much less than a millimeter . however , each colored ray in this bundle remains parallel to the original path . this little sub-millimeter bundle then enters the pupil of the eye , carrying pretty much the same light as before , all traveling in parallel . what does your eye do with it ? it forms the same white colored point image as before , since the light is all traveling in parallel . think for example of red and blue light entering opposite sides of a magnifying glass : both will end up near the center . there will be some chromatic aberration , sure , but it turns out that vertebrate eyes are very , very good at eliminating that form of chromatic aberration at the image level . the bottom line becomes this : as long as the plate is not too thick , the physical separation of chromatic components will fall within the size of the human eye pupil , and the image will appear to be white - color free and pretty much just like the original , with just a bit more blurring . that also leads to an experimental prediction that i have not yet tried : if you hold a pinhole in front of your eye while observing a pinhole light on the other side of an angled piece of glass , you may be able to see a short colored line instead of a white dot . i can not guarantee it , but it is likely enough that it would be interesting to try . now to the final part of the analysis : what if the glass is so hugely thick that there is no way the separated components can be captured by the human eye all at once ? should not that lead to some visible color effects , such as blue and red fringes on either side of a point of white light ? specifically , for a white dot or light , a blue fringe should appear towards the side angled away from the viewer , and a red fringe on the side of the glass that is closer to the viewer . for a black line on a light background this would be reversed , with the red on the glass-is-farther edge of the black line ( since that is the nearer edge of the lighter part ) , and blue on the glass-is-nearer edge of the black line . ( you can work out why that is with simple dispersion diagrams . ) but since the space-form chromatic separation effect is going to be small even for a quite thick piece of glass , where can you find something thick enough to show such fringes ? fish lovers have conveniently provided a solution : they are called aquariums ! the combination of glass and water makes a quite good approximation of a very thick piece of glass with decent chromatic dispersion . but does it really work ? if like me you do not currently have an aquarium , here is a convenient online image of a see-through aquarium that is angled sharply away on the right . on the other side of the tank are both vertical bright lights from curtain folds ( near the right side ) , and vertical dark lines from a picture frame ( on the left ) . if you magnify the image , you will see blue fringes on the right sides of the curtain folds . neither effect is intense , but both are definitely in this image . if you do happen to have an aquarium , you should of course try it for yourself , since good direct experiment always trumps theory if they disagree ! do not look directly into a light , since modern led lights are very bright and should never be gazed at directly . instead , place a thin vertical stripe of white paper on a black background and illuminate that with a bright light pointing away from the observer . you can also try holding a small pinhole in aluminum foil in front of you eye to enhance any color fringing effect you may see . and with that . . . i think i will give this one a rest . further discussion , especially actual results from experimenters with real aquariums , would be great though !
not only is the constant nature of the speed of light guaranteed by theory , it is also shown experimentally . in fact , as you may know , it was the experimental discovery that the speed of light is constant irrespective of the ( inertial ) frame of reference which formed the inspiration for the development of special relativity by albert einstein . mathematically , purely from the relativistic formula for velocity-addition it can be seen that the light would still travel at a speed $c$ in a vacuum . indeed , say the astronaut has a speed of $-v$ with respect to the ground . he observes the light leaving his flashlight with a speed of $u = +c$ ( with respect to him ) . $^1$ the relativistic theory then tells us the light is travelling at a speed $s$ with respect to the ground , given by $$\begin{align} s and = \frac{ ( -v ) + ( +c ) }{1+\frac{ ( -v ) ( +c ) }{c^2}} \\ \\ and = \frac{c-v}{1-\frac{v}{c}} \\ \\ and = \frac{c-v}{\frac{c-v}{c}} \\ \\ and = \frac{c-v}{c-v}c \\ \\ s and = c . \end{align}$$ you can walk at whatever speed you like , you will never get a different result . except if you insert $-c$ instead of $-v$ , then the answer is undefined - however , you do still get $c$ if you calculate it using limits . $^1$ why can we say this ? well , remember that in the reference frame of the astronaut , he himself is not moving at all and he would expect the light to leave his flashlight at a speed of $c$ ( w . r.t. him ) . this becomes clearer when you replace the flashlight with a small cannon and the light with a little ball . suppose you can set the exit speed for this ball out of the cannon . say you set it at $v$ . when you walk around carrying this small cannon , you expect the ball to still leave the cannon at a speed $v$ with respect to you when you fire it . classically , if you’re walking at a speed $u$ with respect to the ground , the ball will leave the cannon at a speed $u+v$ with respect to the ground . relativistically , the only thing that is invalid in all the above is this simple summation of speeds . you need the formula i used in the main text of this post , which reduces to the simple summation if both $u$ and $v$ are much smaller than $c$ . so it’s perfectly alright to say that the light leaves the astronaut’s flashlight at a speed $c$ w . r.t. the astronaut , even without knowing about the constancy of the speed of light in a vacuum .
this is just a the statement that if i define functions $f_i$ of $n$ real variables $\mathbf x = ( x_1 , \dots , x_n ) $ as \begin{align} f_i ( \mathbf x ) = x_i \end{align} then \begin{align} \frac{\partial f_i}{\partial x_j} ( \mathbf x ) = \delta_i^j \end{align} the expression you wrote is just a notational shorthand for this result . why is this equation true ? well , the expression on the left hand side is zero if $i\neq j$ , and one if $i=j$ . on the other hand , $\delta^i_j$ is the kronecker delta which is defined to be zero if $i\neq j$ and one if $i=j$ , therefore the left and right hand sides match . how does this relate to your expression for the kronecker delta that involves the metric ? well , the object $g^{\mu\sigma}$ is defined to be the components of the inverse of the metric having components $g_{\sigma\nu}$ , the mathematical expression for this definition is precisely that $g^{\mu\sigma} g_{\sigma\nu} = \delta^\mu_\nu$ .
answer for questions $1$ , $2$ , and $3a$ 1 ) looking at $2.7.22$ to $2.7.24$ ( and also $2.7.18abc$ ) , one define the ghost number $n^g = \frac{-1}{2\pi i} \int_0^{2 \pi}dz :b ( z ) c ( z ) :$ , and that all the operators $c_n$ increase the ghost number by one . $ [ n^g , c_m ] = c_m$ , so the field $c ( z ) $ , made of operators $c_m$ , increase the ghost number by one unit . so $c^\alpha$ " has " ghost number +1 . now , $b_a$ and $f^a$ do not change the nature of ghost states , so their ghost number is zero . the total ghost number of the ( faddeev-popov ) action $s_3$ is zero , so $b_a$ and $c^\alpha$ must have opposite ghost number , so $b_a$ has ghost number $-1$ finally , looking for instance , at equation $4.2.6c$ , the ghost number must be the same for the two sides of the equation , so the ghost number of $\epsilon$ is $-1$ ( you can check with the other equations $4.2.6x$ that it works ) 2 ) we have $s_2 =-ib_a f^a ( \phi ) $ . so $\delta_bs_2 = -ib_a\delta_b f^a ( \phi ) = -ib_a ( \partial^i f^a ) \delta_b \phi_i = -ib_a ( \partial^i f^a ) ( -i \epsilon c^{\alpha} \delta_{\alpha} \phi_i ) $ $=-\epsilon b_ac^\alpha ( \partial^i f^a ) \delta_{\alpha} \phi_i = -\epsilon b_ac^\alpha \delta_\alpha f^a$ ( note that $\epsilon$ commute with the $b_a , f^a$ ) the variation of $s_3$ due to $b_a$ is : $\delta_b s_3 = \delta_b ( b_a ) c^{\alpha} \delta_{\alpha} f^a ( \phi ) = \epsilon b_ac^\alpha \delta_\alpha f^a$ so the variation of $s_2$ cancels the variation of $b_a$ in $s_3$ [ edit ] 3 ) a ) the variation of $s_3$ relatively to $c^\alpha$ is : $b_a ( \delta_b c^\alpha ) ( \delta_\alpha f^a ) = b_a\frac{i}{2} \epsilon f^\alpha_{\beta\gamma}c^\beta c^\gamma ( \delta_\alpha f^a ) = b_a\frac{i}{2}\epsilon~ c^\beta c^\gamma [ \partial_\beta , \partial_\gamma ] ( f^a ) =b_a i\epsilon~c^\beta c^\gamma \partial_\beta \partial_\gamma ( f^a ) =0$ we have used $4.2.1$ , and the fact that $c^\beta c^\gamma = - c^\gamma c^\beta $
you are correct that you can not really use a gaussian of length 10 meters . instead , consider using a very short gaussian cylinder near your area of interest . in this limit , you should still be able to approximate the electric field as " nice and symmetric " with respect to the area vectors $d\vec{a}$ . by the way , the same thing is done for finite sheets of charge . if you use a small enough gaussian surface that is located near the center of the sheet , the procedure and results become the same as that due to an infinite sheet . the only difference is the limited region of applicability .
i agree that the definition " particles are a small oscillation around a vacuum " is not correct , it is some attempt to apply semi-classical reasoning via the path integral to qft . however , it does not mean anything for a field to be " physical , " fields are not physical . they are a computational tool that allow us to ensure that amplitudes we write down are local and causal . therefore , zero-vev cannot be a physical principle , nor can it be derived from one . the physical question is : does the vacuum of the theory respect the symmetry in question . if the answer is no , the physical consequence is that there exist massless excitations . to see an example of this , consider the s-matrix field redefinition theorem . lsz reduction basically tells you that it does not matter which field operator you use to do calculations , as long as the two fields have overlap on the 1- particle states and you compensate your choice with an appropriate normalization factor and put external lines on-shell ( this is explained properly in weinberg vol 1 ) . the definition of the s-matrix involves a specification of asymptotic states . specifically , you assume that the asymptotic states are 1 particle states of some free hamiltonian ( usually assumed to be the hamiltonian you are perturbing about , although this assumption fails for theories like qcd in the ir ) . in particular , it assumes a particular vacuum state from which particle excitations may be built using creation operators associated with the free hamiltonian . so while it does not matter which field you use to calculate correlators , in order for lsz reduction to work you have to put the external states on shell . since you understand how ssb works , you know that you can show in many different ways that there must be a massless particle in the spectrum . so the point is this : it is not that the field with no vev is any more " physical " than the regular field , this is a meaningless statement . but if you are intending to read off the masses of external states from the tree level lagrangian , you will be doing the wrong calculation unless some of the fields in the lagrangian are massless . if you have assumed that ssb has occurred and the vacuum is no longer a singlet , then you need to be using m=0 when you compute amplitudes for the goldstone mode scattering . and you need the other feynman rules to be consistent with this feynman rule . so you make the expansion you refer to so that you may derive feynman rules that consistently treat the external state as massless and consistently describe the other interactions . if you do not do this , you are doing the wrong calculation , using asymptotic states that do not actually exist in your theory . hope this helps !
actually , " unitary representation " is meant with respect to the spinors , which do not form a finite-dimensional space and therefore allow a unitary representation of the proper lorentz group . the action is defined by $d ( \lambda ) \psi ( x ) =u ( \lambda ) \psi ( \lambda^{-1}x ) $ , and you can simply calculate that this is unitary on your spinor space . however , this does not ( ! ) mean that the matrix $u$ is actually unitary . therefore i also assume , that you mean $u^{-1}$ instead of $u^\dagger$ . to your problem : just notice that $u$ is actually a matrix constant with respect to $x'$ , therefore it commutes with $\partial'_\mu$ , and therefore you have $ ( iu^{-1}\gamma^\mu u\partial'_\mu-m ) = ( iu^{-1}\gamma^\mu \partial'_\mu u-u^{-1} mu ) =u^{-1} ( i\gamma^\mu \partial'_\mu-m ) u$ .
the particles of light waves - the photons - have the rest mass $m_0$ equal to zero . however , at the speed of light , $v=c$ , the total mass $$ m= \frac{m_0}{\sqrt{1-v^2/c^2}} $$ is increased to an indeterminate form , $0/0$ , which should be evaluated as a finite number . the photons - and everything else - carry the total mass that is proportional to the total energy via the famous $e=mc^2$ relation . yes , this mass may be measured . for example , uranium nuclear power plants burn the uranium and reduce its mass by 0.1 percent or so because the waste products ( the nuclei ) are actually a little bit lighter . this energy may be completely transformed to the radiation coming from light bulbs - and the light from these light bulbs carry 0.1 percent of the uranium mass away . this mass is a source of gravitational field and adds inertia to boxes with this light etc . sound is different . the speed of sound is much smaller than the speed of light . while " phonons " in low-temperature condensed matter physics - particles of sound - are analogous to photons in many respects , and $e=mc^2$ still applies , the same is not true for sound waves in the air etc . because the temperature of the air is nonzero , the " ground state " - the lowest-energy state at fixed conditions , with the minimum number of " sound quanta " or " phonons " - is not really unique . instead , there are many states of the air " without any sound " which correspond to chaotic configurations of the air molecules . so one can not consistently divide the energy of the air to the energy of its ground state and the energy of the phonons . but of course , if you produce some loud sounds , they will carry lots of energy in the air and the mass of the air will inevitably increase by $m=e/c^2$ which is , well , not too high because $c^2$ is a large number .
the rutherford model of the atom did not respect any quantization : it was a classical planetary model . the bohr-sommerfeld model had the quantization of the allowed orbit from your first picture ; however , you conflated these two models and spoke about " rutherford-bohr " model which has never existed . the third thing that you conflated is the actual quantum mechanical equation that describes the atom correctly - in the non-relativistic limit - while neither the rutherford model nor the bohr model are correct in details . the states $1s , 2s , 2p , 3s , 3p , 3d , \dots$ that you refer to on your second and third picture only exist in the correct quantum mechanical model that predicts three quantum numbers for the electron , $n , l , m$ ( if we ignore the spin ) . the bohr model only predicts ( incorrectly ) one quantum number $n$ , so it would only have states $n=1,2,3,4$ and no extra $s , p , d$ labels that distinguish different values of $l$ . in some sense , the bohr model has angular momentum $l=n$ and it does not allow any values $l&lt ; n$ while the correct quantum mechanical models only allows $l&lt ; n$ but all of them - it predicts $l=0,1,2 , \dots n-1$ . so you should recognize the different models . the rutherford model is classical and hopeless - and only included the insight that the nuclei are smaller than the atoms . it did not know anything correct about the motion of the electron . the bohr-sommerfeld model knew something " qualitative " about the motion of the electron , namely that there was something quantized about it , but it was still too classical and it was the wrong model that only happened to " predict " the right energies after some adjustments but this agreement for the hydrogen atom was completely coincidental and related to the fact that the hydrogen atom may be solved exactly ( and the answer for the allowed energies is very simple ) . so the answer how you can see $1s , 2s , 2p , \dots$ states in the bohr ( or even rutherford ) model is obviously that you can not see them because the bohr and rutherford models are invalid models of these detailed features of the atom . if you decided to learn quantum mechanics and abandoned the naive ideas such as the rutherford and bohr-sommerfeld models , you could also discuss other properties of the electron states in the hydrogen atom . for example , the states $2px , 2py , 2pz$ from your first picture are particular complex linear combinations of the usual basis of states $2p_{m=-1} , 2p_{m=+1} , 2p_{m=0}$ . in fact , $2pz=2p_{m=0}$ while $2px\pm i\cdot 2py = 2p_{m=\pm 1}$ , up to normalization factors .
you need an equation for the density of the gas as a function of temperature and pressure . assuming the tyre is full of air , this is reasonably close to an ideal gas so the molar volume is given by : $$v_m = \frac{rt}{p}$$ where r is the ideal gas constant and and the average molecule weight of air ( 20% oxygen , 80% nitrogen ) is about 14.4 . from this you can work out the density at the pressure and temperature of your car tyre , and since you know the volume this immediately gives you the mass .
the first formula indeed follows from the second formula if we let $\omega\to0$ . to see that , expand the fractions as $$ \frac1{\pm\hbar\omega + e^a - e^b} = \frac1{e^a-e^b}\left ( 1 \mp \frac{\hbar\omega}{e^a-e^b}\right ) + \mathcal o ( \omega^2 ) $$ to obtain $\sigma_{xy} = \sigma^1 + \sigma^2$ as the sum of a potentially divergent term $$ \sigma^1 = \frac{-ie^2}{v\omega} \sum_{a , b} f ( e^a ) \frac{\langle a|v_x|b \rangle \langle b|v_y|a \rangle + \langle a|v_y|b \rangle \langle b|v_x|a \rangle}{e^a - e^b} $$ and a term that looks like the first formula $$ \sigma^2 = \frac{-ie^2\hbar}{v} \sum_{a , b} f ( e^a ) \frac{- \langle a|v_x|b \rangle \langle b|v_y|a \rangle + \langle a|v_y|b \rangle \langle b|v_x|a \rangle}{ ( e^a - e^b ) ^2} . $$ to see that the first term vanishes instead of diverging , we have to use the heisenberg equation of motion $v_x = \frac{d}{dt}x = [ h_0 , x ] $ which gives $$ \langle a | v_x | b \rangle = \langle a | h_0 x - x h_0 | b \rangle = ( e^a-e^b ) \langle a | x | b \rangle $$ and thus $$ \langle a|v_x|b \rangle \langle b|v_y|a \rangle + \langle a|v_y|b \rangle \langle b|v_x|a \rangle = ( e^a-e^b ) ( \langle a|x|b \rangle \langle b|v_y|a \rangle - \langle a|v_y|b \rangle \langle b|x|a \rangle ) . $$ the factors $ ( e^b-e^b ) $ cancel and the remaining sum over $b$ becomes a sum over the identity $\sum_b |b\rangle\langle b| = 1$ . thus , we arrive at $$ \sigma^1 = \frac{-ie^2}{v\omega} \sum_{a , b} f ( e^a ) \left ( \langle a|xv_y - v_yx |a \rangle \right ) = 0 . $$ since the commutator $ [ x , v_y ] $ vanishes . to see that the second term is correct , we have to get the summation indices right . to do that , we have to rearrange the summation to obtain $$ \sigma^2 = \frac{ie^2\hbar}{v} \sum_{a , b} ( f ( e^a ) -f ( e^b ) ) \frac{\langle a|v_x|b \rangle \langle b|v_y|a \rangle}{ ( e^a - e^b ) ^2} . $$ in the limit $t\to0$ , the difference of fermi-dirac distributions $f ( e^a ) -f ( e^b ) $ will be equal to $1$ if $e^a &lt ; e_f &lt ; e^b$ $-1$ if $e^b &lt ; e_f &lt ; e^a$ $0$ otherwise using this and rearranging the summation again gives the kubo formula in the first form .
before answering the question more or less directly , i would like to point out that this is a good question that provides an object lesson and opens a foray into the topics of singular integral equations , analytic continuation and dispersion relations . here are some references of these more advanced topics : muskhelishvili , singular integral equations ; courant and hilbert , methods of mathematical physics , vol i , ch 3 ; dispersion theory in high energy physics , queen and violini ; eden et . al . , the analytic s-matrix . there is also a condensed discussion of `invariant functions ' in schweber , an intro to relativistic qft ch13d . the quick answer is that , for $m^2 \in\mathbb{r}$ , there is no " shortcut . " one must choose a path around the singularities in the denominator . the appropriate choice is governed by the boundary conditions of the problem at hand . the $+i\epsilon$ " trick " ( it is not a " trick" ) simply encodes the boundary conditions relevant for causal propagation of particles and antiparticles in field theory . we briefly study the analytic form of $g ( x-y ; m ) $ to demonstrate some of these features . note , first , that for real values of $p^2$ , the singularity in the denominator of the integrand signals the presence of ( a ) branch point ( s ) . in fact , [ huang , quantum field theory : from operators to path integrals , p29 ] the feynman propagator for the scalar field ( your equation ) may be explicitly evaluated : \begin{align} g ( x-y ; m ) and = \lim_{\epsilon \to 0} \frac{1}{ ( 2 \pi ) ^4} \int d^4p \ , \frac{e^{-ip\cdot ( x-y ) }}{p^2 - m^2 + i\epsilon} \nonumber \\ and = \left \{ \begin{matrix} -\frac{1}{4 \pi} \delta ( s ) + \frac{m}{8 \pi \sqrt{s}} h_1^{ ( 1 ) } ( m \sqrt{s} ) and \textrm{ if }\ , s \geq 0 \\ -\frac{i m}{ 4 \pi^2 \sqrt{-s}} k_1 ( m \sqrt{-s} ) and \textrm{if }\ , s &lt ; 0 . \end{matrix} \right . \end{align} where $s= ( x-y ) ^2$ . the first-order hankel function of the first kind $h^{ ( 1 ) }_1$ has a logarithmic branch point at $x=0$ ; so does the modified bessel function of the second kind , $k_1$ . ( look at the small $x$ behavior of these functions to see this . ) a branch point indicates that the cauchy-riemann conditions have broken down at $x=0$ ( or $z=x+iy=0$ ) . and the fact that these singularities are logarithmic is an indication that we have an endpoint singularity [ eg . eden et . al . , ch 2.1 ] . ( to see this , consider $m=0$ , then the integrand , $p^{-2}$ , has a zero at the lower limit of integration in $dp^2$ . ) coming back to the question of boundary conditions , there is a good discussion in sakurai , advanced quantum mechanics , ch4.4 [ nb : " east coast " metric ] . you can see that for large values of $s&gt ; 0$ from the above expression that we have an outgoing wave from the asymptotic form of the hankel function . connecting it back to the original references i cited above , the $+i\epsilon$ form is a version of the plemelj formula [ muskhelishvili ] . and the expression for the propagator is a type of cauchy integral [ musk . ; eden et . al . ] . and this notions lead quickly to the topics i mentioned above -- certainly a rich landscape for research .
for the first question , you normally do not have a " real " short circuit , but a very low load ( say a milliohm ) . in this case there is a very low ( non measurable with normal instruments ) difference of potential and this will cause the current to flow through your load . the amount of current flowing in the load will depend on the internal resistance of the generator . if the internal resistance is $r_i$ and the load resistance is $r_l$ , with a open circuit potential difference of $v$ , the current will be $i=\frac{v}{r_i + r_l}$ . if you had a proper zero resistance load ( a real short circuit ) , the difference of potential across the load would be exactly zero but , being the resistance also zero , you would not need a potential difference to support a current ( think of electrons flowing frictionless in your load , so you do not need to provide energy to allow the electrons to win the friction , therefore current without difference of potential ) . in this case the current would be $i=\frac{v}{r_i}$ . this should help understanding the second question as well . if your load is constituted by two resistors in series ( say of resistances $r_1$ and $r_2$ ) , the total current in the circuit would be $i=\frac{v}{r_i + r_1 + r_2}$ , the difference of potential across the load would be $v_l=i ( r_1+r_2 ) $ and the drops across the resistors would be $v_1=i r_1$ and $v_2=i r_2$ .
1 ) motors orient the camera independently of the satellite . the satellite has to be oriented so its big dish antenna is pointed toward earth . 2 ) yes they know the position , and they pre-program the photographs , because instructions take hours to get there . 3 ) is there a gravitational sensor ? not for pointing cameras . 4 ) for orientation , they have inertial ( gyroscope ) sensors and visual trackers of known stars , like canopus . check out voyager spacecraft .
conservation laws typically hold only for closed systems . in this case the inner drum is losing mass so it can not be considered a closed system ; instead , you must also include the angular momentum of the outer drum . on the other hand , you are right that there is no torque on the inner drum , and that therefore its angular momentum - that of the drum itself , ignoring the sand - is also constant . together , these two constraints are enough to determine the two final velocities .
it is true that at the speed of sound , you will have a huge amount of drag . the reason is that the air in front of you has to move out of the way , and if you are moving at the speed of sound , the pressure wave that pushes the air out of the way is moving at exactly the same speed as you . so in the continuum mechanics limit , you can not push the air out of the way , and you might as well be plowing into a brick wall . but we do not live in a continuum mechanics universe , we live in a world made of atoms , and the atoms in a gas bounce off your airplane . at the speed of sound , you get a large finite push-back which is a barrier , and above this , you still have to do the work to push a mass of air out of the way equal to your plane 's cross section with ballistic particles . as you go faster , the amount of drag decreases , since the atomic collisions do not lead to a pile-up on the nose-cone . but if you look at wikipedia 's plot here , the maximum drag at the supersonic transition is only a factor of 2 or 3 higher than the drag at higher supersonic speed , so it is possible to travel at mach 1 , it is just not very fuel efficient .
vector spaces because we need superposition . tensor product because this is how one combines smaller systems to obtain a bigger system when the systems are represented by vector space . hermitation operator because this allows for the possibility of having discrete-valued observables . hilbert space because we need scalar products to get probability amplitudes . complex numbers because we need interference ( look up double slit experiment ) . the dimension of the vector space corresponds to the size of the phase space , so to speak . spin of an electron can be either up or down and these are all the possibilities there are , therefore the dimension is 2 . if you have $k$ electrons then each of them can be up or down and consequently the phase space is $2^k$-dimensional ( this relates to the fact that the space of the total system is obtained as a tensor product of the subsystems ) . if one is instead dealing with particle with position that can be any $x \in \mathbb r^3$ then the vector space must be infinite-dimensional to encode all the independent possibilities . edit concerning hermitation operators and eigenvalues . this is actually where the term quantum comes from : classically all observables are commutative functions on the phase space , so there is no way to get purely discrete energy levels ( i.e. . with gaps in-between the neighboring values ) that are required to produce e.g. atomic absorption/emission lines . to get this kind of behavior , some kind of generalization of observable is required and it turns out that representing the energy levels of a system with a spectrum of an operator is the right way to do it . this also falls in neatly with rest of the story , e.g. the heisenberg 's uncertainty principle more or less forces one to have non-commutative observables and for this again operator algebra is required . this procedure of replacing commutative algebra of classical continuous functions with the non-commutative algebra of quantum operators is called quantization . [ note that even on quantum level operators can still have continuous spectrum , which is e.g. required for an operator representing position . so the word " quantum " does not really imply that everything is discrete . it just refers the fact that the quantum theory is able to incorportate this possibility . ]
the field strength is the force on a unit charge , so the field strength at the surface of sphere 1 is : $$ f_1 = \frac{1}{4\pi\epsilon_0} \frac{q_1 . 1}{r_1^2} $$ and the field strength at the surface of the second sphere is : $$ f_2 = \frac{1}{4\pi\epsilon_0} \frac{q_2 . 1}{r_2^2} $$ lets take the ratio $f_1/f_2$ to see which is greater . the constants cancel to give us : $$ \frac{f_1}{f_2} = \frac{\frac{q_1}{r_1^2}}{\frac{q_2}{r_2^2}} $$ and i am going to rewrite this slightly to make it obvious how you use your equality $q_1/r_1 = q_2/r_2$: $$ \frac{f_1}{f_2} = \frac{\frac{1}{r_1}\frac{q_1}{r_1}}{\frac{1}{r_2}\frac{q_2}{r_2}} $$ because $q_1/r_1 = q_2/r_2$ we can cancel them on the top and bottom of the fraction and we are left with : $$ \frac{f_1}{f_2} = \frac{r_2}{r_1} $$ and because $r_2 &lt ; r_1$ this means the field strength at the surface of sphere 2 is greater than at the surface of sphere 1 .
however , they must somehow occupy space , as i have read that light waves can collide with one another . that is not true . yes , light waves can " collide " and interact with each other ( rarely ) , but that itself does not imply that they need to occupy space . it is not even entirely clear what it means for a subatomic particle to occupy space . a particle like a photon is a disturbance in a quantum field , and is " spread out " across space in a sense ; it does not have a definite size in the same sense that a macroscopic material object does . but you will probably agree that , if it is possible to make any sensible definition of " occupying space " for a subatomic particle , it should involve preventing other things from also occupying that same space . photons do not do that . they are bosons , and as a consequence of that they are not subject to the pauli exclusion principle , so if you have a photon occupying some space ( whatever that may mean ) , you can in theory pack an unlimited number of additional photons into the same space .
the sentence in peskin 's and schroeder 's book that " the weak interactions preserve cp and t " is a bit misleading but there is a sense in which it is right . experimentally , cp and t is known to be violated and cpt is always a symmetry . theoretically , cpt is always a symmetry , too – it is proven by the cpt theorem . the cpt transformation is effectively a rotation by $\pi$ in the $t_ez$ plane in the euclideanized spacetime which is a symmetry due to the lorentz symmetry and analyticity of the theory ( the charge conjugation is automatically added because by sending the particles backwards in time , they become antiparticles , so the geometric operation is physically interpreted as cpt and not just pt ) . theoretically , we know that cp and t may be violated . note that because cpt is always a symmetry , a theory is symmetric under cp if and only if it is symmetric under t . there are various potential physical phenomena that violate cp and t – including the " theta angle " of qcd ( which would mean that the strong force also breaks cp and t ) – but the only experimentally observed source of cp violation is the " complex phase in the ckm matrix " . the ckm matrix is a unitary transformation that transforms the upper-type quark mass eigenstates to the $su ( 2 ) _w$ upper partners of the down-type quark mass eigenstates . all the quark masses are generated by the higgs mechanism – from the yukawa couplings and the vev – and all the higgs-related things may be incorporated into the " weak interaction " . but at least up to some extent , the known breaking of the cp occurs due to mass terms which are not " interactions at all " ( they are quadratic terms in the lagrangian while interactions have to be higher-order ) . in this sense , the cp and t violation is not caused by the " weak interaction " only – only by a subtle combination of the weak interaction and subtleties in the mass matrices that would not matter separately if there were no interactions . at any rate , the cp and t are known to be violated much more " unambiguously " than the formulation in the peskin-schroeder textbook seems to suggest . the breaking of this symmetry is there ; its effects are just a few orders of magnitude weaker ( and less qualitative ) than the effects of the breaking of c and p . the latter symmetry violations are " immediately obvious " when we consider the weak force – for example because the observed neutrinos are always left-handed ( and antineutrinos are right-handed ) .
we know that we can describe a spin $1/2$ massless particle using only a single weyl field ( lets say left-handed $\psi_{l}$ ) . to introduce a mass term we have to use two spinor fields ( one left-handed and one right-handed ) and this gives the dirac mass term . the question is now that if we can describe a massive particle with a single weyl field . well yes , due to the fact that given a left-handed weyl spinor , it is possible to construct a right-handed spinor $\psi_{r}=i\sigma^{2}\psi_{l}^{*}$ . thus , we can write the dirac equation using $i\sigma^{2}\psi_{l}^{*}$ $$\hspace{43mm} \bar{\sigma}^{\mu}i\partial_{\mu}\psi_{l}=im\sigma^{2}\psi_{l}^{*} \hspace{30mm} ( 1 ) $$ the known algebraic methods performed for the dirac equation to prove that it implies a massive klein-gordon equation can be performed here without any problems . thus , the above equation implies $ ( \box+m^{2} ) \psi_{l}=0$ . here we have constructed a mass term using only $\psi_{l}$ and this is known as majorana mass . the similarity with the dirac mass can be seen by writting $ ( 1 ) $ in terms of the four component majorana spinor $\psi_{m}$ in the chiral representation $$\psi_{m}=\begin{pmatrix} \psi_{l}\\i\sigma^{2}\psi_{l}^{*} \end{pmatrix}$$ now , equation $ ( 1 ) $ becomes $$ ( i\gamma_{\mu}\partial^{\mu}-m ) \psi_{m}=0$$ the majorana mass has a very important physical difference when compared to the dirac mass . we know that the dirac action with a mass term is invariant under a global $u ( 1 ) $ transformations of $\psi_{l}$ and $\psi_{r}$ ( i.e. . $\psi_{l}\rightarrow e^{i\alpha}\psi_{l} , \hspace{2mm}\psi_{r}\rightarrow e^{i\alpha}\psi_{r}$ ) . but for majorana spinors , $\psi_{l}$ and $\psi_{r}$ are not independent , they are related by complex conjugation . so , if $\psi_{l}$ transforms as $\psi_{l}\rightarrow e^{i\alpha}\psi_{l}$then $\psi_{r}$ transforms like $\psi_{r}\rightarrow e^{-i\alpha}\psi_{r}$ . the majorana equation $ ( 1 ) $ is not invariant under global $u ( 1 ) $ symmetries . this fact implies that a spin $1/2$ particle with a $u ( 1 ) $ conserved charge cannot have a majorana mass . all spin $1/2$ particles with an electric charge cannot have a majorana mass . also leptons that have a majorana mass violate the lepton number ( because this is a $u ( 1 ) $ symmetry ) . one possible particle that could have a majorana mass is the neutrino . but this is yet to be determined . ( i did not answer your questions point by point but i hope this clarifies some of them ) .
the premise of the question is not correct , but there is a general shape to rivers . from leopold and langbein , writing in scientific american : a sample of 50 typical meanders on many different rivers and streams has yielded an average value for this ratio of ahout 4.7 to one . the ratio they use in that article is different from the definition you ( and wikipedia , and comments ) are using . i am sure it is simple algebra to convert one ratio to the other . but that article also notes that : for the large majority of meandering rivcrs the value of this ratio ranges between 1.3 to one and four to one . whatever the conversion comes out to , there appears to be quite a range of real-life meander ratios . that scientific american article is a summary of a longer , more technical article by the same authors . the method they use is to fix beginning and end points a and b , and allow the river to random walk from a to b . the most probable shape for such a path is what they call a " sine-generated " curve . at a given point , the angle between the tangent to the river and the mean direction of the river the sine of the distance along the channel . the resulting curve is not quite a semi-circular curve , so the meander ratio is not predicted to be $\pi$ . a more recent study by garret williams confirms leopold and langbein 's results , and reports that the most common value for the ratio of the radius of curvature to the channel width is between 2 and 3 . the other major effect driving river shape is how easily the river can erode the soil that it passes through . the river will tend to flow more directly downhill if the surrounding soil is difficult to erode . in areas where the soils erodes quite easily , the river will assume this sine-generated curvature . as an example of that , you can look at the mississippi river in the united states . it has the classic sine-generated shape all the way from about cairo , il south to new orleans , la . but it is much straighter up along the illinois/iowa border .
according to the laws of quantum mechanics ( you might have heard of the uncertainty principle ) if you place a particle in a given position , e.g. near the corner , you are doing so by measuring its position with great precission . this means that you loose all precission about its velocity , therefore in a given instant after you placed the particle it could actually be anywhere within the box , not necessarily the center . there will be a certain probability density that it will be found in any given place of the box . if you add a second particle , then things depend of the type of particles you are dealing with ( recall all particles are either fermions or bosons ) . if both particles are distinct ( no matter if they are bosons or fermions ) then they can " occupy the same point in space " without problems , the same happens if they are bosons ( whether of the same type or not ) , but if you placed two identical fermions in the box there will be some kind of effective repulsion between them ( coming from pauli 's exclusion principle ) , so in some sense there are some " rings " preventing the particles to get too close to each other ( i.e. . of having the same quantum state ) , this is the force that makes everyday matter seem solid . second question : if you place an electron and a proton , chances are they will eventually bind into a hydrogen atom , radiating a photon in the process . the rotation sense only has a meaning if you define it with respect to some axis in quantum mechanics , say the $z$ axis , in such case the angular momentum of the system is conserved , so the final angular momentum depends of the initial angular momentum of the particles before they bind ( if they bind ) . pairs of particle with the same charge will not bind in the conditions of the experiment . photons will not bind under any known circumstances , so far as i know . third and fourth questions : in a real life experiment with reactive walls things will complicate , in principle there is a chance the particles can escape of the box by tunneling effect ( this also depends very strongly of the energy the particles have ) , but most importantly it is almost certain that they will interact with the walls and eventually they will not be the only particles in the box , there will be " parts of the box " inside . chances are it will be very difficult to even be sure that you are observing the same particle in the conditions of the experiment , unless more constrained conditions are inforced on the particles ( as a light-trap for cooling )
yes it is certainly possible to construct a universal clock . if you are at rest with respect to the average matter in the universe ( basically this means being at rest with respect to the cosmic microwave background ) , and not in a gravitational field , then you are a comoving observer . every comoving observer will agree on the time since the big bang ( 13.798 $\pm$ 0.037 billion years ) , and they will all agree on the rate that time passes i.e. the length of the second ( defined using your caesium standard ) . so we can use the number of seconds since the big bang as a universal clock that applies everywhere in the universe . this is not a particularly practical clock , since we have little hope of every knowing the age of the universe to an accuracy comparable to one second . still , in principle it could be done . if you get in a spaceship and fly around the universe then your local clock will get out of sync with the universal clocks due to relativistic time dilation , but you can in principle calculate the loss of sync and correct your clock as you go . likewise if you are in a gravitational field your local clock will run slower than the universal clock , but again this can in principle be corrected .
no , this is not possible . it is only possible for $\mathbf{e}$ to be proportional to $\mathrm{r}^{-2}$ .
the universe today is believed to be dominated by dark energy . in fact , it is believed that the dark energy may take the form of a cosmological constant , in which case its energy density has been a constant throughout the history of the universe . radiation and matter are also present today and have been present for most of the history of the universe . however , as the universe expands , matter and radiation are diluted ; their densities decrease at later times . if you know about the scale-factor of the universe , $a$ , we can say that $\rho_{\gamma}\propto a^{-4}$ , $\rho_{m}\propto a^{-3}$ , and $\rho_{\lambda} = constant$ , where $\rho_{\gamma}$ is the density of radiation , $\rho_{m}$ is the density of matter , $\rho_{\lambda}$ is the density of the cosmological constant . tracing back from out dark-energy dominated universe , we get to a matter dominated universe , and then further back still , we get radiation domination . however , it is also believed that the very early universe went through a period of accelerated expansion called inflation . here , the universe expands as if it were dark-energy dominated ; its volume increases rapidly and it is supercooled . this process does not last long . also , when inflation comes to an end , the energy is used to " reheat " the universe .
what you should be comparing is the time it takes for direct propagation ( which i would guess is the " energy transmitted without total internal reflection" ) versus the time it takes for guided propagation at the critical angle , which is the longest delay/broadening you will get out of the fibre at the other end . modes at angles higher than $\theta_c$ will leak energy into the substrate and will not make it to the other end , so you do not need to consider them . your error is in the calculation of the times each beam travels . for each length $l$ that the direct beam travels , the critical-angle beam travels a length $l'$ given by $$ \frac l{l'}=\sin ( \theta_c ) . $$ thus , if the direct beam travels a total length $d$ , the critical-angle beam will travel a length $d'=\frac{d}{\sin ( \theta_c ) }&gt ; d$ . since they are both travelling in the same medium , the real index of refraction is the same , and hence their travel times are $$ t_\text{direct}=\frac{d}{v}=\frac {dn_f}{ c}\text{ and } t_\text{c . a . }=\frac{d'}{v}=\frac{dn_f}{c}\frac1{\sin ( \theta_c ) } . \tag0 $$ the critical angle will be given by the total internal reflection limit at the boundary with either the substrate or the cover , whichever has a larger index of refraction . assuming wlog that $n_s&gt ; n_f&gt ; n_c$ , the critical angle is given by $\sin ( \theta_c ) =n_s/n_f$ . this means that the time delay is $$ \delta t=t_\text{c . a . }-t_\text{direct}=\frac{dn_f}{c}\left ( \frac{n_f}{n_s}-1\right ) =d\frac{n_f}{n_s}\frac{n_f-n_s}{c} . \tag1 $$ the inverse of this is the bandwidth of the fibre , given by $$ \frac{d}{\delta t}=\frac{n_s}{n_f}\frac{c}{n_f-n_s} . \tag2 $$ this is pretty close to the result you were asked for , $\frac{c}{n_f-n_s}$ . for one , it has a factor of $d$ , which is eliminated in your result , effectively , by calculating the ' bandwidth per unit length ' of the fibre , $1\text{ km}/\delta t$ , in the understanding that the actual bandwidth will vary inversely with the actual length . this makes a lot of sense : longer fibres make for longer distances travelled by the different beams and therefore longer delays . this is to be expected and should be factored out . other than that , some of the prefactors do not quite match up . for one , i must note that one of the equalities that you write as exact is not really so : $$ \frac{{2c{n_s}}}{{{{\left ( {{\rm{an}}} \right ) }^2}}}=\frac{2cn_s}{n_f^2-n_s^2}=\frac{2n_s}{n_f+n_s}\frac{c}{n_f-n_s} , $$ and this only equals $\cfrac{c}{n_f-n_s}$ in the limit where $n_f$ and $n_s$ are really quite close together . similarly , in that limit , $\cfrac{n_s}{n_f}\approx 1$ , so in that sense all three answers match . a bit further along those lines , the factor of $\cfrac{2n_s}{n_f+n_s}=\cfrac{2 n_s/n_f}{1+\frac{n_s}{n_f}}$ from the $1/\rm{an}^2$ answer sits kind of " in between " the exact answer , $n_s/n_f$ , so it is not so bad an approximation . i would therefore sum up the situation as saying that $$ \frac{d}{\delta t} =\frac{n_s}{n_f}\frac{c}{n_f-n_s} \approx \frac{{2c{n_s}}}{{{{\left ( {{\rm{an}}} \right ) }^2}}} =\frac{2n_s}{n_f+n_s}\frac{c}{n_f-n_s} \approx \frac{c}{n_f-n_s} , $$ where each approximation accumulates a slight loss of accuracy , from left to right , though of course everything tends to equality as $n_s/n_f\to1^-$ . thus , if it is convenient for some reason to include the numerical aperture in the formula for the bandwidth , then it makes some sense to put it in the picture .
i will start with the second one . $\int\phi^\ast\psi\ , \mathrm{d}x$ is , as chris says in the comments , the scalar ( or dot ) product of $\phi$ and $\psi$ . in the dirac notation , it is written as $\langle\phi|\psi\rangle$ and it gives the overlap of the two wavefunctions . in other words , it gives the probability amplitude ( i.e. . , what you call square root of probability ) that starting from $|\psi\rangle$ we will measure the state to be $|\phi\rangle$ . the first integral works in pretty much the same way ; it is just the scalar product of $|\phi\rangle$ with $\hat{a}|\psi\rangle$ . this means that you first apply $\hat{a}$ to the state $|\psi\rangle$ and then measure its overlap with $|\phi\rangle$ . but this is not the same as calculating the amplitude probability of measuring $|\phi\rangle$ after the measurement of $\hat{a}$ is performed on $|\psi\rangle$ . if you measured $\hat{a}$ , that would be described by a set of projectors $\hat{\pi}_i = |\chi_i\rangle\langle\chi_i|$ , where $|\chi_i\rangle$ is an eigenvector of $\hat{a}$ . the state $|\psi\rangle$ collapses to $|\chi_i\rangle$ with probability $p_i = |\langle\chi_i|\psi\rangle|^2$ which then collapses to $|\phi\rangle$ with probability $q_i = |\langle\phi|\chi_i\rangle|^2$ . the overall probability is then $p = \sum_i p_i q_i$ , which is not the same as $|\langle\phi|\hat{a}|\psi\rangle|^2$ , as you can see if you write $\hat{a}|\psi\rangle = \sum_i \chi_i|\chi_i\rangle\langle\chi_i|\psi\rangle$ , with $\chi_i$ being the eigenvalues of $\hat{a}$ . edit : the interpretation of $\hat{a}|\psi\rangle$ can be divided into several cases : if $\hat{a}$ is unitary , $\hat{a}|\psi\rangle$ can be understood as a time evolution of $|\psi\rangle$ governed by a hamiltonian $\hat{h}$ fulfilling $\hat{a} = \exp ( i\hat{h}t ) $ . if $\hat{a}$ is not unitary , but is trace-decreasing ( i.e. . , $\langle\psi|\hat{a}^\dagger\hat{a}|\psi\rangle \le \langle\psi|\psi\rangle$ for each $|\psi\rangle$ ) it can describe some probabilistic evolution $|\psi\rangle\to|\varphi_i\rangle$ with probability $p_i\le 1$ . the general case cannot be , i believe , interpreted generally . it requires renormalization ( as does case 2 ) but this time the norm cannot be interpreted as the probability . thus , it depends on the specific operator used . as daaxix pointed out in the comments , the expression $\langle\phi|\hat{a}|\psi\rangle$ can be understood as a form of generalized mean value . assuming $\hat{a}$ is hermitian and writing $|\psi\rangle = \sum_k c_k|\chi_k\rangle$ , $|\phi\rangle = \sum_k d_k|\chi_k\rangle$ , we have $$ \langle\phi|\hat{a}|\psi\rangle = \sum_{k , l} c_k d_l^\ast \langle\chi_l|\hat{a}|\chi_k\rangle . $$ due to orthogonality of $|\chi_k\rangle$ , we can write $\langle\chi_l|\hat{a}|\chi_k\rangle = \delta_{kl} \langle\chi_k|\hat{a}|\chi_k\rangle$ , so we have $$ \langle\phi|\hat{a}|\psi\rangle = \sum_k c_k d_k^\ast \langle\chi_k|\hat{a}|\chi_k\rangle . $$ this gives a sum of the expectation values $\langle\chi_k|\hat{a}|\chi_k\rangle$ ( i.e. . , the eigenvalues $\chi_k$ ) , with weight coefficients given by the expansion of the vectors $|\psi\rangle$ , $|\phi\rangle$ , namely $c_k d_k^\ast$ .
the radiation density has two components : the present-day photon density $\rho_\gamma$ and the neutrino density $\rho_\nu$ . the photon density as a function of frequency can be derived directly from the cmb : the photon number density follows the planck law $$ n ( \nu ) \ , \text{d}\nu = \frac{8\pi\nu^2\ , \text{d}\nu}{e^{h\nu/k_b t_0}-1} , $$ with $k_b$ the stefan-boltzmann constant , and $t_0$ the current cmb temperature . the photon energy density is then $$ \rho_\gamma\ , c^2 = \int_0^{\infty}h\nu\ , n ( \nu ) \ , \text{d}\nu = a_b\ , t_0^4 , $$ where $$ a_b = \frac{8\pi^5 k_b^4}{15h^3c^3} = 7.56577\times 10^{-16}\ ; \text{j}\ , \text{m}^{-3}\ , \text{k}^{-4} $$ is the radiation energy constant . with $t_0=2.7255\ , \text{k}$ , we get $$ \rho_\gamma = \frac{a_b\ , t_0^4}{c^2} = 4.64511\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ the neutrino density is related to the photon density : in eq . ( 1 ) on page 5 in the paper , you see that $$ \rho_\nu = 3.046\frac{7}{8}\left ( \frac{4}{11}\right ) ^{4/3}\rho_\gamma . $$ this relation can be derived from physics in the early universe , when neutrinos and photons were in thermal equilibrium . so $$ \rho_\nu = 3.21334\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} , $$ and the total present-day radiation density is $$ \rho_{r , 0} = \rho_\gamma + \rho_\nu = 7.85846\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ we can also express this relative to the present-day critical density $$ \rho_{c , 0} = \frac{3h_0}{8\pi g} = 1.87847\ , h^2\times 10^{-26}\ ; \text{kg}\ , \text{m}^{-3} , $$ where the hubble constant is expressed in terms of the dimensionless parameter $h$ , as $$ h_0 = 100\ , h\ ; \text{km}\ , \text{s}^{-1}\ , \text{mpc}^{-1} , $$ so we get $$ \begin{align} \omega_{\gamma}\ , h^2 and = \dfrac{\rho_\gamma}{\rho_{c , 0}}h^2 = 1.71061\times 10^{-5} , \\ \omega_{\nu}\ , h^2 and = \dfrac{\rho_\nu}{\rho_{c , 0}}h^2 = 2.47282\times 10^{-5} , \\ \omega_{r , 0}\ , h^2 and = \omega_{\gamma}\ , h^2 + \omega_{\nu}\ , h^2 = 4.18343\times 10^{-5} . \end{align} $$ for a hubble value $h=0.673$ , one finds $\omega_{r , 0} = 9.23640\times 10^{-5}$ .
in an ideal situation ( no air resistance ) there will be absolutely no difference in the place where the coin lands ! whether you toss the coin up from inside the train or while standing on the roof , the coin will land back in your hand ( provided you have tossed it perfectly vertically ) . however , in practice , while standing on a fairly fast train 's roof , there is a lot of wind because you are moving at high speed . the moment you toss the coin , the wind force acts on it , and creates an acceleration in the backward direction , making it go slower than the train ( and you ) . however , it will not land in exactly the same place from where you tossed it every time ! this purely depends on the retarding force acting on the coin - in this case , the wind .
suppose you want to use real measurements in this equation , then the density is actually the mass $m_0$ contained in a mesh unit volume ( or voxel ) divided by the volume of the voxel , $a^3$ such that $\rho=m_0/a^3$ . as far as i know in the lattice boltzmann method there is a finite number of velocities $\vec c_i$ and $f_i$ is the mass $m_i$ of matter moving with velocity $\vec c_i$ divided by $a^3$ , $f_i=m_i/a^3$ . as $\sum_im_i=m_0$ , this explains why $\sum_f_i=\rho$ . i hope this answers the interpretation part of your question .
let me give you an intuition of what components are and then i will answer your question : i pull a dog on a leash with a tension of 5n at an angle of 53.1 degrees . this is equivalent to pulling the dog simultaneously horizontally with a rope with a force of 3n ( 5cosθ ) and vertically with a rope with a tension force of 4n ( 5sinθ ) in both scenarios , the dog " feels " the same . in other words , the two components are equivalent to the single force of 5n at an angle of 53.1 degrees . in your question , the weight only acts downwards . we can " break " the force down using a coordinate system and find two components which are equivalent to the weight . the vertical weight vector can be divided into components along the slope and perpendicular to it . by replacing the weight vector by these components , and dividing the other forces into the same coordinate system , we can determine the motion of the object along and perpendicular to the plane .
i have found the solution to my problem . first of all , i had a factor 2 discrepancy due to the pixel dimension and , more important from an optical point of view , the laser beam was underfilling the entrance pupil of the objective . this means the focusing was worse !
think of a two dimensional horizontal elastic sheet in three dimensions . suppose we specify the height everywhere with $\phi ( x , y ) $ . then if the sheet moves up and down there is kinetic energy . this kinetic energy is proportional to $\dot{\phi}^2$ because $\dot{\phi}$ is telling you the velocity of that point on the sheet . also , suppose we want to pull one point on the sheet up while keeping a neighboring point fixed . this will cost elastic potential energy . the amount of potential energy is proportional to $ ( \nabla\phi ) ^2$ . thus we get a contribution to the lagrangian of the form $ ( \partial_t \phi ) ^2 - ( \nabla\phi ) ^2 = \partial_\mu \phi \partial^\mu \phi$ . i googled and found a pdf ( titled week 7 lecture : concepts of qft by some one named andrew forrester ) which does this in more detail . i have not read it though , so i can not vouch for it too much . you would only need to read the second and third pages .
i do this sort of work in uhecr anisotropy . there are numerous techniques ( which i am currently working on advancing ) . expansion in spherical harmonics is popular ( and what i am working on ) . for that we use the fact that the spherical harmonics are complete to write $$f=\sum_{\ell , m}a_{\ell m}y_{\ell m}$$ and then from orthogonality we can write ( assuming a given normalization ) $$a_{\ell m}=\int d\omega fy_{\ell m}$$ once we have the coefficients $a_{\ell m}$ we have a number of approaches . one obvious one is to follow the approach of the cmb people and write the power spectrum as $$c_\ell=\frac1{2\ell+1}\sum_ma_{\ell m}^2$$ which is axis independent . then you can compare this to that from isotropy . in the pure case you can write for monopoles that $c_0&gt ; 0 , c_\ell=0\forall\ell&gt ; 0$ where the value depends on the normalization . the remaining terms all show deviations from anisotropy . for the sum of discrete events a mc simulation is useful here . another common standard definition that is quite simple is to define $$\alpha=\frac{\max f-\min f}{\max f+\min f}$$ isotropy gives $\alpha=0$ as desired . in addition , for the simple dipole case , we can write $f=1+a\cos\theta$ where $a$ measures the strength of the dipole . $a=0$ is obviously isotropy . $a=1$ gives $f=2$ at $\theta=0$ ( the " direction " of the dipole ) and $f=0$ at $\theta=\pi$ . note that the 1 ( and the implicit requirement $a\le1$ ) is to ensure that $f$ is positive-definite ( which may or may not be necessary for your situation ) . then , at $a=1$ , we get $\alpha=1$ ( and in fact it is easy to show that $a=\alpha$ ) . note that the three $ ( \ell , m ) = ( 1 , m ) $ spherical harmonics are all degenerate . a similar thing can be done for the quadrupole case , at least for the $m=0$ case , with $f=1-b\cos^2\theta$ where $b$ and $\alpha$ have a more complicated ( but still simple ) relationship . for more complicated shapes you can try to match your geometry with spherical harmonics with what is known as the $k$-matrix approach presented here ( arxiv abs ) in section 3 . in that example they consider a sphere with non-uniform exposure . i suspect that the same ( or a similar ) approach could be used for your case . i should warn though that the reconstruction power falls off very quickly with $\ell_{\rm{max}}$ in the $k$-matrix , so do not go any higher than you have to .
you have the right ideas , but you do not dare to put them in maths ! momentum conservation is vectorial . here , you have a 2d system , so let 's write it with 2 component vectors . i will denote $\vec{p}_{i/f}$ the initial and final momentum respectively . so the momentum conservation yields $ \vec{p}_i = \vec{p}_f $ i choose to represent the $x$-axis as the first component of my vector , and $y$ the second one . thus , $\begin{pmatrix} m_av_a \\ 0 \end{pmatrix} = \begin{pmatrix} m_bvb_x + m_cvc_x \\ m_bvb_y + mcvc_y \end{pmatrix} \quad \quad \quad ( 1 ) $ now the exercise tells you several things . first of all , you know $m_a , v_a , m_b$ and the angle at which block $b$ takes off . you have to trigonometrically decompose the speeds to find out how to write them in vector form . for instance , $vb_x = v_b\cdot\cos ( 35 ) $ , and $vb_y = vb\cdot\sin ( 35 ) $ . with the same reasoning , $vc_x = 0$ and $vc_y = vc$ because of momentum conservation being vectorial , momentum is conserved on the $x$-axis as well as the $y$-axis independently . from ( 1 ) , you see that $\left\{ \begin{array}{l} m_av_a = m_bv_b\cdot\cos ( 35 ) \\ 0 = mbv_b\sin ( 35 ) + m_cv_c \end{array} \right . $ the only unknowns are $v_b$ and $v_c$ . all you need to do is solve this equation system .
the standard way is to use generating functions ( in this case a la coherent states ) . usually one would like the resulting formula to be normal-ordered . recall the following version $$\tag{1} e^ae^b~=~e^{ [ a , b ] }e^be^a$$ of the baker-campbell-hausdorff formula . the formula ( 1 ) holds if the commutator $ [ a , b ] $ commutes with both the operators $a$ and $b$ . put $a=\alpha a $ and $b=\beta a^{\dagger}$ , where $\alpha , \beta\in\mathbb{c}$ . let $ [ a , a^{\dagger} ] =\hbar {\bf 1}$ , so that the commutator $ [ a , b ] =\alpha\beta\hbar {\bf 1}$ is a $c$-number . now taylor-expand the exponential factors in eq . ( 1 ) . for fixed orders $n , m\in \mathbb{n}_0$ , consider terms in eq . ( 1 ) proportional to $\alpha^n\beta^m$ . deduce that the the antinormal-ordered operator $a^n ( a^{\dagger} ) ^m$ can be normal-ordered as $$\tag{2} a^n ( a^{\dagger} ) ^m~=~\sum_{k=0}^{\min ( n , m ) } \frac{n ! m ! \hbar^k}{ ( n-k ) ! ( m-k ) ! k ! } ( a^{\dagger} ) ^{m-k}a^{n-k} . $$ finally , deduce that the normal-ordered commutator is $$\tag{3} [ a^n , ( a^{\dagger} ) ^m ] ~=~\sum_{k=1}^{\min ( n , m ) } \frac{n ! m ! \hbar^k}{ ( n-k ) ! ( m-k ) ! k ! } ( a^{\dagger} ) ^{m-k}a^{n-k} . $$
yes . if you define $f=-\partial_\mu a^\mu$ then you can write the equation in the form $$ \partial_\mu\partial^\mu\psi = f$$ this is the klein-gordon equation with a nonzero source ( $f$ ) and can be solved via green 's function methods . once you have the klein-gordon propagator* $g ( x ) $ ( this is derived in any e.g. quantum field theory textbook ) appropriate to the boundary conditions the solution can be written as $$ \psi ( x ) =\int d^4 x ' g ( x-x' ) f ( x' ) $$ since green 's functions by definition satisfy $$\partial_\mu\partial^\mu g ( x-x' ) = \delta ( x-x' ) $$ where we take all differentiations to be with respect to x . *you need the propagator in the position space representation to write this down . it is usually more convenient to write it in momentum space ; you can go back and forth using ( inverse ) fourier transforms .
the initial phase $\phi$ should be $-\pi/2$ , or in other words , the solution should be $a\sin ( \omega t ) $ because $\cos ( \theta-\pi/2 ) =\sin ( \theta ) $ . how did you find that $\phi$ is zero ? you are supposed to use the fact that $x$ is zero when $t$ is zero . which is what let 's you set $\phi $ to $-\pi/2$ ( modulo $\pi$ ) .
no . radio waves are light , and so they travel at the speed of light . possible exception : the speed of light $c$ that is always talked about as the " speed limit of the universe " is the speed of light in vacuum . light travels slower in a medium , and how much slower can be dependent on the wavelength of the light . so , in the right medium , radio waves could travel faster than optical waves , but i would not call this " faster than light" ; that would be very misleading . . .
the order of magnitude given by gugg is correct . the molar volume for the succinic acid is $v_m=\frac{m}{\rho}=\frac{118.09}{1.59}\frac{cm^3}{mol}=74.27cm^{3}/mol$ where $m$ is the molar mass and $\rho$ the density . from this , you find the volume of the molecule to be $v_{molec}=\frac{v_m}{n_a}=1.23\cdot10^{-22}cm^3 $
dear wade , your good question is easily answered if you consider " pressure " to be a derived quantity , and let us derive it . an average molecule ( or atom ) of an ideal gas - and your proposition only holds for an ideal gas - has kinetic energy equal to $$mv^2/2=3kt/2$$ it is because every degree of freedom carries $kt/2$ and there are three degrees of fredom in translations . note that lighter molecules will move faster than the heavier ones . how do we calculate the pressure ? well , put the molecule in a cubic box of volume $a^3$ . it will hit the walls - the total surface of the cube is $6a^2$ . we need to compute the average ( over molecules ) transfered momentum per unit time - this is called the force - and divide it by the area to compute the pressure of one molecule . the force on the wall may be in $x , y , z$ directions . let 's consider the $x$ direction . the velocity of a particular molecule in the $x$-direction is $v_x$ , so it takes $a/v_x$ of time to get from one side to the other side of the box in the $x$ direction . once it gets to the other side , it bounces off the wall and changes the sign of the $x$-component of the velocity ( and momentum ) . at this moment , the momentum $p_x$ clearly changes the sign - i.e. changes by $2p_x$ . so the change of momentum $p_x$ per unit time is $$f_x = 2p_x / ( a/v_x ) = 2p_x/ ( am/p_x ) = 2p_x^2/am$$ that is equal to $4/a$ times the kinetic energy $k_x$ in the $x$-direction . the pressure is $$p = ( f_x+f_y+f_z ) / 6a^2 = 3\times 4/a\times k_x / 6a^2 = 4k/a / 6a^2 = 2k/3a^3 $$ but as i have said , the average kinetic ( motion ) energy per molecule is $k=3kt/2$ where $t$ is the absolute temperature , so the pressure is $$p = 2k/3a^3 = kt/a^3= kt/v$$ note that we have just derived $pv=kt$ for one molecule or $pv=nkt$ for $n$ molecules - which is what we wanted . the mass of the molecule canceled : if the molecule is heavier , the average velocity at a given temperature is slower . but that does not matter - because the molecule has a greater momentum ( because of the higher mass ) which is compensated by the longer time it needs to get from one side to the other to transfer this momentum . so for a fixed temperature , the pressure is independent of the molecule type .
a derivation is here : http://en.wikipedia.org/wiki/lamb_shift#derivation or in landau-lifshitz . bethe is original derivation is found e.g. in matt schwartz 's harvard lecture here http://isites.harvard.edu/fs/docs/icb.topic792163.files/20-lambshift.pdf the leading contribution to the lamb shift is the one-loop level ( the first non-classical correction ) but of course , the effect receives corrections at every higher order , too . one may derive it in the operator approach or path integral approach , much like pretty much everything in physics . these are just equivalent languages to do physics . the lamb shift has to deal with an atom which is not quite an elementary particle . so the usual perturbative rules of qed have to be " generalized " to deal with the composite object . however , otherwise it is about a virtual photon emitted and reabsorbed by the atom . if the atom were elementary , it would be a simple " photon loop " correction to the atom 's propagator . a divergent term has to be removed – equivalently , one has to find out sensible limits of the integral – and what is left is some " truncated logarithmic divergence " that produces those 1,000 mhz for the relevant levels .
its questions like this one that keep me coming back to this site ! your first question is : is there an easy way to understand and/or visualize the reciprocal lattice of a two or three dimensional solid-state lattice ? yes ! the reciprocal lattice is simply the dual of the original lattice . and the dual lattice has a simple visual algorithm . given a lattice $l$ , for each unit cell of $l$ find the point corresponding to that cell 's " center of mass " ( see below ) . connect each such " center of mass " to its nearest neighbors . the resulting lattice is the dual of $l$ . to find center of mass of unit cell ( we consider 2d case , generalizes to arbitrary dimension ) : draw the perpendicular bisectors of the edges which bound the unit cell . for regular lattices these lines should intersect at a single point in the interior of the cell . this point is the " center of mass " of the cell . performing these simple steps you find that the dual of a square lattice is also a square lattice , and that the triangular and hexagonal lattices are each others duals ! you can see a nice illustration of this fact here . your second question is : what is the significance of the reciprocal lattice , and why do solid state physicists express things in terms of the reciprocal lattice rather than the real-space lattice ? as mentioned by others this has to do with fourier transforms . in solid-state physics we want to understand the excitations ( waveforms ) that a certain material , whose structure is given by some lattice $l$ , can support . for a lattice only certain momenta are allowed due to its discrete structure . these allowed momenta correspond to the vertices of the dual lattice ! for more see the wikipedia page or check out the first couple of chapters of little kittel or ashcroft and mermin .  Cheers,  edit : this to clarify some doubts about my answer @wsc has expressed in the comments . first of all , it is incorrect that reciprocal lattice vectors in 3d have dimensions $1/l^2$ . consider a 3d lattice with basis vectors $\{a_i\}$ . the reciprocal lattice has basis vectors given by $$ b_i = \frac{1}{2v} \epsilon_i{}^{jk} \ , a_j a_k $$ in index notation , with summation convention . a more familiar way to write this is in vector notation : $$ \mathbf{b}_i = 2\pi \frac{\mathbf{a}_j \times \mathbf{a}_k}{\mathbf{a}_i \cdot ( \mathbf{a}_j \times \mathbf{a}_k ) } $$ where $ ( i , j , k ) $ are cyclic permutations of $ ( 1,2,3 ) $ . we can see that $$ \dim [ \mathbf{b}_i ] = \frac{\dim [ \mathbf{a} ] ^2}{\dim [ \mathbf{a} ] ^3} = \frac{1}{l} $$ and in terms of the lattice spacing $a$ , $\vert\mathbf{b}\vert \sim \frac{1}{a}$ . in fact , this is a basic fact true in any dimension . we can also understand the normalization of the reciprocal lattice vectors by the factor $\mathbf{a}_i \cdot ( \mathbf{a}_j \times \mathbf{a}_k ) $ as being nothing more than $v$ - the volume of the unit cell . why ? so that the transformation between the lattice and reciprocal lattice vector spaces is invertible and the methods of fourier analysis can be put to use . for all regular lattices afaik the " dual " and " reciprocal " lattices are identical . for irregular lattices - with defects and disorder - this correspondence would possibly break down .
for symmetric or antisymmetric tensor product , the most useful definition of an unentangled state is a state of the form $$ |\psi\rangle = \sum_{p} ( -1 ) ^{2j\cdot |p|} \prod^\otimes_i |\psi_{p ( i ) }\rangle $$ which is just a convoluted symbolic expression for the totally symmetrized ( for bosons ) or totally antisymmetrized ( for fermions ) tensor product . the sum goes over all permutations $i\mapsto p ( i ) $ . the power of $ ( -1 ) $ is $1$ for bosons i.e. $j\in {\mathbb z}$ while it is $ ( -1 ) ^{|p|}$ , i.e. the sign of the permutation , for fermions with $j\in {\mathbb z}+1/2$ . the product is a tensor product . all states that can not be written in this form are entangled . if we kept the usual definition of unentangled states as " strict tensor products " , there would be too few unentangled states because most multi-boson states and almost all multi-fermion states refuse to be strict tensor products . so the definition above makes the form of unentangled states more general . however , it may be too general for various purposes . in general , the tensor factors $|\psi_{p ( i ) }\rangle$ should correspond to states where objects are localized in different regions of space , otherwise the state that is unentangled according to the definition above could be entangled " morally " . in particular , in quantum field theory , unentangled states may be obtained as $$ a^\dagger b^\dagger c^\dagger \dots |0\rangle $$ where $a^\dagger$ and others are polynomials in creation operators but $a^\dagger , b^\dagger , c^\dagger$ are only made of creation operators acting on disjoint , non-overlapping regions . the actual wave function of the state will have to be symmetrized or antisymmetrized so it will not be a strict tensor product but the state of the form above will still enjoy many features of unentangled states . the discussion about the non-overlapping regions may get important and subtle because , for example , the simple singlet state of two spins , $|up\rangle |down\rangle - |down\rangle |up\rangle$ , is entangled according to the normal definition but it could end up being unentangled because it is an antisymmetrized tensor product . however , it usually only makes sense to call such a state unentangled if the up-spinning and down-spinning electron are also located at different positions . if they share the same location , the strict singlet state should be called entangled in all conventions .
the total incident energy per second , $w_{tot}$ , is the energy per unit area multiplied by the area , so : $$ w_{tot} = 2 w/m^2 \times 10^{-4} m^2 $$ the total number of photons is $w_{tot}$ divided by the energy per photon ( in joules ) , and the energy per photon is : $$ e = 10.6ev \times 1.602 \times 10^{-19} $$ where $1.602 \times 10^{-19}$ is the conversion factor from ev to joules . so the total number of photons per second is : $$ n = \frac{w_{tot}}{e} = \frac{2 w/m^2 \times 10^{-4} m^2}{10.6ev \times 1.602 \times 10^{-19}} $$ multiply by 0.53/100 because only 0.53% of photons expel a photoelectron and you get your formula .
so this will take a really simplistic look at it , ignoring things like flexibility in the tires/wheels/bike and assuming that you do not go too fast to slide out . the work done to turn is the force to turn times the distance of the turn . the force is $mv^2/r$ where $m$ is the mass of the system , $v$ is the speed of the turn , and $r$ is the radius of the turn . so the force gets higher as either the speed increases or the turn gets smaller , as expected . the distance of the turn is the arc-length of a circle ( assumption of course ) of the radius , which is $d = \theta r$ . this means the work done is $$w = \frac{mv^2}{r} \theta r$$ or , in other words , if you are not going to slide out and you can generate the forces you need to make the turn , it does not actually matter what radius you choose , the work done is the same either way . now this is where the assumptions become obviously violated . we can not turn instantly ( $r = 0$ ) because there is not enough friction to keep us upright . and the amount of friction increases as the turn gets sharper . the turning friction dominates the friction along the arc-length ( unless your tires are slipping riding in a straight line ) , so all of this comes together to imply that you save energy by taking a bigger turn faster than a narrower turn slower . but you obviously want to take the tightest turn you can at the fastest speed you can without crashing to maximize everything .
your physics is correct : all colors ( wavelengths ) that make up the " blue " sky are " darkened " with altitude in ( approximately ) the same proportion because there is lass air to scatter . however , the brain does not directly perceive how much red , green , and blue there is . the brain transforms this information into something like the " hcl " color space . " h " means " hue" ; different colors on the rainbow are different hues . " c " means " chroma " , low chroma colors are washed out , zero chroma means shades of grey . " l " means luminosity , on a scale of dark vs light . if the red , green , and blue components are all reduced we notice less " l " but the same " h " and " c " ( as long as it stays bright enough for cones to work ) . see : http://tools.medialab.sciences-po.fr/iwanthue/theory.php for more information .
an idealized projectile launched from ground with speed $v$ at angle $\theta$ measured from zenith will reach ground at time : $$ t=\frac{2v\sin\theta}{g} $$ assuming the ground is level . the horizontal distance it will have traveled in that time is : $$ d=\frac{2v^2\sin\theta\cos\theta}{g}=\frac{v^2\sin 2\theta}{g} $$ for small $\theta$ you could approximate this with : $$ d\approx\frac{2v^2\theta}{g} $$ but i guess you need a formula that works also for larger $\theta$ . if the ground is sloped with a constant angle $\alpha$ ( positive angles for uphill slopes ) from the ejection point to where it hits the ground , you can use the following formula : $$ d=\frac{2v^2\sin\theta ( \cos\theta-\sin\theta\tan\alpha ) }{g} $$ do you want me to show you how i derived the formula ?
yes , it will affect angular velocity since different mass distribution have different moment of inertia $i$ in general . the effect of torque $\tau$ on the angular velocity $\omega$ of the object is given by $$\tau=\frac{d}{dt} ( i\omega ) $$ the moment of inertia of a point mass is given by $i=mr^2$ , so in your case , the radius differ by 10 time so moment of inertia differ by 100 times and so does the angular velocity . note that when you mention torque , you dont necessary need to specify which point it acts on .
once boiling starts , there is an equilibrium between the liquid and vapor phase . the pressure , temperature and partial molar gibbs energy are equal for each phase so that water molecules have no preference for one phase or the other . that is for intensive variables . however , the total enthalpy of the liquid and vapor is not fixed : it keeps increasing as more energy is brought in . the proportion of vapor to liquid is fixed through this energy balance . if the pressure is maintained externally , say by a piston , there is no possibility for the water bubbles to form at any other pressure than the vapor pressure . however when you boil water at the bottom of a pot , two phenomena alter the situation slightly . the first is the hydrostatic height of the water column , which increases the pressure at the bottom ( where bubbles form ) and raises the equilibrium temperature . the second is the surface tension of water , which increases the pressure required to form a bubble . if the bottom surface is perfectly smooth , nucleation of bubbles is difficult , and the onset of boiling can be delayed to higher temperatures as water remains in a metastable liquid state . note that when boiling water you will often see bubbles form at an early stage and then disappear as the temperature increases : those bubbles are not water vapor but dissolved gases .
so , whenever i want to find a nice introduction to a concept in physics , i check the american journal of physics , as it is full of articles with clever descriptions of phenomenon appropriate for presentation in university courses . in this case , this yields many results . in particular , i found the following three articles very helpful : the mathematics of brownian motion and johnson noise - daniel gillepsie [ doi ] [ pdf ] two models of brownian motion - david mermin [ doi ] [ pdf ] fluctuation and dissipation in brownian motion - daniel gillepsie [ doi ] [ pdf ] for completeness , i will give my version of a modern approach to describing brownian motion , where i will borrow heavily from the above . if you want to think in terms of newton 's laws , we will take an approach that in spirit is the same that langevin gave three years after einstein 's paper ( a translation of which also appeared in the american journal of physics [ doi ] [ pdf ] ) , that gives the same result . if we imagine a pollen particle suspended in a liquid , we can assume that the forces on the pollen particle are given by a dissipative friction force , and some random jostling by impacts from the water molecules , we will write $$ m \dot v = -\gamma v + f \gamma ( t ) $$ where $\gamma$ is the drag coefficient , $f$ is some constant we will have to determine , and $\gamma ( t ) $ represents a random gaussian process . that is , we will assume the effect of all of the jostling by the water molecules amounts to drawing a random variable at every instant of time . then , the next step of the argument is to make this whole deal consistent with statistical mechanics , namely the equipartition theorem , so in particular , if we look at long times , we should have $$ \frac 12 m \left\langle v^2 ( \infty ) \right \rangle = \frac 12 k t $$ or in words , the average kinetic energy at long times should be a half $kt$ if we are going to be consistent with statistical mechanics . so , we need only compute the average fluctuations in our velocity for long times . you can follow the papers to see a detailed mathematical account , but for just a taste , we can get the answer from dimensional analysis . we are interested in determining $ \langle v^2 ( \infty ) \rangle $ , and this answer should only depend on the parameters in our equation for the forces the particle feels , namely $m$ , $\gamma$ and $f$ . the dimensions of $m$ and $\gamma$ are easy to read off of the equation $$ [ m ] = [ m ] \qquad [ \gamma ] = [ m t^{-1} ] $$ but what about that $f$ ? well , it depends on the dimensions of our random gaussian noise term , which is a bit tricky . but , the way we tried to describe it , the noise was supposed to be completely uncorrelated in time , so though i did not detail it , this means that $$ \langle \gamma ( t ) \gamma ( t' ) \rangle = \delta ( t-t' ) $$ in detail . and since we know that $\int dt\ , \delta ( t ) = 1$ , we have the dimensions $$ [ \delta ( t ) ] = [ t^{-1} ] \qquad [ \gamma ( t ) ] = [ t^{-1/2} ] $$ which tells us that $$ [ f^2 ] = [ m l t^{-3} ] $$ which seems funny , but it enables us to determine that $$ \langle v^2 \rangle \propto \frac{ f^2 }{ \gamma m } $$ and in particular , we will assume that the proportionality constant is 1 , which using equipartition , gives us $$ m \langle v^2 \rangle = f^2/\gamma = k t $$ or $$ f = \sqrt{ \gamma k t } $$ if we had done all of the math properly , the real answer turns out to be $$ \boxed{ f = \sqrt{ 2 \gamma k t } } $$ which is pretty darn close . the point of all of that , and of einstein 's original paper is that we have shown that the fluctuations ( $f$ ) causes by the jostling of the unseen water molecules is directly related to the dissipation ( $\gamma$ ) you can observe in ordinary fluid experiments . this is the major result of einsteins and langevin 's papers . with a bit more work , we can relate this to the diffusion constant , which tells us how the root mean square position increases linearly with time : $$ \langle x^2 ( \infty ) \rangle = d t $$ doing our dimensional analysis again , we discover we need a relation of the form $$ d \propto \frac{f^2}{\gamma^2} $$ or , getting rid of this silly $f$ thing , using our other result above $$ \boxed{ d = \frac{ kt }{\gamma } } $$ which turns out to be right even if you do the math right ( the proportionality constant is 1 ) . this was the actual formula einstein got famous for in his paper , relating the diffusion constant , something you can measure in experiment , to the drag coefficient , something you can also measure from a different set of experiments . giving in the end a quantitative theory of brownian motion that worked to help solidify the atomic hypothesis and some of the early results of statistical mechanics .
as requested , but please do not mark this as the answer because it is a rehash of manishearth 's post : when physicists say for example " time is proportional to $\sqrt{h}$" what they mean is that : $$t = c \times \sqrt{h}$$ where c is just a number , called the constant of proportionality , and it does not change . the only way to find out the value of the constant $c$ is to measure $t$ for a variety of values of $\sqrt{h}$ . typically you would draw a graph of $t$ against $\sqrt{h}$ and you should get a straight line . the gradient of the line gives you the value of $c$ . if you do not get a straight line then you have made a mistake somewhere and $c$ is not constant . so the lecturer in the video starts by guessing that $t$ is proportional to $h^\alpha m^\beta g^\gamma$ , and what he means by this is : $$t = c \times h^\alpha m^\beta g^\gamma$$ where $c$ is the constant of proportionality . dimensional analysis can not tell you what the constant of proportionality is . the only way to find the value of $c$ is to do the experiment .
your term is allowed , and in fact present in qed . it gives the anomalous magnetic form factor contribution to the current . in qed , the number $2f ( 0 ) $ is the anomalous magnetic moment of the electron . see peskin/schroeder p . 186-188 .
we could say : $$ \frac{\partial \mathbf{b}}{\partial t} \neq 0 \implies \mathbf{\nabla} \times \mathbf{e} \neq 0 \implies \mathbf e \neq 0 $$ where the first implication follows from the transitivity of inequality and faraday 's law : $$ \mathbf \nabla \times \mathbf e = \frac{\partial \mathbf b }{\partial t } $$ and the second implication is the contrapositive of $$ \mathbf e = 0 \implies \mathbf \nabla \times \mathbf e = 0 $$ you are right to point out that we could have just as well said : $$ \nabla \times \mathbf e \neq 0 \implies \frac{\partial \mathbf b}{\partial t} \neq 0 \implies \mathbf b \neq 0 $$ this does not present a problem . we tend to interpret any implication derived from physical law as causal , and while you will regularly find passages in books with things like : " the changing magnetic field causes an electric field " , you will equally likely find other passages of the same book that say something like " the curl in the electric field causes a change in the magnetic field " . and i think calling these causal is justified , as we can set up an experiment where we modify a magnetic field and measure an electric field , or for the second one , you could perform an experiment where you rotate a capacitor and measure a magnetic field , for instance . as for jefimenko 's equations , as far as i know they are completely equivalent to maxwell 's equations in their predictions . in fact , you can derive them from maxwell 's equations and vice versa . † similarly describing electromagnetism by use of the electromagnetic tensor $f_{\mu\nu}$ and four current is completely equivalent to the other two in its predictions . ( in this way , we were both lucky and blind , to have developed a classical theory by 1865 that was completely relativistic , but not develop relativity until 1905 . ) †: for instance , see section 6.5 of jackson 's classical electrodynamics , third edition at that point , physics can no longer distinguish which is true , and you are free to believe whichever you like , or to switch beliefs between the three systems as you please , or as proves convenient . they are , in effect , three different , completely equivalent formulations of electromagnetism . this is standard . for instance , newtonian mechanics , lagrangian mechanics , hamiltonian mechanics , or hamilton-jacobi mechanics all give the same predictions , but all seem to tell a very different story about how the world works . each is useful for solving particular problems , and each gives you a different way to view the world , and each is , as far as science can ascertain , completely equivalent and valid ways to view the world . some people like to believe in one interpretation over another , but i think its healthy to keep in mind that they are all as valid as the other . but you are more than welcome to keep one as your favorite .
at the risk of being snarky ( each definition is from wikipedia ) . . . comet - a comet is an icy small solar system body that , when close enough to the sun , displays a visible coma ( a thin , fuzzy , temporary atmosphere ) and sometimes also a tail . these phenomena are both due to the effects of solar radiation and the solar wind upon the nucleus of the comet . comet nuclei range from a few hundred meters to tens of kilometers across and are composed of loose collections of ice , dust , and small rocky particles . asteroid - asteroids ( from greek ἀστήρ ' star ' and εἶδος ' like , in form' ) are a class of small solar system bodies in orbit around the sun . they have also been called planetoids , especially the larger ones . these terms have historically been applied to any astronomical object orbiting the sun that did not show the disk of a planet and was not observed to have the characteristics of an active comet , but as small objects in the outer solar system were discovered , their volatile-based surfaces were found to more closely resemble comets , and so were often distinguished from traditional asteroids meteor - a meteoroid is a sand- to boulder-sized particle of debris in the solar system . the visible path of a meteoroid that enters earth 's ( or another body 's ) atmosphere is called a meteor , or colloquially a shooting star or falling star . if a meteoroid reaches the ground and survives impact , then it is called a meteorite . many meteors appearing seconds or minutes apart are called a meteor shower . the root word meteor comes from the greek meteo¯ros , meaning " high in the air " .
your original question was : afaik , the particles vibrate according to their energy level . is this vibration in 3d space ? one has to state whether you are talking classical particles or quantum mechanical elementary particles or quantum mechanical atoms and molecules . classical particles move in three dimensions , any motion . it might be constrained because of boundary conditions but it is three dimensions . elementary particles , i.e. the ones of the standard model move through space with their kinetic energy , in three dimensions . when unbound , they do not vibrate . quarks , which are always bound within nuclei , vibrate in three dimentional space . atoms and molecules have vibrational states which again are three dimensional but following boundary conditions . all the above vibrations are with the normal definition of vibration , the center of mass being displaced from its average position with the energy of vibration , not connected with the de broglie wavelength . the " vibrations " you are asking , i.e. . the de broglie wavelength defined " displacement " is not similar to the above . the wave is a probability wave , in three dimensional space , but the wave nature appears in specially designed experiments in the distribution of the probabilities of finding elementary particles , atoms , molecules . here is what happens when electrons of the same energy are sent through the double slit : electron buildup over time the probability wave pattern , connected with the energy of the electrons , builds up slowly one electron at a time , proving that the electrons can appear as a probability wave , in three dimensions , but the electron itself is intact , hitting one spot at a time . there is no 3d shape to the electron itself . it is its interactions with the slits that show up the wave nature .
an obvious example is the hubble bubble though this does not invalidate the flrw model , it just means the homogeneity is on a larger scale than the observable universe . however i get the impression the hubble bubble idea is not widely considered likely .
the measure is not arbitrary . in classical mechanics , the symplectic structure of phase space defines the liouville measure . in quantum mechanics , the hilbert space norm plays this role .
there is no simple equation for how a paper airplane flies like there is for a simple projectile because the airplane can interact with the air in complicated ways . the physics of a paper airplane is described by newton 's laws of motion . these laws apply to both the airplane and the air it travels through . the plane is acted on by a constant gravitational force and by contact forces with the air , especially drag and lift . the nature of the force between the air and the plane can be quite complicated , and requires an extremely detailed analysis for accurate simulation . for example , by constructing the plane slightly differently , you can make it fly faster , slower , further , curve left or right , or bob up and down . the basic physical ideas are those of fluid dynamics and the basic equation involved is the navier-stokes equation . modeling something like an airplane accurately is mostly the domain of expertise of aeronautical engineers . to make a simple model for a game , you might want to start with a simple constant gravity force , a drag force proportional to the square of the velocity , and a lift force also proportional to the square of velocity ( which comes from here ) , and then play around with the parameters until you find something pleasing to your eye .
it is only a question of definition . there is the operator of interaction of particle with an externally-produced magnetic field : $\hat{h}_{int}=-\hat{\boldsymbol{\mu}}\cdot\mathbf{h}$ , where $\mathbf{h}$ is a magnetic field and $\hat{\boldsymbol{\mu}}$ is an operator : $\hat{\boldsymbol{\mu}}=\displaystyle \frac{g e}{2 m} \hat{\mathbf{s}}$ by the «value» of the magnetic moment of particle , people usually imply the maximum of the following diagonal matrix element : $\mu=\langle\psi\vert \hat{\boldsymbol{\mu}}_{z} \vert\psi\rangle , $ which is of course $g \mu_n/2$
the xy pattern is used with directional microphones ( usually cardioid ) pointed in different directions . cardioid microphones reject signal coming from different directions . therefore , even if the mics somehow occupied the same space , they would still be hearing different things ( as long as they are pointed in different directions ) . your statement that " the two signals obtained are equal in amplitude " is therefore incorrect : when instruments are mic'ed using the xy pattern , there are differences in amplitude if the sound source ( including reverberant reflections of sound from surfaces in the room ) are coming " off axis " of the stereo pair .
yes . for photons in vacuum , the energy per photon is proportional to the photon 's classical , electromagnetic frequency , as $e = \hbar \omega = h f$ . here , we see a connection between two classical properties of light : the energy and frequency . what is surprising is that the relation holds for matter , where there is no classical equivalent of the frequency . nevertheless , in an interferometry experiment , an relative energy shift of $\delta e$ can lead to an observable frequency difference $\delta f$ , so that the phase of an interferometer operated for a time $t$ is $\phi = \delta e\ , t/\hbar$ . this was originally observed in neutrons and has more recently been seen in electrons and atoms . even the rest mass energy $mc^2$ has a equivalent frequency , which is known as the compton frequency $\omega_c = mc^2/\hbar$ . while we can not ( currently ) experimentally measure it , it can be inferred from atom interferometry experiments . the general idea of a matter-wave frequency occurs where it is possible to make and readout a superposition state , which does not occur classically .
i believe that in that text , $i$ refers to the magnitude of the current ( a scalar ) , which is assumed to be in the same direction as the length vector $\vec{l}$ ( a vector ) . there is no need for both $i$ and $\vec{l}$ to be vectors . think of current flowing through a wire—if $i$ were a vector ( $\vec{i}$ ) , then the direction of $\vec{i}$ would always be the same as the direction of the wire , because current always flows along a wire . the direction of the wire is already captured by $\vec{l}$ , so it is not necessary to make $i$ a vector quantity also .
no . the standard metric for cosmology is given by : $$ds^{2} = - dt^{2} + a ( t ) ^{2}\left ( d^{3}{\vec x}^{2}\right ) $$ where the term inside the parenthees represents the 3-metric of a homogenous three space . as you can see , there is no difficulty with evaluating the age of the universe : $$ t = \int\sqrt{-g}\ , \ , x^{a}y^{a}z^{a}\epsilon_{abcd} = \int dt$$ where the integral is evaluated from the time when $a = 0$ to now .
since omega centarui is at a declination of -47.5 degrees , you would need to be at a lattitude of 23.5 degrees north ( i.e. . on the tropic of cancer ) for it to get to 20 degrees above the horizon when it crosses the meridian ( due south ) . in theory , if you had a perfectly flat horizon ( and not atmosphere ) , you could see it just on the horizon as it crosses the meridian at a latitude of 42.5 degrees north . to see it from a hill with a negative horizion the lattitude would depend on the amount of negative horizon you had . this assumes that there is no atmosphere . even at 30 degrees above the horizion you are looking through twice as much air as straight up . at 20 degrees you are looking through 3 times as much air and it gets worse as you go to the horizion , this will effectively limit the minimum angle you can look at . omega cen is a 3.7 magnitude object so it is fairly bright but even so , you will loose it in the atmosphere before you get all the way to the horizon .
they accrete gas from a disk , fed by either a wind or roche lobe overflow from their companion . almost all known millisecond pulsars are in binary systems , but i think some in globular clusters may have been disrupted by three-body encounters , so appear to be isolated .
this question has been modified into a more specific form concerning two structures : haemoglobin and the ppn formalism for gravitation . so i shall make some general comments about how to consider this : a linked supplementary question might then be the best means to progress . haemoglobin is a large ( class of ) molecules , which have high complexity . approx molecular weight 68000 ( where hydrogen=1 ) . they are in many terrestrial life forms and they are a key component of blood . there are undoubtedly many outstanding scientific questions concerning the origin and dynamics of such a large molecule . one aspect in particular is the whole issue of " protein folding " which would help form them efficiently and successfully . in terms of physics there are two aspects : fundamental physics aspects and local environment aspects . the latter are the most important in practice with e . g temperature undoubtedly being important ; although pressure ( ie blood pressure ) is likely a function of local gravitation too . so there is the whole question of the biomechanics of a body in differing gravitational fields ( surface of earth , surface of moon , in vacuum , on jupiter , etc ) to consider . as the body is the " manufacturing unit " for this molecule its well being is important too - and vacuum conditions and jupiter conditions are generally considered hostile to life as we understand it . in terms of the fundamental physics and laws , the most important here are those of chemistry which come from quantum mechanics . in a hypothetical different universe with a different electric charge for example undoubtedly different molecules would form ; perhaps no molecules can form , only atoms . in terms of our universe ( whatever gravitational laws really apply : newton , einstein , etc ) the local strength of the gravitational field will be an important parameter in the body formation . in terms of biochemistry it probably only has an indirect effect from its contribution to environment ( air , sea ) pressure and other thermodynamic variables . all these issues from the persective of life history and presence on other planets and environments is important research in the field of astrobiology . the ppn formalism was invented to compare einstein 's general relativity with competing theories via delicate astronomical measurements : but gr remains the best theory in terms of these tests . the best way for a gravity theory parameter to affect quantum parameters would be if , in a theory of quantum gravity , it was found that electric charge , say , was a function of the gravitation constant ( which then might not be constant ) . so that theory might have additional solutions with unusual parameters which might or might not allow the formation of complex molecules .
well everyone at least knows a mnemonic for the planets in the solar system . " my very energetic mother just served us nine pizzas " http://en.wikipedia.org/wiki/planetary_mnemonic also to remember where the minus sign goes in $\sigma_2$ of the pauli matrices i learned : " the minus i rides high on the sigma y " edit : also found this list of some mnemonics http://members.chello.nl/r.kuijt/index-physics.htm
the key idea here is the concept of " power-conjugate " stress and strain-rate measures . for the cauchy stress $\sigma$ , the stress power is given by : $$ \dot w/v = \sigma:d $$ where $d$ is the rate of deformation tensor defined as the symmetric part of the velocity gradient . $$d = sym ( l ) = \frac{1}{2} ( l + l^t ) $$ the quantity $\sigma:d$ gives the stress power per unit volume . therefore , using the explicit time integration scheme : $$ \frac{\delta w}{\delta t} = ( \sigma:d ) v $$ the tensor contraction can be re-written as $\sigma:d = tr ( \sigma^td ) $ . this is most easily observed if you work it out in index notation : $$ \begin{align} \sigma:d and = \sigma_{ij}d_{ij} \\ and = ( \sigma^t ) _{ji}d_{ij} \\ and = ( \sigma^t ) _{ji}d_{ik}\delta_{kj} \\ and = ( \sigma^td ) _{jk}\delta_{kj} \\ and =tr ( \sigma^td ) \end{align} $$ where $\delta_{kj}$ is the kronecker delta . due to the symmetry of the stress tensor , you do not really have to compute $d$ explicitly because $\sigma:d = \sigma:l$ . in short , the off-diagonal terms of the stress tensor do factor into the new energy , but it is just not obvious due to the way the formula is evaluating the stress power .
method the method is based on measuring variations in perceived revolution time of io around jupiter . io is the innermost of the four galilean moons of jupiter and it takes around 42.5 hours to orbit jupiter . the revolution time can be measured by calculating the time interval between the moments io enters or leaves jupiter 's shadow . depending on the relative position of earth and jupiter , you will either be able to see io entering the shadow but not leaving it or you will be able to see it leaving the shadow , but not entering . this is because jupiter will obstruct the view in one of the cases . you might expect that if you keep looking at io for a few weeks or months you will see it enter/leave jupiter 's shadow at roughly regular intervals matching io 's revolution around jupiter . however , even after introducing corrections for earth 's and jupiter 's orbit eccentricity , you still notice that for a few weeks as earth moves away from jupiter the time between observations becomes longer ( eventually by a few minutes ) . at other time of year , you notice that for a few weeks as earth moves towards jupiter the time between observations becomes shorter ( again , eventually by a few minutes ) . this few minutes difference comes from the fact that when earth is further away from jupiter it takes light more time to reach you than when earth is closer to jupiter . say you have made two consecutive observations of io entering jupiter 's shadow at t 0 and t 1 separated by n io 's revolutions about jupiter t . if the speed of light was infinite , one would expect \begin{equation} t_1 = t_0 + nt \end{equation} this is however not the case and the difference \begin{equation} \delta t = t_1 - t_0 - nt \end{equation} can be used to measure the speed of light since it is the extra time that light needs to travel the distance equal to the difference in the separation of earth and jupiter at t 1 and t 0 : \begin{equation} c = \frac{\delta d}{\delta t} = \frac{d_{ej} ( t_1 ) -d_{ej} ( t_0 ) }{\delta t} \end{equation} ( both numerator and denominator can be negative representing earth approaching or receding from jupiter ) in reality more than two observations are needed since t is not known . it can be approximated by averaging observations equally distributed around earth 's orbit accounting for eccentricity or simply solved for as another variable . practical considerations note that you will not manage to see io enter/leave jupiter 's shadow every io 's orbit ( i.e. . roughly every 42.5 hours ) since some of your observation times will fall on a day or will be made impossible by weather conditions . this is of no concern however . you should simply number all io 's revolutions around jupiter ( timed by io entering/leaving jupiter 's shadow ) and note which ones you managed to observe . for successful observations you should record precise time . it might be good to use utc to avoid problems with daylight saving time changes . after a few weeks you will notice cumulative effect of the speed of light in that the average intervals between io entering/leaving jupiter 's shadow will become longer or shorter . cumulative effect is easier to notice . at minimum you should try to make two observations relatively close to each other ( separated by just a few io revolutions ) and then at least one more observation a few weeks or months later ( a few dozens of io revolutions ) . this will let you calculate the average time interval between observations within a short and long time period by dividing the length of the time period by the number of revolutions io has made around jupiter in that period . the average computed over the long time period will exhibit cumulative effect of the speed of light by being noticeably longer or shorter than the average computed over the short time period . more observations will help you make a more accurate determination of the speed of light . you must plan all of the observations ahead since you can not make the observations when earth and jupiter are close to conjunction or opposition . calculations once you collected the observations you should determine the position of earth and jupiter at the times of the observations ( for example using jpl 's horizons system ) . you can then use the positions to determine the distance between the planets at the time the observations were made . finally , you can use the distance and the variation in io 's perceived revolution period to compute the speed of light . you will notice that roughly every 18 millions kms change in the distance of earth and jupiter makes an observation happen 1 minute earlier or later . cost the cost of the experiment is largely the cost of buying a telescope that allows you to see io . note that the experiment takes a few months and requires measuring time of the observations with the accuracy of seconds . history see this wikipedia article for historical account of the determination of the speed of light by rømer using io .
energetic cosmic rays are rare , as @john rennie states in his answer , and the detectors to measure their effect cover kilometers . one high energy entrant creates what is called an air shower and it is measured as @dmckee describes . it is not possible to use them as an incoming beam because we do not know what they are ( except in the case of gamma rays and neutrinos , because they need special detectors ) , so we do not know the incoming beam , and do not know where the interaction happened . they are interesting events but new physics can not be garnered from the observations . the observations are useful for astrophysics though and cosmological theories .
luboš' comment really answers your question , but to more specifically address your comment : the big difference between sound and light is that sound requires a medium to travel in while light does not . in fact light travels best in a vacuum where by definition there is no medium . the reason we can see back 13.5 billion years is because the light has been travelling through an almost perfect vacuum so there is nothing to impede it . the problem with anything travelling through a medium is that no medium is perfectly elastic and there are always energy losses due to viscous damping . this is why when you strike a bell it may ring for a while but will not ring for anything like 13.5 billion years . you could ring the bell and wait for the sound wave to travel right round the earth . you had hear the sound about 120,000 seconds later , so this would allow you to hear a day and a bit into the past . however , unless it was an extraordinarily loud bell the sound would have decayed to below thermal noise in that time so you would not be able to hear it . this is not really an answer to your question , but i mention it because it is cool : cosmic events like supernovae form shock waves when they hit ares of concentrated interstellar gas , and this is arguably a form of sound . in that case you can hear sound from many years ago , though you had need an awfully big ear !
it is mainly an electrostatic interaction through a dipole-dipole interaction . however , the dipole moment can be permanent or induced . depending on its nature the force has a different name : force between two permanent dipoles ( keesom force ) force between a permanent dipole and a corresponding induced dipole ( debye force ) force between two instantaneously induced dipoles ( london dispersion force ) you can also refer vanderwaals force an atom or molecule is usually globaly neutral : ie there is exactly the same number of positive and negative charges . however the center of charge ( barycenter of the qi ) for the positive and negative charges does not always coincide . this gives rise to an electrostatic dipole that can interact with an external electric field . to answer to your first bullet , the electrostaic dipole is not linked to electrons changing orbital . the cohesive forces in solids have two different origin : orbital coupling ( not of electromagnetic nature ) or ionic bonds in ionic crystals ( na+ , cl- ) for instance . in the latter the cohesion of the crystal is due to electrostaic interaction ( not dipolar ) . for the liquids , the electrostatic forces are responsible for the observed properties . polar solutions are made of molecules with a dipolar moment and are able to dissolve ionic crystals .
this is a solenoid and its magnetic field lines . this is a toroid and its magnetic field lines a solenoid by construction has two magnetic poles at the edges when current is flowing through its windings . one can think of a toroid as a solenoid that has been curved and joined so no poles are open . a toroid can have magnetic fields outside its geometrical boundary according to the way the currents are flowing , if there is a circumferential current that has not been neutralized . once neutralized there is magnetic field only inside . ( as described in the link given above ) . a neutralizing design is shown below . in contrast a solenoid will always have two open poles .
let 's suppose we have a source particle and and a test particle a distance $r$ from each other . upon measuring the force on the test particle , we find some value $f_\text{one source}$ . by varying the distance , we discover it depends on the inverse distance squared:$$f_\text{one source}=\frac{c}{r^2} , $$ where $c$ is just some constant . ( no ' charge ' appears here because we have not introduced the concept of charge . ) now let 's bring an additional source particle that is identical to the original , and place it right on top of the original . again , we measure the force , but now we call it $f_\text{two sources}$ . how is this new force related to the original one ? we have no idea , because i have not told you anything about the particles or the nature of the forces . suppose now it is an experimental fact that the force doubles:$$f_\text{two sources}=2f_\text{one source}=\frac{2c}{r^2}=\frac{c}{r^2}+\frac{c}{r^2} . $$ here , the possibility of superposition exists . i have made the superposition part ( where you just add up the contribution due to each source alone ) very explicit . now , suppose instead of the force doubling , the force happened to quadruple :$$f_\text{two sources}=4f_\text{one source} = \frac{4c}{r^2}\ne\frac{c}{r^2}+\frac{c}{r^2} . $$ thus , if the force did not double when you added another identical source particle , superposition would not be possible . it does not matter how you end up ' expanding ' $c$: if you try to do $c=c'q$ or $c=c'q^2$ , you can not get the math to work out ( i.e. . , you can not get the net force on your test particle to be the sum of individual forces ) . this second example ( where the force quadrupled ) is an example of what griffiths means when he says " . . . proportional to the square of the source charge . " one could say $f_\text{two sources}= ( c+c ) ^2/r^2$ and get the right expression for the force , but you could not then go and write the force using superposition .
entropy is an extensive property . if you double the size of your system but keep the conditions within it the same then you will double the entropy . you can not do this without changing other properties of the system because lots of things , e.g. free energy , are also extensive . if you keep the size of your system constant but pack in twice as much gas then the entropy will change for two reasons , firstly because you have more gas , but secondly because you have changed the pressure and/or temperature . you need to specify the state of your system after addition of the gas to work out the change in the entropy .
brian cox was talking about stellar nucleosynthesis . in theory you could combine the nucleus of a helium atom ( also called an alpha particle ) and the nucleus of a carbon atom to produce an oxygen nucleus and some energy in the form of gamma radiation $$ ^4_2he + ^{12}_6c \rightarrow ^{16}_8o +\gamma$$ this is an alpha process , and does happen inside certain stars . in practice , you are unlikely to be able to do this personally , as you probably cannot get the nuclei moving fast enough or accurately enough to enable fusion .
temperature is proportional to the average kinetic energy , not velocity , of the particles . kinetic energy is unbounded ; it goes to infinity as velocity approaches light-speed , proportional to $ ( 1 - v^2/c^2 ) ^{-1/2}$ .
take the right pedal as an example . it uses a right-hand thread , so turning the pedal spindle clockwise ( cw ) relative to the crank will screw the spindle in , counter-clockwise ( ccw ) will unscrew it . say you put the bike in a repair stand , grab the right pedal and gently simulate the motion of someone riding the bike ( always keeping the pedal platform horizontal as if your foot were there ) . the body of the pedal will turn ccw relative to the crank . intuitively you would think this might help unscrew the pedal ! but in reality the clamping torque of the threads will be far , far greater than any friction in the bearings could generate . even if the bearings were to seize , it would be very difficult to unscrew the pedal ( unless it was never tight to begin with ) . now consider someone riding the bike , putting weight on the right pedal . this applies a force perpendicular to the ground , no matter where the foot is in the pedal stroke . relative to the crank arm , this radial force rotates ccw , which - via the process of mechanical precession - creates a cw torque on the pedal spindle ( thus tightening it ) . the rotations on the left side are all reversed , so it must use the opposite threading to prevent the pedal from coming loose . also see " precession " on wikipedia , from where the below illustration was taken ( cc-sa ) .
to establish a frame of reference , you need a reference object and reference direction . which one is inertial is actually a tough question . newton 's first law defines a inertial frame , in which all free particles stay stationary or move at constant velocity . but " free " , or " subject to no force " is ill-defined and impossible : gravitation is universal and you can never know whether the gravitational force of the whole universe combined balance out or not . in practice , we regard some frames as inertial with enough precision . the earth is a rough inertial frame ( more accurately , with the direction pointing to distant stars ) , the sun is better and the milky way is even better . in modern view ( that is , in general relativity 's view ) , all frames are equivalent , and any inertial frame is infinitesimally small . this view solves many logic difficulties with the classical idea of privileged sets of frames of reference .
take the trace of the equation by contracting it with $g^{\mu\nu}$: $$ g^{\mu\nu}r_{\mu\nu}-\dfrac{1}{2}g^{\mu\nu}g_{\mu\nu}r=\dfrac{8\pi g}{c^4}g^{\mu\nu}t_{\mu\nu} $$ as $g^{\mu\nu}r_{\mu\nu} = r$ , $g^{\mu\nu}t_{\mu\nu} \equiv t $ and $g^{\mu\nu}g_{\mu\nu} = 4$ , the previous equation gives you $r = -\dfrac{8\pi g}{c^4}t$ . substituting this into einstein 's equation shall give you the result .
muonic atoms should be stable in electron-degenerate matter ( white dwarf material ) as long as the fermi energy is more than $m_\mu - m_e$ . this is more or less exactly a analogy with neutron stability in the nucleus where the the protons are effectively in a degenerate state . any answer has to forbid electrons ( which is not going to be possible as they share almost all quantum numbers with the muon ) or have electron be degenerate with the stated fermi energy .
physicists collide particles to study their behavior under extreme conditions . new unknown particles can be created in high energy collisions or new unknown processes may be observed . our equations describing the particles are predicting certain behavior and physicists are testing if the particles really behave like that . and they are hoping to find a discrepancy to locate a problem in our theories and improve them . the short lived particles are short lived not only in our laboratories , but also in space . they are created also in collisions , but not in accelerators . certain processes in the space are powerful enough to accelerate and collide particles at much higher energies than our most powerful accelerator . if a short lived particle is created in such process , it decays quickly , like in the earth laboratories . long lived high energy particles ( protons , neutrions , electrons ) are flying through the space everywhere and they are even hitting the earth . every day there are lots of particles arriving to the earth from the space that have much higher energy than protons in lhc .
good to ask about these historical controversies . it makes sense to review the controversy . in the medium , the density of momentum was ( i set $c=\hbar=1$ everywhere ) $$e\times h$$ according to max abraham - that is equal to the poynting vector which determines the flow of energy according to everyone - and $$d\times b$$ according to hermann minkowski who argued that in the medium , the stress-energy tensor is asymmetric . in both cases , one can start with the product of the vacuum fields $e\times b$ and replace one of them by the material " variation " of the field . the two guys differed by whether they replaced the electric or the magnetic factor . consequently , a photon according to the first , abraham form will have momentum $$p=\omega/n$$ while the second , minkowski definition will give $$p=\omega n$$ now , one may realize that the ( complexified ) electromagnetic wave depends on the position and the time as $$\exp ( -i\omega t+i k x ) $$ where $k=\omega n$ . the phase velocity is simply $\omega/k$ and it should be $1/n$ , smaller than one , so it is clear that $k=\omega n$ . so the wave function shows clearly that the conserved momentum of a single photon agrees with the minkowski 's template . by definition , the momentum is given by the $k$ . barnett agrees that the minkowski momentum is what generates normal translations - of the vector potential and other fields - by conjugation . if you try to find out what are the arguments ( and interpretations ) that support the abraham form , i think that all of them are incompatible with modern physics . as far as i can say , they are all based on the idea that the momentum of any particle should be equal to the " mass times velocity " . this is an extremely shaky assumption that is not really true in this case . in modern physics , the momentum has to be defined by a solid definition - and it is the " quantity conserved , as showed by noether 's theorem , because of the spatial translational symmetry " . one can show that this is $m_{total}v$ for ordinary particles in the vacuum but one cannot show this thing for a photon in a material - especially because it is not true . it is very bizarre for barnett to say that the " canonical momentum is more subtle than the $mv$ momentun " . the " canonical momentum " is the only momentum that is acceptable in modern physics and that may be generalized to new contexts . it is the only noether-derived momentum . in this case , it is the minkowski 's momentum . the " kinetic momentum " , as he calls $mv$ , is just an attempt to return to the 17th century as much as he can . for charged particles , there could be issues about $i\partial$ versus $i\partial-ea$ , the velocity operator , but this does not occur for $a_i$ in the dielectric because the wave function of the photon cannot be transformed away by any gauge symmetries . under the u ( 1 ) gauge symmetry with an oscillating parameter , $a$ may shift by a constant but it will not change the speed of its oscillations . the other arguments supporting the abraham form seem to be bogus , too . for example , the abraham form is supported by the argument that the center-of-mass-energy continues in a uniform motion as the photon enters the material . however , this is an invalid assumption , too . the uniform motion of center-of-mass only holds - again according to noether 's theorem - in systems which respect the equivalence of inertial frames e.g. the lorentz ( or galileo ) symmetry . this symmetry is explicitly broken by the boundary between the vacuum and the dielectric material , so there is no reason why the corresponding conservation law - the conservation of the velocity of the center-of-mass motion - should hold . to summarize , minkowski was simply right while abraham was wrong . but it is probably ok to " redo " the budget in such a way that a part of the momentum of the photon is attributed to the dielectric material when the photon enters it , and then it is returned back to the photon . in this way , one may justify the abraham 's form - and probably many other forms - but why should one really do it ? stephen barnett who wrote the 2010 paper above is a respectable optician but this whole thing was much ado about nothing . if you care about the sociology , i am pretty confident that almost no people in optics will oppose barnett , and the people from other disciplines may say that this was really a trivial dispute . minkowski was really right , and by the way , he was a much better physicist than abraham from all other angles , too . for example , abraham 's model of the electron and the arguments used to support this model were truly painful . that is a very different league than minkowski who was one of the first people to have understood special relativity . i do not think that throughout the century , this problem was being studied by top physicists . to make things worse , the experiments are never testing these things " directly " , especially because one may always choose different interpretations where the momentum goes when the photon is changing the medium . best wishes , lubos p.s. : let me mention one detail : minkowski 's stress-energy tensor is asymmetric , so if you define the density of the angular momentum purely in terms of this tensor , multiplied by $x$ in the usual way , the angular momentum will not be conserved . however , it is not a problem because this can be compensated by an extra contribution to the angular momentum that comes from the volume and surface density of the spins - including the internal angular momenta of the atoms of the materials . there are many ways how to attribute the conserved quantities ( energy , momentum , angular momentum ) to regions - only the total has to be conserved . the diverse interpretations will differ about all the functions , but they should ultimately predict the same predictions for the experiments when done right .
this equation is called the ray equation and it can indeed be derived from fermat 's principle . i guess you can find more about its derivation in , e.g. , born and wolf 's principles of optics or in fundamentals of photonics by saleh and teich .
you can not see clearly underwater for a couple of reasons . one is the thickness of your lens , but the main one is the index of refraction of your cornea . for reference , here 's the wikipedia picture of a human eye . according to wikipedia , two-thirds of the refractive power of your eye is in your cornea , and the cornea 's refractive index is about 1.376 . the refractive index of water ( according to google ) is 1.33 . in water , your cornea bends light as much as a lens in air whose refractive index is $$\frac{1.376-1.33}{1.33} + 1 = 1.034$$ that means you are losing about 90% of your cornea 's refractive power , or 60% of your total refractive power , when you enter the water . the question becomes whether your lens can compensate for that . i did not find a direct quote on how much you can change the focal distance of your lens , but we can estimate that your cornea is doing essentially nothing , and ask whether your lens ought to be able to do all the focusing itself . for a spherical lens with index of refraction $n$ sitting in a medium with index of refraction $n_0$ , the effective focal length is $$f = \frac{nd}{4 ( n-n_0 ) }$$ the refractive index of your vitreous humor is about 1.33 ( like water ) , and the refractive index of your lens , according to wikipedia , varies between 1.386 and 1.406 . let 's take 1.40 as an average . then , plugging in the numbers , the effective focal distance of a spherical eye lens would be five times its diameter . the wikipedia picture of a human eye makes this look reasonable - a spherical lens might be able to do all the focusing a human eye needs , even without the cornea . the problem is that your eye 's lens is not spherical . from the same wikipedia article in many aquatic vertebrates , the lens is considerably thicker , almost spherical , to increase the refraction of light . this difference compensates for the smaller angle of refraction between the eye 's cornea and the watery medium , as they have similar refractive indices . [ 2 ] even among terrestrial animals , however , the lens of primates such as humans is unusually flat . [ 3 ] so , the reason you can not see well underwater is that your eye lens is too flat . if you wear goggles , the light is refracted much more as it enters the cornea - the same amount as normal . if you want to wear some sort of corrective lenses directly on your eye like contact lenses , they should have a refractive index as low as possible . googling for " underwater contact lens " , i found an article about contact lenses made with a layer of air , allowing divers to see sharply underwater .
this must be impossible , even for lady castafiore with her earthquake voice . for a glass to break by sheer sound you need to produce a tone equal to the glass 's natural frequency - the frequency at which a body vibrates with the least amount of energy . in other words : there you get the most vibration with a minimum of effort . this is also called resonance . however , it is much harder to create resonance in mixed materials because each component has a different fundamental frequency . so two layers of different types of glass will effectively prevent resonance . of course , apart from that the mere thickness of the material will be a problem for our nightingale .
$\varphi=15°$ ; $m_t=30 kg $ ; $\mu _c =0,25 $ ; $ d=130m$ ; newton 's 1st law : $$\sum f_x = 0 \implies t_x-\mu _c m_tg cos \varphi - m_tgsin \varphi= 0$$ $$\sum f_y = 0 \implies t_y-m_tg cos \varphi = 0$$ $$\begin{pmatrix} t_x \\ t_y \end{pmatrix} = \begin{pmatrix} \mu _c m_tg cos \varphi +m_tgsin \varphi \\ m_tg cos \varphi \end{pmatrix} = \begin{pmatrix} 1.23 \\ 0.96 \end{pmatrix} n $$ after you find $t_x$ and $t_y$ , you can find the resultant vector which will be in the direction of $\vec d$ . the work done then is simply : $$w=\vec t \bullet \vec d = |t||d|$$
the mass $m$ in the formula is not the rest mass $m_0$ and therefore dependent on the velocity : $$ m = \frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}} \equiv \gamma m_0 $$ this means you cannot simply take the normal mass of a nitrogen atom and put it in there , if you assume a speed $v \neq 0$ . the $c$ in the formula does not mean that the particle travels at light speed . it is a factor , nothing more . the actual particle velocity comes into play by the mass $m$ with the above relation . there is also an expression for the energy in terms of the rest mass and momentum : $$ e^2 = p^2c^2 + m_0^2c^4 $$ this might remind you of pythagoras ; ) in fact this equation is even better because it is also valid for massless particles like the photon , where $e = mc^2$ is only valid for massive objects . you can easily see with the help of the first equation that if $v$ approaches $c$ the term for $m$ becomes $\infty$ and therefore $e$ becomes also $\infty$ . this means that it is impossible to accelerate a particel with a rest mass $m_0 \neq 0$ to light speed .
line integrals of the magnetic field strength are magnetic voltage drops . just google for " magnetic voltage drop " ( including the double-quotes ) . in the quasi-static case ( $\dot{\vec{d}}=\vec{0}$ ) the $\vec{h}$-field within a simply path-connected domain with zero current density has a magnetic potential . in this case you can calculate the magnetic voltage drops as potential differences . the magnetic voltage caused by a winding is called current-linkage or ampere-tuns . from the mathematical point of view the closed path integral of the magnetic field strength is a circulation .
you are definitely taking this picture too seriously . hybridization of atomic orbitals is just an approximation based on an independent particle model created by pauling to rationalize some structural trends in chemistry with quantum mechanics . we can only assign orbitals unambiguously for one-electron systems , though of course independent particle models were widely employed and still are to explain reactivity trends , etc . anyway , if you prepare an electron in an sp3 state , which is an equally weighted linear combination of the px , py pz and s orbitals , the probability to find and electron at a certain angular momentum might be calculated by taking the square of the projection of this angular momentum on the wave function , as qm tells us to do . do not worry too much if you find weak spots in general theories developed to explain molecules , reactivity , etc , since they are highly approximated in most cases ( unless you do an expensive computation for a system , but then it is not general anymore ) , and the guys that developed those such as pauling already knew that . as you know these highly approximated theories ( e . g . , molecular orbital and valence bond theory ) were and still are very successful , even if they are not very rigorous . it gets really hard to be rigorous in chemistry beyond a certain point . . . @tedbunn of course the basic formalism of qm would predict properties of this hypothetical electronic state , i did not say anything that would imply the opposite . still , to think about bonding in terms of sp3 hybrid orbitals such as one does in general chemistry is really a very crude picture , unless you are dealing with molecules that have very special properties such as td symmetry ( methane , for example ) , and still in those cases if you perform a calculation by using vb or mo ( hartree-fock ) theory ( and it does not even need to be with a computer , since symmetry is going to make life very easy here ) with only the sp3 orbitals of carbon and s of hydrogen in your set of basis functions , you will see a very big quantitative error in predicted ionization energies when you compare to photoelectron spectroscopy measurements of ionization energies . another good test would be to perform high level quantum chemistry calculation of methane using a software and employing two different basis sets , one containing only s and p functions centered on carbon and s functions centered on hydrogen atoms , and another in which the basis set has functions of several angular momenta centered on each atom . if you compare your results with experiments you will see that the first method will have a much lower accuracy compared to the second one . s and p functions might still be the ones that contribute mostly to the bonding molecular orbitals in the different orbital configurations ( set of occupied molecular orbitals ) that contribute to a good description of methane , but we can only say that for sure for very special cases . when you go to molecules with very distinct geometries , it is vital to have d functions centered on carbon , for example , in order to determine even qualitatively right chemistry trends . quantum chemistry semi-empirical methods widely used in the past often had this problem of only employing valence basis functions centered on an atom in molecule calculations , not giving enough flexibility for the description of molecular geometries , and therefore providing bad results whenever the geometry deviated too much from what the qualitative models ( such as the hybridization model ) predicted . in fact , i would really be surprised ( but not too much ) if any modern paper that discusses chemical bonding , or that qualitatively discusses bonding orbitals to explain computed trends , do that by employing hybridization theory arguments . those that i have seen almost always do that by looking at molecular orbitals or some refined valence bond treatments . therefore , although hybridization is a deep and important topic that every chemist should dominate , in the way it was developed by pauling it has severe limitations that people are , or should be , aware of .
there are two considerations : a ) if you follow a bit the current theories , the manifestation of the higgs decays can vary , depending on the parameters of the models , and there are many models . b ) hadron colliders , in contrast to e+e- ones are " dirty " , there are enormous backgrounds that have to be understood , and the few clear decay channels proposed by theories have small crossections and have to be fished out of these backgrounds . so more data than the ones existing are necessary to be able to answer definitively the higgs question . lhc is now building up the statistics necessary . so what i foresee will happen , if we are lucky , is first a new resonance in some of the channels checked will be declared with at least 4 sigma . then everybody will fall on it , experimentalists to find other decay channels and theorists to fit specific models to decide whether it is the higgs or not .
if you are not interested in relativistic effects , the answer to your question is easy to workout . according to wikipedia , alpha centauri is 4.24 ly away ( 4.0114x$10^{16}\mathrm{m}$ ) . so to get there in 60 years ( $1892160000\mathrm{s}$ ) . so your non-relativistic answer is $v = \frac{d}{t} = \frac{4.0114 \times 10^{16}}{1892160000} = 21200000 \mathrm{m}\ , \mathrm{s}^{-1}$ . this is 21200 $\mathrm{km}\ , \mathrm{s}^{−1}$ . the fastest recored space flight was 24,791mph which is around 11$\mathrm{km}\ , \mathrm{s}^{−1}$ which is 0.05% of 21200$\mathrm{km}\ , \mathrm{s}^{−1}$ . this means we have to be able to get spaceships to travel 2,000 times faster than the fastest current spaceship . note , i believe satellites in geostationary orbits do $\approx 17\mathrm{km}\ , \mathrm{s}^{−1}$ .
it is possible to make an estimate of the amount of baryons in the observable universe . but it is more difficult to make an estimate of anything else . it gets particularly more difficult when you consider things like photons , because they can pop in and out of existance , i.e. the number of photons is not constant . actually , that is true for all elementary particles , since they are considered as excitations of quantum fields in most modern physics theories . thus , the number of particles is not constant . but the heavier the particle , the less likely it will pop in existance . and if it is too heavy , it'll decay in lighter , more stable particles . so maybe you will have something a couple of orders bigger than the amount of baryons , but probably not much larger . then , there is dark matter , of which we do not really know much . so , i have no idea if an estimate has been attempted of the amount of dark matter particles . any estimate will be highly dependent on the theory we have for these particles . in his book the emperor 's new mind , penrose estimates the number of baryons in the observable universe to be of the order of $10^{80}$ . this seems to confirm it .
what constituent of internal energy does an electron excitation represent ? you can think of electrons as just like planets orbiting the sun and get the correct answer to this question . an electron in a higher energy level has less kinetic energy , but more potential energy as it is ( generally ) farther from the nucleus . the net result is more energy . is it only lower frequency photons that interact with chemical bonds ? vibrational transitions typically require an amount of energy that corresponds to a photon in the infrared region of the spectrum . rotational levels are even closer , and are usually measured using combined rotational-vibrational transitions . this leads to the usual ladder of spectroscopy : vibration ( with or without rotation ) : infrared electronic excitation ( outer electrons , or small atoms ) : visible / ultraviolet ( uv ) electronic excitation ( inner electrons , large atoms ) : x-ray nuclear excitation : gamma-ray
*when peeling a sticker off its base , the immediate reaction is that it curls ; why is this ? on a somewhat related note , is this a similar reason for why a ribbon curls when glided over with a blade ? * the two phenomena are not different : in the second case you you pull the ribbon across the blunt side of the blade of the scissors ( or other tool ) , and at the same time you press a finger or your palm against the ribbon on the other side , creating a sharp angle of roughly 180° ( shaped like a u ) around which the ribbon has to bend . this bending takes the fabric beyond its elastic limit and leaves it permanently bent . in the first case you have the same phenomenon , the glue might some special properties but its role is negligible : usually when you remove a sticker you pull it almost horizontally holding it with your thumb while sliding your index finger on its surface , and thus making an angle of roughly 170° ( allowing for the thickness of your finger ) . plastic material is made by polymers and these are deformable only to a certain extent : synthetic fibres can withstand a certain amount of stretching or bending without being permanently deformed . if the fibres are deformed too much , the polymer molecules cannot be straightened again , so the shape of the fibres is permanently changed . from wiki : plasticity in amorphous materials crazing in amorphous materials , the discussion of “dislocations” is inapplicable , since the entire material lacks long range order . these materials can still undergo plastic deformation . since amorphous materials , like polymers , are not well-ordered , they contain a large amount of free volume , or wasted space . pulling these materials in tension opens up these regions and can give materials a hazy appearance . this haziness is the result of crazing , where fibrils are formed within the material in regions of high hydrostatic stress . the material may go from an ordered appearance to a " crazy " pattern of strain and stretch mark please note that ribbons for decoration are called curling ribbons because the are expecially made to curl , or usually poly ribbons as they are made from polymers , like the stickers . sometimes they are still made of cotton or silk ( which is very expensive )
for the sorts of vehicles we are used to , like cars and aeroplanes , there are two contributions to drag . there is the drag caused by turbulence , and the drag caused by the effort of pushing the air out of the way . the streamlining in cars and aeroplanes is designed to reduce the drag due to turbulence . the effort of pushing the air out of the way is basically down to the cross sectional area of whatever is pushing it is way through the air . turbulence requires energy transfer between gas molecules , so you can not get turbulence on length scales shorter than the mean free path of the gas molecules . the wikipedia article on mean free paths helpfully lists values of the mean free path for the sort of gas densities you get in space . the gas density is very variable , ranging from $10^6$ molecules per cm$^3$ in nebulae to ( much ) less than one molecule per cm$^3$ in intergalactic space , bt if we take the value of $10^4$ in the table on wikipedia the mean free path is 100,000km . so unless your spaceship is very big indeed we can ignore drag due to turbulence . a sidenote : turbulence is extremely important in nebulae , and a quick glance at any of the hubble pictures of nebulae shows turbulent motion . however the length scale of the turblence is of the order of light years , so it is nothing to worry a spaceship . so your spaceship designer does not have to worry about the sort of streamlining used in aeroplanes , but what about the drag due to hitting gas molecules ? let 's start with a non-relativistic calculation , say at 0.5c , and use the density of $10^4$ i mentioned above , and let 's suppose that the gas is atomic hydrogen . if the mass per cubic metre is $\rho$ and you are travelling at a speed $v$ m/sec then the mass you hit per second is : $$ m = \rho v $$ suppose when you hit the gas molecules you accelerate them to match your speed , then the rate of change of momentum is this mass times your speed , $v$ , and the rate of change of momentum is just the force so : $$ f = \rho v^2 $$ a density of $10^4$ atoms/cm$^3$ is $10^8$ per m$^3$ or about $1.7 \times 10^{-19}$kg and 0.5c is $1.5 \times 10^8$m/sec so $f$ is about 0.004n per square metre . so unless your spaceship is very big the drag from hitting atoms is insignificant as well , so not only do you not worry about streamlining , you do not have to worry about the cross section either . however so far i have only talked about non-relativistic speeds , and at relativistic speeds you get two effects : the gas density goes up due to lorentz contraction the relativistic mass of the hydrogen atoms goes up so it gets increasingly harder to accelerate them to match your speed these two effects add a factor of $\gamma^2$ to the equation for the force : $$ f = \rho v^2 \gamma^2 $$ so if you take v = 0.999c then you get $f$ is about 7.5n/m$^2$ , which is still pretty small . however $\gamma$ increases without limit as you approach the speed of light so eventually the drag will be enough to stop you accelerating any more . incidentally , if you have a friendly university library to hand have a look at powell , c . ( 1975 ) heating and drag at relativistic speeds . j . british interplanetary soc . , 28 , 546-552 . annoyingly , i have googled in vain for an online copy .
faraday 's law fails here . let 's go back to basics . we use the lorentz force . and what is happening is as the rod rotates , the charges in it rotate too . however , the rod is neutral so there is no net current flowing . now field of the bar magnet is towards left in the wire , the lorentz force applies on the protons and electrons inside the wire , causes the electrons only to move in the wire circularly as the force on the electron is towards centre of the winding ( radially inwards ) and the force on the protons is radially outwards which gets balanced by constraint forces of the wire . thus only , electrons flow . this causes , a net current to flow and thus we see the effect as an emf . after a certain instant , there is an accumulation of negative charges at one end after which no more accumulation will take place . now regarding the direction of the current flow , it could have flown both ways by this logic . to find direction , now use the numerical values which are give as only one direction will give the matching value of the current in the question as the $\vec{v}$ for the electrons , you need to take it from the frame of the magnet .
anomalies ( not anamolies ) are a whole subject whose basics are covered by one or several chapters of almost any good enough quantum field theory textbook so it is counterproductive to retype this whole chapter here . but generally , in quantum field theory , anomalies are quantum mechanical effects breaking symmetries that exist in the classical theory – quantum mechanical contributions that are zero in the classical theory because of the symmetry but that are inevitably nonzero in the quantum theory because the quantum completion does not allow all the symmetries to be preserved . an anomaly in a global symmetry is a physical effect that changes the dynamics but keeps it logically consistent ; a gauge anomaly – the quantum mechanical breakdown of a gauge symmetry – renders the theory inconsistent because the gauge symmetry is needed to decoupled the unphysical ( negative-norm ) polarizations of the gauge boson so it should never be broken . anomalies appear because certain divergent integrals cannot be simultaneously set to zero . they appear in theories that admit left-right asymmetry , especially in even spacetime dimensions . the simplest feynman diagrams that quantify the anomaly are $n$-gons , polygons , where $n=d/2+1$ where $d$ is the spacetime dimensions . so importantly enough , anomalies in the $d=10$ dimensional effective theories resulting from superstring theory are calculated using hexagon feynman diagrams . in the undergraduate example of $d=4$ , anomalies are given by $n=3$ i.e. triangle feynman diagrams . a fermion – left-right-asymmetric particle – is running in the loop of the feynman diagram . three gauge bosons are attached . the diagram is proportional to something like $${\rm tr} ( q^3 ) $$ where $q$ is a generator of a $u ( 1 ) $ subgroup of the gauge symmetry , a charge . all such traces of cubic expressions have to vanish for consistency , and they indeed vanish in the standard model – but a cancellation between quarks and leptons is necessary for that . the standard model with leptons only or quarks only would be inconsistent . the mssm has the same chiral spectrum as the standard model with one exception : there are extra higgsinos . they contribute some extra term to the anomalies that are not cancelled by anything . the arguably simplest way to cancel the anomaly is to add not one higgs doublet ( one doublet of superfields ) but two doublets , with mutually opposite values of the charges ( assuming the same chirality ) . so these two mssm higgsinos cancel the anomalies against one another . the two higgs doublets in the mssm are important for another reason : one higgs doublet is only able to give masses to the upper quarks only ; or the lower quarks only . one actually needs both higgs doublets , the up-type and the down-type , to allow all quarks to become massive .
first off , physics tends to provide a very good background for people who move on to study problems in other areas , which is perhaps why there is a lot of cross-over to computer science . however , there are also a number of areas at the interface of computer science and physics which attract people from both sides : computer hardware ( which is generally based on semiconductor physics ) . large scale simulations physics of computation ( quantum computing , reversible computing , etc . ) theoretical computer science etc . of these , perhaps the last one ( tcs ) seems the most surprising . however , in recent years , there has been significant success in applying ideas from thermodynamics and statistical mechanics to problems in computational complexity . an example of this would be the simulated annealing algorithm which works extremely well for optimization problems , as well as work done on phase transitions in 3sat .
the use of the term " heisenberg limit " is somewhat misleading for outsiders ( that is non-quantum interferometers ) . if we recall the heisenberg uncertainty principle is a limit on simultaneous measurement of two complementary variables . in the case of ( quantum ) metrology one is only interested in the measurement of a single variable to high accuracy , and this does not ( directly ) conflict with the hup . in the case of interferometry the variable of interest is $\delta \phi$ the phase difference between two waves detected in two arms . in basic interferometry there were some limits as to how accurately this could be measured : quantum shot noise : $\delta \phi = 1/n^{1/2}$ heisenberg limit : $\delta \phi = 1/n $ here the n corresponds to how many quanta are required for the given accuracy , so the second is more accurate when it can be achieved , as was eventually done using entangled states , and perhaps squeezed light . if you cannot use these features of qm one gets just the quantum shot noise accuracy . well a few years ago it was noticed that the assumption behind the heisenberg limit calculation was that the hamiltonian was quadratic in its ( key ) variables : this corresponded to the assumption of linearity amongst the measuring quanta . if the hamiltonian could be made non-linear then an improvement on the heisenberg limit would be possible . this interaction between the measuring photons is discussed in the given paper , in arxiv form here .
this is almost a duplicate to can you magnetize iron with a hammer . have a look at it . the only difference is that the rail lines are fixed in their north south direction for years . the iron in the lines themselves become magnetized and so the argument with the hammer should also hold , i.e. the small magnetic domains , momentarily freed by the impact , reorient to the magnetic field direction of north south of the earth 's magnetic field . if the coin partially melts under the weight of the train , even better . it should of course be made from a ferromagnetic metal .
no , a massive body is able to bend light around it , which is called gravitational lensing . this has been observed multiple times . edit photons are massless . otherwise , they would not travel at the maximum speed , which is called speed of light . keep in mind , that gravitational lensing is not a part of newtonian mechanics . you need general relativity for that . and in the context of general relativity , it is not mass , which exerts gravitation , but energy . and photons clearly carry energy . so , you could produce a gravitational well ( even a black hole ) entirely without massive particles .
neglecting air resistance ( whether this is a good idea or not is besides the point ) , suppose you drop the egg from height 10.0m , and it fell to a height of 0.10m , and then rebounded to a height of 9.0m . what can you deduce from this fact ? edit : since you want something a bit more along the modelling side , if you know how the tension changes with temperature , you can deduce the change in entropy with change in length at fixed temperature using maxwell relations . you can assume the rubber band obeys $du = \delta q + t dl$ where $t$ is the tension and $l$ is it is length . you can then use more thermodynamic cleverness to determine the change in temperature with length for adiabatic stretching , which allows you to figure out the irreversible component ( lost as heat ) . google for thermodynamics+rubber+elasticity should get you started . more links : http://physics.oregonstate.edu/~roundyd/courses/ph423/lab-2.pdf ftp://ftp . ccmr . cornell . edu/tmp/mse-4020/4020-notes-7-text-book-rubber-elasticity . pdf
the main effect of an electromagnetic wave is basically that the electric field in the electromagnetic wave shoves charged particles around ( ions and/or electrons ) . that is called " electric dipole coupling " . electric dipole coupling is almost always much stronger than other effects of the electromagnetic wave . for example , the electromagnetic wave has a magnetic field too , which can exert forces on molecules . this " magnetic dipole coupling " is a much smaller effect than the electric dipole coupling . shoving an electron around ( the electric dipole coupling ) does not directly change or rotate the electron 's spin . it only changes the spin a little bit , due to " spin-orbit coupling " , an effect related to special relativity ( in the " rest frame " of the moving electron , the electric fields get converted to magnetic fields , which torques the spin ) . therefore , since spin-orbit coupling is a weak effect ( unless the electron is traveling near the speed of light ) , the coupling between electromagnetic waves and molecules cannot usually involve a change of spin . that means that a spin-singlet molecule absorbing light almost definitely goes into a spin-singlet excited state . conversely , it is exceedingly unlikely for a spin-triplet excited state to emit light while changing into a spin-singlet ground state . by the way , another possibility is for the magnetic field of the light wave to directly torque the spin of the electron . this effect is normally even weaker than the effect of the wave via the spin-orbit effect .
it is true that by noether 's theorem energy is conserved when the action is depednent on position only and not time . however , i think they want you to write down the equation of motion ( use $f=ma$ ) and then use that to show that the total energy is a constant . write down the expression for total energy and take its derivative w.r.t. time .
with a potential $v ( x ) = - \frac{\alpha}{|x|}$ , with the notation $a = \large \frac{\hbar^2}{m \alpha}$ , solutions are : $$u^+_n ( x , t ) \sim x e^{ - \large \frac{x}{na}} ~l_{n -1}^1 ( \frac{2x }{na} ) e^{ -\frac{1}{\hbar} \large e_nt}~~for~~ x&gt ; 0$$ $$u^+_n ( x , t ) = 0~for~~ x\le0$$ and : $$u^-_n ( x , t ) \sim x e^{ + \large \frac{x}{na}} ~l_{n -1}^1 ( \frac{2x }{na} ) e^{ -\frac{1}{\hbar} \large e_nt}~~for~~ x&lt ; 0$$ $$u^-_n ( x , t ) = 0~for~~ x\ge0$$ whose energy is : $$e_n = - \frac{1}{n^2} ( \frac{m \alpha^2}{2 \hbar^2} ) $$ $l_n^\gamma$ is the generalized laguerre polynomial [ edit ] there are 2 different set of basis functions , see this reference page $192$ formulae $20a$ and $20b$
assuming the projectile had enough velocity to escape the gravitational potential of the " projector " ( system that fired it ) . and assuming that space-time is otherwise basically flat . then , yes .
the wedge is tangent to the sphere . using that it touches at height r/5 you can easily work out the slope of the wedge . the velocity of the sphere follows directly ( 20m/s times slope ) .
one should not expect to have a " good " formula for the local isometric embeddings of a constant negative curvature surface in euclidean $\mathbb{r}^3$ . this is due to a little theorem proved by david hilbert around 1901: theorem there does not exist a smooth immersion of the hyperbolic plane into euclidean 3 space . the theorem has been further studied in the years following . in 1961 efimov showed that any complete surface with curvature strictly bounded above ( that is to say , if there exists a negative number $k_0 &lt ; 0$ such that the gaussian curvature is always strictly less than $k_0$ ) cannot admit a smooth ( twice continuously differentiable ) isometric immersion into euclidean three space . that is to say , if you try to " extend " any surface in euclidean 3 space that satisfies constant negative curvature , you are guaranteed to hit a singularity . in particular , you cannnot expect the surface to be described by $f ( x , y , z ) = 0$ where $f ( x , y , z ) $ has a nice algebraic expression ( say , polynomial ) and has smooth level sets . typically the image one usually use to illustrate the notion of negative ( but not constant ) curvature is the graph $$ z = x^2 - y^2 $$ which produces a classical saddle , or the catenoid whose gaussian curvature , while everywhere negative , is not constant . ( though it has constant [ in fact everywhere vanishing ] mean curvature . ) lastly , however , despite the above , it is possible to embed " patches " of hyperbolic plane into euclidean 3 space . there are many ways of doing so ( one can search for the term pseudosphere ; though some people use the same term for the hyperboloid/de sitter spaces embedded in higher dimensional minkowski space ) , but one of the more well-known is the tractricoid . ( see wiki entry here . ) parametrically in cylindrical coordinates $ ( z , r , \theta ) $ the surface can be described by : $$ \mathbb{r}_+\times\mathbb{s}^1 \ni ( t , \omega ) \mapsto \left ( z=\frac{1}{\cosh t} , r=t-\tanh t , \theta = \omega\right ) $$ and has constant negative curvature .
i am not sure , but i think your objection here is that the times measured by these observers for the interval between the " a1 meets b1" and " a2 meets b2" events is the same , even though they are in frames that are moving relative to one another . so should not there be some kind of time dilation ? this is not a problem , though . the familiar relativistic time dilation formula has to do with the time t ' you will measure between two events , relative to the time t between the events in the frame where they occur at the same location--that is , their rest frame , the frame in which a single stationary clock could mark time for the two events . t'/t = gamma , the time dilation factor . what is the rest frame defined by these two events ? it is not the frame moving with either train . it is a third frame : the frame in which the two trains are moving in opposite directions with the same speed ! ( call it the " center of velocity " frame . ) in this frame , a1 passes b1 and a2 passes b2 at exactly the same place . relative to this center-of-velocity frame , both train frames are moving at the same speed , just in opposite directions . so the elapsed time between the two events gets time-dilated to exactly the same extent to observers on the two trains . the measured time will not be the same in every frame ; but it will be the same in those two particular frames !
the central point of the question is somewhat ambiguous , but here is an effort to answer it . i am sorry in advance if i have misunderstood it . does light/photons travel ? the question whether light travels from place a to place b or not , can be answered mainly by experience and experiment/observation . when you hold a torch in the dark and you aim it at some point in the background where it is dark , you can see its effects almost immediately . from having being dark , now it is bright and you can see the objects that exist there . that means that light not only travelled there and illuminated the area , it also came back to your eye to give you the information about the objects . this means that light has not always been there , suspended in the air , waiting for you to turn the torch on and make it become reality . i don’t think this is how you envision it . does light “feel” the existence of space ? this type of questions touch on the borders of ontology , somewhat . it is not very easy to formulate answers because one has to talk in terms of metaphysical notions and concepts which , unfortunately , fall outside the scientific method of thinking . but let us take a look at it from this point of view : imagine we send a laser beam from one side of our room to the other . watching it without an apparatus it looks as if light did not have to travel at all , it looks as if the event evolved instantly . a very sensitive apparatus , however , can sense that light has actually taken some time to go there and back . the situation can become more obvious if we try to send the laser beam to the moon and back ( this has been done . ) even we , without any apparatus , can tell that the distance involved must be huge . so space becomes important and even light “feels” the vastness of it . in the experiments you mentioned , the extremely sensitive detectors can distinguish photons arriving with a time difference just a few nanoseconds or less , due to the slightly different paths they take ( space becomes very important in less obvious ways ) light can even “feel” the geometry of space-time , as is demonstrated by the deflection of light-rays passing near the surface of the sun , during a total solar eclipse . light can “feel” the immense density of a bose-einstein condensate by slowing down to incredibly low speed . you can run fast enough and catch up with it ! ! the question whether or not light takes a well defined path to go from a to b involves quantum mechanics , and from your comment i read that it does not interest you at the moment ( ? )
concerning $\tilde{x}$ , it is self-adjoint ( the proof is easy ) if it is defined on its natural domain $d ( \tilde x ) $ . for $\gamma_n ( x ) := ( n+1/2 ) \delta$ if $n \delta \leq x &lt ; ( n+1 ) \delta$ and $n \in \mathbb z$ $$d ( \tilde{x} ) = \left\{\psi\in l^2 ( \mathbb r ) \:\left|\:\int_{\mathbb r} |\gamma_n ( x ) \psi ( x ) |^2 dx &lt ; +\infty \right . \right\}$$ the spectrum is $\sigma ( \tilde{x} ) = \{\delta ( n+1/2 ) \:|\: n \in \mathbb z\}$ as expected . the fact that your claimed approximated momentum operator $\tilde{p}$ makes any sense as it stands is questionable . the point is that $\tilde{p}$ defined this way is not self-adjoint but only symmetric , while observables must be self-adjoint in order to exploit the spectral calculus technology . to obtain a symmetric operator ( i.e. . hermitean and densely defined ) i think that a possibility is to define its domaine $d ( \tilde{p} ) $ as a space of $c^1$ functions ( that vanish in $x_n= \delta n$ for all $n \in \mathbb z$ with their first derivative for a reason i discuss shortly ) also requiring that these functions rapidly vanish with the first derivative for $|x|\to \infty$ . it should be possible to relax the smoothness condition using weak derivatives , but the situation does not seem to change remarkably . consider the anti-linear operator $ ( c\psi ) ( x ) := \overline{\psi ( -x ) }$ , the bar denoting the complex conjugation . it is norm preserving and $cc=i$ , so it is a conjugation . it seems to me that $c\tilde{p}= \tilde{p}c$ with the domain i introduced above . therefore , in view of a theorem due to von neumann , $\tilde{p}$ admits some self-adjoint extension . a careful analysis of the defect indices of $\tilde{p}$ would classify these extensions . therefore we are not authorized to say that $\tilde{p}$ has the meaning of an observable , we have to choose a self-adjoint extension of it . unfortunately , the spectrum depends on this choice . it is by no means obvious what the spectrum of a self-adjoint extension of $\tilde{p}$ is . what i am saying is that since we do not know the spectrum of the claimed approximated momentum we cannot say how ( if ) this claimed approximation works as soon as $\delta \to 0$ . it is possible to prove that if the domain is chosen as $c_0^\infty ( \mathbb r ) $ or ${\cal s} ( \mathbb r ) $ then there is a unique self-adjoint extension and coincides with the standard one . but we cannot make this choice in view of the form of $\tilde{x}$ . in fact , when computing $ [ \tilde{x} , \tilde{p} ] =0$ we are assuming that the domain of $\tilde{p}$ is invariant under the action of $\tilde{x}$ . this indeed happens if $d ( \tilde{p} ) $ is made of the $c^1$ functions rapidly vanishing as $|x|\to \infty$ also vanishing at each $\delta n$ with their first derivative . the last condition eliminates the discontinuities introduced by the action of $\tilde{x}$ giving rise to a function in the same domain . we conclude that $ [ \tilde{x} , \tilde{p} ] =0$ holds on a domain which does not fix a unique self-adjoint extension of $\tilde{p}$ ( and its spectrum consequently ) as it would be if this domain were $c_0^\infty ( \mathbb r ) $ or ${\cal s} ( \mathbb r ) $ . an alternative definition of $\tilde{p}$ is obtained re-defining its domain $d ( \tilde{p} ) $ by including the ( vanishing sufficiently fast at infinity ) functions $c^1$ in each open interval $ ( n\delta , ( n+1 ) \delta ) $ that are separately periodic in each such interval . the values these functions attain in the set of points $\delta n$ is irrelevant as this set has zero measure and $l^2$ does not care of zero measure set : we can safely assume that $\tilde{p}\psi$ vanishes thereon by definition as already assumed by the op . this only concerns mathematics while , physically speaking , this assumption has devastating implications . with this definition $\tilde{p}$ become essentially self-adjoint , i.e. it has a unique self adjoint extension . the spectrum should be $\sigma ( \tilde{p}_{ext} ) = \{ \frac{2\pi m}{\delta} \hbar\:|\: m \in \mathbb z\}$ , since in each said interval we find the standard momentum operator on a segment with periodic boundary conditions ( to be completely sure i should check some conditions but i am reasonably confident ) with this definition you find $ [ \tilde{x} , \tilde{p} ] =0$ on $d ( \tilde{p} ) $ . the problem with this construction is that $\tilde{p}$ does not generate space displacements of the wavefunctions , $ ( e^{-ix_0\tilde{p}}\psi ) ( x ) = \psi ( x- x_0 ) $ . and it does not seem that the situation improves as soon as $\delta \to 0$ . instead $\tilde{p}$ generates " periodic " displacements in each interval $ ( n\delta , ( n+1 ) \delta ) $ ( as it were a circle ) . for $\delta \to 0$ these periodic displacements become denser and denser but they have nothing to do with geometric translations along $x$ , as the ones generated by the true momentum operator .
as i mentioned in the comments , p and s are working in the schrodinger picture which means that the operator fields are time-independent . of course , in the heisenberg picture , the solution of the klein-gordon equation is dependent on time ( and then it will have four-vectors ) . in order to see this , let us write down the klein-gordon equation : \begin{equation} \left ( \partial^2 + m^2 \right ) \phi ( x ) =0 \end{equation} where $g=\mathrm{diag} ( +1 , -1 , -1 , -1 ) $ . then the solutions in the heisenberg picture can be written as : \begin{equation} \phi ( x ) = e^{\pm i p_\mu x^\mu} \end{equation} which can be easily verified : \begin{equation} \begin{aligned} \partial^2 \phi and = \partial_\mu \partial^\mu \left ( e^{\pm i p_\nu x^\nu}\right ) \\ and = \partial_\mu \left ( \pm i p^\mu\right ) \left ( e^{\pm i p_\nu x^\nu}\right ) \\ and = \left ( \pm i p^\mu\right ) \left ( \pm i p_\mu\right ) e^{\pm i p_\nu x^\nu} \\ and = - p_\mu p^\mu e^{\pm i p_\nu x^\nu} \\ and = - ( e^2 - \mathbf{p}^2 ) e^{\pm i p_\nu x^\nu} \\ and = -m^2 e^{\pm i p_\nu x^\nu} \\ and = -m^2 \phi \end{aligned} \end{equation} and so : \begin{equation} \left ( \partial^2 +m^2\right ) \phi = \left ( -m^2 +m^2\right ) \phi = 0 \end{equation} it is normal to write the solution in terms of positive frequency solutions and negative frequency solutions : \begin{equation} \phi ( x ) =\phi_+ ( x ) + \phi_- ( x ) = a e^{- i p_\nu x^\nu} + b e^{+ i p_\nu x^\nu} \tag{1} \end{equation} of course , we also need to sum over all energy-momentum values $p_\mu$ ( because equation $ ( 1 ) $ is a solution for any value of $p_\mu$ ) . hence , the general solution is : \begin{equation} \phi ( \mathbf{x} , t ) = \int \frac{\mathrm{d}^3 \mathbf{p}}{n} \ ; \left [ a ( \mathbf{p} ) e^{- i e_{\mathbf{p}} t + i \mathbf{p} \cdot \mathbf{x}} + b ( \mathbf{p} ) e^{i e_{\mathbf{p}} t - i \mathbf{p} \cdot \mathbf{x}}\right ] \end{equation} where $n$ is a normalization constant . in order to see how to switch between the dirac and schrodinger picture , i refer you to section $2.4$ of p and s . edit i could not help my self and will quickly add this : p and s are discussing the real klein-gordon field , which means : $$ \phi = \phi^* $$ and so : \begin{equation} \int \frac{\mathrm{d}^3 \mathbf{p}}{n} \ ; \left [ a ( \mathbf{p} ) e^{- i p^\mu x_\mu} + b ( \mathbf{p} ) e^{ip^\mu x_\mu}\right ] = \int \frac{\mathrm{d}^3 \mathbf{p}}{n^*} \ ; \left [ a^* ( \mathbf{p} ) e^{ ip^\mu x_\mu} + b^* ( \mathbf{p} ) e^{-i p^\mu x_\mu}\right ] \end{equation} which implies : \begin{equation} \begin{array}{cc} a ( \mathbf{p} ) = b^* ( \mathbf{p} ) \ ; , and b ( \mathbf{p} ) = a^* ( \mathbf{p} ) \end{array} \end{equation} and $n$ must be a real . so the real field can be written as : \begin{equation} \phi ( \mathbf{x} , t ) = \int \frac{\mathrm{d}^3 \mathbf{p}}{n} \ ; \left [ a ( \mathbf{p} ) e^{- i e_{\mathbf{p}} t + i \mathbf{p} \cdot \mathbf{x}} + a^* ( \mathbf{p} ) e^{i e_{\mathbf{p}} t - i \mathbf{p} \cdot \mathbf{x}}\right ] \end{equation}
this question can be answered , but only with the addition of some extra assumptions . what we know for sure is that the entropy of the universe as a whole cannot decrease , and hence fluids can not be unmixed without increasing the entropy of some other system by an amount equal to $\delta s_\text{mixing}$ . this does not immediately tell us anything about work . however , let us now assume we have to hand a source of mechanical work , and a large heat reservoir at temperature $t$ . i will assume that we will use the work ( somehow ) to unmix the fluids , and that any excess heat that gets generated will be dumped in the heat reservoir . in general the mixed and unmixed states of the fluid system will have different energies . i will call the difference $\delta h$ , with positive $\delta h$ meaning that the unmixed state has a higher energy than the mixed one . ( i am guessing that for real miscible fluids this is usually a small positive value , but it need not necessarily be . ) we will assume it takes us an amount $w$ of work to unmix the fluids . $w$ has been lost from the work source , and $\delta h$ has been gained from the fluid system , so the first law says that the energy of the heat bath must increase by $w-\delta h$ . we can now calculate the total change in entropy . the fluids ' entropy has decreased by $\delta s_\text{mixing}$ , and the heat bath 's entropy has increased by $ ( w - \delta h ) /t$ , so the total is $\delta s_\text{total} = \frac{w - \delta h}{t} - \delta s_\text{mixing} \ge 0 , $ or $w \ge t\delta s_\text{mixing} + \delta h . $ changing this to an equality gives you the minimum amount of work required to unmix the fluids . but note that the expression involves $t$ , which is the temperature of the heat bath that i assumed to exist . without the heat bath there would be nowhere for the energy from the work source to go once it is used up . also note that $t$ is not necessarily the temperature of the mixed fluids , which could be different from that of the heat bath . i did not need to make any assumptions about the fluids ' temperature in order to work out the above . in practice you had usually assume the fluids to be in contact with the heat bath , so that the two temperatures are in fact equal . in this case ( and only in this case ) , the minimum work required is equal to the difference in helmholtz free energy between the mixed and unmixed states . it is possible for this minimum bound to be negative , meaning that you can actually get work out of the system by unmixing the fluids - but in this case the fluids would be immiscible , so the mixed state would be the unstable one . finally , let 's make the additional assumption that the fluids are gases and that we have two very special semi-permeable pistons to hand , one at either end of the container . one is permeable to the first gas but completely blocks the second , and the other is permeable to the second gas but blocks the first . we can now very slowly push the two pistons in opposite directions until they meet in the middle , while holding the system in contact with a heat bath . if we do it slowly enough we will have reversibly unmixed the two gases by something similar to reverse osmosis , using an amount of work equal to the minimum $w$ calculated above . ( i got that last idea from this brilliant paper on the entropy of mixing by edwin jaynes : http://bayes.wustl.edu/etj/articles/gibbs.paradox.pdf )
there is another question on this site about whether the laws of physics change over time . i think that the answers to that one ( including mine ) apply pretty much perfectly to this question about whether the laws change in space . we expect the fundamental laws of physics to be the same throughout space . in fact , if we found that they were not , we would strongly expect that that meant that the laws we had discovered were not the fundamental ones . it is very sensible to ask whether the laws as we currently understand them vary with respect to position . people do try to test these things experimentally from time to time . for instance , some experiments to test whether fundamental constants change with time are also sensitive variations in the fundamental constants with position . some cosmological theories , especially some of those that come under the heading of " multiverse " theories do allow for the possibility that the laws are different in different regions of space , although generally only on scales much larger than what we can observe . in general , in such theories , the truly fundamental laws are the same everywhere , but the way the evolution of the universe played out in different regions is so different that the laws appear quite different . one way this can happen is by the mechanism of spontaneous symmetry breaking . when the universe cooled down from very high temperatures , it probably underwent various transitions , more or less like phase transitions , in which an initially symmetric state turns into a less-symmetric state . in those transitions , there may be different ways that the final state can come out , and they may be quite dramatically different -- completely different sorts of particles may exist , for instance . there could be different regions of the universe in which the symmetry breaking went different ways , in which case the " apparent " laws would be utterly different in different regions , but probably only on scales many orders of magnitude larger than what we can see .
in his original work , fermi considered only vectors $f^{\mu}$ which are orthogonal to the curve $f^{\mu} v_{\mu} = 0$ . his analysis is relevant to the spin or photon polarization vectors which are orthogonal to the four-velocity by definition . walker generalized fermi 's work to vectors which are not necessarily orthogonal to the velocity . ( thus the second term in the fermi-walker transport is not identically $0$ ) . this fact and more historical remarks on fermi 's work on accelerated bodies in general relativity , the fermi-walker transport and their connections to modern works on spinning bodies in general relativity as well as references to their original work can be found in the following article by : bini and jantzen .
there are a lot of comments . the end result is that your first proposition is correct as far as the physics models we have validated with experimental evidence go . even classically , when one has an extended body whose trajectory is assumed by the center of mass , the paradox becomes trivial , even if the center of mass never reaches the goal , half of the solid body minus an infinitesimal essentially does . the heisenberg uncertainty principle is involved at the elementary particle state , and yes the classical motion is no longer relevant and does not describe the behavior of nature at the microscopic scale . as for your second proposition , what disallows discreteness of space and time is not lack of imagination . it is at the moment incompatibility of known and validated physics and mathematical models proposed with such discreteness incorporated . at the moment as far as i know , locality for lorenz invariance , which has been validated an innumerable number of times , is the main obstacle for such theories , but also i am not aware if there exist proposals that can also incorporate the standard model of particle physics , which is an encapsulation of all the experimental measurements we have up to now . thus discreteness in time exists in some models but is not validated by any data and is not a proposition accepted by the mainstream physics community . in any case even if you take time as a variable the argument with the heisenberg uncertainty principle is sufficient as there also exist a delta ( e ) delta ( t ) > h_bar form of it .
this types of problems are solved by observing projectile movements in $x$ and $y$ direction separately . in $x$ direction you have constant velocity movement $$v_x = v_{x0} = v_0 \cos ( \theta ) , \ ; ( 1 ) $$ $$x = v_{x0} t +x_0 = v_0 \cos ( \theta ) \ ; t +x_0 , \ ; ( 2 ) $$ and in $y$ direction you have constant acceleration movement with negative acceleration $-g$ $$v_y = - g t + v_{y0} = - g t + v_0 \sin ( \theta ) , \ ; ( 3 ) $$ $$y = - \frac{1}{2} g t^2 + v_{y0} t + y_0 = - \frac{1}{2} g t^2 + v_0 \sin ( \theta ) \ ; t + y_0 . \ ; ( 4 ) $$ your initial conditions are $$x_0 = 0 , \ ; y_0 \ne 0 , $$ and final conditions ( at moment $t=t$ projectile falls back on the ground ) are $$t = t , \ ; x = d , \ ; y = 0 . $$ if you put initial and final conditions into equations ( 2 ) and ( 4 ) you end up with two equations and two unknowns $v_0 , t$ . by eliminating $t$ you get expression for $v_0$ . my calculations show that $$v_0 = \frac{1}{\cos ( \theta ) }\sqrt{\frac{\frac{1}{2} g d^2}{d \tan ( \theta ) +y_0}}$$ which is i believe equal to your equation . maybe your problem is that $d$ means displacement in direction $x$ , while the total displacement is $\sqrt{d^2+y_0^2}$ ?
yeah , neils bohr and john von neumann were skeptics : many prominent physicists thought it could not even work , based on their knowledge of physical principles . in quantum mechanics , the uncertainty principle developed by einstein , says that the energy ( and therefore the frequency , by e=hv ) of a photon can not be known to great precision in a short time . in masers , photons last for a very short time . therefore , no less than neils bohr and john von neumann thought it could not work , even after it had been created . the solution to this apparent paradox is that , though the photons all have the same frequency and direction , which atoms do the emitting and when remains unknown . the emitting atoms maintain an anonymity that avoids uncertainty > violation . read more : why was the laser light invented ? | ehow . com http://www.ehow.com/about_5250763_laser-light-invented_.html#ixzz1uic0yare
the radar method is a general approach that works for non-inertial observers and curved spacetime . two co-ordinates of an event are given by your clock time at which the event intersects your future and past light cone , called retarded time and advanced time , ( $\tau^+ , \tau^-$ , resp . ) . or use a diagonal combination thereof : $\tau^\star = \frac{\tau^+ + \tau^-}{2}$ , called radar time , and $\rho = c\frac{\tau^+ - \tau^-}{2}$ , called radar distance . this diagonal combination has the property that , in the case of an unaccelerated observer in flat spacetime , $\tau^\star$ and $\rho$ are equal to the usual measures ( the " infinite grid of rulers and clocks " business ) . two other co-ordinates can be given by the incoming angles ( $\omega^+$ ) of the null geodesic from the event to you . this is the reception or retarded co-ordinate system . the dual system , the trasmission or advanced co-ordinate system , would use the outgoing angles ( $\omega^-$ ) of the null geodesic from you to the event . for a non-rotating unaccelerated observer in flat spacetime , the two pairs of angles are equal to each other and to the usual polar co-ordinate angles . in flat spacetime this will assign a unique co-ordinate to every reachable event , that is , every event in the observer 's causal diamond . in curved spacetime it will assign at least one co-ordinate to every reachable event , however there may be duplicates . one can restrict to the boundary of the causal past and future , as described in answer i linked to above . then , under certain causality assumptions , every reachable event gets a unique $\tau^\star$ and $\rho$ . the surfaces of constant $\tau^\star$ and $\rho$ are then 2-d globally spacelike surfaces , but not always topologically $\mathcal{s}^2$ , rather , they will be some subquotient of $\mathcal{s}^2$ . that is , for a given $\tau^\star$ and $\rho$ some angle pairs $\omega^+$ will not be valid ( corresponding to parts of the light cone that have " fallen behind" ) , and some events on the boundary of validity will have more than one angle pair .
there is no mistake on the wikipedia page and all the equations and statements are consistent with each other . in $$a_{\rm heis . } ( t ) = e^{iht/\hbar} a e^{-iht/\hbar}$$ the letter $a$ in the middle of the product represents the schrödinger picture operator $a = a_{\rm schr . }$ that is not evolving with time because in the schrödinger picture , the dynamical evolution is guaranteed by the evolution of the state vector $|\psi\rangle$ . however , this does not mean that the time derivative $da_{\rm schr . }/dt=0$ . instead , we have $$ \frac{da_{\rm schr . }}{dt} = \frac{\partial a_{\rm schr . }}{\partial t} $$ here , $a_{\rm schr . }$ is meant to be a function of $x_i , p_j$ , and $t$ . in most cases , there is no dependence of the schrödinger picture operators on $t$ - which we call an " explicit dependence " - but it is possible to consider a more general case in which this explicit dependence does exist ( some terms in the energy , e.g. the electrostatic energy in an external field , may be naturally time-dependent ) . in schrödinger 's picture , $dx_{i , \rm schr . }/dt=0$ and $dp_{j , \rm schr . }/dt=0$ which is why the total derivative of $a_{\rm schr . }$ with respect to time is given just by the partial derivative with respect to time . imagine , for example , $$ a_{\rm schr . } ( t ) = c_1 x^2 + c_2 p^2 + c_3 ( t ) ( xp+px ) $$ we would have $$ \frac{da_{\rm schr . } ( t ) }{dt} = \frac{\partial c_3 ( t ) }{\partial t} ( xp+px ) . $$ these schrödinger 's picture operators are called " untransformed " on that wikipedia page . the transformed ones are the heisenberg picture operators given by $$a_{\rm heis . } ( t ) = e^{iht/\hbar} a_{\rm schr . } ( t ) e^{-iht/\hbar}$$ their time derivative , $da_{\rm heis . } ( t ) /dt$ , is more complicated . an easy differentiation gives exactly the formula involving $ [ h , a_{\rm heis . } ] $ that you quoted as well . $$\frac{d}{dt} a_{\rm heis . } ( t ) = \frac{i}{\hbar} [ h , a_{\rm heis . } ( t ) ] + \frac{\partial a_{\rm heis . } ( t ) }{\partial t} . $$ the two terms in the commutator arise from the $t$-derivatives of the two exponentials in the formula for the heisenberg $a_{\rm heis . } ( t ) $ while the partial derivative arises from $da_{\rm schr . }/dt$ we have always had . ( these simple equations remain this simple even for a time-dependent $a_{\rm schr . }$ ; however , we have to assume that the total $h$ is time-independent , otherwise all the equations would get more complicated . ) the two exponentials on both sides never disappear by any kind of derivative , so obviously , all the appearances of $a$ in the differential equation above are $a_{\rm heis . }$ . the displayed equation above is the ( only ) dynamical equation for the heisenberg picture so it is self-contained and does not include any objects from other pictures . in the heisenberg picture , it is no longer the case that $dx_{\rm heis . } ( t ) /dt=0$ ( not ! ) and the similar identity fails for $p_{\rm heis . } ( t ) $ as well . $a_{\rm heis . } ( t ) $ is a general function of all the basic operators $x_{i , \rm heis . } ( t ) $ and $p_{j , \rm heis . } ( t ) $ , as well as time $t$ .
the weld acts on the rod with force components $b_x$ and $b_y$ as well as a moment $m$ . the equations of motion for the rod are $$ b_x = m_{rod} \left ( - \cos\theta \ , \ddot{q} \right ) $$ $$ b_y = m_{rod} \left ( \sin\theta \ , \ddot{q} + g \right ) $$ $$ m - \frac{l}{2} b_y = i_{rod} \dot{\omega}_{rod} = 0 $$ where $q$ is the distance along the guide of travel and the last equation is the sum of moments about the center of gravity of the rod . gravity is included above as $g$ . the equations of motion for the block are $$ -f\ , \cos\theta + n \sin\theta - b_x = m_{block} \left ( - \cos\theta\ , \ddot{q} \right ) $$ $$ f\ , \sin\theta + n \cos\theta - b_y = m_{block} \left ( \cos\theta\ , \ddot{q} + g\right ) $$ where $n$ is the contact normal force . combined the above is $$ m_{rod} \left ( \ddot{q} + g\sin\theta \right ) - f = -m_{block} \left ( \ddot{q} + g\sin\theta \right ) $$ $$ n-m_{rod} g \cos\theta = m_{block} g \cos\theta $$ with solution $$ n = \left ( m_{rod}+m_{block}\right ) \ , g \cos\theta $$ $$ \ddot{q} = \frac{f}{m_{rod}+m_{block}} - g \sin \theta $$ now going back to moment , $m=\frac{l}{2} b_y$ which you can solve now .
yes , indeed . there is a working group called the ' particle data group ' operating the particle data listing where they sum up all experimental results , have short reviews about the physics behind it , also explaining the implicit assumptions made and give bounds on the most popular extensions of the standard model .
whatever direction you look , you are looking back to a time closer to the big bang . i am guessing that is what you read . after all , there is no location in space of the big bang ( i can elaborate further on this if desired , but i suspect it has been answered in another question ) , so anywhere you move will not get you any closer to a point that does not exist . as to the second part of your question , sadly the answer is no , not at all . anywhere you can get to by going no faster than the speed of light will be further along in its cosmic evolution than we are here and now . there is no way your future can be in the past so to speak . the whole universe is aging uniformly , and so the stars will eventually die everywhere . [ regarding those star deaths , though , that will not happen for a very long time , even by the universe 's standards . stars somewhat smaller than our sun have lifetimes on the order of 100 billion years . for comparison , the universe is only about 14 billion years old right now . even if stars stopped forming today , solar systems around such stars will go on pretty much unchanged for a very long time . ]
a lot is known about qfts ( including qed ) at finite time . it is tractable approximately ( just like scattering ) . though in 4d no rigorous treatment is available ( neither is there one for scattering ) . one can compute - nonrigorously , in renormalized perturbation theory - many time-dependent things , namely via the schwinger-keldysh ( or closed time path = ctp ) formalism . for example , e . calzetta and b . l . hu , nonequilibrium quantum fields : closed-time-path effective action , wigner function , and boltzmann equation , phys . rev . d 37 ( 1988 ) , 2878-2900 . derive finite-time boltzmann-type kinetic equations from quantum field theory using the ctp formalism . there are also successful nonrelativistic approximations with relativistic corrections , within the framework of nrqed and nrqcd , which are used to compute bound state properties and spectral shifts . see , e.g. , hep-ph/9209266 , hep-ph/9805424 , hep-ph/9707481 , and hep-ph/9907240 . there is also an interesting particle-based approximation to qed by barut , which might well turn out to become the germ of an exact particle interpretation of standard renormalized qed . see a.o. barut and j.f. van huele , phys . rev . a 32 ( 1985 ) , 3187-3195 , and the discussion in phys . rev . a 34 ( 1986 ) , 3500-3501,3502-3503 . approximately renormalized hamiltonians , and with them an approximate dynamics , can also be constructed via similarity renormalization ; see , e.g. , s.d. glazek and k.g. wilson , phys . rev . d 48 ( 1993 ) , 5863-5872 . hep-th/9706149 in 2d , the situation is well understood even rigorously : for all theories where wightman functions can be constructed rigorously , there is an associated hilbert space on which corresponding ( smeared ) wightman fields and generators of the poincare group are densely defined . this implies that there is a well-defined hamiltonian h=cp_0 that provides via the schroedinger equation the dynamics of wave functions in time . in particular , if the wightman functions are constructed via the osterwalder-schrader reconstruction theorem , both the hilbert space and the hamiltonian are available in terms of the probability measure on the function space of integrable functions of the corresponding euclidean fields . for details , see , e.g. , section 6.1 of j . glimm and a jaffe , quantum physics : a functional integral point of view , springer , berlin 1987 . in particular , ( 6.1.6 ) , ( 6.1.11 ) and theorem 6.1.3 are relevant . [ information extracted from the section ''relativistic qft at finite times ? '' of chapter b3: ''basics on quantum fields'' of my theoretical physics faq at http://www.mat.univie.ac.at/~neum/physfaq/physics-faq.html ] [ edit october 9 , 2012: ] on the other hand , a lot is unknown about qfts ( including qed ) at finite time . let me quote from the 1999 article ' 'some problems in statistical mechanics that i would like to see solved'' by elliot lieb http://www.sciencedirect.com/science/article/pii/s0378437198005172 : but there is one huge problem that everyone avoids , because so far it is much too difficult to handle . that problem is quantum electrodynamics , and the problem exists whether we are talking about non-relativistic or relativistic quantum mechanics . [ . . . ] the physical picture that begs to be understood on some decent level is that the electron is surrounded by a huge cloud of photons with an enormous energy . we are looking for small effects , called ' radiative corrections ' , and these effects are like a flea on an elephant . perturbation theory treats the elephant as a perturbation of the flea . [ . . . ] after renormalizing the mass so that the ' effective mass ' ( a concept familiar from solid state physics ) equals the measured mass of the electron we are supposed to obtain an ' effective low energy hamiltonian ' ( again , a familiar concept ) that equals the schroedinger hamiltonian plus some tiny corrections , such as the lamb shift . from there we should go on to verify the levels of hydrogen ( which , except for the ground state , have become resonances ) , stability of matter and thermodynamics and all those other good things . but no one has a clue how to implement this program . [ . . . ] on the other hand matter does exist and the sun is shining , so the theory must exist , too . i would like to see it someday
i did some research and calculations : to summarize : the relativistic rocket will not break apart , uniform acceleration along it is possible . but the observers will measure different accelerations due to the gravitational time dilation . in more detail : let 's assume the observer at the bottom measures $\alpha$ acceleration . so for an inertial observer outside ( who draws the minkowski chart ) this accelerating observer 's motion will be hyperbolic . the semi mayor axis of this hyperbola will be $c^2/\alpha$ . let 's say the length of the relativistic rocket is $h$ this is measured before the launch , and the mechanical stresses during the travel try to keep this value constant in the reference frame of the rocket , otherwise the rocket would break apart ( we assume it do not break apart ) . as the rocket accelerates the plane of simultaneity rotates from the viewpoint of the inertial observer . so the two ends of the rocket will not trace two identical hyperbolas . but the two ends always connect two points on the two hyperbolas whose slope is the same ( you can see this on the rindler chart ) . so all parts of the rocket travel with the same speed at the local frame simultaneously , so the rocket 's acceleration will be uniform and it will not break apart . but the two hyperbolas are different . the bottom traces a hyperbola whose semi-mayor axis is $c^2/\alpha$ , the top traces a hyperbola whose semi-mayor axis is $c^2/\alpha + h$ . the acceleration that corresponds to the second hyperbola is $1 + \alpha h / c^2$ times smaller than $\alpha$ . this a bit paradoxical situation , because i stated that the acceleration is uniform along the rocket , now i state it is different due to the different hyperbolas . this paradox can be resolved if i introduce gravitational time dilation , so i assume that the clock of the observer at the top ages faster with the rate i mentioned above . so the top observer measures less acceleration this way . there is an event horizon at the rindler-horizon where $h = -c^2/\alpha$ , there the time stands still . this is somewhat analogous with the black hole 's event ho+rizon . the gravitational time dilation formula mentioned in the wikipedia and the one mentioned in the comment is the same , but that exponentional formula never reaches zero , that would mean the rindler-horizon does not exist . . . which would be a bit odd . so i still need some research . update : the wikipedia article has been fixed since last update . so the general formula to the gravitational time dilation is $e^{\int^h_0 g ( h ) dh / c^2}$ , where $g ( h ) $ is the measured gravitational acceleration at the given level . for rindler observers $g ( h ) = c^2/ ( h+h ) $ where $h = c^2/\alpha$ . doing the integral gives $e^{ln ( h+h ) - ln ( h ) } = ( h+h ) /h$ . substituting $h$ back i will get the original $1 + \alpha h / c^2$ i mentioned earlier .
it seems that few people were interested in this problem . in the early days , the main focus was in experimentally testing bell inequalities . the papers i know that treat the problem abstractly are from the 80s onward . the story goes like this : in 1969 , clauser , horne , shimony and holt publish their famous paper that introduced the chsh inequality . in it , they stated the the maximal violation for a singlet is $2\sqrt{2}$ . as they were analysing a specific experiment , they did not bother proving it for any two-qubit state , nor even stated the bound explicitly . they were inspired by bell ( of course ) and bohm 's 1957 paper with aharonov . amusingly , bohm 's paper displays an inequality whose quantum bound is 2.85 , and the bound for some local hidden-variable models he tried is 2 . unfortunately , these numbers are just a numerical coincidence , as his inequality has nothing to do with chsh , not being even a bell inequality . in 1978 , boris tsirelson gives a seminar on bell inequalities . the chairman ( a . vershik ) asks him if it is possible to develop analogue inequalities for quantum theory , i.e. , to bound the strength of quantum correlations . in 1980 , boris tsirelson publishes a paper that proves that in the chsh case , it is possible to bound the correlations by $2\sqrt{2}$ , for any quantum state . in 1985 , summers and werner rediscover tsirelson 's bound independently . ( apparently it was they who popularised it outside the soviet union ; landau in 1987 cites both tsirelson and summers . )
where did you get that figure for the speed of the earth 's surface ? the circumference of the earth at the equator ( it is greatest circumference ) is 40,075km and it rotates once in 24 hours , so the speed is 40,075/24 or about 1,670 km/hour or 464 m/sec . if you calculate the time dilation at 464 m/sec it is insignificant , as you had expect given that this speed is 0.0000015$c$ . response to comment : the velocity of rotation around the sun is about 29,800 m/sec ( 0.0001$c$ ) , and at this speed the time dilation factor is about 0.999999995 . this gives an error of about 23 years over the 4.54 billion lifespan of the solar system . but in any case you need to define what you mean by the time . if a physicist uses radioactive decay to measure a date then because the physicist is moving at the same speed as the radionuclide the physicist and the radionuclide experience time flowing at the same rate . that means that by definition there can not be any dating errors . response to response to comment : suppose a block or uranium-238 split in two at the beginning of the solar system , 4.54 billion years ago , and one half ended up on earth while another half ended up on pluto . on the earth a physicist dated the formation of the solar system by measuring how much of the u-238 had decayed , while on pluto an alien physicist did the same experiment . the two physicists would come up with different ages i.e. they would measure that different amounts of the u-238 had decayed . this is a real difference : time really does flow at a slightly different rate on earth and on pluto , though the difference is so small that you had never actually be able to measure it . some of the difference would be due to the different orbital velocities of earth and pluto , and some would be because earth is deeper into the sun 's potential well and gravity affects time as well . i would have to sit down with a pen and paper to work out which effect was stronger . although the time dilation is insignificant for the solar system gravitational time dilation can be significant for neutron stars and especially for black holes . in fact if you sat some distance from a black hole and watched someone falling into it you had see time for the falling astronaut slow to a stop as they approached the black hole event horizon . one last comment : you might be interested in this article . i have already mentioned that the gravitational field of the earth affects time , and this has been used to measure the earth 's geoid by measuring the differences in the rate atomic clocks run .
again , keep in mind this : let us considere a $f$ function depending on a single variable $x$ . then , you have the following implication : $$\forall x , \ ; \frac{df}{dx} ( x ) =0 \rightarrow f ( x ) =c^{st}\ ; ( \text{regarding}\ ; x ) $$ i.e. $f$ does not explictly depend on $x$ if now $f$ depends on a set of $n$ independant variables $ ( x_i ) _{i\in [ 1 . . n ] }$ , you have : $$\forall i , \forall ( x_1 . . . x_n ) \ ; \frac{\partial f}{\partial x_i} ( x_1 . . . x_n ) =0 \rightarrow f ( x_1 . . . x_n ) =c^{st}\ ; \text{regarding}\ ; x_i$$ i.e. $f$ is only explictly depending on $n-1$ variables : $ ( x_1 . . . x_{i-1}\ , x_{i+1} . . . x_n ) $ now , if $f$ depends on a set of $n$ variables which are depending on a parameter $t$ : $ ( x_1 ( t ) . . . x_n ( t ) ) $ , then what as been said before is no longer true . in such case , you have to follow a chain rule to perform a derivative in respect to $t$ . in particular : $$\frac{df}{dt} ( x_1 ( t ) . . . x_n ( t ) ) =0\nrightarrow f\ ; \text{is a constant regarding any}\ ; x_i ( t ) $$ of course . but , $$\frac{df}{dt} ( x_1 ( t ) . . . x_n ( t ) ) =\sum_{i=1}^n\frac{\partial f}{\partial x_i}\frac{\ dx_i}{dt}=0\rightarrow f\ ; \text{is a constant regarding}\ ; t$$ i.e. $f$ is " conserved " in respect of $t$ . finally , for completeness , if now $f$ explictly depends on the parameter $t$ , i.e. $f ( x_1 ( t ) . . . x_n ( t ) , t ) $ , then : $$\frac{df}{dt} ( x_1 ( t ) . . . x_n ( t ) , t ) =\frac{\partial f}{\partial t}+\sum_{i=1}^n\frac{\partial f}{\partial x_i}\frac{\ dx_i}{dt}=0\\\rightarrow f\ ; \text{is globally a constant regarding}\ ; t$$ i.e. $f$ is " conserved " in respect of $t$ again . but , a priori $f$ still does explictly depend on $t$ because in that case you must verify the indentity : $$\frac{\partial f}{\partial t}=-\sum_{i=1}^n\frac{\partial f}{\partial x_i}\frac{\ dx_i}{dt}\neq 0$$ in conclusion , all of this shows that $\frac{df}{dt}=0$ for a multi-variable dependent function $f$ means only that the growth rate of $f$ in respect of $t$ is zero but does not mean that $f$ does not explictly depend on $t$ .
the state $|\phi\rangle$ is a coherent state , which has a non-zero overlap with the vacuum state $|0\rangle$ . the vacuum state is defined by $\hat \phi ( x ) |0\rangle=0$ for all $x$ . the vacuum state is also given by the coherent state $|\phi ( x ) =0\rangle$ for all $x$ . furthermore , one should keep in mind that the coherent state basis is overcomplete , and therefore $\langle \phi|\phi'\rangle\neq0$ for any $\phi$ and $\phi'$ . all this properties solve the op 's problem . to see that , one can look at the simpler case of the harmonic oscillator , and everything should be clear .
( i can not quite comment on the previous post , so i will have to write a new answer ) . if we set the curvature of the earth to be non-negligible in our problem , yes , gravity would slow the baseball down by an extremely tiny amount , but , if we exclude this case ( which , again , i stress to be many orders of magnitude below anything considerable ) , then no , gravity itself does not slow the ball down since the force of attraction ( the direction of the vector of acceleration ) points exactly downwards and contributes nothing to the horizontal component .
it is very similar to the first newton 's law : he says that in a body at rest or moving with a linear uniform motion there is no force acting on it or the vector sum of all forces acting on it is zero . however , this law ignores galilean relativity , because if one object and an observer are falling at the same acceleration with same initial velocity , so the object is at rest in the observer 's reference frame , so he can say that no force is acting on it .
your mistake here is to assume that the multiplication $\vec v\cdot \vec \nabla$ is commutative . it is not ; the dot product here is just a convenient mathematical notation . this part of the wikipedia article on navier-stokes equations explains how to interpret this term .
i ) let us just consider $1$ dimension for simplicity . ( the generalization to higher dimensions is straightforward ) . then the volume factor $v$ is just a length factor $l$ . ii ) the standard fourier series formulas can be derived from $ ( 12.1.7 ) $ and $ ( 12.1.6 ) $ by taken the length $l$ to be $l=2\pi$ . then $ ( 12.1.7 ) $ and $ ( 12.1.6 ) $ become the standard fourier series formulas $$\tag{12.1.7'} c_{n} ~=~ \frac{1}{2\pi}\int_{-\pi}^{\pi} \ ! dx~ f ( x ) e^{-in x} , $$ $$\tag{12.1.6'} f ( x ) ~=~\sum_{n\in\mathbb{z}} c_n~e^{in x} ~=~f ( x+2\pi ) , $$ via the identifications $$\ell~=~ n~\in~\mathbb{z} , \qquad \tilde{f} ( \ell ) ~=~2\pi c_{n} . $$ iii ) going back to $3$ dimensions , the $1/v$ normalization in $ ( 12.1.6 ) $ is important . of course , in another convention , it could be put in $ ( 12.1.7 ) $ instead , or alternatively , symmetrically as $1/\sqrt{v}$ in both formulas $ ( 12.1.6 ) $ and $ ( 12.1.7 ) $ .
i am not a nuclear engineer , or a power systems engineer but lets try to work the question as simply as possible . you want the maximum efficiency out of the turbine . that is a large fraction of the thermodynamic ( carnot ) limit which is set by the temperature of steam entering the turbine and the design of the turbine . the only parameter you can change dynamically here is the temperature of the input steam ( and you want it to be as hot as possible ) . the steam can be no hotter than the water leaving the core of the reactor , and will actually be a bit less because of latent heat . assuming that you want to maintain the core-cooling loop liquid the whole way ( i believe that you do ) , the only way to raise its temperature is to increase it is pressure ( because this increases the boiling point , which is your upper limit ) . so , to maintain high power generation efficiency you must maintain high pressure on the core cooling system . you need to keep in mind that there are two separate loops at work here . one is in contact with the core and potentially contaminated , so it is kept carefully inside the containment vessel . the other one is nominally clean and actually powers the turbine . you can see this in the figure you posted : one is colored in oranges , yellows and reds ; the other colored in shades of blue ( and actually a third water system that provides the cold sink for the back of the turbine also colored blue , but it plays only a passive roll here ) . steam is generated in the clean loop by exchanging heat with the hot liquid water in the dirty loop . latent heat only comes into play on the clean side . i do not know if that system is actively pressurized or not , but there will be some back pressure in the system from the turbine .
i would not say that fusors are a waste , but so far they have turned out to not be very efficient . there are several factors working against them , compared to linear accelerator-based devices . basically what it boils down to is the neutron production cross section . in a fusor , the ions are in a plasma , and the mean free path is huge . so the effective cross section is small ( in other words , ions have a relatively small probability of hitting each other and fusing ) , and you are spending a lot of energy accelerating ions ( and electrons ) that fuse very rarely . perhaps efficiency can be improved , but the results speak for themselves -- the best efficiency i could find was 10^4 neutrons/w [ 2 ] . despite a lot of work over the years , we do not have a good qualitative understanding of what goes on inside a fusor . linear neutron generators have a huge advantage because they use solid tritiated metal hydride targets [ 1 ] . the ratio of hydride atoms to metal atoms is typically ~2 , which presents a very large effective cross section compared to a plasma . in addition , the generated beam ( s ) of deuterons can be optimized for the target geometry . furthermore , we have a better understanding of what goes on : it amounts to a fixed-target setup , which has a long history in nuclear physics . there is the side issue of d-d vs d-t . since the fusor is fed by gases and typically purges to atmosphere , hobbyists ( and most scientists ) operating fusors are restricted to the use of deuterium due to safety concerns . for a given energy in the typical range , the microscopic cross section for d-d is lower by an order of magnitude or more . but when the efficiency deficit is 10^4 , 10^5 , there is still a big gap even if d-t was used in both cases . [ 1 ] see chapter 7 of phd thesis by j.m. verbeke , uc berkeley , 2000 [ 2 ] miley group , uiuc , 2004
well here 's an unexpected source : steve spangler science ! apparently after shaking , there are bubbles left on the sides of the can ( or bottle ) , under the liquid . since the soda is supersaturated with $co_{2}$ the bubbles become nucleation sites for the conversion of aqueous $co_{2}$ to gaseous $co_{2}$ . since there are many bubbles , and since they are under the liquid level , there is a quick increase in the volume of gas inside the can , and the liquid is pushed outwards as the gas tries to escape the liquid . evidently tapping the can before opening it dislodges the bubbles under the liquid , decreasing the amount of nucleation sites below the liquid level . therefore when opening the can , there is no sudden overwhelming conversion of aqueous $co_{2}$ to the gaseous form inside the liquid , and the liquid does not gush out . of course , if one waits long enough after shaking a can , the bubbles stuck on the side will eventually rise to the surface , and the can becomes safe to open again all by itself . this does require some time though , more than the few seconds that someone would spend flicking their can .
if one is a mathematician one can study any set of theories to his/her heart 's content and end up with a qed at the end . physics is about studying understanding and modeling nature ( physis in greek ) preferably with mathematical models which are predictive of new behaviors . the standard model ( sm ) is one such mathematical model . what does this mean ? it means that it is a shorthand for a huge number of physical data painstakingly gathered from individual experiments . it fits the results and all the predictions it has given for the lhc are within the experimental errors . why are physicists interested in string theories and not satisfied with the sm ? because they believe that all interactions in nature should be modeled by a single model and the sm describes and predicts data only for three out of the four . gravity is not within sm . for many years now theoretical physicists have been studying various mathematical models that will include gravity in one unified theory with all four interactions . string theories are the best candidate for modeling all interactions because their group structures can accommodate the groups of the sm , and thus all known data can be embedded in a string theory model . if you do not know in depth what your are trying to model , you may be a good technician in mathematical modeling , but not a good physicist with a physicist 's intuition . it all depends on your further goals , if you want to be a research physicist or are just studying some physics for other requirements . if the former , yes the sm course is necessary . the work for theoretical physicists in string theory models lies in finding which specific branch and finally form of a string theory embeds best the sm and predicts new phenomena to be found and studied . if one has no data bank of the known phenomena and the way they are connected mathematically one will have developed no intuition on finding the needle in the haystack of string theory possibilities , or be able to suggest experiments that will lead to a validation for a specific st model . .
a pendulum of ( long ) length l will tick with a period of $2\pi\sqrt{l\over g}$ , and air resistance can be made negligible for a mm-sized oscillation of a heavy object on a several-meter long rigid arm . you need to determine the location of the center of mass accurately to know the effective value of $l$ , but this can be done arbitrarily accurately by balancing the arm and weight on a fulcrum ( or by accurately finding the cm of each of the parts and measuring the configuration of the parts accurately ) . then you just have to count the number of oscillations over a long enough period of time . this is a practical method that allows the determination of g to 5 significant figures ( assuming the error on l is the signifcant one , the lever is 10m , and the position measurements are at the . 1mm scale ) with no technology .
have derived it myself : $${\tau}=\dfrac{\underbrace {\large { i}}_{\text{through inductor at steady state}}}{\underbrace {{di_{\small }}/{dt}}_{\text{initial}}}$$ also $$v_{\text{inductor}}=l\dfrac{di}{dt}$$ where $i$ is current through inductor .
paragraphs " creating a general single particle state ( discrete solution form ) " ( $3.108 \to 3.110 $ ) and " destroying a general single particle state ( discrete ) " ( $3.111 \to 3.112 $ ) are two independent paragraphs , are should not be mixed . it is not possible to start with a normalized state $c\equiv\sum_\mathbf{k}a_\mathbf{k}a_\mathbf{k}^\dagger$ , with $\sum_\mathbf{k}\left|a_\mathbf{k}\right|^2=1$ , then applying the operator $d\equiv\sum_\mathbf{k}a_k$ , and find that the resulting state $\sum\limits_i a_i|0\rangle$ is also normalized ( except in the trivial case where there is only one term in the sum ) . the main reason is that the operator $d$ is not unitary , so there is no reason why it should transform a normalized state into an other normalized state . or said , differently : $\sum_\mathbf{k}\left|a_\mathbf{k}\right|^2 \neq |\sum\limits_{k} a_\mathbf{k} |^2$
a fresnell lens is essentially a " collapsed " plano-convex lens . if you were to raise each ring of the fresnell lens so the outer diameter of the ring were at the height of the inner diameter of the next larger ring , you had have reconstructed the original plano-convex lens . obviously , there are some diffraction effects and various other higher-order aberrations , but at the simple level i think you are asking , the fresnell lens produces a full image of the object . that is another way of saying yes , it would produce an image of the sun even if the sun were not on the optic axis .
for your first question : obtaining equation ( 2 ) from ( 1 ) is basically just a matter of taking the derivative . consider this expression that relates an infinitesimal change in the value of a function $f ( x , b ) $ to the infinitesimal changes in the values of its arguments : $$\delta f ( x , b ) = \frac{\partial f ( x , b ) }{\partial x}\delta x + \frac{\partial f ( x , b ) }{\partial b}\delta b$$ if you have ever done anything with uncertainty analysis , in particular error propagation , you should be familiar with this sort of thing , but if not , it is still not too complicated - it is just the chain rule of multivariable calculus . try plugging in the left side of equation ( 1 ) for $f ( x , b ) $ and see that it works out . a different way to do the same derivation is to consider the right triangle whose legs are formed by the rod and the " ceiling " and whose hypotenuse is formed by the string . originally , the triangle has side lengths $a$ , $b$ , and $l - x$ ; after an infinitesimal displacement of the system , it has side lengths $a$ , $b + \delta b$ , and $l - x - \delta x$ . using the pythagorean theorem on the first set of lengths , you get $$a^2 + b^2 = ( l - x ) ^2$$ and on the second set of lengths , you get $$a^2 + ( b + \delta b ) ^2 = ( l - x + \delta x ) ^2$$ try combining these to see that you get equation ( 2 ) out of it . remember that because the $\delta$ quantities are infinitesimal , you can ignore terms involving $\delta x^2$ and $\delta b^2$ . as for the second question : probably the easiest way to think about it is that for a static system , the principle of virtual work is equivalent to potential energy minimization . to find the state of the system , you find what values of the coordinates minimize the potential energy , which in this case is $-mgx - mgb$ . ordinarily , when you want to minimize a function , you take the derivative and set it equal to zero , but you can just as well calculate an infinitesimal change using the formula above and set that equal to zero . try setting $f ( x , b ) $ in that formula equal to the potential energy . if you are still concerned about tension , though , you could make the following argument , staying true to the spirit of virtual work . as the ring moves downward by a small amount $\delta b$ , the work done on it is $$\delta w_m = mg\delta b - t\sin\theta\ , \delta b$$ where the factor $\sin\theta = \frac{b}{\sqrt{a^2 + b^2}}$ picks out the vertical component of the tension . as this happens , the string gets pulled through the pulley , thus raising the larger mass by a displacement $\delta x$ . the work done on the larger mass is going to be $$\delta w_m = -mg\delta x + t\delta x$$ when the system is in equilibrium , it will be in a state such that the no work gets done if it shifts a tiny amount in either direction . that means that the total work for an infinitesimal displacement should be zero , $$\delta w_m + \delta w_m = 0$$ using this and equation ( 2 ) , namely $\delta x = -\frac{b}{\sqrt{a^2 + b^2}}\delta b$ , you can show that the two tension terms , one from $\delta w_m$ and one from $\delta w_m$ , cancel out . any work done by the tension on one mass is canceled out by work done by tension on the other mass . so even if you do not actually take the tension into account , you still wind up getting the right result .
you may indeed have heard that an electron is antisymmetric , whereas a photon is symmetric . what does this mean ? suppose i have a system of several electrons . they could be orbiting a nucleus , for example . their behaviour is described by a wave-function , $\psi$ . if i swap the positions of two electrons labled $a$ and $b$ in the system , the wave-function will pick up a minus sign , $$ \psi_{ab} = - \psi_{ba} . $$ thus we say that electrons are antisymmetric . by the spin-statistics theorem , all fermions ( electrons , leptons , quarks etc ) are antisymmetric . if , on the other hand , the behaviour was simply , $$ \psi_{ab} = \psi_{ba} , $$ the particle is said to be symmetric . by the spin-statistics theorem , all bosons ( photons , $w$ , $z$ etc ) are symmetric . what is the physical meaning of this ? what are the implications ? well , one famous implication is the pauli exclusion principle . suppose i have two indistinguishable electrons ( with the same quantum numbers ) , they cannot be labelled independently , we simply have $a=b$ . what is their wave-function ? it must be antisymmetric , so we have $$ \psi_{ab} = - \psi_{ba} \rightarrow \psi_{aa} = - \psi_{aa} = 0 $$ i.e. it is impossible to have antisymmetric particles in identical quantum states .
if we restrict ourselves to newtonian gravity , then it is indeed temporally symmetric . orbits require the orbiting body to be gravitational bound to the central object---i . e . they must have negative energy ( i.e. . the magnitude of the potential energy is greater than the kinetic ) relative to the central body . one way gravity can do this is by exchanging energy with a third body . this is still time-symmetric . ( thanks @michaelbrown for reminding me ) . in general , for something to ' enter ' an orbit without such a three-body type interaction , there needs to be a mechanism of dissipating its initial ( non-negative ) energy . in practice , how this would happen depends on the context . for stellar systems , it could happen from tidal dissipation ; for satellites , i guess it could be atmospheric drag ; or for a spaceship it could be its engines .
there is a quite instructive paper g . a . alekseev and v . a . belinski , equilibrium configurations of two charged masses in general relativity , phys . rev . d76 ( 2007 ) 021501 ; arxiv:0706.1981 [ gr-qc ] , e.g. they mentioned a work about non-existence of static equilibrium configurations of two charged black holes by p . chrusciel and p . tod , commun . math . phys . , 271 577 ( 2007 ) ; arxiv:gr-qc/0512043 and found condition for equilibrium of two charged masses : $m_1 m_2 = ( e_1-\gamma ) ( e_2+\gamma ) $ with $\gamma = ( m_2 e_1-m_1e_2 ) / ( l+m_1+m_2 ) $ .
this question was asked four months ago , but none of the existing answers mentions trebuchets . to my knowledge the trebuchet design is the only design that is purely mechanical . other projectile throwing devices store elastic energy and on release transfer that to the projectile . so a trebuchet it is . ( in effect ' lever ' and ' trebuchet ' are synonymous . a trebuchet gets its leverage by being a lever ( pun intended ) ) . it may be worthwhile to research the noble art of pumpkin chunkin ' . there is a town in the us where there is a yearly pumpkin chunkin ' contest that also features a trebuchet category . can a trebuchet deliver close to 100% efficency ? for one thing , frictional losses can be kept quite low , that is good . the problem is this : to throw the projectile the lever arm must be accelerated to a large angular velocity . after the projectile has been released the lever arm is swinging violently . that kinetic energy of the lever arm is energy from the counterweight that was not transferred to the projectile . the trebuchet design involves trade-offs . a longer lever arm gives the potential for faster throws , but a longer arm also has a bigger moment of inertia . an ideal trebuchet would transfer all of its kinetic energy to the projectile , so that right after releasing the projectile it would just sit there , nearly motionless . i am not aware of any trebuchet design that achieves that . to reduce the inefficiency the mass of the lever arm must be as low as possible . that is what the pumpkin chunkers do : they make the lever arm as flimsy as they dare . for every throw they stand well back , as their machines tend to self-destruct when the trigger is pulled .
the important factor is not the absolute speed of the atoms but the speed relative to each other . i would guess the article on the rubidium atoms is related to making a bose-einstein condensate , and slow relative speed is needed otherwise your cloud of atoms just disperses instead of making a condensate . increasing the speed of an isolated atom makes absolutely no difference to it , but if you increase the relative speed of atoms in some assemblage the atoms will collide with each other at that speed . as you increase the relative speed from the low levels of the rubidium atoms the collision energy will grow to the point where it ionises the atoms , so the atoms form a plasma . if you carry on increasing the speed up to near light speed the collisions between the nuclei will be so violent that they completely destroy the atomic nuclei and break them into showers of hadrons . we can be confident about what happens in near light speed collisions , because this is exactly what the rhic does . the lhc has also done heavy ion collisions in between finding the higgs boson . it is worth a note that tokamak fusion reactors work by raising deuterium and tritium ions to very high relative speeds so that the collisions cause the nuclei to fuse . the corresponding temperature is about 100 million degrees , though from a quick google i could not find the velocities of the ions . you do not get fusion in the rhic/lhc experiments because the energies are so high that the nuclei are completely blown apart .
i would guess that the article is referring to solitons . i am not sure if every non-linear system gives soliton solutions , but many do . the wikipedia article i have linked gives lots of examples of classical solitons , but i am not sure to what extent ( if at all ) they are important in the standard model . perhaps one of the qft specialists hereabouts could comment .
as you approach the event horizon , the doppler shift of any electromagnetic signals you send back out approaches infinity . any distant observer will get old and die while monitoring your signals that get slooower and sloooooower . your own experience is that you exist for some finite time ( not very long , for a black hole of a typical size ) , and then you hit the singularity and die . ( you may die earlier because of tidal forces . that depends on the size of the black hole . ) to you , the experience is that the doppler shifts of signals coming in from the outside become greater and greater . however , you do not get to see the arbitrarily distant future of the outside universe . for any location outside the event horizon , there is some latest time at which their signals can reach you before you go splat into the event horizon . i have never liked descriptions of this sort of thing that use phrases like " it seems to you like " or " you see " or " in your frame of reference . " general relativity does not have global frames of reference . it is meaningless to talk about whether something far away is happening " now " as judged " according to you . " all you can do is receive or transmit signals . you know they are just signals . btw , there is a classic science fiction novel , gateway , by frederik pohl , in which the protagonist makes himself really , really miserable by imagining that his lover , whom he abandoned to fall into a black hole , is forever cursing him " now . " a better understanding of general relativity would have been the best therapy for the poor guy -- but i guess that would have spoiled the book .
as you have mentioned , topological insulators ( ti ) are " topological " because they can not be smoothly connected to trivial band insulators without closing the band gap ( and without breaking certain symmetry ) . simply generalize this to the many-body case , we may say that the topologically ordered states are called " topological " because they can not be smoothly connected to the trivial product state without closing the many-body gap . to gain a better understanding , one should realize that " topology " is a complement to " geometry " . by geometry , we mean that there is a sense of measurement of the distance and angle etc . , and the shape and the size of the object matters . while by topology , we mean that one can continuously deform the object , and the shape or the size does not matter . so the topological properties are those properties that can endure continuous deformation of the state ( by continuity we mean without encountering a quantum phase transition ) . to protect the topological properties against deformation , a gap between the ground states and the excited states is always required . so topological property is only defined for gapped quantum matters ( both ti and topological order are within this scope ) . on the other hand , gapless quantum matters do not have topological properties , and their properties are geometrical . the topological/geometrical distinction is also reflected from the mathematical tools we used to study the physics . for gapped quantum matters , we use topological tools like homotopy , cohomology , k-theory , category theory etc . for gapless quantum matters , we use geometrical tools like gravity theory ( ads/cft ) .
the term vanished because we can translate this term to one making a statement about the fields at the boundary and assume that the fields themselves vanish in spatial and temporal infinity . by stokes ' theorem , we can translate volume integrals into surface integrals . more specifically gauss ' theorem states that the integral of a divergence of a field over a volume ( denoted $v$ ) to an integral of the field itself over the surface of that volume ( denoted $\partial v$ ) $$\int_v \textrm{div} \vec{a}\ , \textrm{d}v= \int_{\partial v} \vec{a}\ , \textrm{d}\vec{s}$$ this holds true in any dimension and metric . in minkowski-space the divergence ( called a four-divergence ) is exactly $\partial_\mu\phi$ thus , you can translate $$\int_v \partial_\mu\left ( \frac{\partial\mathcal{l}}{\partial ( \partial_\mu\phi ) }\right ) \ , \textrm{d}v = \int_{\partial v} \frac{\partial\mathcal{l}}{\partial ( \partial_\mu\phi ) }\ , \textrm{d}\sigma_\mu$$ i.e. if we assume that the fields ( and thus the lagrangian density ) vanishes in infinity , this term vanishes .
a hydrogen atom can be made to show chaotic behaviour if it is excited to very near it is ionisation energy . the maths is somewhat beyond me , but this paper discusses calculations of the hydrogen atom showing the onset of chaotic behaviour . you can find more by googling for " rydberg atom " combined with " chaos " or " chaotic behaviour " .
whenever it seems like two water levels should be equal but are not , either there is a physical restriction preventing flow ( like a dam keeping upstream waters higher than downstream , or surface tension causing meniscuses or capillary action ) , or there is energy being expended to put water back upstream as fast as gravity is pulling water downstream . in a sump , the lower water level can be maintained despite water continously flowing into it is due to there being a pump at the bottom , consuming electrical energy to move water from that lower section back to the higher section . so there appears to be continuous flow of water yet no change in the different water level . the maximum height difference then depends on the power of the pump ( how much work it can do per unit time , or technically how much water it can move back up per unit time ) . note that since energy is flowing into the system at one point , the new equilibrium is unequal water levels ( it stays unequal so long as you keep the pump on ) . if you turned the pump off , the water level equalises quickly . same thing for a waterfall . why is there always water at the top of the waterfall to keep it flowing even though all the water should logically be at the bottom ? the sun is providing the energy to evaporate water from the bottom creating clouds and rain to put water back on top--rinse and repeat . always look for an energy source--it is the reason for a different equilibrium condition .
i will try to address your question , though , as david says in the comments , it is evident that you have very little background in elementary particle physics . i will bring over an event much simpler than a display of an event that could show a higgs particle decay . here is a simple antiproton annihilation event whose end particles are recorded by their passage through a bubble chamber which also has a magnetic field perpendicular to the picture . the antiproton enters from below and hits a proton which is at rest , so not visible , in the bubble chamber liquid . it annihilates and eight pions come out , their momentum measured by the curvature , their mass by the ionisation track . where is the higgs field in this picture ? it permeates everything and at the point of interaction when the pions materialize it has supplied the masses to the quarks and antiquarks that they are made up of . the simulated higgs event display you have attached shows the decay products of the higgs boson . this particle is predicted by the standard model and it is necessary to find and confirm it in order to validate the sm . it appears because a higgs field exists , but it is a particle in the data set of particles predicted and mostly found by the sm . in the real experiment , a number of events with two photons , for example , have been accumulated so that the claim of seeing a higgs like particle has been established statistically . a lot of work remains to make sure that the bump seen has really the decay branching ratios and spin and statistics expected from the sm before the discovery of the higgs boson is established unequivocally . then we could state with some certainty that the standard model which depends on the existence of a higgs field is validated . so it should be clear that each individual event is not like a spider that can be dissected . it is an instant of the materialization of the fields and the experiment has to accumulate enough events to statistically establish an observation that validates a hypothesis .
i believe most of the em-spectrum from a nuke is low energy . so when you take a photo you get a lot of visible light and heat , and only small amounts of high energy radiation on your film . also the lens might be transparent to visible light , but non transparent to high-energy em-waves . edit : just found http://www.fas.org/nuke/intro/nuke/thermal.htm it says that most of the inital photons are indeed in the x-ray spectrum , but these high energy photons are quickly absorbed by the atmosphere . so the spectrum reaching the photographer is determined by the atmosphere transparency .
grassman $d\theta$ has opposite mass dimension to $\theta$ , which is why the notation is not 100% optimal , it confuses on this issue . but if you know how to evaluate the integral , that it goes like the derivative , then you know how change of scale works , and it is the opposite of normal change of scale : $$\int d ( k\theta ) f ( k\theta ) = {1\over k} \int d\theta f ( \theta ) $$ and this is why the volume determinant for the integration ends up being the reciprocal of the bose case .
it applies to everything in the sense that it imposes an upper limit i.e. no field can propagate faster than the speed of light . a massive field obviously will not propagate at the speed of light , but massless fields do propagate at the speed of light ( provided other effects do not slow them ) . however there is a complication you should be aware of in that we define two velocities for wave motion , the group velocity and the phase velocity . the phase velocity can exceed the speed of light but it can not carry any information . the group velocity cannot exceed the speed of light . in the qm context the group velocity corresponds to the particle velocity .
the final condition for a realistic theory in physics is that its predictions ( of scattering amplitudes or correlators etc . ) have to agree with observations . this implies that the predictions have to be consistent and obey some general consistency conditions ( unitarity , non-negativity of probabilities , some symmetries , locality or approximate locality , and so on ) . for theoretical theories , it is just the general consistencies that hold . now , the task is to classify all possible theories and learn how to calculate with them . it turns out that the euclidean spacetime or world sheet is simply a simpler , more straightforward , more free-of-subtleties approach to produce a machine that calculates some scattering amplitudes or other observables . at least formally , the euclidean theories may be continued to analytic ones and vice versa . for nontrivial spacetime topologies , the euclidean objects are likely to be more manageable . for example , the world sheets in string theory ( think about a torus or pants diagrams etc . ) are much more well-behaved in the euclidean signature so we may consider this approach " primary " . covariant calculations in string theory are almost always done with the euclidean ones . the result may be continued to the minkowski momenta etc . and some of the consistency conditions above are still guaranteed to hold because of some properties of the complex calculus . in the light-cone gauge , we may work directly with the minkowski-signature world sheets . but we pay the price that the interaction points where strings split or join are singular and the direction of the " future time " is ambiguous . we must also include contact terms , higher-order interaction terms , to deal with some divergences caused by the singular world sheets , but when these things are summed over , we may prove that the resulting amplitudes agree with the covariantly computed ones ( in the euclidean signature ) . gravity in $d\geq 4$ ( and maybe 3 ) suffers from the " negative norm conformal factor " . the euclideanized einstein-hilbert action $\int r \sqrt{g}$ is no longer positively definite . in particular , if you consider scalar waves that scale the metric by an overall number , $g_{\mu\nu}=e^f\eta_{\mu\nu}$ , and derive the kinetic term for $f$ , it will have the opposite sign than the kinetic term for other components of the metric tensor ( the physical polarizations of the gravitational waves , like $g_{xy}$ ) . it follows that the action will be bounded neither from below nor from above , and $\exp ( -s_e ) $ in the euclidean path integral will diverge in some region of the configuration space . in this sense , people believe that the minkowskian path integral must be the " more kosher one " for higher-dimensional gravity . but this is a bit empty statement because at the quantum level , higher-dimensional gravity obtained as a direct quantization of einstein 's equations is inconsistent , anyway . and string theory which is consistent and contains gravity does not give us any tool to directly rewrite the path integral in terms of spacetime fields including the metric ; it is not a field theory in the ordinary sense . so the preference for the " minkowski signature " is a bit vacuous . after all , the minkowskian action is not bounded from either side , either . this is considered " not to be a problem " because the integrand is $\exp ( is ) $ which still has the absolute value equal to one , so it does not diverge . but i would personally say that the unboundedness of the euclidean action is the " same " problem for the minkowskian path integral . quite generally , the wick rotation is extremely important in quantum field theory and it is actually even more important in quantum gravity or places with many spacetime ( or world sheet ) topologies , i.e. in situations where one has many different " time variables " in which we might try to expand things . one should not be afraid but at the end , whatever theory he deals with , he gets some amplitudes whose self-consistency ( and/or consistency with observations ) must be verified . with some " good rules of behavior " while wick-rotating , one may be " pretty sure " that some tests will be passed .
i will give you the derivation from my book which includes a nice way to see how the delta functions arise : . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . we can derive the potential field $\vec{a}$ and the electromagnetic fields $\vec{e}$ and $\vec{b}$ of a vector point dipole and an axial point dipole in the same way as we derive these in the case of a the point monopole . we start with a static point charge $\delta ( \vec{r} ) $ and derive the dipole charge/current densities with the help of differential operators . we apply the same differential operators for $\vec{a}$ , $\vec{e}$ and $\vec{b}$ to obtain the dipole fields from the monopole fields . first we recall the fields of the monopole . the point charge obtains ( over time ) a potential field given by . $ \mbox{field}\big\{\ , \delta ( \vec{r} ) \ , \big\} ~~=~~ \frac{1}{4\pi r} $ the reversed operator which derives the source from the field is just the laplacian operator . $ -\nabla^2\big\{\ , \frac{1}{4\pi r}\ , \big\} ~~=~~ \delta ( \vec{r} ) $ integrating the delta function over space shows equal contributions from the three spatial components . $ \int \delta ( \vec{r} ) ~d\vec{r} ~=~ -\int\left [ \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}\right ] \frac{d\vec{r}}{4\pi r} ~=~ \frac13+\frac13+\frac13 ~=~ 1 $ we can define vector dipole and axial dipole sources by using differential operators on the monopole $\delta ( \vec{r} ) $ and derive their potential and electromagnetic fields . the vector dipole is obtained by differentiating the monopole along the direction that we want the dipole to have . the result is a combination of a positive and a negative delta function . the axial dipole is defined by the curl of the monopole so that it gives a circular point current in the direction of the dipole . charge/current densities , potentials and fields of dipoles : $ \begin{array}{|lcll|} \hline and and and \\ j^o and = and ~~~\mbox{div}\ , \left ( ~\vec{\mu} \ , \delta ( \vec{r} ) ~\right ) and \mbox{vector dipole charge density} \\ \vec{j} and = and ~~~\mbox{curl}\left ( ~\vec{\mu}\ , \delta ( \vec{r} ) ~\right ) and \mbox{axial dipole current density} \\ and and and \\ a^o and = and ~~~\mbox{div}\ , \left ( ~\vec{\mu} \ , \frac{1}{4\pi r}\ , \right ) and \mbox{vector dipole electric potential} \\ \vec{a} and = and ~~~\mbox{curl}\left ( ~\vec{\mu} \ , \frac{1}{4\pi r}\ , \right ) and \mbox{axial dipole magnetic potential} \\ and and and \\ \mathsf{e} and = and -~\mbox{grad}\left ( \mbox{div}\ , \left ( ~\vec{\mu} \ , \frac{1}{4\pi r}\ , \right ) \right ) and \mbox{vector dipole electric field} \\ \mathsf{b} and = and +~\ , \mbox{curl}\left ( \mbox{curl} \left ( ~\vec{\mu}\ , \frac{1}{4\pi r}\ , \right ) \right ) and \mbox{axial dipole magnetic field} \\ and and and \\ \hline \end{array} $ the potential fields are obtained by applying the same differential operators on the field $1/4\pi r$ of the monopole rather than on the charge distribution . for a dipole moment in the $z$-direction we obtain in si-units . potential fields of the electric and magnetic dipoles : $\phi ~=~ \frac{z}{4\pi\epsilon_o r^3}\ , \mu~~~~~~~~ \vec{a} ~=~ \big ( -y ~ , ~ x~ ~ , ~ 0 \big ) ~ \frac{\mu_o}{4\pi r^3}\ , \mu $ the expressions for the electromagnetic fields do implicitly contain delta functions at the center with the right magnitude . these delta functions are easily lost if the derivation is not careful enough . the e and b fields are related to each other by the standard vector identity . $\mbox{curl} ( \mbox{curl}\vec{x} ) ~=~ \mbox{grad} ( \mbox{div}\vec{x} ) -\nabla^2\vec{x} $ we have seen that the last term ( the laplacian ) yields $\delta ( \vec{r} ) $ . the two therefor differ only at the center by a delta function in the $\vec{\mu}$ direction . we can see this explicitly if we align the dipoles with the z-axis , so that $\vec{\mu}$ has only a z-component , and write out the fields . the only difference is in the z-components . the total difference between the two is the laplacian and thus $\delta ( \vec{r} ) $ . the vector dipole gets -1/3 while the axial dipole gets +2/3 . $\begin{aligned} \mathsf{e} and = and \frac{1}{\epsilon_o}\bigg [ ~ \mathbf{\hat{x}}\ , \frac{\partial}{\partial x}\frac{\partial}{\partial z} ~+~ \mathbf{\hat{y}}\ , \frac{\partial}{\partial y}\frac{\partial}{\partial z} ~+~~~ \mathbf{\hat{z}}\ , \frac{\partial^2}{\partial z^2} ~~~~~~~~~~~~~ and \bigg ] ~ \frac{\mu}{4\pi r}\\ \\ \mathsf{b} and = and \mu_o\bigg [ ~ \mathbf{\hat{x}}\ , \frac{\partial}{\partial x}\frac{\partial}{\partial z} ~+~ \mathbf{\hat{y}}\ , \frac{\partial}{\partial y}\frac{\partial}{\partial z} ~-~ \mathbf{\hat{z}}\left ( \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}\right ) ~ and \bigg ] ~ \frac{\mu}{4\pi r}\\ \end{aligned} $ which we can see from the third equation with the three 1/3 parts . in the general case , with arbitrary dipole direction $\vec{\mu}$ we get for the electromagnetic fields in vector form . electromagnetic dipole fields : $ \begin{aligned} \mathsf{e} and = and \frac{1}{\epsilon_o}\left ( ~\frac{3\left ( \vec{\mu}\cdot\hat{r}\right ) \hat{r}-\vec{\mu}}{4\pi r^3} ~-~\frac13\vec{\mu}\ , \delta ( \vec{r} ) ~\right ) \\ \\ \mathsf{b} and = and \mu_o\left ( ~\frac{3\left ( \vec{\mu}\cdot\hat{r}\right ) \hat{r}-\vec{\mu}}{4\pi r^3} ~+~\frac23\vec{\mu}\ , \delta ( \vec{r} ) ~\right ) \end{aligned} $ alternatively we can look at the fields explicitly expressed in the individual $x$ , $y$ and $z$ components which gives . ( with the dipole moment in the $z$-direction ) $\begin{aligned} \mathsf{e} and = and \frac{\mu}{4\pi\epsilon_o}\bigg [ ~ \mathbf{\hat{x}}\ , \frac{3xz}{r^5} ~+~ \mathbf{\hat{y}}\ , \frac{3yz}{r^5} ~+~~~ \mathbf{\hat{z}}\ , \left ( \frac{3zz}{r^5}-\frac{1}{r^3} - \frac{4\pi}{3}\delta ( r ) \right ) ~ and \bigg ] \\ \\ \mathsf{b} and = and \frac{\mu_o\mu}{~4\pi~}\bigg [ ~ \mathbf{\hat{x}}\ , \frac{3xz}{r^5} ~+~ \mathbf{\hat{y}}\ , \frac{3yz}{r^5} ~+~~~ \mathbf{\hat{z}}\ , \left ( \frac{3zz}{r^5}-\frac{1}{r^3} + \frac{8\pi}{3}\delta ( r ) \right ) ~ and \bigg ] \\ \end{aligned} $ in general the fields decrease with the third order of $r$ compared to $1/r^2$ for the charge which makes it possible not many effects at larger scale . at the other hand , when we go to smaller scales , the magnetic dipole fields become more powerful relative to the charge .
it is a little tricky giving a formal answer to this , but here is a sketch : . the electromagnetic potential of the kerr-newman hole is given by : $$a_{a}dx^{a}=-\frac{qr}{r^{2}+a^{2}\cos^{2}\theta}\left ( dt-a \sin^{2}\theta d\phi\right ) $$ this field will acquire a magnetic field from the fact that $\frac{\partial a_{\phi}}{\partial r}$ and $\frac{\partial a_{\phi}}{\partial \theta}$ are both nonzero . the problem is that the magnetic field , when re-phrased in terms of vectors and not one-forms , will fall off as $\frac{1}{r^{3}}$ . at that point , if we are keeping terms that fall off that quickly , then we need to have a discussion about that asymptotic form of the metric you imply above , because there are terms in the metric that we need to keep , arising from frame-dragging effects of the black hole . if you want , i can go into more detail . edit : ok , so once we have the vector potential , we can calculate the magnetic field according to the rule : $b^{i}=\frac{1}{\sqrt{\left|g\right|}}\epsilon^{ijk}\left ( \frac{\partial a_{j}}{dx^{k}}-\frac{\partial a_{k}}{dx^{j}}\right ) $ , where $\epsilon^{r\theta\phi}=1$ , $\epsilon^{ijk}=-\epsilon^{jik}=-\epsilon^{ikj}$ and $\epsilon^{ijk}=0$ if $i=j$ , $i=k$ or $j=k$ . so , now , we just plug in the above expression for the spatial components of $a_{a}$ , the metric tensor , and turn the crank . after doing this , and taking the limit that $r$ is larger than everything else , we find that $${\vec b}=\frac{2qa\cos\theta}{r^{3}}{\hat e}_{r} + \frac{qa \sin \theta}{r^{3}}{\hat e}_{\theta}$$ sensibly , this is zero if either $q=0$ or $a=0$ . and i will once again assert that there are $\frac{1}{r^{3}}$ corrections to the gravitational force that must be taken into account in your high $r$ limit if you are going to keep this magnetic field .
i think what you are getting at is not some kind of mathematically rigorous equivalence , but more what it means for a particle physics experiment like atlas to collect 1 inverse femtobarn of data . and actually this is computable quire easily . the design frequency of the lhc is 40mhz ( which corresponds to 25ns bunch spacing , but now i is at 50ns ) . but since most events are uninteresting background all modern experiments have a system called a " trigger " which only records events which pass some rough requirements which would render them interesting ( maybe a high-momentum electron or jet ) . atlas is routinely recording at 300hz ( a $10^5$ reduction of rate from the initial collision rate ) . that is 300 events per second . the size of an event in terms of storage space varies from experiment to experiment and depends on the software it uses , but for atlas it is something of the order of 1.5mb/event . currently the lhc runs at peak luminosities of 12600 $\mu b^{-1}/s$ ( microbarn per second ) , this decreases over time since the beam intensities decrease , let 's just run with 1000$\mu b^{-1}/s$ . an inverse femtobarn is $10^9\mu b^{-1}$ so we have : $$\frac{300 \text{ events}}{s}\frac{1.5 \text{ mb}}{\text{ event}}\frac{s}{1000\mu\text{b}^{-1}} \approx 0.5 \frac{\text{mb}}{\mu\text{b}^{-1}}$$ so for $10^9 \mu b^{-1}$ we have $$0.5\cdot10^9\text{mb}$$ so 500 tb of data ps : this is just an back-of-the-envelope calculation of course . the rates are constantly changing and the luminosities as well . so collecting 1/fb of data in a low lumi setting requires much more data ( since one would still max out the bandwidth of 300hz recording ) than in high lumi settings ( where one is still bound by the 300hz boundary , so the trigger would have to do a tighter selection )
the resolution is controlled by diffraction at the smallest part of the lens system . the wikipedia article on angular resolution goes into this in some detail . to quote the headline from this article , for a camera the spatial resolution at the detector ( or film ) is given by : $$ \delta \ell = 1.22 \frac{f\lambda}{d} $$ where $f$ is the distance from the plane of the lens to the detector , $\lambda$ is the wavelength of the light and $d$ is the camera aperture . making the pixel size smaller than $ \delta \ell$ will not do any harm , but it will not make the pictures any sharper . i do not know if smartphone cameras contain a variable aperture . with conventional cameras larger apertures produce less diffraction so the picture quality should actually improve in low light . however larger apertures expose a larger area of lens and optical aberration dominates the quality . the end result is that there is an optimum aperture below which diffraction dominates and above which optical aberration dominates . incidentally , the poor performance at low light probably is not due to diffraction . i would guess it is just that the signal to noise ratio of the detected light falls so far the pictures get very noisy .
the answer to your question is very wide and include many phenomena . there is no single mechanism that converts absorbed energy into heat . i will give you a general overview on this topic . heat is transferred by radiation which is in general an electromagnetic wave ( not light only ) . for example , ir radiation alone provides 49% of the heat provided to earth from sun radiation . have a look at heat section of this page . in electromagnetic spectrum , the wavelength varies significantly , which means that materials respond differently to different ranges of wavelengths . for example ( listing from long wavelength to short wavelength ) : radio waves : the wavelength here is very long such that the materials constituents ( atoms and molecules ) do not sense the waves . thus most solids are transparent to radio waves , which means there is no wave-material interaction . there is no heat generated . the transparency is clear as you can get cellphone reception in your home as the wave simply travel through walls and our bodies . microwaves : the wavelength here is shorter , the photons energy in this level are comparable to the energy levels of the molecular rotational levels , which means that a molecule could absorb a microwave photon and start to rotate . this rotation is a form of kinetic energy of molecules which is translated as heat when you look at the whole ensemble of molecules . this mechanism is the mechanism known for microwave heating in your kitchen . ir : the photon energy in this range is comparable to molecular vibrational levels . so if a molecule absorbed an ir photon it will vibrate , which is translated into heat when you look at the whole ensemble of molecules . visible light : things get complicated here , the photon energy here is comparable to electronic levels within an atom . there is no simple direct explanation of how photon energy is transformed into heat . one can argue for visible light that an electron can absorb a photon and radiate it without increasing kinetic energy of atoms ( no increase oh heat ) . that is true but that is a very simplistic picture of what happens in reality . first because the visible light is continuous spectrum , while the photons that can be absorbed by electrons are discrete . so there are many visible light photons with wavelengths that electrons can not absorb , which could be absorbed by ions or affect polar bonds somehow if they existed in the material . second , some materials like solids ( where heat by radiation is efficient ) have energy bands rather than the simplistic electron levels concept . so the reaction of materials to visible light in this case is complex . but one of the mechanism of transforming photon energy to heat that i can think of here is the absorption of visible light by ion lattice in some crystals . uv : the photons in this range have enough energy to ionize the atoms they hit ( by liberating an electron from its orbital ) . again , there is no simple direct explanation of how photon energy is transformed into heat . one possible route is the inelastic scattering of electrons with materials atoms . for example , have a look at page 5 of this report where it is mentioned that inelastic electron scattering can induce phonons ( lattice vibration , which is a form of kinetic energy of atoms ) x rays : the photon energy in this range is much larger than the energy required to move electron from one level to other . x rays photons can ionize an atom and become lower energy photon through compton scattering . there is no simple direct explanation of how photon energy is transformed into heat . briefly , heating by radiation in general can not be attributed to a simple explanation . the mechanisms through which photons energy is transformed into heat depend on the energy of the photon and the properties of the material . the simplistic picture of a photon being absorbed by electron is narrow to explain conversion to heat because it only describes a single electron in a free atom . that is a narrow view of material properties . have a look here and the nice figure of non-ionizing radiation section of this page hopefully that was helpful
i had not realized this had been happening , very interesting . hand waving : when the intensity of the laser is high enough electrons can be accelerated to relativistic energies and create e+e- pairs interacting with the collective electric and magnetic fields of the laser pulse . here is a link with some semiclassical calculations . electrons have to be supplied from the atoms of a gas on which the laser is shining . the interaction is a collective field one with the electron , in these calculations , not with the nucleus . and another using quantum electrodynamics : production of electron-positron pairs from vacuum in the combined electromagnetic fields of a high-intensity laser pulse and an atomic nucleus is studied within the framework of laser-dressed quantum electrodynamics . the focus lies on the influence exerted by a finite laser pulse length on the energy spectra of created electrons and positrons , which is examined in a broad range of field frequencies and intensities . the results for an isolated short laser pulse are also compared with corresponding calculations for an infinite train of laser pulses . it is shown that the laser pulse length and its carrier-envelope phase have a substantial effect on the pair creation process , leading to both quantitative and qualitative differences in the particle spectra . it is behind a pay wall so i cannot see what " atomic nucleus " means in this case . certainly the diagrams will be different than the gamma-nucleus diagrams .
when can a natural satellite possibly be in polar orbit around it is primary ? well , normally you do not expect to see this . as explained in why are our planets in the solar system all on the same disc/plane/layer ? and the questions linked to it ( such as accretion disk physics - stellar formation ) , we expect that when clouds of gas and dust condense into stars , planets , and moons , everything will be confined to a disk with the same angular momentum ( or at least the same direction of the vector ) . if you do see a polar orbit , or more generally a highly inclined* orbit , then it is an indication that one of the following probably occurred : the smaller body was captured after the main body formed , the smaller body 's orbit was altered by ( usually gravitational ) interactions with other bodies , or the larger body 's spin was changed after the formation of the system . are there any such observations on record ? why , yes indeed ! in our own solar system , my favorite example ( there are many others ) is neptune 's moon triton . it is in a retrograde orbit , and the general consensus is that it falls into category ( 1 ) above . it was orbiting the sun in the outskirts of the solar system in what was probably an eccentric orbit , it got too close to neptune , and it is close approach to neptune completely by chance had it on the side of the planet such that it is orbit ended up going opposite to neptune 's spin . while the planets in our solar system are not particularly inclined with respect to one another or the sun 's spin , this is not true of some other planetary systems we have observed . the past few years have seen a small explosion of measurements of exoplanetary systems ' " spin-orbit misalignment " using the rossiter–mclaughlin effect . for a nice light summary of this effect , you can take a look at [ 1 ] , especially the figures . since that paper was written , many more exoplanets have been discovered , and many more astronomers have taken spin-orbit alignment data . the general consensus is that most systems are aligned , but there are some notable retrograde and polar orbits . it remains to be seen what this implies for our models of planetary formation and migration . there may even be an explanation for these inclined systems in ( uninteresting ) observer bias or ( interesting ) stellar physics , as discussed in [ 2 ] , which also gives a list of measured inclinations for systems with relatively precise data . * inclination is defined as the angle between the main body 's spin axis and the orbiting body 's orbital axis . $0^\circ$ means perfect alignment in an equatorial orbit , $90^\circ$ is for a polar orbit , and $180^\circ$ is for a retrograde orbit ( you are back in the equatorial plane , but going around the " wrong " way ) . [ 1 ] winn , 2006 , " exoplanets and the rossiter-mclaughlin effect . " [ 2 ] winn et al . , 2010 , " hot stars with hot jupiters have high obliquities . "
the solution is to realize that the steady-state solution of a harmonically driven system must also oscillate harmonically . ( as regards your solution , this means that spectrally the $c_i ( \omega ) $ are delta functions , which resolves your contradiction . ) thus one usually begins by postulating the oscillatory ansatz $$c_a ( t ) =c_a e^{-i\omega_a t} , \ , \ , c_b ( t ) =c_b e^{-i\omega_b t} , $$ where the $c_a$ and $c_b$ are now constants . as an ansatz this is harmless and if it turns out to not be a solution you can drop it ( but as it happens it will ) . your schrödinger equation thus reads $$ \begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v e^{i t \omega _d} \\ \frac{1}{2} v e^{-i t \omega _d} and \frac{\omega _0}{2} \end{pmatrix} \begin{pmatrix} c_a ( t ) \\ c_b ( t ) \end{pmatrix} = i\frac d{dt} \begin{pmatrix} c_a ( t ) \\ c_b ( t ) \end{pmatrix} $$ so $$ \begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v e^{i t \omega _d} \\ \frac{1}{2} v e^{-i t \omega _d} and \frac{\omega _0}{2} \end{pmatrix} \begin{pmatrix} c_a e^{-i\omega_a t}\\ c_b e^{-i\omega_b t} \end{pmatrix} = \begin{pmatrix} \omega_a c_a e^{-i\omega_a t} \\ \omega_b c_b e^{-i\omega_b t} \end{pmatrix} $$ or $$ \left\{ \begin{array}{ccc} -\frac{\omega _0}{2} c_a e^{-i\omega_a t} and +\frac{1}{2} v e^{i t \omega _d}c_b e^{-i\omega_b t} and = \omega_a c_a e^{-i\omega_a t} , \\ \frac{1}{2} v e^{-i t \omega _d} c_a e^{-i\omega_a t} and + \frac{\omega _0}{2} c_b e^{-i\omega_b t} and = \omega_b c_b e^{-i\omega_b t} . \end{array} \right . $$ this needs you to set $\omega_a+\omega_d=\omega_b$ , after which you can eliminate the time dependence . that leaves you with the simple linear system $$ \left\{ \begin{array}{ccc} -\frac{\omega _0}{2} c_a +\frac{1}{2} v c_b = \omega_a c_a , \\ \phantom+ \frac{1}{2} v c_a + \frac{\omega _0}{2} c_b = ( \omega_a+\omega_d ) c_b , \end{array} \right . $$ which is an eigenvalue system for the hamiltonian $h=\begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v \\ \frac{1}{2} v and \frac{\omega _0}{2} -\omega_d \end{pmatrix}$ . since you tagged this as homework , i will leave the calculation here , as i am sure you are better off calculating eigenvectors and eigenvalues on your own .
there is no simple relationship between the worldsheet objects and spacetime objects ( such as superpotentials ) in this curved case ( and many other general cases ) . the lg-like models correspond to calabi-yau compactifications whose typical radii are comparable to the string length so it is very hard to see the spacetime geometry - and moreover , the mirror symmetric geometry is as easy or as hard to see as the " original " one , whichever is which . moreover , much of the world sheet objects are auxiliary and do not survive into spacetime . in particular , the whole world sheet supersymmetry is an auxiliary symmetry - a gauge symmetry - that does not really survive in spacetime . its existence has consequences in spacettime ( nonrenormalization theorems etc . ) but its detailed building blocks do not have any counterparts because they are partly " unphysical states " from the spacetime viewpoint . so the way to proceed when identifying the topology of gepner-like models is to find the hodge numbers and primary operators , choose a corresponding topology of a calabi-yau space that matches the constraints , and verify that this candidate is indeed equivalent . to see that there can not be any coordinate-by-coordinate map between the world sheet pieces and spacetime physics , note that the central charges of the world sheet cft are fractional . the relationship between gepner models and particular calabi-yaus is a form of duality , and as any duality , it must be nontrivial to be seen , otherwise it would not be interesting and it would not really be a " duality " .
it is not sound propagation but rather momentum diffusion that makes a fish swim . the rate at which momentum diffuses is determined by the kinematic viscosity , which for water is about $10^{-6} m^2/s$ . it takes minutes for momentum to diffuse in water over distances of centimeters , while the time scale over which a fish wiggles is tenths of a second . so , during a wiggle the water surrounding the fish does not ' communicate ' with any wall , and the fish does not notice any anomalous hydrodynamic effects .
like prahar had said , the problem reduces fairly simply in momentum-space . we note that , in such space : $\hat x = i\hbar\frac{\partial}{\partial p}$ and $\hat p=p$ , thus , using some auxiliary function $f$: $$ [ \hat x , \hat g ( \hat p ) ] f=i\hbar\frac{\partial ( \hat gf ) }{\partial p}-i\hbar\ , \hat g\frac{\partial f}{\partial p}=i\hbar\frac{\partial \hat g}{\partial p}f $$ by applying the product rule and reducing , this yields the correct result .
symmetry of the canonical energy-momentum tensor can be related to the spin of the object ( s ) that contribute to it ( in other words , the representation of the lorentz group under the fields transform ) . note that the canonical em tensor is obtained by using the noether 's procedure for translational symmetry $$ t_{\mu\nu} = \sum\limits_r \frac{\delta {\cal l}}{\delta \left ( \partial^\mu \phi_r \right ) } \partial_\nu \phi_r - g_{\mu\nu} {\cal l} $$ this expression is clearly not symmetric . however , we can comment about its ( non- ) symmetry by looking at the conserved quantity corresponding to lorentz transformation . by the noether 's procedure , we can show that this is $$ m_{\mu\nu\alpha} = t_{\mu\alpha} x_\nu - t_{\mu\nu} x_\alpha - \frac{\delta {\cal l}}{ \delta \left ( \partial^\mu \phi_r \right ) } \left ( j_{\nu\alpha} \right ) ^{rs} \phi_s $$ here , $\left ( j_{\nu\alpha} \right ) ^{rs}$ is the representation of the lorentz algebra under which the set of fields $\phi_r$ transforms . conservation of this quantity implies $$ 0 = \partial^\mu m_{\mu\nu\alpha} = t_{\nu\alpha} - t_{\alpha\nu} - \partial^\mu \left ( \frac{\delta {\cal l}}{ \delta \left ( \partial^\mu \phi_r \right ) } \left ( j_{\nu\alpha} \right ) ^{rs} \phi_s \right ) $$ this implies $$ t_{\nu\alpha} - t_{\alpha\nu} = \partial^\mu \left ( \frac{\delta {\cal l}}{ \delta \left ( \partial^\mu \phi_r \right ) } \left ( j_{\nu\alpha} \right ) ^{rs} \phi_s \right ) $$ non-symmetry of the stress-energy tensor is an indication that the fields that are contributing to it transform non-trivially under the lorentz group . in particular , the canonical em tensor is symmetric only if the theory contains only scalars . the way i like to think about the process of " symmetrizing the em tensor " is the following . the canonical em tensor does not contain any " spin information " and one needs the angular momentum tensor for that information . however , a symmetrized em tensor is essentially defined to " absorb " in the spin information of the field content so that the angular momentum tensor is no longer needed ( it contains more information ? ) . the reason i think of this like this is that in terms of the symmetric em tensor we can define another conserved quantity $$ {\tilde m}_{\mu\nu\alpha} = {\tilde t}_{\mu\alpha} x_\nu - {\tilde t}_{\mu\nu} x_\alpha $$ since ${\tilde t}_{\mu\nu}$ is symmetric the tensor above is trivially conserved and does not contain any new information . however , this modified angular momentum tensor still generates all the conserved quantities as $m_{\mu\nu\alpha}$ . it then seems to me that ${\tilde t}_{\mu\nu}$ already contains information about the conserved angular momentum . this also reconciles with the fact that the symmetrized em tensor is often the same as one obtains by varying the metric , usually defined as $$ t_{\mu\nu} = \frac{2}{\sqrt{-g}} \frac{\delta s}{\delta g^{\mu\nu} } $$ since the metric ( gravitation ) couples to all particles in a universal fashion , the above definition of the em tensor should involve spin information as well , and therefore , must be equal to ( or atleast closely related to ) ${\tilde t}_{\mu\nu}$ described above .
for 2d/3d cases the ellipse has 5/7 degrees of freedom . you say you have 3 points and their " tangents " . a single 2d/3d point gives 3/5 equations . 3 points give 9/15 equations . so that in both 2d and 3d cases you have an overdetermined equation system ( actually it is overdetermined even with 2 points ) . in the general case it may be solved for instance by one of those methods .
a newton is 1 ${\rm kg\cdot \rm m/s^2}$ , not 1 ${\rm kg\cdot s^2/m}$ . also , you need to take the square root at the end .
in this book they first non-dimensionalize the ns equations and then , assuming terminal velocity , small temperature differences , and using scaling arguments they arrive at the following relationship for the terminal speed of a particle moving buoyantly in a stratified flow : $v = \frac{g \alpha \delta t r^2}{6 \pi \nu}$ $\alpha$: coefficient of thermal expansion $\delta t$: temperature difference $g$: gravitational acceleration $r$: radius of spherical fluid element under consideration $\nu$: kinematic viscosity this is roughly what happens , i assume a lot of things as it is necessary for a sane understanding of the subject . initially , a ( stationary ) stably stratified homogeneous fluid is above the stove element . there are roughly three stages , generation , evolution , steady state . in the first stage , the element is turned on , fluid particles nearest to the element experience a change in temperature ( positive ) , kinetic energy increases and nearby fluid parcels propagate such information upwards ( can not go down , forget sides ) via diffusive heat transfer ( in the initial stages ) . at this point in time particles have barely moved from the surface ( considering the total timescale we are interested in ) . however , this process effectively gives rise to considerable upwards fluid velocity in fluid that is adjacent to the heat element . in the next stage , some fluid has accelerated to the point that we can now physically discern fluid motion . diffusion is no longer important ( it never really was ) , at this point advection takes the lead . a fluid parcel along the bottom , the warmest one with respect to all the other fluid parcels along the bottom , will , at this stage , begin to feel a little different from its neighbors . since the fluid parcel is warmer , it is also lighter than all the fluid around it , so , according to archimedes it must go up . this is very similar to the way the sun heats up the earth every morning and generates thermals . there will be lots of different convection cells owing to imperfections in the heating element , which will in turn give rise to eddying motion and turbulence . at steady state , the fluid will be moving away from the element at a steady pace in a way that you can average out all the nuances of fluid dynamics , so that you can arrive at that formula . sometimes though , i just wish we could give someone a million dollars .
per wikipedia , the electromagnetic tensor $f^{\mu \nu}$ contributes to the stress energy tensor $t^{\mu \nu}$ by $$t^{\mu \nu} = \frac{1}{\mu_0} \left ( f^{\mu \alpha} g_{\alpha \beta} f^{\nu \beta} - \frac{1}{4} g^{\mu \nu} f^{\gamma \delta} f_{\gamma \delta} \right ) $$ the einstein equations govern how the stress-energy tensor is coupled to spacetime curvature . since the magnetic field is entirely captured by the electromagnetic tensor , the answer is yes , magnetic fields contribute to gravitation .
there is quite a big controversy these days about the correct definition of the entropy in the microcanonical ensemble ( the debate between the gibbs and boltzmann entropy ) , which is closely related to the question . everyone agrees that the correct definition of the density matrix is given by $$\rho ( e ) =\frac{\delta ( e-h ) }{\omega ( e ) } , $$ where $h$ is the hamiltonian and $$ \omega ( e ) =tr\ , \delta ( e-h ) . $$ then the question is the correct definition of the entropy . boltzmann says $s_b=\ln \omega ( e ) $ , whereas gibbs argued $s_g=\ln \omega ( e ) $ where $$ \omega ( e ) =\int_0^e\omega ( e ) de . $$ in the text quoted by the op , the partition function corresponds to $\omega ( e ) $ . note that in most cases , in the thermodynamics limit , both entropies gives the same result . the question arises in the case of small systems and special cases with bounded from above spectra . hilbert et al . ( arxiv:1408.5382 and arxiv:1304.2066 ) argue that only the gibbs entropy is thermodynamically consistent . i must say that i find their arguments compelling , and that of their opponents , given in at least two comments of their papers , not at all .
if $f_{ii}=0$ then you are right that the $i\neq j$ constraint is unnecessary although it does make the physical interpretation clearer : all atoms ( other than $i$ ) act on $i$ . however , in practice , the force is usually given as a function of separation , $f ( r ) $ . and so when you evaluate $f_{ii}$ , you effectively evaluate $f ( 0 ) $ . the problem is that $f ( 0 ) $ is almost never zero . for instance , the coulomb and lennard-jones interactions are undefined for $r=0$ and so , when evaluating them ( either by hand or in code ) , you must explicitly skip the $i=j$ case .
the reason is because it is the lightest baryon with strangeness 3 , the mass energy of lightest strange object of s=1 ( the kaons ) is greater than 1/3 it is mass , so it can not decay into these . it is a general principle of energy conservation and strangeness conservation : the lightest example of any conserved quantum numbers can not decay without changing this number . in this case you need the strangeness to go down , and this requires weak decay , because , as dmckee says in his comment , the strong interaction respects flavor .
the einstein-hilbert action of general relativity , to make the variational principle fully rigorous , must be supplemented by a boundary term , $$s = \frac{1}{8\pi g} \int_{\partial m} d^3 x \sqrt{-h} \ , k$$ where $h_{\mu \nu}$ is the first fundamental form of a submanifold which we take to be $\partial m$ , the boundary of the spacetime manifold . the cuvature $k$ is the trace of the extrinsic curvature . so your concerns are justified , strictly speaking , one should include a boundary term , unless the manifold has no boundary . ( the boundary term was first derived by gibbons , hawking and york . for additional information , i highly recommend the gravitational physics lectures online from the perimeter institute by professor ruth gregory - her lectures are excellent . )
as the comments have mentioned , acceleration , torque and force can all be zero , but the key point is that some zeros are more fundamental than others . take acceleration : you state in a comment that newton 's law of gravity has an infinite range so the force/acceleration generated by some gravitating body can never be zero . this is quite true , but acceleration/force can be positive or negative so two non-zero accelerations can sum to zero . this is what dmckee means in his comment . even though the gravitational fields of the earth and moon are infinite , somewhere in between them is a point where the net force is zero . but ( ignoring some technical definitions ) you can not have a negative temperature . so i can not take a chunk of matter and make it is temperature zero by mixing it with another chunk of matter that has a negative temperature . the only way i can cool my chunk of matter is by putting it in contact with something that is colder , but of course still has a positive temperature . so 0k is special because there is nothing colder than it .
the heat capacity of an einstein solid is given by \begin{equation} c = nk \left ( \frac{\epsilon}{kt}\right ) ^{2} \frac{e^{\epsilon/kt}}{ ( e^{\epsilon/kt}-1 ) ^{2}} , \end{equation} where $n$ is the number of degrees of freedom . so the value of the energy quantum $\epsilon$ , or more precisely the ratio $x\equiv\epsilon/kt$ matters ! the above equation tends to the equipartition value $nk$ as $x \rightarrow 0$ , and the half maximum corresponds the value of $x$ satisfying \begin{equation} x^{2}\frac{e^{x}}{ ( e^{x}-1 ) ^{2}} =\frac{1}{2} . \end{equation} it turns out that the solution is $x_{\rm{half}} = 2.98287\ldots \approx 3$ , and this is the origin of the relation $kt/\epsilon\approx 1/3$ given to you .
energy exchange is quantized when moving a electron from one bound state to another bound state . this is not because the exchange is inherently quantized , but because the states the electron may occupy are quantized . thus the standard photo-electric effect in which a photon can not excite an atom unless it has a minimum energy . however , . . . there are multi-photon processes by which sub-threshold light can excite transitions . cross-section for them go by intensity-squared ( or worse ) and are very small for any reasonable light intensity . to study or employ them you get powerful , short pulsed laser systems . where short pulsed means nano-second or faster pulses and powerful means " do not look into beam with remaining eye " . even then you do not get a lot of rate . these processes are utterly negligible for the kind of benchtop experiment we use to teach the photoelectric effect : you just can not get enough intensity . ( see below for how negligible . ) the conceptual model here is that the first photon bumps the electron to a short-lived , unstable state without well defined quantum numbers , and the second comes along before that state decays and finishes the job . we are currently exploring the application of such a process to calibrating light yields , opacities in a large volume of scintillating material . from new j . phys . 12:113024 , 2010 : for gases the one-photon absorption cross-section $\sigma_1$ is typically of the order of $10^{−17}\text{ cm}^2$ , whereas the two-photon and the three-photon cross-sections are of the order of $\sigma_s = w/f_2 \approx 10^{−50}\text{ cm}^4\text{ s}$ and $\sigma_3 = w/f_3 \approx 10^{−83}\text{ cm}^6\text{ s}^2$ , respectively . where $f$ is intensity in photons/second and w is excitation rate in reciprocal seconds .
well to be hones both of your questions are related . let me start by rewriting your equations of $y_1$ and $y_2$ ( this will make the discussion easier ) , your version of $y_1$ and $y_2$ can be rewritten as : $$y_1=a\sin\left ( \frac{2\pi}{\lambda} ( x+vt ) \right ) , $$$$y_2=-a\sin\left ( \frac{2\pi}{\lambda} ( x-vt ) \right ) , $$where i put a minus outside of $y_2$ by using $\sin ( -x ) =\sin ( x ) $ . let 's now first look at question 2 . question 2 : the answer to this question can be found by looking at the wavefronts of $y_1$ and $y_2$ , which in its turn can be done by looking at a constant value for the arguments of $y_1$ and $y_2$ ( since a constant argument yields a constant value of $y_1$ and $y_2$ and hence a wavefront ) . let 's call this constant value of the argument $x_0$ , then the arguments of $y_1$ and $y_2$ become : $$x_0=x+vt \text{ for the argument of $y_1$} , $$$$x_0=x-vt \text{ for the argument of $y_2$} . $$ both arguments can be rewritten as:$$x=x_0-vt \text{ for the argument of $y_1$} , $$$$x=x_0+vt \text{ for the argument of $y_2$} , $$ where we see that the introduced constant $x_0$ denotes the position of the wavefront at $t=0$ . what this tells us is that $y_1$ represents a wave travelling in the negative $x$-direction ( when the time increases the value of $x$ decreases ) and $y_2$ represents a wave travelling in the positive $x$-direction ( when time increases the value of $x$ increases ) . in general . you can do this analysis for every kind of wave , and you will find that waves with an argument of the form $ ( x+vt ) $ are waves travelling in the negative x-direction ( als called ''left travalling waves'' ) and waves with arguments of the form $ ( x-vt ) $ are wave travelling in the positive x-direction ( also called ''right travelling waves'' ) . on hyperphysics a few more drawings and discussions are available ( should you be interested ) . $$$$ question 1 : the answer to question 1 can be given by the fact that te amplitudes of waves can be summed . this is because of the fact that each wave tells you what displacement $y$ is causes at a given point $x$ on a time $t$ , when you have two waves which interact , the displacements should be summed . now when you look at reflection ( of sound our light or whatever wave you are looking at ) , there are 3 things that can happen : ( first case on the figure ) : when you reflect the reflected wave picks up a phase $\phi=\pi/2$ ( this happens when you reflect on a dense medium ) . in that case your reflected wave picks up a minus sign since $\sin ( x+\pi/2 ) =-\sin ( x ) $ , this is probably the case you are looking at ) . ( second case on the figure ) : when you reflect the reflected and indecent wave are in anti-phase , so they cancel eachother out . ( third case on the figure ) : when you reflect , the reflected wave does not pick up a sign ( and they are in phase ) , the amplitude of the wave doubles .
when integrating in ( 2d ) polar coordinates you need to use a surface element : $$da = r\ ; dr\ ; d\theta$$ the reason for this can be seen geometrically : the surface element has the same shape as one of the spaces between two red and two blue lines ( a sort of curved rectangle ) . in the infinitesimal limit the area of one such segment is just its length multiplied by its width . the length is easy , it is $dr$ , and is always the same ( notice that the length of a blue segment between the evenly spaced red lines is always the same ) . the width is a bit more subtle . first , you might notice that the " inner width " and " outer width " are different . we do not need to worry about this because in the infinitesimal limit , they approach the same length . but the length of the arc between two evenly spaced blue lines clearly increases as the radial coordinate increases . it should be easy to convince yourself that the length of an arc that spans angle $theta$ is $r\theta$ ( of course $theta$ in radians ) . it follows that the width of the surface element is $r\ ; d\theta$ ( the angle shrinks to an infinitesimal , but the radial coordinate does not - it simply takes the value of the radius wherever we place our surface element ) . so why does not your integral using a line work ? well , this treats surface elements at all distances from the origin as having the same size ( i am speaking very loosely here , since infinitesimals do not really have a size ) . but looking at the diagram , clearly surface elements close to the origin need to be smaller than ones further away , otherwise you end up " overcounting " area near $r=0$ and under-counting area further out . put another way , take a bunch of long thin rectangles of the same size cut out of paper and try to arrange them into an approximate segment of a circle without overlapping them . you should find it is impossible . even as you make them infinitesimally thin this fails . you need to use little wedge shapes pieces .
your $ t=m_aa $ is wrong . it is $ t - f= m_aa $ . because tension due to $ m_b $ is pulling it forward and friction is trying to resist that force . using $v^2 = u^2 + 2as$ for calculating velocity after moving $ s = 0.03 m $ forward with initial velocity $ u = 0 m/s $ is correct ! .
i assume that what you are asking here is more an operational question rather than a conceptual question . bear in mind that $\phi$ depends on $x$ and $t$ , i.e. $\phi = \phi ( x , t ) $ , and that with two variables $t$ and $x = x ( t ) $: $d_t = \partial_t + \partial_x \dot{x}$ you can then expand the $\dot{\phi}^2 ( x , t ) $ term into $\dot{\phi}^2 ( x , t ) = ( \partial_t \phi ) ^2 + 2 ( \partial_t \phi ) ( \partial_x \phi ) \dot{x} + ( \partial_x \phi ) ^2 \dot{x}^2$ now going back to the lagrangian density $\mathcal{l} = \frac{1}{2} [ \mu \dot{\phi}^2 ( x , t ) - y\partial_x \phi ( x , t ) ] $ . bear in mind that the partial derivatives operates on explicit dependence of variables in question . as an example , let me do the first term of the el equation here : $\begin{align*} \frac{\partial \mathcal{l}}{\partial ( \partial_t \phi ) } and = \frac{1}{2} \mu \frac{\partial}{\partial ( \partial_t \phi ) } [ ( \partial_t \phi ) ^2 + 2 ( \partial_t \phi ) ( \partial_x \phi ) \dot{x} + ( \partial_x \phi ) ^2\dot{x}^2 ] + \frac{1}{2} y \frac{\partial}{\partial ( \partial_t \phi ) } ( \partial_x \phi ) ^2\\ and =\frac{1}{2} \mu [ \frac{\partial}{\partial ( \partial_t \phi ) } ( \partial_t \phi ) ^2 + 2\frac{\partial}{\partial ( \partial_t \phi ) } ( \partial_t \phi ) ( \partial_x \phi ) \dot{x} + \frac{\partial}{\partial ( \partial_t \phi ) } ( \partial_x \phi ) ^2 \dot{x}^2 ] + 0 \\ and =\frac{1}{2} \mu [ 2 ( \partial_t \phi ) + 2 ( \partial_x \phi ) \dot{x} + 0 ] \\ and =\mu [ ( \partial_t \phi ) + ( \partial_x \phi ) \dot{x} ] \end{align*} $ so far , the point here is that you should treat $\partial_t \phi$ to be an explicit variable , and apply partial derivatives as you normally would just like $x$ or $t$ . the term $\frac{1}{2} y \frac{\partial}{\partial ( \partial_t \phi ) } ( \partial_x \phi ) ^2$ in the first line , for instance , evaluates to be zero , for there is no dependence on $\partial_t \phi$ . you can halt at this stage and carry on to evaluate the derivative $\frac{\partial \mathcal{l}}{\partial ( \partial_x \phi ) }$ first , and you should yield something like $\mu [ ( \partial_t \phi ) \dot{x} + ( \partial_x \phi ) \dot{x}^2 ] - y \partial_x \phi$ . and since the lagrangian density does not depend on $\phi$ , $\frac{\partial \mathcal{l}}{\partial \phi} = 0$ . ( do try to calculate them yourself ! i may be wrong here :p ) combining what you have , you should be able to get the el equation to be : $\partial_t \mu [ \partial_t \phi + ( \partial_x \phi ) \dot{x} ] + \partial_x \mu [ \partial_t \phi + ( \partial_x \phi ) \dot{x} ] \dot{x} - y \partial_{xx} \phi = 0 $ the third term is obviously the rhs of the equation that you need to prove . what is perhaps messy is the first two terms , but you should refer back to the first equation that we had discussed previously , i.e. $d_t = \partial_t + \partial_x \dot{x}$ ; with this , it is pretty straightforward to show that $\mu [ \partial_t \dot{\phi} + ( \partial_x \dot{\phi} ) \dot{x} ] = y \partial_{xx} \phi$ do the simplification once more and you will obtain the equation that you need to prove : )
i think it is bizarre that a particle does not have a definite composition . yeah , it is . as qftme said , that is quantum mechanics for you . it really does not make sense until you immerse yourself in the subject for long enough ( and even then , only somewhat ) . but it does appear to be the way the universe works . anyway , just so everyone is on the same page , let me start from the basics . if you are familiar with linear algebra , you know that a vector in a 2-dimensional vector space , for example , can be written as a linear combination $\alpha|0\rangle + \beta|1\rangle$ of two basis elements $|0\rangle$ and $|1\rangle$ . for example , a direction vector of length 1 that points northeast can be written as $$\frac{|\text{north}\rangle + |\text{east}\rangle}{\sqrt{2}}$$ or it could be written as $$|\text{northeast}\rangle$$ or $$\alpha|\text{north-northeast}\rangle + \beta|\text{east-southeast}\rangle$$ etc . you could figure out what the coefficients $\alpha$ and $\beta$ are in that last case , but it does not matter . the point is , there are an infinite number of ways to decompose any vector . the pion state is an example of such a vector . it is often considered to be a member of a three-dimensional vector space . one possible basis for that vector space is $u\bar{u}$ , $d\bar{d}$ , and $s\bar{s}$ . but another possible basis is $$\pi^0 = \frac{u\bar{u} - d\bar{d}}{\sqrt{2}}$$ $$\eta = \frac{u\bar{u} + d\bar{d} - 2s\bar{s}}{\sqrt{6}}$$ $$\eta&#39 ; = \frac{u\bar{u} + d\bar{d} + s\bar{s}}{\sqrt{3}}$$ this basis is useful because these particular combinations happen to be relatively stable ; in other words , when a particle consisting of any combination of $u\bar{u}$ , $d\bar{d}$ , and $s\bar{s}$ is detected in a cloud chamber ( if you are old-school ) or a calorimeter or something like that , it will behave like one of these three particles . it is possible that what was actually emitted was the quantum state $u\bar{u}$ , but in terms of the " stable " states , that is $$u\bar{u} = \frac{1}{\sqrt{2}}\pi^0 + \frac{1}{\sqrt{6}}\eta + \frac{1}{\sqrt{3}}\eta&#39 ; $$ ( hopefully i did the math right ) . so you would have a probability of $\frac{1}{2}$ that it acts like ( or technically , collapses to ) a pion , $\frac{1}{6}$ that it collapses to an eta meson , and $\frac{1}{3}$ that it collapses to an eta prime meson . one of those three possibilities is what you had actually observe in your detector . you can do this the other way around , too : suppose that instead of $u\bar{u}$ , you started with a pion , and instead of measuring the " stable " meson type , you were able to directly measure the quark content . since the pion state contains equal components of $u\bar{u}$ and $d\bar{d}$ , your hypothetical quark flavor measurement would give you one of those outcomes with 50% probability each : half the time you had find that you had an up quark and an anti-up quark , and the other half of the time you had find a down and anti-down quark . that is what the state $\frac{u\bar{u} - d\bar{d}}{\sqrt{2}}$ actually means : it governs the probabilities that the pion will interact with a quark flavor measurement as each particular quark type .
( source : polchinski ) consider a toroidal compactification for a bosonic closed string . we make the identification : $x \sim x +2\pi r$ , $x$ being one of the 25 spatial dimensions , say $x^{25}$ the left and right momenta are : $k_l =\frac{n}{r} +\frac{wr}{\alpha'} = 0$ , $k_r =\frac{n}{r} - \frac{wr}{\alpha'} = 0$ the on-shell mass conditions are written : $m^2 = k_l^2 + \frac{4}{\alpha'} ( n - 1 ) $ , $m^2 = k_r^2 + \frac{4}{\alpha'} ( \tilde n - 1 ) $ from this we get : $0 = k_l^2 - k_r^2 + \frac{4}{\alpha'} ( n - \tilde n ) $ using a " dimensionless " momentum $l_{l , r} = k_{l , r} ( \frac{\alpha'}{2} ) ^{\frac{1}{2}}$ , we get : $0 = l_l^2 - l_r^2 + 2 ( n - \tilde n ) $ if we compactify 16 dimensions , we will have vectors $\vec l_l , \vec l_r$ , with : $0 = \vec l_l^2 - \vec l_r^2 + 2 ( n - \tilde n ) $ now , in the heterotic string , we consider only left - movers , so $\vec l_r = \vec 0$ , so we have : $0 = \vec l_l^2 + 2 ( n - \tilde n ) $ if we consider a lattice $\gamma$ made up with the $\vec l_l$ , we see that it must be a even lattice . note : the expression of the dimensionless momentum may be justifyed by looking at the operator product expansion ( ope ) : $x_l ( z_1 ) x_l ( z_2 ) \sim -\frac{\alpha'}{2} \ln z_{12}$ and $x_r ( z_1 ) x_r ( z_2 ) \sim -\frac{\alpha'}{2} \ln \bar z_{12}$ note that we have also : $:e^{ik_lx_l ( z ) +ik_rx_r ( \bar z ) }: :e^{ik_l'x_l ( 0 ) +ik_r'x_r ( \bar 0 ) }:~ \sim z^{\alpha'k_lk_l'/2} ( \bar z ) ^{\alpha'k_rk_r'/2} ~:e^{i ( k_l +k_l' ) x_l ( 0 ) +i ( k_r + k_r' ) x_r ( 0 ) }:$ where the $z , \bar z$ term could be written $z^{l_ll_l'} \bar z^{l_rl_r'}$ in fact , single-valuedness of the last ope under a circle means that : $e^{2i\pi ( l_ll_l ' - l_rl_r' ) } = 1$ , so $l_ll_l ' - l_rl_r'$ is in $\mathbb z$
would quantum field theory allow more powerful computation than the standard quantum computing model ? the question above is how i understood your question after your edit . it is the question i try to answer below . as far as i know , the question is still formally open since it was asked in nielsen and chuang 's book . however , in the 13 years ( 2000-2013 ) since the publication of this books , progress have been made . this 2000 paper deal with the simulation of topological field theories and this one from 2011 deals with scalar field theories . both paper provide efficient simulation algorithm ( to run on a quantum computer ) of some quantum field theories , thereby showing that these theories do not allow to create a computer more powerful than the standard quantum computer . if you want an accessible account on this question , you can read this answer by @scott aaronson , on cstheory.sx and this post on his blog .
texts on qcd do not divide the generators of $su ( 3 ) $ – and therefore " bicolors of gluons " – into two groups because this separation is completely unphysical and mathematically artificial ( basis-dependent ) . moreover , the number of " bicolors of gluons " i.e. generators of $su ( 3 ) $ , the gauge group of qcd , is not nine as you seem to think but only eight . the group $u ( 3 ) $ has nine generators but $su ( 3 ) $ is the subgroup of matrices with the unit determinant so one generator is removed . at the level of gluons or lie algebra generators , the special condition $s$ means that the trace is zero . so the combinations $$ a ( r\bar r ) + b ( g\bar g ) + c ( b \bar b ) $$ are only allowed " bicolors of gluons " if $a+b+c=0$ . now , in this 8-dimensional space of " bicolors of gluons " , there are no directions ( "bicolors" ) that are better than others . for any direction in this space , there exists an $su ( 3 ) $ transformation that transforms this direction into a direction non-orthogonal to any chosen direction you choose . this is true because the 8-dimensional representation is an irreducible one ( the adjective " irreducible " means that one should not try to split it to two or several separated collections ! ) . and there does not exist any consistent yang-mills theory that would only contain the six off-diagonal " bicolors " because the corresponding six generators are not closed under the commutator . the actual calculations of the processes with virtual gluons ( "forces " between quarks etc . ) therefore never divide terms to your two types because this separation is just an artifact of your not having learned group theory . instead , all the expressions are summing over three colors of quarks , $i=1,2,3$ indices of some kind , and there is never any condition $i\neq j$ in the sums because such a condition would break the $su ( 3 ) $ symmetry . now , the $r\bar r , g\bar g , b\bar b$ " bicolors of gluons " ( only two combinations of the three are allowed ) are actually closer to the photons than the mixed colors . so it is these bicolors that produce an attractive force of a very similar kind as photons – they are generators of the $u ( 1 ) ^2$ " cartan subalgebra " of the $su ( 3 ) $ group and each $u ( 1 ) $ behaves like electromagnetism . that is why these components of gluons cause attraction between opposite-sign charges and repulsion between the like charges . the six off-diagonal " bicolors of gluons " ( and let me repeat that the actual formulae for the interactions never separate them from the rest – they are included in the same color-agreement-blind sum over color indices ) cause neither attraction nor repulsion : they change the colors of the interacting quarks so the color labels of the initial and final states are different . it makes no sense to compare them , with the idea that only momentum changes , because that would be comparing apples and oranges ( whether the force looks attractive or not depends on the relative phases of the amplitudes for the different color arrangements of the quarks ) . at any rate , particles like protons contain quarks of colors that are " different from each other " so they are closer to the opposite-sign charges and one mostly gets attraction . however , the situation is more complicated than it is for the photons and electromagnetism because of the six off-diagonal components of the gluons ; and because gluons are charged themselves so the theory including just them is nonlinear i.e. interacting .
i think this is related to elastic fatigue . here 's the wikipedia page .
no , because a lorentz transformation is continuous with a continuous inverse . while an open ball is not mapped to itself , it is mapped to some other open set , in an invertible way . ( that a lorentz transformation is continuous of course follows from that it is linear . )
first of all , to have standing waves you must be talking about a wave carrying system with spatial extent : something like a guitar string . such a system has a set of possible vibrational modes . the first two modes are shown in the figure . we can describe each mode shape with a function $\phi_n ( x ) $ where $n$ is a label that indexes the modes , ie . $n$ is an integer going from 0 to $\infty$ . note that for each mode , $n$ is the number of nodes other than the endpoints . figure : first two normal modes of a string . the black line indicates the rest position . the blue solid line is the zeroth ( "fundamental" ) mode . the broken red line is the first mode . suppose the string is in some arbitrary state , where the deflection from rest at each point $x$ is given by a function $y ( x ) $ . it turns out that you can write any $y ( x ) $ like this : $$y ( x ) = \sum_{n=0}^{\infty}c_n \phi_n ( x ) . $$ that sum is called a fourier series , and the point is that you can write the state of a string as a linear sum of the normal modes . what is special about the modes is that if you start the string in exactly one of those modes , it will keep that shape forever ; only the deflection amplitude will oscillate . for example , if we start the string in $\phi_0$ the string 's deflection for all time is $$y ( x , t ) = \phi_0 ( x ) \cos ( \omega_0 t ) $$ where $\omega_0$ is a frequency associated to the $0^{\text{th}}$ normal mode . the fact that this shape is maintained forever means that the mode is a standing wave , by definition . if you start the string in the generic shape given in our first equation above , the string 's shape for the rest of time will be $$y ( x , t ) = \sum_{n=0}^{\infty}c_n\phi_n ( x ) \cos ( \omega_n t ) . $$ to recap : you can always write the shape of a string as s sum of normal modes ( aka standing waves ) each with its own shape and vibrational frequency . now on to your question . . . if you pluck the string in some way such that the initial shape is not one of the normal modes , or in your words , not one of the usual harmonic standing waves , you can still write it as a sum of other modes ( sum of other standing waves ) . the future evolution of the string 's shape is then given by our third equation . so , the answer to your question is basically that if you excite the string in a non-standing wave way , it will vibrate as a sum of standing waves , and that sum is just whatever set of standing waves superimpose to match how you are exciting the string . note : everything said here pertains to linear systems . most things in nature are at least approximately linear as long as the amplitude of the waves is not too big . there are exceptions to that , however . if that does not make sense , please post a comment : )
the traveller can travel 10 billion light years arbitrarily fast in his/her own experienced time . however , to the observer who stays at home on earth , the traveller 's speed will simply get asymptotically closer to the speed of light . this does , if you think about it , line up very fine with the twin " paradox " ( which is not really a paradox at all ) . as the traveller keeps accelerating the engine , when $v \rightarrow c$ , the time his travel takes will not be shortened to the twin who stays at home . a journey of , say , 40 light years will mean that the twin who stays home will always have aged at least 80 years when the traveller returns home . the traveller , however , can experience an arbitrarily short travel time , tending asymptotically to zero as $v$ tends to $c$ .
nothing as easy from basic principles as for conduction or radiation . you can multiple the mean heat carried by the convecting liquid by it is mass rate of flow $$ \delta t = c_p * ( t_{in} - t_{out} ) * \frac{\delta v}{\delta t} \rho \delta t . $$ where $t_{in}$ is the mean temperature of the liquid moving toward the sink ( cool side ) and $t_{out}$ is the temperature of the liquid moving the other way . the problem is that you can not get the temperature ( s ) of the convecting liquid or the flow rate without detailed calculations or measurements . for the generaly case you pretty much have to go to cfd .
well , to be really pedantic , almost anything is data . any measurement or observation is data . i suppose i would define data as any piece of information , regardless of how it was obtained or whether it is valid . of course , i think your question was really getting at what would be considered evidence , in the context of providing support or opposition to a theoretical model . i would say that the answer is very situation dependent . using simulation results as an example since it was in your question , i would certainty take simulation results in support of some theory , but only on the condition that the simulation has been very well validated against experiment . the details of how and why the simulation results are favorable to the theory is also certainly important . ultimately , all science must be validated by experiment or observation , but that can sometimes be a long journey , and there is definitely a place for other sorts of data along the way .
in direct dissociation the system is excited from a bound state to an unbound state . because the translational energy of the unbound state is not quantised all energies above the dissociation energy are absorbed , so there is a continuous absorption spectrum with no sharp lines . in pre-dissociation there is usually some selection rule that blocks direct dissociation , so there is a transition from the ground state to a an excited but bound state . this transition gives sharp lines like any other transition between bound states so you get a spectrum with sharp lines . however the excited state may very quickly decay to the unbound state , and if this happens lifetime broadening will blur the lines in the spectrum . typically you find that the probability for decay of the excited to unbound states depends on energy , so as you increase the energy you find the lines start sharp , then blur out , and finally get sharp again .
your mistake is in the equation $$22.22t = 27.78t - 236.1$$ everything up to there made good sense , but if the police officer has already traveled 236 meters , you should add that to his distance traveled , not subtract it . you will also need to account for the way the police officer only began traveling at full speed 15 seconds into the chase . anyway , it is much easier to do the problem by thinking about the relative speeds . during the first ten seconds , the car is going 80kph and the police officer is going 40kph on average . so the police officer loses ground at an average of 40kph for 10 seconds . we can think of this as 10 seconds ' worth of loss , and ask how many seconds ' worth of loss the police officer gains as he speeds up further . in the next segment , the police officer gains ground at an average of 10kph for 5 seconds . he is gaining ground 1/4 as fast as he lost it earlier and does it for 5 seconds , so this makes up for 5/4 of a second 's worth of loss , leaving 8 3/4 seconds ' lost ground remaining . finally he gains ground at 20kph until he catches up . he is gaining here at half the rate he was originally losing ground , so it takes him double the remaining seconds ' worth time , or 17.5 seconds , to finish the pursuit . this method is much simpler to calculate , eliminating many opportunities for errors .
if the temperature is not much below freezing , the rate of heat transfer from your plants ( and particularly from the earth around their roots ) is low , if there is a lot of water present , the high heat of fusion means that it will take a long time to actually freeze much of it . so maybe the plant makes it through the night without too much damage . note that if it does not warm up enough the next day the second night will kill them because it starts close to freezing .
so what are spin-networks ? briefly , they are graphs with representations ( "spins" ) of some gauge group ( generally su ( 2 ) or sl ( 2 , c ) in lqg ) living on each edge . at each non-trivial vertex , one has three or more edges meeting up . what is the simplest purpose of the intertwiner ? it is to ensure that angular momentum is conserved at each vertex . for the case of four-valent edge we have four spins : $ ( j_1 , j_2 , j_3 , j_4 ) $ . there is a simple visual picture of the intertwiner in this case . picture a tetrahedron enclosing the given vertex , such that each edge pierces precisely one face of the tetrahedron . now , the natural prescription for what happens when a surface is punctured by a spin is to associate the casimir of that spin $ \mathbf{j}^2 $ with the puncture . the casimir for spin $j$ has eigenvalues $ j ( j+1 ) $ . you can also see these as energy eigenvalues for the quantum rotor model . these eigenvalues are identified with the area associated with a puncture . in order for the said edges and vertices to correspond to a consistent geometry it is important that certain constraints be satisfied . for instance , for a triangle we require that the edge lengths satisfy the triangle inequality $ a + b \lt c $ and the angles should add up to $ \angle a + \angle b + \angle c = \kappa \pi$ , with $\kappa = 1$ if the triangle is embedded in a flat space and $\kappa \ne 1$ denoting the deviation of the space from zero curvature ( positively or negatively curved ) . in a similar manner , for a classical tetrahedron , now it is the sums of the areas of the faces which should satisfy " closure " constraints . for a quantum tetrahedron these constraints translate into relations between the operators $j_i$ which endow the faces with area . now for a triangle giving its three edge lengths $ ( a , b , c ) $ completely fixes the angles and there is no more freedom . however , specifying all four areas of a tetrahedron does not fix all the freedom . the tetrahedron can still be bent and distorted in ways that preserve the closure constraints ( not so for a triangle ! ) . these are the physical degrees of freedom that an intertwiner possesses - the various shapes that are consistent with a tetrahedron with face areas given by the spins , or more generally a polyhedron for n-valent edges . some of the key players in this arena include , among others , laurent friedel , eugenio bianchi , e . magliaro , c . perini , f . conrady , j . engle , rovelli , r . pereira , k . krasnov and etera livine . i hope this provides some intuition for these structures . also , i should add , that at present i am working on a review article on lqg for and by " the bewildered " . i reserve the right to use any or all of the contents of my answers to this and other questions on physics . se in said work , with proper acknowledgements to all who contribute with questions and comments . this legalese is necessary so nobody comes after me with a bullsh*t plagiarism charge when my article does appear :p
if we consider two spin $1/2$ particles with spin up and spin down states , then there are four possibilities in total : \begin{equation} \begin{array}{cccc} |++\rangle \ ; , and |+-\rangle \ ; , and |-+\rangle \ ; , and |--\rangle \end{array} \end{equation} where this notation means for instance : \begin{equation} |+-\rangle = |s_1=1/2 , m_1=1/2 ; s_2=1/2 , m_2=-1/2\rangle \end{equation} we will suppose that the system consisting of both particle has zero orbital angular momentum and let $s_z$ denote that operator acting on the system . then it is very easy to evaluate the following eigenvalue equations : \begin{align} s_z|++\rangle and =\hbar|++\rangle \\ s_z|+-\rangle and =0|+-\rangle\\ s_z|-+\rangle and =0|-+\rangle \\ s_z|--\rangle and =-\hbar|--\rangle \\ \end{align} and so $s_z$ can be written as : \begin{equation} s_z = \hbar \begin{pmatrix} 1 and 0 and 0 and 0 \\ 0 and 0 and 0 and 0 \\ 0 and 0 and 0 and 0 \\ 0 and 0 and 0 and -1 \end{pmatrix} \end{equation} now , if $s=1$ ( remember $s$ denotes the quantum number for the total system ) , then the three states of the system are : \begin{align} |s=1 , m=1\rangle and = |++\rangle \\ |s=1 , m=0\rangle and = \sqrt{\frac{1}{2}} ( |+-\rangle+|-+\rangle ) \\ |s=1 , m=-1\rangle and = |--\rangle \end{align} with eigenvalues : \begin{align} s_z |1,1\rangle and = \hbar |1,1\rangle \\ s_z |1,0\rangle and = 0|1,0\rangle \\ s_z |1 , -1\rangle and = -\hbar |1 , -1\rangle \end{align} where i have switched to a more compact notation : \begin{equation} |s=1 , m=1\rangle \equiv |1,1\rangle \end{equation} however , we can also get an eigenstate with $s=0$: \begin{equation} |0,0\rangle = \sqrt{\frac{1}{2}} ( |+-\rangle-|-+\rangle ) \end{equation} with eigenvalue : \begin{equation} s_z |0,0\rangle = 0|0,0\rangle \end{equation} now , we can also verify that : \begin{equation} s^2 \equiv s_x^2 + s_y^2 + s_z^2 \end{equation} satisfies : \begin{align} s^2 |1,1\rangle and = 2 \hbar^2 |1,1\rangle \\ s^2 |1,0\rangle and = 2 \hbar^2 |1,0\rangle \\ s^2 |1 , -1\rangle and = 2 \hbar^2 |1 , -1\rangle \\ s^2 |0,0\rangle and =0|0,0\rangle \end{align} therefore , we see that the following two important equations are satisfied : \begin{equation} \begin{array}{cc} s^2 |s , m \rangle = \hbar^2 s ( s+1 ) |s , m \rangle \ ; , and s_z |s , m \rangle = \hbar m |s , m \rangle \end{array} \end{equation} to sum up , we have found the possible eigenvalues for the magnitude and $z$-component of the system and the eigenstates corresponding to these values : the allowed values for total spin are $s=1$ and $s=0$ , while the allowed value of $s_z$ are $\hbar$ , $0$ , and $-\hbar$ and the corresponding eigenstates in the product basis $|1,1\rangle$ , $|1,0\rangle$ , $|1 , -1\rangle$ and $|0,0\rangle$ . thus , the meaning of these triplet and singlet states is that they are the possible states of the system consisting of the two aforementioned particles . this is often written as : \begin{equation} \frac{1}{2} \otimes \frac{1}{2} = 1 \oplus 0 \end{equation} which means that the tensor product of two spin-$1/2$ hilbert spaces is a direct sum of a spin-$1$ space and a spin-$0$ space .
in the weak-field case , $$\mathrm{d}s^2 = -\left ( 1+2\frac{\phi}{c^2}\right ) c^2\mathrm{d}t^2 - \frac{4}{c}a_i\mathrm{d}t\mathrm{d}x^i + \left ( 1-2\frac{\phi}{c^2}\right ) \mathrm{d}s^2\text{ , }$$ where $\phi$ is the newtonian potential and $\mathrm{d}s^2 = \mathrm{d}x^2 + \mathrm{d}y^2 + \mathrm{d}z^2$ is the euclidean metric . in the static case , $a_i = 0$ , which is the form used for gps calculations , but in general it is more interesting as being a direct analogue to classical electromagnetism , first formulated for gravity by heaviside in 1893: $$\begin{eqnarray*}\mathbf{e}_\text{g} = -\nabla\phi - \frac{1}{2c}\frac{\partial\mathbf{a}}{\partial t} and \quad\quad and \mathbf{b}_\text{g} = \nabla\times\mathbf{a}\end{eqnarray*}$$ $$\begin{eqnarray*} \nabla\cdot\mathbf{e}_\text{g} = -4 \pi g \rho_\text{g} and \quad\quad and \nabla \times \mathbf{e}_\text{g} = -\frac{1}{2c}\frac{\partial\mathbf{b}_\text{g}}{\partial t} \\ \nabla\cdot\mathbf{b}_\text{g} = 0 and \quad\quad and \nabla\times\frac{1}{2}\mathbf{b}_\text{g} = -\frac{4\pi g}{c}\mathbf{j}_\text{g} + \frac{1}{c}\frac{\partial\mathbf{e}_\text{g}} {\partial t} \end{eqnarray*}$$ this particular version was taken from einstein 's general theory of relativity by grøn øyvind and sigbjørn hervik ; a few variations in defining these fields exist in the literature . but probably more importantly , the post-newtonian formalism gives a more general approximation scheme , the first few terms of which are : $$\mathrm{d}s^2 = - ( 1+2\phi+2\beta\phi^2+\ldots ) \mathrm{d}t^2 + ( 1-2\gamma\phi+\ldots ) \mathrm{d}s^2 + ( \ldots ) \mathrm{d}t\mathrm{d}x^i\text{ , }$$ with many other potentials that i am omitting here . this is very useful for understanding the general predictions of gtr and comparing them to alternative theories of gravity ( e . g . , gtr predicts $\beta = \gamma = 1$ , other theories might not ) .
i think this is a typo in morse and feshbach methods of theoretical physics . the correct expression is $-\frac{1}{6}r^2\nabla ^2\psi$ or $-\frac{1}{6} ( dx^2+dy^2+dz^2 ) \nabla^2 \psi$ .
i think you can get a estimate like this . for a semiconductor with no split in the quasi-fermi levels , the electrons and holes take their intrinsic values ( carrier density ) $n_0$ and $p_0$ ( $cm^{-3}$ ) . the charge carriers are in equilibrium with the thermal photons being absorbed and emitted inside the material . so if we calculate the emission rate of thermal photons then we know the time constant for how long the thermally generated carriers will last before recombining ( because at equilibrium upwards rates and downward rates must balance ) . let 's assume perfect bimolecular recombination , then the rate of thermal emission is , $\frac{\partial n}{\partial t} = bn_0p_0$ , where $b$ is the bimolecular recombination coefficient , for gaas , $b=7\times10^{-10}$$cm^{6}s^{-1}$ , and the intrinsic carrier density is , $n_i=n_0=p_0=2\times10^{6}$$cm^{-3}$ . this gives a transition rate of 2800 $s^{-1}$ . this seems a bit slow . but it is correct for the assumptions , namely because we assumed an un-doped intrinsic semicondutor ( the carrier density is very low ) . for more information i recommend ' light-emitting diodes by e . fred schubert’ , search for the vanroobroeck-shockley equation .
even a normal planet does not permanently lock its atmosphere : a little bit of it is creeping out all the time . the air molecules are distributed according to a maxwell-boltzmann distribution , which falls off to zero exponentially . a small fraction of that air will always be above escape velocity and will disappear into space . the distribution of air re-thermalizes , and thus another fraction is lost to space . the fraction that is above escape velocity depends on the mass of the molecule : it is appreciable for helium on earth ( popped balloons are gone forever ) . for your deep well , you had have to consider the shape of the maxwell-boltzmann distribution and the variation of pressure with altitude ( and include a non-earth " g" ) . frame the problem in terms of the amount of loss that you are comfortable with--- something so small that it will not be missed or can easily be replenished . someone who is actually engineering this might also want to chill the upper layer of gas with some kind of large-scale air conditioning . that would reduce the loss so that the hole would not need to be as deep . maybe a greenhouse effect could be useful to keep the upper layer cold and the lower layer warm . after all , who needs to see the sun ?
as you say , the power produced per cubic metre of the sun 's core is surprisingly low . this is because proton-proton fusion is a very slow process , as has been discussed hereabouts before . the core is so hot because conduction of heat through the core is slow . the average speed with which a photon escapes the core is the astonishingly low value of about 30$\mu$m/s . however the photon net speed is so slow because the dense plasma at the sun 's core scatters photon extremely efficiently . if you were to magic the earth into the sun 's core then the earth would start receiving energy at the rate predicted by the stefan boltzmann law . i make this around $10^{21}$ w/m$^2$ of the earth 's surface , so the earth would start boiling away pretty quickly . the earth would cool the plasma around it , and as that plasma cooled and recombined it would become transparent to the next layer of plasma out , and so on . to a first approximation the heat flux entering the earth would remain at around $10^{21}$ w/m$^2$ until the material of the earth became hot enough to form a plasma itself . at this point that plasma would start scattering photons and the heat flow would start slowing . actually calculating the rate at which the earth would vaporise would be a difficult task . you could treat it as the heating of a sphere with a known thermal conductivity , but as mentioned above once the temperature gets hot enough to ionise the material from the earth this would strongly affect the heat flow from the plasma around it .
the explicit eigenfunctions of the harmonic oscillator hamiltonian are given here , but i would highly discourage you from explicitly doing an integral using these expressions to determine $a$ . it is significantly easier to use the fact that the eigenfunctions are orthogonal ; $$ \int_{-\infty}^\infty dx\ , \phi_m^* ( x ) \phi_n ( x ) = \delta_{mn} $$ if you use this fact , then the integral on the left hand side of the $t=0$ normalization condition you wrote down will be very easy . try this out , and if you still have trouble we can give you more guidance since this is a homework question .
what you are missing is some practical aspects of modern plumbing . every modern plumbing fixture ( toilet , shower/sink drain ) has a trap or " u-bend " in the piping before it connects to the sewer branch or main pipe . this allows a pocket of water to sit in the bend of the pipe to keep sewer gases from escaping back into the room when the fixture is not being used . this water pocket obeys the equilibrium principle you know . when the fixture is used , the water above disturbs this balance and everything flows into the sewer pipe . during your sewer back-up event , something has happened downstream to provide enough pressure reverse the normal flow , and the material will seek any free outlet ( e . g . your shower drain ) . these events are usually very short-lived , so probably provided enough pressure to push stuff through the u-bend in the toilet and shower , then ceased . once that pressure was gone , the normal physics applies and the liquid levels then reach equilibrium with their respective u-bends . your shower basin , despite being lower than the toilet , contained the spill because it had a larger area to contain the volume of the spill . source : plumbing design engineer . more about plumbing traps here .
if you are talking of " black body " as in " black body radiation " better read the link : a black body is an idealized physical body that absorbs all incident electromagnetic radiation , regardless of frequency or angle of incidence . " idealized " is the crucial point here . all matter can be approximated to a " black body " , i.e. an emitter of black body radiation by the multiplication with a constant specific for the material , the emissivity . thermal conductivity is a different concept than the attributes of a black body . it is measured experimentally and depends on the molecular structure . a black body will have a specific modified black body spectrum of radiation but can have any type of conductivity . for example dull wrought iron and black concrete have the same emissivity , . 94 , but very different thermal conductivity .
no . there is no theory of open , oriented strings . any string theory must contain closed strings , while the open strings are optional . if there is a string theory which contains oriented open strings , then it has the problem that the oriented open strings cannot couple to the oriented closed strings . why ? this is my understanding of the explanation given by the by thomas mohaupt in lecture notes " introduction to string theory " : in the closed string spectrum , there is an $\mathcal n = 2a$ algebra and and an $\mathcal n = 2b$ algebra which lead to different string theories . both have 32 supercharges . in each of these , , there are 2 gravitinoes , dilationoes , 1 in the ramond neveu-schwarz sector and 1 more in the neveu-schwarz ramond sector . these 2 gravitinos need 2 different supercurrents to couple to . but the $\mathcal n=1$ supersymmetric algebra with only 16 supercharges clearly cannot allow this ! thus , the open oriented strings would not couple with the closed oriented strings . the solution is to have open unoriented strings instead . this along with the unoriented closed strings is the type i string theory . the " only unoriented closed strings " theory is also inconsistent because of other reasons .
the height at which the balloon floats is determined by the density of the air outside of the balloon . once you are high enough in the air that upward force generated by the difference in densities $\rho_{\text{helium}}$ and $\rho_{\text{air}}$ is exactly canceled by the gravitational force on the balloon plus its counterweight , the balloon will remain at the same height ( assuming it has negligible upward velocity , else it'll overshoot a little , then come back down and oscillate around the equilibrium height ) . now , inside a room , we may assume that the density of the air is the same as outside . therefore , your balloon will float at the same level , regardless of whether you are inside a room or not .
a rigid body has 6 configuration degrees of freedom because its most general configuration can be obtained by translating ( 3 degrees of freedom ) and rotating ( 3 degrees of freedom ) its initial configuration . a mathy way of saying this is that its configuration manifold is $\mathbb r^3\times \mathrm{so} ( 3 ) $ . however , you are right that the phase space of a rigid body is 12-dimensional because each independent configuration degree of freedom corresponds to an independent momentum degree of freedom .
no . the law of the conservation of mass-energy is a purely local law ( i.e. . the divergence of the stress-energy tensor must be zero at any point in spacetime ) . you will find that , as long as your wormhole is a solution to einstein’s equations , this law will hold anywhere in the vicinity of the wormhole or within it . in other words , there is no point at which the mass suddenly disappears . it enters the mouth of the wormhole , pass through its throat , and emerges from its other mouth .
assuming you are using c code : this implements a simple integration of the equations of motion : $$v = \frac{dx}{dt}\\ a = \frac{dv}{dt}$$ i made the time step very small : you can get away with bigger steps ( takes less time ) . also i keep the loop going until the velocity has reached a certain downward value : you can use any other criteria ( time , position , etc ) . update as johannes pointed out , you get slightly more accurate results ( even when you use larger time steps ) when you use the average velocity during a time step to compute the next position . this leads to a small change in code : . below , i show both the old and the new calculation side by side - as you can see , when the time step is coarser , the new method continues to give good results : here are the results at time = 1.80 seconds for different time steps : finally , the " correct " result from integrating the equations of motion gives velocity = 8 - 1.8*9.8 = -9.64 position = 0 + 8 * 1.8 - 0.5 * 9.8 * 1.8^2 = -1.4760  as you can see , smaller steps get better results , and using the " new method " ( as suggested by johannes ) gets you closer to the true position than the " lazy method " i first proposed . last update to implement other forces , you can do something like this : i hope you can see how you could implement lots of different things that might change the total force on the object and that will change the motion . in this case , the last term ( onsupport ) sets both velocity and acceleration to zero , meaning that after a small time step it will still be zero - but if you set onSupport = 0 it will once again be able to move . and like this you could have multiple objects doing different things under the influence of gravity , rocket packs , etc . you obviously want to experiment with the drag=200 factor - i just picked a number from thin air , and with a value like 200 you will end up with a terminal velocity of about 3.5 m/s ( you would survive that no problem ) . as you make drag lower , the terminal velocity will be higher ( also depends on the mass of the object ) .
you should also specify the representation . the representation requires su ( n ) lie group with n×n matrix is called fundamental representation . which is used in standard model u ( 1 ) x su ( 2 ) x su ( 3 ) . you can surely have su ( n ) lie groups with other representation , such as adjoint representation , then in this case su ( n ) are represented by a matrix with a rank of the number of its group element $d ( g ) $ , which is rank-$ ( n^2-1 ) $ . i.e. $ ( n^2-1 ) $ x $ ( n^2-1 ) $ matrix . in this case , adjoint representation for a lie group g is a way of representing the elements of the group as linear transformations of the group 's lie algebra . so the generator $ ( t^a ) _{bc}=-if_{ab}{}^c$ , takes its value as the structure constant . ( here $ [ t^a , t^b ] =i f_{ab}{}^c t^c$ . ) so there is no puzzle at all .
i was wondering if there is an intuitive way to understand the motion of a body influenced by two other massive bodies ( say the moon being influenced by the earth and the sun . no , intuition is not of real use in a three body gravitational problem , more so in many body . in 1887 , mathematicians ernst bruns [ 4 ] and henri poincaré showed that there is no general analytical solution for the three-body problem given by algebraic expressions and integrals . the motion of three bodies is generally non-repeating , except in special cases . [ 5 ] actually the motions of the planets are studied as possible dynamic chaos . planetaria solve the many body problem with numerical approximations .
the givens : $\delta t$ , $d$ , $v_1$ , $v_2$ . and you demand that the acceleration is exponential . let $$ a ( t ) =a_0 e^{\omega t} $$ then if we say that $v_1=0$ we have after integrating $$ v ( t ) =\frac{a_0}{\omega} ( e^{\omega t}-1 ) $$ now if we say that the initial coordinate is zero ( $x_1=0$ ) , then $$ x ( t ) =\frac{a_0}{\omega^2} ( e^{\omega t}-1 ) -\frac{a_0 t}{\omega} $$ now you say that at $t_2$ , $x ( t_2 ) =d$ , and at $v ( t_2 ) =v_2$ , so $$ \frac{\omega^2 d}{a_0}+\omega t_2=e^{\omega t_2}-1 $$ and $$ \frac{\omega v_2}{a_0}=e^{\omega t_2}-1\implies \frac{\omega^2 d}{a_0}+ ( t_{2}-\frac{v_2}{a_0} ) \omega=0 $$ then $$ \omega=0 , \quad \frac{v_2 -a_0 t_2}{d} \equiv \omega_0 $$ then $$ x ( t ) =\frac{a_0}{\omega_{0}^2} ( e^{\omega_0 t}-1 ) -\frac{a_0 t}{\omega_0} $$ this gives you distance in terms of time . for time , set up $$ \frac{\omega_{0}^2 x}{a_0}+1+\omega_0 t=e^{\omega_0 t}=b+at=e^{at} $$ with solution $$t ( x ) =\frac{-\mathcal{w}\left ( -e^{-b}\right ) -b}{a}=\frac{-\mathcal{w}\left ( -\exp\left [ -\left ( \frac{\omega_{0}^2 x}{a_0}+1\right ) \right ] \right ) -\left ( \frac{\omega_{0}^2 x}{a_0}+1\right ) }{\omega_0} $$ with $\mathcal{w}$ the product log function . hopefully this one is helpful : )
for a path integral , the integral $$ \int e^{i\int \lambda ( x ) f ( x ) } d\lambda = \prod_x ( 2\pi ) \delta ( f ( x ) ) $$ where the product is over all points x ( imagine a lattice ) . so that the result is not just a saddle point identity ( saddle points and perturbative approximations are not the same , perturbative is by expanding the exponential in a series ) , it is an exact path integration identity . while the result is not fully rigorous , that is only because of the issue of path integration definition , the limit of continuous space as the lattice gets finer . ignoring this mathematical point , there is no way this identity is avoided in any proper definition of path integration , since it holds without any regulator ambiguity .
as the comments say , you have to be precise about your reference point when you talk about time dilation . time dilation is always relative to something else . but there is an obvious interpretation to your question . suppose you have an observer well outside the solar system and stationary with respect to the sun . for that observer your clock on earth is ticking slowly for two reasons : you are in a gravitional well so there is gravitational time dilation . you are on the earth which is hurtling round the sun at about ( it varies with position in the orbit ) 30 km/sec . the earth 's surface is also moving as the earth rotates , but the maximum velocity ( at the equator ) is only 0.46 km/sec so it is small compared to the orbital velocity and we will ignore it . as it happens the problem of combined gravitational and lotentz time dilation has been treated in the question how does time dilate in a gravitational field having a relative velocity of v with the field ? , but this has some heavy maths so let 's do a simplified calculation here . the gravitational time dilation , i.e. the factor that time slows relative to the observer outside the solar system is : $$ \frac{t}{t_0} = \sqrt{1 - \frac{2gm}{rc^2}} $$ where $m$ is the mass of the object and $r$ is the distance from it . for the sun $m = 1.9891 \times 10^{30}$ kilograms and $r$ ( the orbital radius of the earth ) $\approx 1.5 \times 10^{11}$ so the time dilation factor is $0.99999999017$ . for the earth $m = 5.97219 \times 10^{24}$ kilograms and $r$ ( the radius of the earth ) $\approx 6.4 \times 10^{6}$ so the time dilation factor is $0.999999999305$ . the lorentz factor due to the earth 's orbital motion is : $$\begin{align} \frac{1}{\gamma} and = \sqrt{1 - v^2/c^2} \\ and = 0.999999995 \end{align}$$ and to a first approximation we can simply multiply all these factors together to get the total time dilation factor : $$ \frac{t}{t_0} = 0.999999984 $$ to put this into context , in a lifetime of three score and ten you on earth would age about 34 seconds less than the observer watching from outside .
no . temperature is a concept of statistical mechanics and only exists in the limit of large numbers of objects . so it is an average . and since the number of particles is ( typically ) not known exactly , the possible temperature values ( due to the quantization of energy ) also cannot be known exactly . that is the best case . more generally , a temperature bath can have any real temperature and so temperature is not quantized .
my problem appears to be with the initial state of the system , which i have written as $$\left| \psi\right\rangle=\left|\psi_1\right\rangle\left|\psi_2\right\rangle+\left|\psi_2\right\rangle\left|\psi_1\right\rangle , $$ where $\left|\psi_1\right\rangle$ is packet from one source , and $\left|\psi_2\right\rangle$ is packet from another one . this state says that the system is in a superposition of states , in each of which one of the particles comes from one source , and another necessarily from another source . i.e. the system is highly entangled . such system could be created e.g. by some generator of pairs of particles with opposite momenta . but two independent sources are clearly not such a source of entangled pairs . as the particles are indistinguishable , and there is no symmetry which would allow us to determine that the particles come from different sources , we can not say which source the particle has come from . if we watch single particle emitting from the sources , they might come one after another from different sources , or they could repeatedly come from single source , then several times from another one . i.e. there is no rule that if one particle is from source a , next detected one is from source b . so , the initial state must be in the following form : $$\left|\psi\right\rangle=\left ( \left|\psi_1\right\rangle+e^{i\phi}\left|\psi_2\right\rangle\right ) \otimes\left ( e^{i\psi}\left|\psi_1\right\rangle+e^{i\chi}\left|\psi_2\right\rangle\right ) +\\ +\left ( e^{i\psi}\left|\psi_1\right\rangle+e^{i\chi}\left|\psi_2\right\rangle\right ) \otimes\left ( \left|\psi_1\right\rangle+e^{i\phi}\left|\psi_2\right\rangle\right ) , $$ where $\phi , \psi , \chi$ are constants , which depend on the experimental setup . now from the form of the initial state it is obvious that the interference pattern will be present , and it is confirmed by numerical simulation . thus , it appears that even in this multiparticle experiment particles interfere with themselves , rather than with each other , to produce visible pattern on the screen .
first a comment about the following statements made by kitchi and wouter : lines of force always have to be smooth , there can not be a sharp bend in them like in your second diagram and the comment by @kitchi basically says it all : lines of force should be continuous and differentiable and they should never intersect . these statements are incorrect without qualification , and here are two reasons why : the electric field lines of a point charge consist of rays all of whom intersect at the location of the point charge . moreover , the field is not smooth at the location of the point charge ( it is not even continuous there since there is a singularity ) . there is a discontinuity in the electric field in passing through a surface charge with charge density $\sigma$ , in fact the discontinuity is proportional to the surface charge density ; $$ ( \mathbf{e}_2 - \mathbf e_1 ) \cdot \mathbf n = \frac{\sigma}{\epsilon_0} $$ if you want to make some statement about smoothness of electric field lines , ( which you should try to avoid calling " lines of force" ) , then you really need to make some other qualifications that exclude such cases . second , these smoothness justifications are , to some extent , missing the forest for the trees so to speak . the crux of the issue is really addressed by richard . the way one obtains the field due to a charge distribution is by invoking the principle of superposition . this immediately rules out the second diagram you drew because if you simply add the fields of the two point charges vectorially , then you will see that the field lines look like you draw them in the first diagram .
you have $$u=n\langle e \rangle=n\ ( \langle e_\text{kin} \rangle+\langle e_\text{pot} \rangle ) =n k\ t , $$ $$\tfrac{\partial s ( u ) }{\partial u}=\tfrac{1}{k\ t}=\tfrac{n}{u}\ \ \longrightarrow\ \ s ( u ) =n \ln ( u ) +s_0 , $$ $$\delta s=s ( x\ u ) -s ( u ) =n\ln ( x\ u ) -n\ln ( u ) =n \ln ( x ) . $$ and $$100 \ln ( 2 ) =69.315 . . . $$
i think you are missing that the partial derivative in $d_\mu$ acts on the phase factor . if you start only with the phase transformation of the dirac term : $$\mathcal{l}_d ' = \bar \psi e^{i\alpha ( x ) } \left [ i\gamma^\mu ( \partial_\mu + iea_\mu ) - m \right ] \psi e^{-i\alpha ( x ) } $$ you can almost pull the $e^{i\alpha ( x ) } $ through and cancel them . however , the $\partial_\mu=\partial/\partial x^\mu$ acts on $ e^{i\alpha ( x ) } $ and gives you an additional term ( product , then chain rule ) . so after canceling , $\partial_\mu \rightarrow \partial_\mu + $ something with $\alpha ( x ) $ . then perform the $a_\mu \rightarrow a_\mu'$ substitution , and the terms should exactly cancel out .
1 ) the first derivation is correct . when you make the second one for $\dot x$ written as $\dot x = \dfrac{d x}{d t}$ you make an error when using full derivatives instead of partial ones . in the latter case you would have obtained : $$ \dot x=\dfrac{d x}{d t}=\dfrac{\partial x}{\partial \theta}\dfrac{d \theta}{d t} +\dfrac{\partial x}{\partial \phi}\dfrac{d \phi}{d t}+\dfrac{\partial x}{\partial r}\dfrac{d r}{d t} $$ then , as $x=r \cos \theta$ , you can insert $\dfrac{\partial x}{\partial \phi}=0 , \dfrac{\partial x}{\partial \theta}=-r \sin \theta$ , $\dfrac{\partial x}{\partial r}=\cos \theta$ into the last formula to obtain the same result you had before . 2 ) the derivatives you are using here are not covariant derivatives , they are always partial . the derivative $\dfrac{d}{d t}$ is expressed through partial derivatives , in contrast to full derivative $\dfrac{d}{d t}$ , which is expressed through covariant derivatives . as you are actually not using covariant derivatives in your derivation , you do not have to worry about the basis vectors . 3 ) you could make you derivation alternatively by rewriting your original equations in a covariant form $m\dfrac{d^2 x^i}{d t^2}+c\dfrac{d x^i}{d t}+kx^i=0$ , where $x^i$ stands for $ ( x , y , z ) $ , stating that the equations of motion do not depend on the choice of coordinates , rewriting the equation in spherical coordinates as $m\dfrac{d^2 \tilde x^i}{d t^2}+c\dfrac{d \tilde x^i}{d t}+k \tilde x^i=0$ , where $\tilde x = ( r , \theta , \phi ) $ , and expanding all the derivatives $\dfrac{d}{d t}$ through partial derivatives and christoffel symbols in spherical coordinates . you would have to take care of the basis , as the result would be in natural basis , whereas the equations of motion are usually written in orthonormal basis . 4 ) another powerful way to transform coordinates would be by using lagrangian form of the equations of motion . this would be possible for the case of conservative systems only ( no damping terms ) . in this case you could just rewrite the lagrangian of your system in spherical coordinates and write lagrangian equations from it in a more or less straightforward manner .
there is absolutely no danger from the electromagnetic waves coming from the phone . they are nowhere near powerful enough to heat up the gasoline , much less cause it to explode . they also do not produce more static electricity than anything else . if there is any part of the phone that is hypothetically " dangerous " , it might be the battery , as you say . if the battery happens to fall out , and land precisely ( terminals-first ) onto a conductive surface , it can create a tiny spark which can ignite the vapors . there is much more danger from the static electricity from your own body when handling the gas nozzle . the best practice should be to lay one hand onto your car , then pick up the nozzle with the other hand and insert it into your tank . ( the same when removing the nozzle )
standard deviation adds uncertainties to the measured value : $23.3\pm 0.4\ , {\rm m}$ . one can quickly look at the error ( which has units of ${\rm m}$ in my case ) and think , the value could be as low as $22.9\ , {\rm m}$ or as high as $23.7\ , {\rm m}$ without much thinking . modifying this to being a percentage of the value would be confusing . plus it would be arbitrary . as martin beckett points out , since the celsius and kelvin scales have identical temperature spacing but differing 0-points , making the error ( std . dev . ) scale to the value would be ridiculous : $$20.0\pm0.1\ , {\rm c} \to 20.0\ , {\rm c}\pm0.5\%$$ $$293.0\pm0.1\ , {\rm k}\to293.0\ , {\rm k}\pm0.034\%$$ same value in units systems would lead to differing uncertainties ; this would bring about more confusion .
quantum field theory is the framework that we normally use to study particle physics . it is the idea that the world is described by fields and each field can move into excited states which correspond to particles . since we are working with fields , you not constricted to a fixed particle number as in quantum mechanics . this is very useful since due to $e=mc^2$ , particles constantly appear and disappear in the real world . particle physics on the other hand , is the study of the particles that make up our world . since qft 's are terrific at describing nature , we use them to describe particle physics ; they are tool we need to study particle physics . however , some people study qft for the sake of qft since its a very deep and subtle subject .
classical lagrangians of fermions are always constructed out of grassmann numbers . no exception . in both of op 's cases , the mass term is nonvanishing : in the first case the mass term is proportional to $\psi_1^*\psi_2^*-\psi_2^*\psi_1^* = 2\psi_1^*\psi_2^* \neq 0 . $ in the second case , i write in the chiral basis : $$\gamma^0=\begin{pmatrix} and \sigma^0 \\ \sigma^0 \end{pmatrix}\qquad \gamma^0=\begin{pmatrix} and \sigma^2 \\ -\sigma^2 \end{pmatrix} \enspace \rightarrow \hat{c}=\begin{pmatrix}-i\sigma^0\sigma^2 \\ and i\sigma^0\sigma^2\end{pmatrix}$$ so that ( picking the diagonal element of the mass matrix $i=j$ as the off-diagonal terms are obviously non-vanishing . ) $$ \begin{aligned}\psi_r^t \hat{c} \psi_r and = \begin{pmatrix}0 and 0 and \psi_3 and \psi_4\end{pmatrix} \begin{pmatrix}-i\sigma^0\sigma^2 \\ and i\sigma^0\sigma^2\end{pmatrix} \begin{pmatrix}0\\0\\\psi_3\\\psi_4\end{pmatrix} \\ and = \begin{pmatrix}\psi_3 and \psi_4 \end{pmatrix}i\sigma^0\sigma^2\begin{pmatrix}\psi_3\\\psi_4\end{pmatrix}\\ and =\psi_3\psi_4-\psi_4\psi_3 = 2\ , \psi_3\psi_4 \neq 0 \end{aligned}$$ where in the last step i used the grassmann property of anticommutation .
it is easiest to directly derive the form of the vector fields from the boundary conditions for asymptotically flat spacetimes . see for example this paper http://arxiv.org/abs/1001.1541 or this one http://arxiv.org/abs/1106.0213 infinitesimally diffeomorphisms act on the metric via lie derivative $\mathcal{l}_{\xi}g_{\mu \nu}$ . in the case of bms , you require this transformation to respect the boundary conditions on $g_{\mu \nu}$ , for example $g_{uu}\approx -1 +\mathcal{o} ( r^{-1} ) $ . so the vector field should satisfy $\nabla_u \xi_u =\mathcal{o} ( r^{-1} ) $ . you may set up such equations for each component of the metric and its corresponding boundary conditions . in each case you require the vector field not to touch the " leading " minkowski part of the metric . this is a system of differential equations , which you may then solve . as far as the generators go , this is sort of addressed in the second paper i linked to . though as far as eqn 3.3 goes , you may heuristically think of this just as the adm hamiltonian weighted by a function on the sphere which turns the uniform time translation into a supertranslation ( the spaces andy is studying are christodoulou-klainerman spaces for which $i^0$ is a non-singular point and so you may hope to match $\mathcal{j}^+_-$ to $\mathcal{j}^-_+$ through $i^0$ where the adm hamiltonian is defined ) . demonstrating that these charges generate the correct transformations quantum mechanically is actually very subtle , as is discussed here http://arxiv.org/abs/1401.7026 hope this helps .
the following is a passage from diffusion in the standard map ( pdf ) by itzhack dana and shmuel fishman : a major difficulty in the analysis of chaotic behavior of hamiltonian systems is the proximity of chaotic and regular orbits on various scales [ 2 ] . thus , for $k \approx 1$ , the phase plane of ( 1 ) is an intricate mixture of regular and chaotic orbits [ 3 ] . for $k \gg 1$ chaotic orbits fill almost the entire phase plane , but " islets of stability " , in particular accelerator modes [ 2 , 7 ] , are known to exist for $k$ arbitrarily large . when a chaotic orbit approaches such an islet , it may wander near it in a " regular " fashion . the area of each islet generally decreases when $k$ increases . distinctive features of a stochastic or random motion , which allow a statistical description of it , are the rapid decay of correlations and diffusion . the decay of correlations in the chaotic region is related to the local instability , measure lyapunov characteristic exponent [ 1 , 2 , 8 ] . for $k \gg 1$ , the decay of correlations is actually exponential , with the decay exponent proportional to the lyapunov exponent [ 8 ] . the existence of a definite characteristic time for the decay of correlations and the resulting statistical independence generally imply diffusion in the unbounded direction of the action $i$ for $k &gt ; k_\mathrm{c}$ [ . . . ] . therefore a definite diffusion coefficient $d$ can be associated with the chaotic motion $$ d = \lim_{n\to\infty} \frac{\big\langle ( i_n-i_0 ) ^2\big\rangle}{n} , $$ where the average is taken over some ensemble of initial positions $ ( i_0 , \theta_0 ) $ within the chaotic region . for large $k$ , where the lyapunov exponent is large , diffusion was verified unambiguously [ 2 ] .
to answer your second question : yes , you can write any arbitrary function $f ( x ) $ as a linear combination of eigenfunctions of $\mathcal{p}$: $$f ( x ) =\underset{=f_+ ( x ) }{\underbrace{\frac{f ( x ) +f ( -x ) }{2}}}+\underset{=f_- ( x ) }{\underbrace{\frac{f ( x ) -f ( -x ) }{2}}}$$ with $\mathcal{p}f_+ ( x ) =f_+ ( x ) $ and $\mathcal{p}f_- ( x ) =-f_- ( x ) $ .
narrowness is precisely the essence of the preferred basis problem . consider : some states are narrow , some are not . given that some are narrow and some are not , why should ' narrowness ' come about as a meaningful concept at all ? why should this quality be an interesting one ? consider the position of a pointer . we do not interpret non-narrowly pointed states of the pointer as physical , basically by the very definition of what we mean by " a&nbsp ; pointer " ( after all , we take for granted that some systems can only occupy narrow states , and these are called pointer states in common terminology ) . in quantum mechanics , the non-narrow states of the pointer are perfectly valid . then how does the pointer come in practice to inhabit the narrow states ? the answer is that the pointer has a preferred basis ( or something which is almost but not quite an orthonormal basis , in the case of literal pointers having different positions ) : a basis in which its environment tends preferentially to interact , so that the information about the state of the pointer which is encoded in that basis gets copied in other systems , and is therefore strongly correlated . ( this is the notion of quantum darwinism : the information best suited to be reproduced elsewhere , comes to spread faster than it could be stopped , giving rise in practise to decoherence in the basis in which that information is represented . ) the question then arises : how does one determine that basis , and why should this basis be priviledged in our experience of the world ? for instance , whatever the superposition to which we supposedly belong , according to the mwi , we percieve a strong tendancy for objects to be spatially localised . why ? how does one explain the way in which the myriad potential microscopic worlds merge into distinguishable macroscopic worlds ? why should a superposition state seem , from the inside , like a decomposition with respect to any particular basis ? that is the preferred basis problem , in a nutshell .
it is not quite true that " the square of the wave function gives the probability of finding an electron at a particular position . " the square of the wave function gives you the probability density of find the the electron as a particular position . in order to get the probability of finding the electron in a given region of space , you need to integrate the square of the ( normalized ) wave function over the volume you are interested in . another way to think about why you need to integrate over some volume is that the wave function is giving you the probability density at a given point in space . that point is infinitesimally small ; in mathematical terms , it has measure zero . because it is infinitesimally small , there is an infinitesimally small probability of finding the electron there . you need to integrate over a finite volume in order to get a finite probability .
there is no fallacy , you are just not being particularly careful . you need to include both the electric and magnetic forces of the right magnitude and a covariant result drops out . ( of course historically it went the other way around : people noticed that frame changes were messed up unless the transformation laws were different and this led to the development of special relativity . ) for simplicity let both beams be very thin and have equal uniform charge density $\rho$ in the rest frame and suppose they run exactly parallel separated by a distance $l$ . let the velocity of the beams be $v$ in the lab frame . rest frame : taking the usual gaussian pillbox gives the electric field of one beam at the location of the other as $$ \vec{e}_\text{rest} = \frac{\rho}{2\pi\epsilon_0 l} \hat{r} , $$ where $\hat{r}$ is the unit vector directed away from the source beam . thus the force on a single particle in the second beam ( charge $q$ ) is : $$ \vec{f}_\text{rest} = \frac{\mathrm{d}p}{\mathrm{d}t_\text{rest}} = \frac{\rho q}{2\pi\epsilon_0 l} \hat{r} . $$ lab frame : the charge density of the beam is enhanced by the relativistic $\gamma=1/\sqrt{1-v^2/c^2}$ factor . thus the electric field is : $$ \vec{e}_\text{lab} = \frac{\gamma \rho}{2\pi\epsilon_0 l} \hat{r} . $$ there is also a magnetic field of magnitude $$ b = \frac{\mu_0 \gamma\rho v}{2\pi l} $$ and directed so as to produce an attractive force . plugging these in the lorentz force formula $$ \vec{f}_\text{lab} = q ( \vec{e}_\text{lab} + \vec{v}\times\vec{b} ) = q\left ( \frac{\gamma \rho}{2\pi\epsilon_0 l} - \frac{\mu_0 \gamma\rho v^2}{2\pi l}\right ) \hat{r} = \frac{\rho q}{2\pi\epsilon_0 l}\gamma\left ( 1 - \epsilon_0\mu_0 v^2\right ) \hat{r} . $$ using $\epsilon_0 \mu_0 = c^{-2}$ this reduces to $\vec{f}_\text{lab} = \gamma^{-1} \vec{f}_\text{rest}$ which , on noting the relativistic time dilation $\mathrm{d}t_\text{lab} = \gamma \mathrm{d}t_\text{rest}$ , is exactly right ! note that i have used the fact that the force is orthogonal to the velocity implicity when writing the lorentz transformation law for the force . you can prove the covariance for general motions using the covariant formulation of em . lesson : relativity and electromagnetism go together like hand and glove !
in short , no , at least not at this point . unfortunately i was not able to read the entire article , but what i understood from the abstract is that it was only meant for very small volumes and masses , but if they did manage to levitate a cup of coffee , that is a step in the right direction . but there is another problem with this method - it uses high-intensity sound waves , although i am not sure at exactly what intensity , so it would likely be uncomfortable or possibly even dangerous to levitate a living being in this manner . also , i imagine that the device that would be able to levitate the mass of a human with sound waves would be relatively bulky , and there is the factor of the protective equipment that would likely be necessary . but i do believe it is possible , as just about anything is possible . i just do not think that it is the best way to levitate a human . other non-living materials , sure , it could be a great thing . but i think there will be better ways to do this with new discoveries . as for if you can levitate soon , who knows . there could be a discovery tomorrow that would allow you to levitate , or it could be years . update : i found this bit of information on wikipedia : there is no known theoretical limit to what acoustic levitation can lift given enough vibratory sound , but in practice current technology limits the amount that can be lifted by this force to at most a few kilograms . this was in the article on acoustic levitation that rody posted .
only insofar that the maximum writing speed is limited by the need to unambiguously encode the disk 's surface . if you tried to raise the speed of writing , the bit error rate increases : theoretically as a continuous , monotonic function of writing speed but practically a point is reached where the ber increases abruptly , so the ber as a function of speed tends to look like a step function . the writing data corresponds physically to changing the optical properties of the disk so this takes a nonzero time , and the more time one spends doing it , the more well formed the symbols are . as the ber increases , the error correction procedures , namely the computing of the nearest valid codeword to a corrupted one in coding space and , more importantly , repeat reads if error correction fails , use up more time , so the read gets markedly slower for a disk with a few errors , then fails altogether . this phenomenon is very like the practical example where it gets harder and harder to read someone 's hadnwriting as the writer gets more and more hurried and thus shapes their letters more and more ambiguously .
the potentiometer is not a normal resistor . they are often used simply as variable resistors , but in this example its use does its name justice -- " potentiometer " = voltage measure . the float is attached to a type of slider which contacts the resistor at some point in the middle . the result is that you can treat the potentiometer as two resistors ( i will call them r1 on top and r2 on bottom ) in series , where r1 and r2 always add up to the same total resistance , and therefore where the potential drop across r1 and r2 always add up to the voltage of the source on the left . as an example , if the float is at the top , you might find that r1 = 0 and r2 = 1000 ohms . if the voltage source is 5v , then v1 ( the voltage drop across r1 ) = 0 and v2 = 5v . if the float is exactly in the middle , r1 = r2 = 500 ohms . then v1 = v2 = 2.5v . if the float is at the bottom , then r1 would be 1000 ohms and r2 would be 0 , v1 would be 5v and v2 would be 0 . the movement does not change the voltage across the one whole resistor , but it changes how much of that voltage drop is above and how much is below the float . as drawn in your diagram , the voltmeter/level indicator measures v2 , the voltage drop that is below the float .
there are a two levels for the answer : first , perturbative string theory actually has two different scales : the string scale in which the extended nature of the string comes into play ( even classically strings are different than particles ) , and the planck scale in which the quantum mechanics of the string ( including quantum gravity ) becomes important . the string energy scale is lower than the planck energy scale , their ratio is the string coupling constant ( which is proportional to $\hbar$ ) , so it is meaningful to talk about stringy physics in the sub-planckian regime . your mental picture then depends on the size of the compact geometry . when the cy manifolds are larger the planck length you can use perturbative string theory , in which case the notion of a manifold still kind of makes sense . when reaching the planck scale the string theory is no longer perturbative and large fluctuations in the geometry are part of the story . since we know quite a bit about non-perturbative string theory ( through dualities and non-renormalization theorems ) these expected phenomena can be observed and analyzed precisely . for example , topology change of the cy manifold is a smooth process that can be discussed very precisely in this framework . ( incidentally , in this context we see that in the transition region between two well-defined geometries there is no geometrical description at all - contrary to some people 's intuition about quantum geometry and spacetime foam and all the rest of that . the geometrical description fails and there is some well-defined procedure to do any well-defined calculation without any reference to any type of geometry whatsoever ) . the second level of answer is that even when discussing perturbative strings on sub-planckian cy manifolds , you have to keep in mind that this is a shorthand for something more precise and technical - ( 2,2 ) scft which has a limit in which it becomes a sigma model with a cy manifold as a target space . in english this means that even classical strings in some target space probe the geometry in different ways than point particles , and to describe their physics you need more complicated machinery than just differential geometry . as you suspected the fact that the string is extended means that it is more forgiving to non-smooth features of the manifold , even classically . this is encoded in the fact that the scft is better behaved than differential geometry on that space , and stays well defined even in the presence of some types of singularities ( orbifolds of space are a famous example ) . so , in summary " string theory on cy " really means string theory whose classical point particle limit ( valid when the manifold is very large and smooth ) reduces to cy manifold . good to remember in general that there is some poetic license taken sometimes when physicists describe their work .
i think what is happening in rough qualitative terms us that the water freezes around the sides and the top first leaving a hole in the centre . ice expands by 4%-9% when freezing so as the water below freezes it forces the remaining water up through the hole where is freezes around the edge . the hole shrinks as the water freezes and rises around its edge forming the base of the spike . the spike is hollow so water is pushed up to the top where it freezes at the edge making the spoke grow longer update : if you search for ice spike on youtube you will find some good timelapse videos showing these spikes forming . i especially like this one because you can see the unfrozen water pushing up inside the spike . sometimes in a larger water container you can get an inverted pyramid shape . the explanation is the same and the shape is due to the crystaline nature of the ice . this video shows the phenomena
there is a simple reason why we can consider variations on the whole $a$ rather than the quotient $c=a/g$ and the reason is following : all configurations that are $g$-equivalent have the same value of the action $s$ . that is what we mean by the statement that the theory has the symmetry $g$ . so the variation of the action $s$ in the directions that are equivalent to the action of a gauge transformation in $g$ vanish automatically , by the gauge invariance of the action ! the variation of the action $\delta s$ is therefore a combination of the variations $\delta a_\mu$ only of those kinds that are independent of the directions along $g$ . that is also why the equations of motion that we derive from $\delta s=0$ do not determine the evolution of the fields such as $a_\mu$ unambiguously out of the initial conditions : the equations of motion only constrain $f_{\mu\nu}$ and they always allow us to change $a_\mu$ in the future , by a gauge transformation . this ambiguity arises from the " flat directions of the action " . you could be studying $\delta s =0$ on the quotient $c=a/g$ only but it would be cumbersome and $\delta s$ would be physically and literally the same thing as it is on $a$ . it is the very point of introducing degrees of freedom which include one redundant one ( because of the gauge symmetry ) to simplify the picture . in fact , the equations of motion from $\delta s= 0$ on $a$ are manifestly lorentz-covariant etc . if you were trying to parameterize the space $c=a/g$ by some fields , you would probably have to impose some lorentz-breaking or otherwise unnatural conditions , e.g. $a_0=0$ , and the whole formalism would lose the manifest lorentz symmetry even though the actual phenomena , when looked at properly , would still obey the laws of relativity . so yes , the methods on $a$ and on $c=a/g$ are equivalent , and it is the calculus on $a$ that is the smarter one . if the formalism using $c=a/g$ were more convenient from all points of view , we would never talk about the gauge symmetry because it would be a totally counterproductive concept ! be sure that it is a very useful concept . i can not make sense out of the second part of the question . when we discuss infinitesimal variations of quantities such as energy , they should be linear combinations of the infinitesimal variations of the fields , like $du=\vec e\cdot d\vec d$ . but your expression is " doubly infinitesimal " , it is bilinear in $\delta x$ where $x$ is something , and terms this small can be neglected in the usual infinitesimal calculus in which $\delta a$ is sent to zero because they are of higher order . the energy ( and stress-energy tensor ) is gauge-invariant in electrodynamics , too , so its ( linear , first-order ) variation induced by gauge transformations is equal to zero . talking about some second-order " variations " seems completely misguided to me . also , less seriously , i feel uneasy about your usage of the word " orthogonal " . in general , one does not have an inner product ( needed to determine orthogonality ) on the full configuration space and it is really not needed for most questions of this sort . the directions in $a$ away from a slice that may be used as representatives of $c=a/g$ come in two types : those that are pure gauge transformations , in the direction of the $g$ " fibers " , and those that are not . most of them " are not " but any combination of those that are and those that are not " is not " again and no combination is really " fundamentally different " than others . so it is a bit meaningless to look for " orthogonal " directions to the directions along $g$ . finally , you ask whether " a " gauss law is related to the standard gauss law taught at school but you have not really explained what you mean by " a " gauss law . there is only one gauss ' law . it is the equation of motion obtained by varying the action with respect to $a_0$ , and it gives us something like ${\rm div}\ , \vec d = \rho$ . it is always the same law – which may be generalized to more complicated theories than electrodynamics , e.g. yang-mills theory . this equation ${\rm div}\ , \vec d =\rho$ is interesting because it does not contain any time derivatives . so it really does not dictate the time evolution of anything : it already constrains the initial state . one may prove that if the equation holds at $t=0$ , it will hold at any time : the time-derivative of the gauss ' law may be derived as a spatial derivative of other maxwell 's equations involving $\vec d$ . this non-dynamical = constraint character of the gauss ' law ( the fact it does not contain time derivative ) is related to the fact that this equation of motion is derived from the variation of a field that may be interpreted as a completely redundant one in a particular convention how to fix the gauge symmetry . that is why we can identify it with the statement that the states related by gauge transformations are treated as equivalent states by the theory . this is particularly clear in the quantum theory where we may a priori have states not annihilated by ${\rm div}\ , \vec d - \rho$ but only states that are annihilated by this operator , i.e. states that respect the equivalence of the states related by gauge transformations , may be considered physical .
these are several rather different question . first , bosonic string theory in $d=26$ has both open string tachyons and closed string tachyons . the open string excitations are attached to d-branes . in particular , open strings that can live everywhere are excitations of a spacetime-filling d25-brane . the list of these open string excitations includes a tachyon . this open string tachyon is a sign of instability of the d25-brane with respect to the complete annihilation of the d25-brane . it is a violent process but the released energy only goes like $1/g_{closed}$ , proportionally to the tension of the affected d-brane , which is smaller – for a small $g_{closed}$ – than the energy densities of order $1/g_{closed}^2$ which occur in closed string processes . the difference of potential energies before ( local maximum of the potential ) and after the tachyon condensation was conjectured by ashoke sen to coincide with the tension of the d25-brane . it has been verifified by various informal proofs as well as very sophisticated and mathematically rigorous proofs in open string field theory , especially cubic string field theory where the most complete steps towards the quantitative understanding of the tachyon condensation was achieved by martin schnabl . http://arxiv.org/abs/hep-th/0511286 the understanding of the closed string condensation is much less clear . most likely , there does not exist any nearby local minimum and the potential for the closed string tachyon is unbounded from below in all directions , signalling a neverending instability that destroys the spacetime beyond repair . whether there are fermions in bosonic string theories is a different issue . one may mention type 0 string theory in $d=10$ – which are naively as purely bosonic as the $d=26$ string . however , it was proved by shiraz minwalla and pals that one may find fermionic solitons in those theories : http://arxiv.org/abs/hep-th/0107165 there are also papers claiming to find a dynamical process that interpolates between bosonic string theory and superstring theory . they require some time-dependent configurations , however . a time-dependent tachyon is a part of the picture , if i remember well . see e.g. this paper by simeon hellerman and ian swanson and other papers by the same authors ( and followups and references ) : http://arxiv.org/abs/hep-th/0612051
the bubbles form as air is entrained in the water during the pouring process . the key factor here is how fast the bubbles collapse . this may seem a funny distinction , but bubbles are always thermodynamically unstable compared to the bulk liquid because it always costs energy to create them . the only reason we see long lived bubbles is that there is a kinetic barrier that stops the water films collapsing . this image shows schematically what the water film in a bubble looks like ( this is a soap bubble but the principle applies to all bubbles ) : the water contains a chemical ( soap in this case ) that is surface active i.e. it preferentially adsorbs on the surface . in the case of soap the end that sticks into the water ( the red blob in the picture ) is negatively charged and the negative charges on the opposite sides of the water film repel each and stop the film from thinning and collapsing . in your case you say there is no soap suds present , though note that there may be very small amounts of surfactant present , even on apparently clean glass , because surfactant adsorbs on glass and it can be difficult to wash it all off . however lots of other things are surface active including many biopolymers , and your glass may have traces of some polymer . absolutely clean water in an absolutely clean glass will not form a stable foam , so in your case there must be some contaminent present , either on the glass or in the tap water .
imagine $\theta$ to be quite large , about 80$^\circ$ . any normal car would fall down ( rather slip down against the friction force from tires . ) but if the speed of the car is very high , the centrifugal force would prevent it from slipping ( imagine a horizontal roller coaster loop-the-loop ) . the frictional force would have to act up the incline in this case to counter a component of the weight down the incline . also , consider static friction , not kinetic . the friction is the sideways dragging of the tires and not related to their kinetic/rolling motion .
the difference is ( just ) past the last digit of accuracy in your number , so it is impossible to tell . you can estimate the fraction of the mass of the atmosphere by the ratio of the atmosphere 's height to the radius of the earth ( which gives the order of magnitude of the fraction of the earth 's volume in the atmosphere , about 10km/6000km , or 1/600 ) , times the ratio of density of gas to ordinary solid ( which is about 1/300 ) . this gives 1 part in $10^{-5}$ , and this is an overestimate , because the earth 's core is much denser than an ordinary solid because of the immense pressure . the mass figure you quote is correct to about 1 part in $10^{-5}$ , so it is not clear if it includes the atmosphere or not , because this is a negligible error . the mass of the earth is found by measuring g , and the 1 part in $10^{-5}$ ( or less ) variations due to the atmosphere as you go up will be hard to distinguish from oblateness corrections , or just experimental error . atmospheric pressure multiplying standard atmospheric pressure by the surface area of the earth gives the mass of the atmosphere as $5 \times 10^{18}kg$ , which is almost exactly 1 part in $10^{-6}$ of the mass of the earth that you quote .
there is a crucial difference between the newtonian time-varying field effect and the long distance effect , in that the newtonian effect is what is called " near field " and the radiative transmission of energy is by a " far field " . it is the difference between an electrostatic force and a radio wave ( lubos motl 's answer gets at this , but it is possible to elaborate using electromagnetism as a direct analog . gravity has more components , and is less intuitive , but it is the same idea ) . not all time-varying field responses are true waves . if you hold two charges , they have an electrostatic force . if you move one of the charges around the other , you get a time-varying electrostatic field on the other . this effect can lead to all sorts of oscillations on the second object . but this time varying field is , when the objects are separated by less than the speed of light divided by the typical oscillation period , not an electromagnetic wave . it is just a time-varying electrostatic field . the electrostatic field dies off as $1/r^2$ , and so the energy density in the field dies off as $1/r^4$ , which means that the total energy going past a sphere of radius r dies off as $1/r^2$ . if there were radiation going out , the amount of energy going past concentric large spheres would be roughly constant , as the ratiation passed the spheres , and this requires fields which fall off like $1/r$ , not $1/r^2$ . the difference in falloff of the two kinds of fields is important . there are proposals for near-field electrostatic and magnetostatic communication . in practice , this just means using a radio wavelength bigger than the distance between the objects , so that you would have them nearly touching , and then you can synchronize them with signals that are too small to be registered from far away ( because static fields fall off much quicker than energy carrying waves ) . the magnetic fields generated when you move an electric charge in a circle , together with the induced electric fields from the magnetic field , does only die off as $1/r$ , meaning that the total energy density carried across larger and larger spheres is constant . this energy flux is the electromagnetic wave energy , and it is the far-field , or radiative field component of the electrostatic situation . the near and far field are not continuously related , they cross over , so that the far-field responses are not intuitive as compared to the near field . gravity also has a near-field $1/r^2$ force , and this is also carries negligible energy and has zero detection possibility at long distances . the induction of components other than time-time of the metric tensor is is required to have gravitational waves , and this is not possible in newton 's conception . so it is not correct to say that newton was considering gravitational waves , even when you make gravity propagate at finite speed , because the effects you are considering are all near field effects , while the true gravitational radiation is far-field .
it is well-known that x-rays are blocked by metal . [ ref : kid 's science ] obviously the doctor wants to look at your internal organs , unobscured by a fuzzy outline of your house keys and pendant . so , the sign is requesting that you removing metal items from the external parts of your body , to allow visibility to the internal parts . ( mri is a totally different story . )
i found a compact version of the course notes , re-written in english by the same professor who gave the original lectures . you can find them at this link .
this would be a dyson sphere , there should be no difference , in terms of gravitational forces , if the central object is a star or a black hole ( of the same mass ) . see also niven ring - where rotation is used to provide artificial gravity .
alas , if anyone had a solid answer to this , we had publish it and become famous exoplanet scientists . in our own solar system , certainly most of the planetary mass is in the outer planets . the reason that is often cited has to do with the ice line . beyond a certain distance , water and methane and other such materials are solid rather than gaseous . more solid matter $\rightarrow$ faster accretion $\rightarrow$ bigger final products . however , the relative sizes of the outer planets amongst one another is still not so easily determined . if you believe the nice model , which suggests uranus and neptune have swapped orbits , then the outer planet masses were originally monotonically decreasing with distance from the sun , but there are no quantitative predictions regarding this . as it turns out , though , there are hundreds of planetary systems out there that look nothing like our solar system . radial velocity and transiting surveys have found plenty of " hot jupiters " - jupiter-mass and above planets orbiting closer to their stars than mercury around the sun . the transiting data obtained with the kepler mission is given on the kepler website . note the large masses at small separations . on the other hand , with direct imaging , we have found large objects orbiting their stars much further than neptune orbits the sun . see for instance fomalhaut b and hr 8799 c and b . there may very well be underlying trends , but seeing them amongst all this diversity will require a lot more data . right now exoplanet science is in its infancy - it is more exploratory than systematic , especially given how hard it is to detect small planets . the only hard restriction is if a proposed arrangement of planets ( usually close together and massive ) proves to be dynamically unstable on ( astronomically ) short timescales . since such systems would not last for very long , we do not expect to see many of them . of course , calculating such dynamics can be almost as tricky as doing the dust simulation you wanted to avoid . ultimately , if you are procedurally generating systems for some visualization/gaming software , no one can fault you for having a little poetic license when it comes to inputting parameters for what these systems may look like .
a tesla coil is in many ways the same as a transformer and hence if you know the working of one you can understand the working of the other . now in common transformers , the coils are couple tightly , so that a large amount of energy transfers from the primary to secondary . this works well at low voltages , but at high voltages the insulating air gap may suffer a dielectric breakdown , thereby becoming the reason for heavy losses . the tesla coil avoids this and hence can be used at very high voltages . a tesla coil consists of two lc oscillators very loosely coupled to each other . when a charged capacitor is connected to an inductor an electric current will flow from the capacitor through the inductor creating a magnetic field . when the electric field in the capacitor is exhausted the current stops and the magnetic field collapses . as the magnetic field collapses , it induces a current to flow in the inductor in the opposite direction to the original current . this new current charges the capacitor , creating a new electric field , equal but opposite to the original field . as long as the inductor and capacitor are connected the energy in the system will oscillate between the magnetic field and the electric field as the current constantly reverses . the figure below shows a common schematic of the tesla coil . when the switch is open , the cap is charged . now when the switch is closed , the action discussed earlier will cause a magnetic field to be built up in the primary inductor and this in turn will set up a field in the secondary . since the secondary has a large amount of turns , this will cause an extremely high e field to be built up thereby resulting in large voltages . the new coils have some additional components , but this is the very basic working of the tesla coil with minimal components .
first , $u$ is surely not " any non-singular matrix " . for a given basis , $u$ is almost completely determined i.e. unique . it contains $\gamma_2$ because it is derived from the only imaginary pauli matrix . because of the basic dirac algebra $$ \{ \gamma_\mu , \gamma_\nu \} = 2\cdot 1_{2\times 2} \cdot g_{\mu\nu} $$ one may see that $\gamma_0$ is hermitian , $\gamma_0=\gamma_0^\dagger$ , while the spatial ones are anti-hermitian , $\gamma_i=-\gamma_i^\dagger$ . in your identity , you want to relate $\gamma^\mu$ to its transposition $\gamma^{\mu t}$ . up to the sign that depends on the spatial or temporal character of $\mu$ , the transposition is the same thing as complex conjugation . so a related problem is whether the complex conjugate matrices $\gamma^{\mu*}$ can be related to $\gamma^\mu$ by something like a conjugation . and the answer is yes . the main fact behind the exercise is that $\sigma^2$ is the only imaginary pauli matrix , so complex conjugation of pauli matrices is equivalent to the conjugation by $\sigma^2$ with an extra sign . this may be easily generalized if you also include the temporal 0th component and if you use the normal basis . you should check the identity you want to verify in a particular convenient basis , i.e. with an explicit form of the gamma matrices . the verification is most convenient if you write the gamma matrices in block form , with $2\times 2$ blocks being either multiples of pauli matrices or the unit matrix . in a more general representation , the dirac gamma matrices differ from those in the particular basis you will have verified by a conjugation only , and this may only mean that $u$ is changed in the formula , but the essence of the conjugation is unchanged . these equations are important because $c$ is related to the charge conjugation – the replacement of particles by antiparticles ( e . g . exchange of electrons and positrons ) . mathematically , the most important part of the charge conjugation is complex conjugation which is why we needed to express the " complex conjugate gamma matrices as some conjugations of the normal ones " . theories with a symmetry between matter and antimatter are symmetric under c - the charge conjugation symmetry . spinors are mapped to $\psi\to c\psi$ etc . and the only hard part of the symmetry of the lagrangian is a step that requires you to conjugate the gamma matrices by $c$ which is why it is good that we have a way to simplify $c^{-1t} \gamma^\mu c^t$ .
1 ) op is basically wondering how weinberg on the middle of p . 112 can extend the integration region from$^1$ $${\cal j}^{\pm}_{\beta}~=~ \int_{m_{\alpha}}^{\infty} \ ! de_{\alpha}\frac{e^{-ie_{\alpha}t}g ( e_{\alpha} ) t_{\beta\alpha}^{\pm}} {e_{\alpha}-e_{\beta}\pm i0^{+}}$$ to include the negative real axis $${\cal j}^{\pm}_{\beta}~=~ \int_{-\infty}^{\infty} \ ! de_{\alpha}\frac{e^{-ie_{\alpha}t}g ( e_{\alpha} ) t_{\beta\alpha}^{\pm}} {e_{\alpha}-e_{\beta}\pm i0^{+}} , $$ where $g:e_{\alpha}\mapsto g ( e_{\alpha} ) $ is a meromorphic function ? 2 ) that weinberg ( implicitly ) assumes meromorphicity of the $g:d\subseteq \mathbb{c}\to \mathbb{c}$ function can be deduced further down on p . 112 , where he writes that [ . . . ] we can close the contour of integration for the integration variable $e_{\alpha}$ [ . . . ] , which is a clear reference to the residue theorem , which in turn assumes meromorphicity . also weinberg writes on the same page$^1$ [ . . . ] the functions $g ( e_{\alpha} ) $ and $t_{\beta\alpha}^{\pm}$ may , in general , be expected to have some singularities at values of $e_{\alpha}$ with finite [ . . . ] imaginary parts [ . . . ] so there is little doubt that weinberg assumes meromorphicity of $g$ . 3 ) on the other hand , on the bottom of p . 109 , weinberg writes$^1$ [ . . . ] therefore , we must consider wave-packets , superpositions $\int\ ! de_{\alpha}~g ( e_{\alpha} ) \psi_{\alpha}$ of states , with an amplitude $g ( e_{\alpha} ) $ that is non-zero and smoothly varying over some finite range $\delta e$ of energies . [ . . . ] now according to the identity theorem for holomorphic functions , if a function $g:d\subseteq \mathbb{c}\to \mathbb{c}$ is zero on a subset $s\subseteq d$ that has an accumulation point $c$ in the domain $d$ , then $g\equiv 0$ is identically zero . however , any interval $i\subseteq \mathbb{r}$ on the real line of non-zero length has accumulation points . so if weinberg in above quote literally means that $g$ is mathematically zero outside some finite interval $i\subseteq \mathbb{r}$ , then $g\equiv 0$ would be identically zero in the whole complex plane . of course weinberg does not mean that . he just means that $g$ outside some finite range takes so small values , that to the precision $\epsilon$ that we are working , it does not matter whether we include the integration region $\mathbb{r}\backslash i$ , or not . in particular , mathematically speaking , weinberg has only proven the condition $$\tag{3.1.12} \int_{m_{\alpha}}^{\infty} \ ! de_{\alpha}~ e^{-ie_{\alpha}t} g ( e_{\alpha} ) \psi^{\pm}_{\alpha}~\longrightarrow~ \int_{m_{\alpha}}^{\infty} \ ! de_{\alpha}~ e^{-ie_{\alpha}t} g ( e_{\alpha} ) \phi_{\alpha} ~\text{for}~ t\to\mp\infty \qquad $$ within some precision $\epsilon$ . however , the precision $\epsilon$ can be made arbitrarily fine by preparing more and more sharply defined wavepackets $g$ . 4 ) if one would like to have a concrete example of a $g$ function , one may think of a lorentzian function ( aka . breit–wigner or cauchy distribution ) , $$g ( e_{\alpha} ) ~=~ \frac{1}{\pi}\frac{\delta}{ ( e_{\alpha}-e_0 ) ^2+\delta^2} , \qquad \int\ ! de_{\alpha}~g ( e_{\alpha} ) ~=~1 , $$ for appropriate choices of constants $e_0$ and $\delta$ . 5 ) finally , one should not loose sight of weinberg 's main goal in section 3.1 , namely to argue the $\pm i0^{+}$ prescription in the lippmann-schwinger equations $$\tag{3.1.17}\psi^{\pm}_{\alpha} ~=~\phi_{\alpha}+\int\ ! d\beta\frac{t_{\beta\alpha}^{\pm}\phi_{\beta}} {e_{\alpha}-e_{\beta}\pm i0^{+}} . $$ the lippmann-schwinger equations ( 3.1.17 ) are not an approximation , and they are independent of the choice of wavepacket $g$ . -- $^1$ to simplify the discussion , we have taken the liberty to replace weinberg 's more general $\alpha$-integration with just an $e_{\alpha}$-integration . here $$\tag{3.1.4} \int \ ! d\alpha \cdots \equiv \sum_{n_1\sigma_1n_2\sigma_2\cdots}\int d^3p_1 d^3p_2 \cdots$$ changing integration variable from $e_{\alpha}$ to momenta does not solve op 's problem , essentially because we still have to pick the branch of the pertinent square root that has positive real part , so that it does not bring us any closer in understanding negative energies .
here tension is zero for very sort span of time ( infinitesimally sort time ) , or for an instant only . when ever it moves away from the vertically top position it will fill tension again . so it will move in circular trajectory . and if we consider instantaneous velocity it is tangential . i think this clarify your doubt .
my guess will be that it is mainly the effect of capillarity forces due to the porous quality of the stone surface . this might also be a good read .
you are correct in stating that the resultant effect on incoming rays and their focussing depend on the two lenses and the distance between them . to reduce the two lens system in general we use the principal planes method and then can describe the situation in terms of two parallel planes only . but when they say that the powers are added or the " resultant " focal length is given by $$1/f=1/f_1+1/f_2$$ is valid only in case of thin lenses very close together such that their combined thickness can be ignored . you can easily get this equation by assuming their ( lenses' ) centres two be coincident and then applying some geometry or by using the general refraction -through-curved-surfaces equation and trace your image based on the object distance . in any case , the finiteness of the thickness has to be eliminated to get this equation . if we assume two very thin lenses in contact , than this equation holds true and the " resultant " power or the focal length is just the power/focal length of the single lens that can replace the two-lens system without any change in its optical properties ( in paraxial approximation ) .
we have conservation of energy to contend with . the magnetic energy being released in the upper layers of the suns atmosphere originated in turbulent convection lower down . so in effect the upper layers of the sun ( upper here meaning roughly from the photosphere and downward ) are acting as a heat engine putting some energy into the suns magnetic field . so we can not " twist " magnetic field lines without providing the energy to do so . it would be possible to generate electricity from changes in the earth 's magnetic field caused by the impingement of solar wind born magnetic energy on the earths magnetic field . any change in the total flux of magnetic field going through a current loop , generates a voltage . in fact during geomagnetic storms there is a danger that currents induced in power transmission lines could cause serious damage to th electrical grid system . but this is not a practical way to generate energy , but rather a hazard that our infrastructure has to contend with .
to model a diffuse surface , imagine a house that is on fire inside ( ! ) so that everything inside is emitting light equally in all directions . you can also imagine a very hot oven or kiln in which the interior walls are aglow . now , if you look through the door of the house , the flux of light entering your eye is obviously proportional to the area of the door . in other words , proportional to the cross-sectional area of the column of light passing through the door on its way towards you . if you view the door from an angle , then the effective area has been reduced by a factor of $\cos\theta$ where $\theta$ is the incident angle . the door you see has gotten narrower by a factor of $\cos\theta$ , and the column of light similarly . to see this , draw a right triangle where the hypotenuse is the door , and one side lies parallel to the line between you and the door , while one side is perpendicular to that line . the angle between the hypotenuse and the second side is complimentary to the angle between the normal of the door and the line connecting you and the door . hence the third side has length $hypotenuse \times \cos\theta$ , you can see the geometry of the situation in this image : http://www.idav.ucdavis.edu/education/graphicsnotes/shading/img86.gif now the door was letting out light in all directions equally , so if we instead imagine the door emitting light in all directions equally ( * ) , then this is the same as a diffuse surface . ( * ) this equivalence , i believe , depends on the inverse-square fall-off of intensity .
because $\left ( i\gamma^\mu\frac{\partial}{\partial x^\mu}+m\right ) _{\xi\xi&#39 ; }i\delta ( x-x&#39 ; ) $ is a symbolic expression for a given analytical right-hand side . it is written so for convenience ( not yet calculated ) but it is a specific expression like $\delta ( x-x&#39 ; ) $ or $\delta ( x-x&#39 ; ) &#39 ; $ . it should not acquire any " gauge extension " by definition . this expression does not contain a " particle momentum " .
the pressure coefficient at a certain point ( at which the value of the pressure is $p$ ) is defined as $c_p=\frac{p-p_\infty}{\frac12\rho_\infty u_\infty^2} , $ where the $\infty$-symbol denotes freestream quantities . for an incompressible and steady fluid and assuming zero viscosity , bernoulli 's equation is given by $p+\frac12\rho u^2=p_\infty+\frac12\rho u_\infty^2 , $ which we can rearrange as $\frac{p-p_\infty}{\frac12\rho u_\infty^2}=1-\frac{u^2}{u_\infty^2} . $ in order for this expression to be valid for given fluid , we have to show that it is incompressible and steady . incompressibility means that the laplacian of the flow potential $\phi$ vanishes , this can be shown to be true for the problem at hand . furthermore , a fluid is steady if its flow does not depend explicitely on time , which is also the case .
voltage is the unit of electric potential , the electric potential difference ( in your case , the potential difference between the two ends of resistor in a circuit ) can be called the voltage drop . the potential difference produces an electric field $\vec{e}$ , and the direction of $\vec{e}$ points from high potential to low potential . the electric field applies a force on charged particles ( i.e. . electrons in circuits ) such that the electrons are driven by this force and move , thereby producing a current . so you can see the potential ( voltage ) difference is the reason why there is a current . by the way , you cannot say the " current 's voltage " , since the current is defined as $i = dq/dt$ . that is , it only describes the flow of charge per unit time . when electrons move through a resistor they are scattered by the other electrons and nuclei , causing the electrons lose some of their kinetic energy . but the presence of the electric field will then accelerate the electrons again . we can calculate the average kinetic energy statistically , and assume the electrons are moving at a single average velocity . thus after each collision there is a loss of kinetic energy ( it is converted to heat ) but which is recovered due to the work done by the electric field . and this work is equal to the potential energy difference . you can see that the electrons have the same kinetic energies both when they enter and when they leave the resistor , but different potential energies . so we can say the voltage drop at the two ends of a resistor is caused by the potential energy difference .
i only know one particular reason : the transition responsible for the h1 line is highly forbidden and shows an extreme lifetime ( 10^7 years ) , so the absorption rate in interstellar clouds , which can be very opaque for every other radiation , is very small . looking at the h1 line allows you to see objects which are for example hidden behind dust clouds that absorb a lot of the radiation of the object behind .
you are incorrect to suppose that this spacetime is curved . in fact , up to some conditions on the coordinate ranges , this is simply a piece of minkowski spacetime . let me put it in this form : $$ds^2 = dt^2 - t^2 ( d\psi^2 + \sinh^2\psi\ , d\omega^2 ) \text{ , }$$ where $d\omega^2 = d\theta^2 + \sin^2\theta\ , d\phi^2$ is the metric for a unit $2$-sphere , and we can go from spherical minkowski form $dt^2 - dr^2 - r^2\ , d\omega^2$ to yours by the substitution $$\begin{eqnarray*}r = t\sinh\psi and \quad\quad and t = t\cosh\psi\end{eqnarray*}$$ simply , the ricci tensor is zero because $r_{\mu\nu} = 0$ is a coordinate-invariant condition : if a tensor zero in some coordinate chart ( e . g . , minkowski ) , then it is zero in every coordinate chart ( e . g . , yours ) . naturally , the riemann tensor is also zero . interestingly , this is the also the hyperbolic flrw ( "big bang" ) metric in the limit of zero density , where a galaxies do not interact gravitationally but every galaxy sees itself a center of cosmological expansion . for details , see the milne universe model . . . . the article at the link below says that this interval corresponds to an open flat ( space curvature = -1/r^2 &lt ; 0 ) expanding universe ( look at the picture in the question ) . can you explain that ? sure : spacetime curvature is not the same thing as space curvature , or more generally the curvature of a manifold is not the same thing as the curvature of a submanifold of lower dimensionality . take euclidean space $e^3$ in spherical coordinates , $ds^2 = dr^2 + r^2d\omega^2$ , and look at submanifolds of constant radius : $$d\sigma^2 = r^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) \text{ , }\quad r = \text{constant}$$ one can verify by direct computation that the scalar curvature is positive , as one might expect from them being spheres . obviously , if you ' slice up ' $e^3$ by planes , each will have zero curvature instead . in your case , what is going on is just the same thing , except we are ' slicing up'/'foliating ' the minkowski spacetime $e^{1,3}$ by spacelike hypersurfaces of the form $$d\sigma^2 = t^2\left [ d\psi^2 + \sinh^2\psi\ , d\omega^2\right ] \text{ , }\quad t = \text{constant}$$ in this form , it is not hard to guess that the curvature for each hypersurface is same as above except negative . ( let me know if you need help calculating it explicitly , but for the moment i think intuitively motivating the fact that this makes sense would be more fruitful . ) since coordinate changes do not change the scalar curvature , we can let $\rho = t\sinh\psi$ to get $d\rho = t\cosh\psi\ , d\psi$ and $$t^2\ , d\psi^2 = \frac{d\rho^2}{\cosh^2\psi} = \frac{d\rho^2}{1 + \sinh^2\psi} = \frac{d\rho^2}{1+\rho^2/t^2}\text{ , }$$ putting it in the same form as ( 80 ) except for variable names ; the other part of the metric should be more obvious .
temperature is not a concept that has a lot of utility at the level of single atoms because it represents the mean kinetic energy of a group of particles ( to within a coefficient ) . you can define it , it just does not help much . at the level of two atoms you revert to a more fundamental model such as the forces between them . one atom transfers energy to another through electromagnetic forces between them . when that energy manifests as randomized kinetic energy at the microscopic scale we refer to it as " heat " at the macroscopic scale . in a solid it is usually reasonable to treat the forces between individual pairs of atoms as being spring-like ( i.e. . they obey $f_{i , j} = -k ( r_{i , j} - r_0 ) $ ) . starting from there you can build various models of solid behavior . for instance the einstein model of a crystal .
let me here just derive the equation ( 6.11 ) that follows the sentence , you mention . the navier-stokes equation ( 6.6a ) reads $$\partial_t v_i + v_j\partial_j v_i = -\partial_i p + f_i + \nu~\partial_j\partial_j v_i . $$ the incompressibility condition ( 6.6b ) reads $$\partial_jv_j=0 . $$ hence we have in the unprimed and the primed point that $$\partial_t v_i = -\partial_j ( v_j v_i ) -\partial_i p + f_i + \nu~\partial_j\partial_j v_i , $$ and $$\partial_t v&#39 ; _i =-\partial&#39 ; _j ( v&#39 ; _j v&#39 ; _i ) -\partial&#39 ; _i p&#39 ; + f&#39 ; _i + \nu~\partial&#39 ; _j\partial&#39 ; _j v&#39 ; _i , $$ respectively . therefore , averaging yields $$\begin{align} \partial_t \langle v_i v&#39 ; _i \rangle and = \langle v&#39 ; _i \partial_t v_i \rangle + \langle v_i \partial_t v&#39 ; _i \rangle \\ and = - \langle v&#39 ; _i\partial_j ( v_j v_i ) \rangle - \langle v_i \partial_j&#39 ; ( v&#39 ; _j v&#39 ; _i ) \rangle \\ and -\langle v&#39 ; _i \partial_i p \rangle -\langle v_i \partial&#39 ; _i p&#39 ; \rangle\\ and +\langle v&#39 ; _i f_i \rangle +\langle v_i f&#39 ; _i \rangle \\ and +\nu \langle v&#39 ; _i\partial_j\partial_jv_i\rangle + \nu \langle v_i\partial&#39 ; _j\partial&#39 ; _j v&#39 ; _i\rangle \\ and = -\partial_j \langle v_i v_j v&#39 ; _i\rangle -\partial_j&#39 ; \langle v&#39 ; _i v&#39 ; _j v_i\rangle \\ and -\langle v&#39 ; _i \partial_i p \rangle -\langle v_i \partial&#39 ; _i p&#39 ; \rangle\\ and +\langle v&#39 ; _i f_i \rangle +\langle v_i f&#39 ; _i \rangle \\ and +\nu\left ( \partial_j\partial_j + \partial&#39 ; _j\partial&#39 ; _j\right ) \langle v_iv&#39 ; _i\rangle , \end{align} $$ where we have used that averaging and differentiation commute , and also that primed velocities are independent of unprimed derivatives , and vice-versa . the full karman-howarth-monin relation is derived by marc brachet on p . 8-9 here , essentially following the book by uriel frisch ( 1995 ) .
hooke 's law is frequently used to model multi-dimensional materials because the stress tensor is simple ( linear ) . the full expression can be found on wikipedia . the simplification for 2d is straight forward ( drop any terms with a 3 in the subscript ) . note that whether deformation in one dimension affects the others is a property of the material and shows up through poisson 's ratio ( $\nu$ ) . independence between deformations in x and y imply $\nu = 0$ if you imagine two perpendicular springs only , then the terms with $\gamma$ ( or different subscripts like 12 , 23 , 31 , depending on the form of the equation ) drop out of the expression as those are shear terms . the shear terms can be thought of as a spring across the diagonal . the stress tensor $\sigma$ is defined as the force per unit area .
the short answer is that you are basically correct ; you just need to be more careful with your notation and your minus signs . here 's the long answer . by the definition of a conductor , the sphere is at some constant potential . additionally , the potential of the system goes to zero as $r\rightarrow \infty$ . there are a lot of different ways to think about this problem , but the simplest is probably to say that since the problem is spherically symmetric the equipotential surfaces ( i.e. . the surfaces of constant potential ) are concentric spheres centered around the conductor . therefore , the only important length in determining a potential difference is the difference in radius from the center of the sphere . ( think about it : you can move freely over any sphere concentric with the conductor without changing your potential . ) as a result , $a$ and $b$ may as well both lie along the same line connecting them to the origin . as you correctly noted , if the sphere carries a charge $q$ , then the potential at a given point outside of the sphere is given by $v=\frac{kq}{r}$ where $r$ is the distance from the center of the sphere to that point . by definition , $\vec e=-\vec \nabla{v}$ . therefore , $\vec e=-\frac{\partial{v}}{\partial r}\hat r=\frac{kq}{r^2}\hat r$ . now , if you wanted to , you could integrate $-\int_b^a\vec e\cdot d\vec r=-\int_b^a\frac{kq}{r^2}dr$ which would give you back $v|_b^a=v_a-v_b$ . ( since $d\vec r=\hat rdr$ and $\hat r\cdot\hat r=1$ ) but we already know what the functional form of $v$ is , so we could just take that difference immediately . you get the same answer both ways : $$ \delta v=kq\big ( \frac{1}{a}-\frac{1}{b}\big ) $$
i am not an expert in gravity , however , this is what i know . there is a hypothesis about gravity being an entropic property . the paper from verlinde is available at arxiv . that said , i would be surprised for this to be true . the reason is simple . as you probably know , entropy is an emergent property out of statistical probability . if you have non-interacting , adimensional particles into one half of a box , with the other half empty and separated by a valve , it is probability , thus entropy , that drives the transformation . if you look at it from the energetic point of view , the energy is exactly the same before and after the transformation . this works nicely for statistical distribution , but when you have to explain why things are attracted to each other statistically , it is much harder . from the probabilistic point of view , it would be the opposite : the more degrees of freedom your particles have , the more entropy they have . a clump has less degrees of freedom , hence has less entropy , meaning that , in a closed system , the existence of gravity is baffling . this is out of my speculation , and i think i am wrong . the paper seems to be a pleasure to read , but i have not had the chance to go through it .
these concepts refer to completely different aspects of reality . supersymmetry is a ( possible ) symmetry of the microscopic laws of nature , much like the rotational symmetry . entropy is the quantity counting the disorder of a given ( usually macroscopic ) system , the number of rearrangements that do not change the macroscopic appearance of the physical system ( well , the logarithm of the number of these rearrangements ) . entropy may be zero or nonzero for a system that preserves some supersymmetry or does not preserve any supersymmetry , entropy may be zero or nonzero in a theory that is supersymmetric and in a theory that is non-supersymmetric . all the combinations are surely possible . supersymmetry is a particular property of theories ( or states ) , a constraint , but it does not prohibit macroscopic objects . entropy is a measure of any macroscopic physical object . some calculations of entropy may simplify in a supersymmetric theory , especially if the state of the physical system preserves some of the supersymmetries . ( for example , the strominger-vafa black hole in 5d is the first one whose entropy was computed microscopically , and it is largely because it is the simplest black hole with a classically nonzero horizon that still preserves some susy , i.e. it is bps , we say . ) but that is true for all calculations , not only entropy calculations : susy often constrains and simplifies things .
the rms ( root-mean square ) value of an ac voltage , which is what is represented as "110 v " or "120 v " or "240 v " is lower than the electricity 's peak voltage . alternating current has a sinusoidal voltage , that is how it alternates . so yes , it is more than it appears , but not by a terrific amount . 120 v rms turns out to be about 170 v peak-to-ground . i remember hearing once that it is current , not voltage , that is dangerous to the human body . this page describes it well . according to them , if more than 100 ma makes it through your body , ac or dc , you are probably dead . one of the reasons that ac might be considered more dangerous is that it arguably has more ways of getting into your body . since the voltage alternates , it can cause current to enter and exit your body even without a closed loop , since your body ( and what ground it is attached to ) has capacitance . dc cannot do that . also , ac is quite easily stepped up to higher voltages using transformers , while with dc that requires some relatively elaborate electronics . finally , while your skin has a fairly high resistance to protect you , and the air is also a terrific insulator as long as you are not touching any wires , sometimes the inductance of ac transformers can cause high-voltage sparks that break down the air and i imagine can get through your skin a bit as well . also , like you mentioned , the heart is controlled by electric pulses and repeated pulses of electricity can throw this off quite a bit and cause a heart attack . however , i do not think that this is unique to alternating current . i read once about an unfortunate young man that was learning about electricity and wanted to measure the resistance of his own body . he took a multimeter and set a lead to each thumb . by accident or by stupidity , he punctured both thumbs with the leads , and the small ( i imagine it to be 9 v ) battery in the multimeter caused a current in his bloodstream , and he died on the spot . so maybe ignorance is more dangerous than either ac or dc .
the experiment detected more than just solar neutrinos , it also detected those produced by interactions with muons from cosmic rays , radioactive decays in the rocks surrounding the mine , internal radioactive contaminants in their tetracholorethylene fluid , and atmospheric argon decay production , but like any good experiment they controlled and subtracted all of these backgrounds . the full paper is actually available for free here and in section 6 of the paper ( nonsolar production of ${}^{37}$ar ) there is a full discussion of all of their background subtraction methods and calibration measurements . in particular , section 6.1 ( cosmic rays ) discusses how they subtracted the cosmic ray background from direct depth intensity measurements .
topological insulator , by definition , cannot exist in magnetic field . this is because the topological insulator is not topological . a topological insulator is a material with time reversal symmetry and particle number conservation . without time-reversal symmetry , topological insulators cannot exist , since they become the same as trivial band insulators . so a magnetic field destroys the topological insulator . true topological phases ( ie phases with non-trivial topologically orders ) are robust against any perturbations , including magnetic field .
i have an answer to establish expectations from first principles . i have not looked up real values , nor am i hopeful of finding such values . all i am doing here is setting a general expectation for the difference in cross sections of ion-ion fusion and ion-atom or atom-atom fusion . i should note that the one glaring piece of information missing from the question is the energy of the reaction . as such , my answer will be somewhat non-specific on that point , but my position is that it does not affect my overall conclusion that no major reaction rate benefit could be obtained in fusion machines by this proposed mechanism . i used a number of simplifications to craft a simple algebraic form for the difference in cross sections . i should first specify though , i used a " radius " for the cross section defined as $a=\sqrt{\sigma / \pi}$ . yes , it is true that cross-sections are not really areas , but for the relative scales here it can be treated as such . now , here are the assumptions i used : fusion cross section &lt ; &lt ; atomic radius physics are identical to ion-ion interaction after atomic radii meet only the impact of cross section broadening considered , the increase in energy due to atom charge interaction not considered target nuclei does not move , you could try to eliminate this with reduced mass or a adjustment to reaction energy , i will leave that for someone else to try . this assumption is not really necessary but i just did not see a quick way around it . the basic proposition we have is that the ion-ion cross sections of the reactions are already known . this , naturally , depends on the energy . my proposition is that , whatever the ion-ion reaction trajectory is , it will be different from the ion-atom interaction by the electrostatic force operating at distances greater than the atomic radius . i hope my method id already starting to seem clear , but i will use an illustration . the proposition is that we know $a_{++}$ we know the profile of the $y-y_{++}$ with simple electrostatic physics . using the assumption that the atomic radius is small compared to the fusion cross section radius , i can claim $\sin{ \theta} = a/s$ . the electrostatic force from the net charges on the atom/ion will operate along the line between the moving atom/ion and the target atom/ion . the distance between these can be approximated to be equal to $s$ . if we do not bother with the speedup from this force , then we will just consider the component inward . $$f_{in} = \sin{\theta} \frac{ k q_1 q_2 }{ s^2 } = \frac{ k e^2 z_1 z_2 a }{ s^3 } $$ the speed of the incoming atom/ion is roughly constant , so i can give this approximate expression to get the above math in terms of time . this applies from $-\infty$ to $-r/v$ . the $v$ follows from the energy of the reaction . $$s ( t ) = - v t$$ then the basic kinematics follow logically . $$v_{in} ( t ) = \int_{-\infty}^{t} \frac{f_{in} ( t' ) }{m} dt ' = \frac{ k e^2 z_1 z_2 a }{2 v^3 m t^3}$$ $$ y ( r ) - y_{++} ( r ) = \int_{-\infty}^{-r/v} v_{in} ( t ) dt $$ $$ a - a_{++} = v_{in} ( r ) \frac{r}{v} + ( y ( r ) - y_{++} ( r ) ) = \frac{ k e^2 z_1 z_2 /r }{m v^2} a$$ this gives the difference in cross section radius due to atom-ion or atom-atom interaction beyond that atomic radius . this is a particularly useful form if we introduce $e_c=k e^2 / r$ and the kinetic energy of the incoming atom/ion . the difference in cross sections will be about twice this difference in radii . $$\sigma - \sigma_{++} = \pi ( a^2 - a_{++}^2 ) \approx - 2 \pi a^2 z_1 z_2 \frac{e_c}{e_k}$$ $$\frac{ \sigma - \sigma_{++} }{ \sigma_{++} } \approx 2 z_1 z_2 \frac{e_c}{e_k}$$ $$e_c = 0.027 kev$$ this makes intuitive sense . the cross section will be affected by an amount proportional to the electrostatic energy of the two touching atoms and inversely proportional to the kinetic energy of the interaction . the interaction energy in iter will be about $8 kev$ , and the optimal energy for dt fusion is $80 kev$ ( previous graph ) . if you had two neutral atoms interacting , the cross sections will be greater than the ion-ion interaction by 0.6 % at iter energies . it would be an improvement of 0.06 % at optimal dt energies . the best you could ever hope for would be the pb interaction with the b fully ionized ( +5 ) and the hydrogen with ( -1 ) . even this is likely unattainable as some comments have pointed out . if we look at this at iter energies ( impossible but this is best-case ) , that would increase the cross section by about 5% . pretty much all other scenarios would have less of an improvement than this . the bottom line is that the coulomb attraction at distances beyond the atomic radius is very very small compared with the coulomb repulsion that the nuclei have to overcome as they get closer than that . any adjustments to the cross section from this atom-ion or atom-atom interaction will really be &lt ; 1% for most conceivable fusion reactions .
it would be helpful to include what the x/y axes represent -- the physics of the problem may suggest an answer . for a generic approach you could just ( explicitly ) construct $y'=\ln y$ , $x'=\ln x$ and then do polynomial fits to this data $x ' , y'$ . a functional form that might be worth considering is $y = c x^{a +b x}$ which could give you the curvature you are seeing in the log-log plot , but i would only go there if there was some physical motivation for the $x^{bx}$ behaviour .
in my view , the important question to answer here is a special case of the more general question given a space $m$ , what are the physically allowable wavefunctions for a particle moving on $m$ ? aside from issues of smoothness wavefunctions ( which can be tricky ; consider the dirac delta potential well on the real line for example ) , as far as i can tell there are precisely two other conditions that one needs to consider : does the wavefunction in question satisfy the desired boundary conditions ? is the wavefunction in question square integrable ? if a wavefunction satisfies these properties , then i would be inclined to assert that it is physically allowable . in your case where $m$ is the circle $s^1$ , the constant solution is smooth , satisfies the appropriate conditions to be a function on the circle ( periodicity ) , and is square integrable , so it is a physically allowed state . it also happens to be an eigenvector of the hamiltonian operator with zero eigenvalue ; there is nothing wrong with a state having zero energy .
i understand that with " wi-fi power " , you actually mean a " wireless power grid " , and not using you internet wi-fi router to power your cell-phone . if you want to use actual wi-fi devices to power anything , you are out of luck . the energy emmitted by a normal wi-fi device is in the order of 1 watt , top . and the available energy decreases with the square of the distance , so the amount of energy available to charge your mobile phone will be negligible . if you are talking about wireless power grids , that is another matter . it can be done , and experiments have been sucessful , sometimes . it usually involves sending a high energy beam of electromagnetic-radiation ( not electrons ) between parabolic antannaes . it could be used to bring energy from a space-solar-generator , but i would not install this on my house : imagine that somebody crosses the invisible beam !
in addition to what you have listed : use the buckingham pi theorem to create as many non-dimensional numbers as possible from combinations of dimensional numbers . this simplifies the expression but also allows you to reduce the number of variations on variables that need studied . non-dimensionalize all of your variables using suitable reference measures for the problem you are studying . use this to do an order-of-magnitude assessment of terms to decide if some terms are insignificant under certain conditions . perform a perturbation analysis . for example , if you have an equation for a wave , $\psi$ , substitute in $\psi = \overline{\psi}+\psi'$ where $\overline{\psi}$ is the average wave value ( in time , space or any of your other independent variables ) and $\psi'$ is a disturbance on that mean . then expand all terms and collect the means together and the perturbations together . do a lot of manipulation and you will get an expression for the mean behavior and the response to disturbances . this is not always " simpler " but gives tremendous insight . you could then substitute in simple functions , like trigonometric functions , for that disturbance and study how it grows or shrinks and under what conditions . learn what types of terms do what . which terms transport or convect your variable ? which terms produce or dissipate your variable ? these types of terms usually have typical forms in terms of derivatives . you can group terms together to come up with a simple conservation equation ( time change = transport + production - dissipation ) where all those terms can be lumped together . you can then attempt models for each individual group of terms . of course , all of these can be combined with one another to do all sorts of complex study .
you can totally transfer charge using protons . or using na+ or any kind of charged particle . it happens all the time - if you look at how a wet cell battery works , you will find that while charge across the wire is carried by electrons , current flows along the salt bridge via charged ions ( theoretically protons could be among these ) . i bet if you could somehow make a superfluid out of he+ or something you could use that to carry charge like a superconducting wire . i think our intuition that electrons are always the charge carriers comes from the fact that : 1 . ) electrons are very light compared to protons , so if you imagine putting a proton in an electric field and an electron in the same electric field , the electron will be accelerated 1000x more ( same charge , 1/1000th the mass ) . this means that when there is a charge imbalance and either proton or electron flow could alleviate it , the electrons will flow way before the protons are impelled to move . 2 . ) electrons are the mobile charged particles in a solid - all the protons are bound in nuclei . since almost all of our intuition about current flow comes from wires or other solid conductors , we are almost exclusively worrying about electron flow .
firstly the link you have provided for the paper is only accessible for those who have a subscription to the aps journal . so maybe instead you could just give a preview of the equation you are referring to . but i can give you a general overview on projections in quantum mechanics and bell states . i will use dirac 's bra-ket notation for the rest of this post : mathematically speaking , a projection of e.g. one state $\left|v\right&gt ; $ onto another state $\left|u\right&gt ; $ , is just written as follows : $$p_{u} := \left\langle u\right| v\rangle \left|u\right&gt ; $$ where $p_u$ stands for projection onto $\left|u\right&gt ; $ and not a probability . the $\left\langle u\right| v\rangle$ term is just the inner product between the two kets ( states ) in dirac 's notation . now in a physical sense , projection of states in quantum mechanics results from simple measurement of a system 's observables , e.g. a photon 's polarization , in a superposition of states $\left|\psi\right&gt ; = \alpha \left|h\right&gt ; + \beta \left|v\right&gt ; $ once measured , can collapse ( be projected ) onto one of its eigenstates , being horizontal or vertical polarization in this case . as for bell states , each bell state describes a unique maximally entangled state of two qubits ( photons e.g. ) . meaning all 4 bell states together : ( a and b to distinguish between the photons ) $$\left|\phi_{\pm}^{ab}\right&gt ; = \frac{1}{\sqrt{2}}\left ( \left|h^{a} , h^{b}\right&gt ; \pm \left|v^{a} , v^{b}\right&gt ; \right ) $$ $$\left|\psi_{\pm}^{ab}\right&gt ; = \frac{1}{\sqrt{2}}\left ( \left|h^{a} , v^{b}\right&gt ; \pm \left|v^{a} , h^{b}\right&gt ; \right ) $$ form a basis in the hilbert space $h_4^{ab}$ of 2-qubit entangled states . so as shown before , here one can also talk about the projection of two qubit states onto a given bell state , using the same formula . an example to showcase : if the bell states are said to form a complete basis for 2-qubit states , then for any given 2-qubit state , one should be able to decompose and express it in terms of superpositon of bell states . simply meaning that you project the 2-qubit state on each one of the bell states . for example lets see whether the state $\left|v\right&gt ; =\left|h^{a} , v^{b}\right&gt ; $ has a non-vanishing probability of collapsing ( being projected after measurement ) onto the first bell state : \begin{align*} \left\langle v\right| \phi_+^{ab}\rangle and = \frac{1}{\sqrt{2}} \left\langle v^{b} , h^{a}\right| \left ( \left|h^{a} , h^{b}\right&gt ; + \left|v^{a} , v^{b}\right&gt ; \right ) \\ and =\frac{1}{\sqrt{2}}\left ( \left\langle v^{b} , h^{a}\right| \left|h^{a} , h^{b}\right&gt ; + \left\langle v^{b} , h^{a}\right| \left|v^{a} , v^{b}\right&gt ; \right ) \\ and =\frac{1}{\sqrt{2}}\left ( \left\langle h^{a}\right| h^{a}\rangle \left\langle v^{b}\right| h^{b}\rangle + \left\langle h^{a}\right| v^{a}\rangle \left\langle v^{b}\right| v^{b}\rangle\right ) \\ and = 0 \end{align*} so we see that such projection is impossible , a result which is expected as $\left|h^{a} , v^{b}\right&gt ; $ is not an entangled state since it can be written down as a single outer product of the individual kets . i hope this overview gives you enough insight to be able to follow the mathematical steps involved in the paper you are reading .
if you slow down , it will take you longer to type a sentence , will not it ? when the earth slows down ( i.e. . the speed of rotation is decreasing ) , it will take longer to complete a rotation , so the day is longer ( i.e. . the length is increasing ) .
you have different questions in the title and in the body of your post and i will answer both . electronic excitation happens very fast compared to nuclear motion so the molecule remains " frozen " when it is brought from an equilibrium to an excited state . if the equilibrium geometry of the excited state is very similar to the ground state then the molecule will remain still , but if this is not the case , the molecule will start to oscillate around the equilibrium point . this is known as frank-condon principle and wikipedia article has nice figures to illustrate this point . exciting different electrons leads to different electronic states that have different equilibrium geometries so you cannot expect same vibrational distributions . in your title you also ask about relaxation of excited molecule . since the energy of the x-ray photon is much higher than the ionization energy , the excited molecule will be ionized before it can relax vibrationally .
the average of any quantity $s$ is $\frac{\sum\limits_{r=0}^ns_r}{n}$ . if the distribution is continuous , lets say as a function of x , then it becomes $\lim\limits_{n\to\infty}\frac{\sum\limits_{r=0}^ns_r}{n}$ . this can be rewritten as $\frac{\int s ( x ) dx}{\int dx}$ , taking limits as the length of the wire . in your formula , i do not see any $x$ term in the rhs , nor anything that could depend on x , so i do not see how we can proceed . please specify what is constant and what is a function of x . so the final formula is $$\frac{\int t ( x ) dx}{\int dx}$$ if your wire is infinite , you may need to take limits 0 to y , and then limit the expression for average as $y\to\infty$ . update : with the updated formula , assuming the wire spans from x=0 to x=l , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\tanh ( ml ) }{ml}-1\right ) $$ if the wire spans from 0 to y , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\sinh ( my ) }{my\cosh ( my ) }-1\right ) $$ . limiting y to infinity gives us an infinite answer . so i am assuming that i have interpreted it correctly in my previous answer .
to the last comment - i disagree . wind turbines operate in a fairly different flow regime , and compare the table fan to the prop of a ship , they are about the same shape . the key difference is moving fluid volume . the larger blades push against the fluid more strongly , which is desirable in the fan case and in the ship case . wind turbines are different as the builder cares nothing about the exhaust wind . anyway . . . 1 . i do not think it is ' functional ' in any sense other than the fact that the motor is behind it . 2 . i do not believe so and i agree with other commentators that the fan would be more simple and less noisy without them . i will withhold comment on the obvious safety considerations . 3 . everything dealing with pumping or pushing a fluid makes noise , and it is quite considerable . you could remove the blades and listen to the electric motor itself and you would find it constitutes only a small fraction of the noise . noise minimization for fans is a major engineering topic and it is more or less impossible to reduce it . obviously , the faster you get the bigger of a problem it is . i believe we are talking about turbulent flow in general , i do not think many pumps are actually laminar . the metal bars will have an influence , but i could not say how much .
the first issue i see : your speed is in miles/hour , and your time is in seconds .
dear humble , because non-gravitational yet interacting field theories such as qcd or the standard model exist and they do not predict a curved space , the answer to your question is clearly no , interactions do not imply that the spacetime has to be curved . however , the curved spacetime follows from many other assumptions - or combinations of assumptions - for example from the requirement that the gravitational force ( respecting the equivalence principle ) simultaneously exists with the relativistic lorentz invariance . the theorems you mentioned clearly assumed that the spacetime is allowed to become curved . so far , i assumed that you agree that the existence of interactions is a property of a theory , not a property of a configuration . but what about the possibility that you meant the " existence of interactions " to be a property of a state , or a configuration ? because the positive-energy theorem implies that the energy is strictly positive with the single exception of an empty minkowski space , it follows that if you also agree that the minkowski space " has no interactions in it " , then every state that has interactions " in it " has nonzero energy and consequently has to lead to a curved spacetime .
if you mean that qft operators are matrices $m_{ij}$ whose indices $i , j$ are really points in space ( or spacetime ) , so that the operator is really represented by the function $m ( x , x&#39 ; ) $ , then the answer is no . the actual matrices corresponding to qft operators are much much larger than that . an object expressed by $m ( x , x&#39 ; ) $ is pretty much equivalent to operators in ordinary quantum mechanics of a single particle : $$m ( x , x&#39 ; ) = \langle x| \hat{m} |x&#39 ; \rangle$$ the function $m ( x , x&#39 ; ) $ knows everything about the operator $\hat{m}$ because we have evaluated all matrix elements of this operator with respect to a basis - in this case , the position eigenvector basis . ( it is just space , not spacetime . ) however , quantum field theory has a much larger hilbert space . instead of the simple functions above , you may imagine that an operator is expressed by the following functional : $$m [ \phi ( x , y , z ) , \phi&#39 ; ( x , y , z ) ] = \langle \phi ( x , y , z ) |\hat{m}| \phi&#39 ; ( x , y , z ) \rangle$$ note that the object on the left hand side is a functional - it is a function whose two arguments are functions of 3 variables themselves - field configurations of a klein-gordon field , in this case . again , the left hand side knows everything about the operator $\hat{m}$ that acts on the hilbert space of the klein-gordon quantum field theory . the formula above chose a specific basis - a " truly " continuous basis of field configurations of $\phi ( x , y , z ) $ . however , this prescription may make the operators of qft look more complicated than they really are . any operator in any quantum mechanical theory may be specified by its matrix elements with respect to any basis . and there are usually more intuitive bases . in particular , in free quantum field theories , a simple-to-imagine basis is the fock space basis . $$|0\rangle , a^\dagger_{i}|0\rangle , a^\dagger_{i} a^\dagger_{j}|0\rangle , \dots$$ it is the vacuum and all of its excitations by an arbitrary number of creation operators that add particles into any state . in this way , the hilbert space is represented as an infinite-dimensional harmonic oscillator . the transformation " matrix " from the continuous basis of the field configurations to the fock space basis may be found by copying the same transformation for a normal harmonic oscillator infinitely many times and taking the tensor product . ( you do not have to mechanically repeat the same work infinitely many times , so this prescription may sound more intimidating than it is . ) if you meant that the quantum fields $\hat{f} ( x , y , z ) $ are operators and for each $x , y , z$ , then the answer is almost yes . they are operator distributions - that generalize operators much like the distributions such as the delta-function generalize the concept of a function . however , they are damn large matrices . they are matrices expressed with respect to a basis - for example one of the two bases above . but if this paragraph captures what you meant , then you were just asking whether operators are matrices . indeed , at least morally , they are . but this fact is true in any quantum mechanical theory and is not specific to qft . an operator contains a lot of numbers and may always be represented as a matrix . however , the values of the matrix elements always depend on the basis you chose - sometimes in ways that can not be recognized quickly . however , there is something physical about each operator that does not depend on any basis : you may " feel " an operator . you may learn many of its properties . some of them are more easily understood in one basis ; other properties naturally lead to another basis . so it is useful to think about operators in a more general way than matrices in a particular fixed basis - because the latter viewpoint usually discourages one from gaining the insights that are more obvious in another basis . one should not forget that the transition from one basis to another is just a " trivial " linear algebra that does not contain any special " physics " knowledge .
i think it is not quite true that modern physics tries to " avoid big bang singularities " . among many other things , modern physics tries to determine what is happening in the vicinity of the region that the big bang cosmology views as a singularity . by saying that the big bang singularity is completely avoided , you are already presupposing an answer . this possible answer may be a " working hypothesis " but it may also be incorrect . so it is not a " goal of modern physics " . classical general relativity breaks down - and becomes unpredictive - in the presence of singularities . but that does not mean that the full theory that also knows everything about the short-distance physics has to avoid the singularities completely . in string theory , the big bang singularity has not been understood yet - at least to the satisfaction of most string theorists . but many other singularities have been fully understood and it is not true that all of them disappeared . some of them did not disappear but the physics around them began well-defined , anyway . in particular , time-like singularities - such as orbifold singularities ; orientifold singularities ; and conifold points - have been pretty much fully understood . there are new degrees of freedom and new phenomena that take place in their vicinity but in some sense , physics understands them as well as it understands the smooth space today . from some viewpoint - e.g. according to some " probes " ( objects whose reactions we use to evaluate what is happening in the region ) - the singularity may get regulated , replaced by a regular manifold with some typical length scales . other probes see the singularity replaced by a non-commutative geometry that is also regulated . but some singularities according to some probes still look singular ; the best geometrical description is still a geometry that has a full-fledged singularity at the original point . however , it is not necessarily a problem . despite the singularity , physics at some ( not quite ) manifolds such as the conifold may be shown to be completely equivalent to physics at a completely smooth manifold ( e . g . by mirror symmetry ) . this equivalence shows that physics at singular manifolds may be well-defined and predictive . the space-like singularities - like the singularity inside the schwarzschild black hole and the big bang singularity - remain confusing . some people even think that all questions involving the interior of a black hole or the very beginning of the universe are inevitably ill-defined , at least to some extent , so there will never be a set of sharp observables that can be exactly calculated and discussed . i am personally not sure whether it is the case : it could be . but of course , the working hypotheses what is happening near the big bang - probably before inflation - also include some models with bounces ; cyclic universes ; non-commutative geometry ; a beginning of the universe from " zero " that looks totally smooth after the wick rotation to the euclidean space ( the hartle-hawking state ) ; but also an abrupt tunneling into another universe , and many other things . the collection of possibilities is pretty rich and some of them are more motivated than others . of course , physics still does not know for sure which of the tools are truly relevant for the birth of the universe .
a quantum mechanical model in which the magnetic translation operators are observables is a charged particle moving on a two dimensional torus in the background of a uniform magnetic field perpendicular to the torus surface . please , see for example the following article by e . onofri . the hamiltonian is the magnetic schrodinger operator and the ground state is the lowest landau level . the full solution shows that the degeneracy of the lowest landau level is equal to the magnetic flux through the torus surface area . therefore , the magnetic flux must be quantized . this is the dirac quantization condition . ( there are many other ways to prove this result without the need of the full solution because dirac quantization condition is a particular case of the index theorem ) . a basis of wave functions of the lowest landau level can be taken as the jacobi theta functions , $\theta_{\nu} ( z , \tau ) $ where $z=x+iy$ is the complex coordinate on the torus , $\tau$ is proportional to the ratio between the torus generators and $\nu$ takes integer values between $1$ and the magnetic charge $n$ . the main difference of the landau problem on the torus case from the plane is that the infinitesimal magnetic translation operators $\mathbf{p}-e\mathbf{a}$ are not observables because their action on the wave functions lies outside the lowest landau level . however , finite translations $e^{ ( \mathbf{p}-e\mathbf{a} ) . \mathbf{r}}$ are well defined if $e^{i|r|}$ is an $n$-th root of unity . this particular setting , however , cannot be easily implemented in lab at all , because it would require a net magnetic charge inside the torus and free magnetic charges have not been produced until now . however , this model can be translated into momentum space . here the torus is a brillouin zone of a $2d$ rectangular lattice . the restricted dynamics in one band can be described by an effective theory in which a berry connection term is added to the hamiltonian . thus , this problem becomes analogous to the motion on the torus , but in momentum space . here , in contrast to the real space , berry 's connections have nontrivial ( fictitious ) magnetic charges . physical observables in the real space have analogous observables in momentum space . the hall conductance is proportional to the magnetic charge of the berry curvature , thus the dirac quantization condition is responsible for the quantization of the hall conductance .
i originally had something about the constellations changing in the sky to show that the earth orbits the sun , but that would still be the case if the sun orbited the earth instead . now that i think about it , there is one thing that conclusively proves that the earth orbits the sun : parallax . over the course of one year many of the stars will move relative to each other . at the end of the year they will be back where they started . this is because the earth moves around in a 2au diameter circle , so that six months from your first observation , you will be standing 2au away from where you were then , and are viewing the stars from a ( slightly , but observably ) different angle . to show that the moon orbits the earth you could observe its location at the same time every night , and see that it moves , and is always nearly the same distance from earth . it never goes into a retrograde motion . assuming the earth is spherical , the only way this could be true is if the moon orbits the earth . you might also take the phases of the moon into account and model the sun-earth-moon system to explain it .
i think your problem is assuming that $a = b$ . your triangle looks isosceles , but it will not usually be . using your terminology : $\cos \alpha = \overrightarrow{ae} / \overrightarrow{ad}$ . if we let $\mathbf{f} = \overrightarrow{ad}$ , then $|\overrightarrow{ae}| = f \cos \alpha$ , and $|\overrightarrow{ae}|$ is the component of the force in the direction of movement . the reason you can not just fix $a=b$ is that it makes $\alpha = \frac{\pi}{4}$ automatically , while in reality it can be anything , and it will depend on the relationship between the force and the direcion of movement .
you have to first derive the time-dependent current $i ( t ) $ which runs through the wire . the capacitor is described by the relation $$q ( t ) = c v ( t ) \qquad ( 1 ) $$ with $c= \epsilon \pi l^2/d$ and $\epsilon$ the dielectric constant of the medium between the two plates . the wire is described by $$v ( t ) = r i ( t ) . \qquad ( 2 ) $$ charge conservation yields $$ \dot q ( t ) = i ( t ) . \qquad ( 3 ) $$ taking the time derivative of ( 1 ) and plugging in ( 3 ) and then ( 2 ) , we obtain $$ rc \ , \dot{i} ( t ) = i ( t ) $$ with the solution $$ i ( t ) = i_0 e^{-t/rc} , \qquad i_0 = \frac{q}{cr} = \frac{qd}{\epsilon\pi l^2 r} . $$ the electric field $e ( t ) $ in the capacitor is constant throughout the capacitor ( in the limit of $l/d \gg 1$ ) and points along the axis of the capacitor . is is given by $e ( t ) = v ( t ) /d =r i ( t ) /d$ . similarly , the magnetic field $b ( t ) $ is always perpendicular to the electric field and curls around the axis of the capacitor . its size is given by $b ( t ) = \mu_0 i ( t ) /2\pi r$ with $r$ the ( radial ) distance to the axis . together , the magnitude of the poynting vector is given by $$|p ( t ) | = e ( t ) b ( t ) /\mu_0 = \frac{r i ( t ) ^2}{2\pi r d} . $$
in axiomatic approaches to quantum field theory , the basic field operators are usually realized as operator-valued distributions . that is what wightman fields are : operator-valued distributions satisfying the wightman axioms . wightman functions are the correlation functions of wightman fields , nothing more . there is a nice theorem that says if you have a bunch of functions that look like they are the wightman functions of qft , then you can actually reconstruct the hilbert space and the algebra of wightman fields from it . neither of these concepts is anything new physically . they are just more precise ways of speaking about things physicists already know . ( thinking of fields as operator-valued distributions instead of operator-valued functions lets you make precise what goes wrong when you multiply two fields at the same point . ) you can talk about opes or scattering theory in this language , but it will not gain you anything , unless you are trying to publish in math journals . if you are interested in the wightman axioms , they are explained nicely in the first of kazhdan 's lectures in the ias qft year .
if you assume perfect efficiency , then the energy of dissociation of a liter of water is computed as follows : 1 liter of water , molar mass 18g , => 55.6 moles the energy needed is 237 kj per mole ( from your link - see under " thermodynamics" ) . 237 * 55.6 => 31.7 mj of energy for a liter of water . in terms of power , this is 3.67 kw for one hour . this shows you that the energy density of hydrogen ( in terms of energy stored per gram ) is very very high : it is why people have from time to time had such high hopes for a " hydrogen economy " which could leverage this energy density ( much more than the energy per gram of gasoline ) . but storing the stuff safely and cheaply is remarkably difficult . . .
this is likely to be an electronics se or signal processing se question . but i presume we have the plot bending back and asymptoting to the imaginary axis as $\omega\to 0$ and the plot approaches the origin as $\omega\to\infty$ right ? in which case , to get the intersection with the real axis as well as $\omega\to 0$ you have a polynomial in the denominator that is at least third order . i am assuming it will asymptote to the origin of the plane by becoming tangent to the positive imaginary axis . this means again that the system has three more poles than zeros , so it is going to be of the form : $$h ( s ) \propto \frac{1}{s\ , ( s^2 + 2 \zeta \omega_n s + \omega_n^2 ) }$$ assuming the plot crosses the imaginary axis at the point $0-0.5 i$ i.e. $\exists \omega_0 \ , \ni\ , h ( i\ , \omega_0 ) = 0-0.5 i$ how much do you have to scale up the above plot by , i.e. what is the real multiplying factor $k$ such that $h ( i , \omega_0 ) = -1\ , i$ i.e. the plot would be on the brink of encircling the critical point ?
the average height of a molecule is $h$ with your exponential distribution , because $\int_0^\infty dt\ , t\ , \exp ( -t ) =1$ , so its average potential energy is $mgh$ where $m$ is the mass of the molecule . how much does it compare with $3kt/2$ ( or $5kt/2$ etc . ) in the kinetic energy ? and what will happen to $\delta t$ ? the answer is given by the virial theorem ( so in advance , i am telling you that the reduction of $\delta t$ will be of the same order as $\delta t$ itself ) but let me use no pre-derived results . instead , just realize that the exponential decrease in $\exp ( -h/h ) $ is nothing else than the maxwell-boltzmann factor $\exp ( -mgh/kt ) $ which implies that $h=kt/mg$ and the average potential energy is $mgh=kt$ . ( at this stage , i am neglecting the dependence of the temperature itself on the height etc . ) so if you have a monoatomic gas which has $3kt/2$ in the kinetic energy , $kt=2kt/2$ is ultimately given to the potential energy which is $2/5$ of the total energy . as the temperature rises , the scale height will also have to rise , and you can see that $2/5$ of the energy that is initially pumped to the kinetic energy will be converted to the increased potential energy and only $3/5$ of the original increase of the kinetic energy will stay in the kinetic energy . so i think that for the monoatomic gas , the answer is $ ( 3/5 ) \delta t$ . similarly , it will be $ ( 5/7 ) \delta t$ if the kinetic energy of a single molecule is $5kt/2$ , and so on . just to be sure , this is not equivalent to the " global warming " issue because the greenhouse warming is a permanent change of the energy flows ( extra watts per square meter - joule in every single second ) while the energy needed to change the gravitational potential of the gas when its distribution at different altitudes changes is just a one-time event . however , the general observation that the ultimate increase of the temperature ( or anything else ) will be smaller than the most naively calculated one is valid : the extra heat is ultimately redistributed to and consumed by " many consumers " ( in this case , potential energy ) , which means that one particular consumer ultimately gets less . this is the reason why the feedbacks of stable systems tend to be negative - an insight known as the le chatelier 's principle ( in chemistry ) or homeostasis ( generally ) or lenz 's law ( in electromagnetism coupled to mechanics ) etc .
walter lewin 's lectures .
aeroplanes fly by thrusting air downwards and by thus being borne up by the newton 's-third-law begotten upwards reaction force of the downthrusted air on the aeroplane . there are many excellent answers to the physics se question " what really allows airplanes to fly ? " that you should read . but basically the simplest estimates arise from calculating the ram pressure thrust upwards on the aeroplane given the above principle . the variables you need to know are density of the air at the height , the relative speed of the aeroplane to the air , the angle of attack that the wing makes with the velocity vector of the air relative to a frame comoving with the aeroplane and the scale factor that yields the effective surface area of the wing - which at subsonic speeds is considerably larger than the wing itself because the disturbance to the fluid flow pattern that arises from the wing is felt over a region that is considerably bigger than the wing . the last variable - effective area - can also be expressed as the wing 's coefficient of lift . to illustrate these points , we can do a back of the envelope estimation of ram pressure in this case : see my drawing below of a simple aerofoil with significant angle of attack being held stationary in a wind tunnel . this is the kind of analysis you should do to get an idea of your specific situation . your air density is going to be rather less than that for the following calculation ( commercial jetliners reach their top speed at heights of about 8000m ) : lets suppose the airflow is deflected through some angle $\theta$ radians to model an aeroplane 's attitude ( not altitude ! ) on its last approach to landing or as it takes off , flying at $300\mathrm{km\ , h^{-1}}$ airspeed or roughly $80\mathrm{m\ , s^{-1}}$ . i have drawn it with a steep angle of attack . air near sea-level atmospheric pressure has a density of about $1.25\mathrm{kg\ , m^{-3}}$ ( molar volume of $0.0224\mathrm{m^{-3}} ) $ . the change in momentum diagram is shown , whence the change in vertical and horizontal momentum components are ( assuming the speed of flow stays roughly constant ) : $$\delta p_v = p_b \sin\theta ; \quad\quad\delta p_h = p_b \ , ( 1-\cos\theta ) $$ at the same time , the deflecting wing presents an effective blocking area to the fluid of $\alpha\ , a\ , \sin\theta$ where $a$ is the wing 's actual area and $\alpha$ the scale factor to account for the fact that in the steady state not only fluid right next to the wing is distrubed so that the wing 's effective area will be bigger than its actual area . therefore , the mass of air deflected each second is $\rho\ , \alpha\ , a\ , v\ , \sin\theta$ and the lift $l$ and drag $d$ ( which force the engines must afford on takeoff ) must be : $$l = \rho\ , \alpha\ , a\ , v^2\ , ( \sin\theta ) ^2 ; \quad\quad d = \rho\ , \alpha\ , a\ , v^2\ , ( 1-\cos\theta ) \ , \sin\theta$$ if we plug in an angle of attack of 30 degrees , assume $\alpha = 1$ and use $a = 1000\mathrm{m^3}$ ( roughly the figure for an airbus a380 wing area ) , we get a lifting force $l$ for $\rho = 1.25\mathrm{kg\ , m^{-3}}$ and $v = 80\mathrm{m\ , s^{-1}}$ of 200 tonne weight . this is rather less than the takeoff weight of a fully laden a380 airbus ( which is 592 tonnes , according to the a380 wikipedia page ) but it is an astonishingly high weight just the same and within the right order of magnitude . we see that the wing 's effective vertical cross section is bigger than the actual wing by a factor of 2 to 3 . this is not surprising at steady state , well below speed of sound flow : the fluid bunches up and the disturbance is much bigger than just around the wing 's neighbourhood . so , plugging in an $\alpha = 3$ ( given the experimental fact that the a380 can lift off at 592 tonnes gross laden weight ) , we get a drag $d$ of 54 tonne weight ( 538kn ) - about half of the airbus 's full thrust of 1.2mn , so this ties in well with the airbus 's actual specifications , given there must be a comfortable margin to lift the aeroplane out of difficulty when needed .
the result can be proved in a general way using , well , math . in particular the theory of semigroups of linear operators on banach spaces ( i know that seems advanced and maybe not physical , but it is an elegant way of proving the result you seek ; - ) ) . define the banach space $\mathscr{l}^1 ( \mathscr{h} ) $ of trace class operators over a separable hilbert space the set of all bounded operators $u$ such that $\mathrm{tr}\lvert u\rvert&lt ; \infty$ . the fact that ( 3 ) holds for all observables ( i will not discuss about domains here for the sake of simplicity ) implies that it holds also for trace class operators that does not depend explicitly on time . in this case ( 3 ) is equivalent to the following cauchy problem on the banach space $\mathscr{l}^1 ( \mathscr{h} ) $: $$\frac{du ( t ) }{dt}=l u ( t ) \ ; , \ ; u ( 0 ) =x$$ where $\mathscr{l}^1 ( \mathscr{h} ) \ni x\equiv \hat{a}_0$ that does not depend on time , $u ( t ) \equiv \hat{a}_h ( t ) $ and $l$ is the linear operator that acts as $i [ \hat{h} , \cdot ] $ ( i am assuming $\hslash=1$ ) . if $x\in d ( l ) $ , i.e. $x$ such that $\mathrm{tr}\lvert [ \hat{h} , x ] \rvert &lt ; \infty$ , the solution of the cauchy problem above is unique , because $h$ is self-adjoint . we know by stone 's theorem an explicit solution , namely $$u ( t ) =\hat{u} ( t ) x \hat{u}^\dagger ( t ) \ ; , $$ where $\hat{u} ( t ) =e^{-it\hat{h}}$ is the unitary group generated by $\hat{h}$ , that satisfies ( 1 ) on $d ( \hat{h} ) $ . that solution is unique , so assuming ( 3 ) for all observables implies that the operator $\hat{u}$ you used to define heisenberg picture operators has to be exactly the group generated by $\hat{h}$ , i.e. satisfy ( 1 ) . just for the sake of completeness : once you have solved the cauchy problem for $x\in d ( l ) $ , you can extend the solution to trace class operators or compact operators or bounded operators ; also to unbounded operators ( provided $\hat{u} ( t ) x \hat{u}^\dagger ( t ) $ makes sense on some dense domain ) . this is what is called a mild solution of the cauchy problem , because we do not know a priori if we are allowed to take the derivative . however uniqueness is usually proved under general assumptions and for mild or even weak solutions , so i think it is quite safe to conclude that $\hat{u} ( t ) x \hat{u}^\dagger ( t ) $ is the unique solution of ( 3 ) in a suitable sense .
why do you want to renormalize a theory that gives uv finite answers ?
one of the standard texts for this kind of thing ( the quantum mechanics of lasers , without all the technical details you had need to know to design a real one ) is loudon 's quantum theory of light . i have got the 2nd edition , i think he is up to the 3rd . in my edition he does the " pre-quantum " explanation of lasers ( i.e. . in terms of einstein a and b coefficients ) in about 40 pages . over the next 200 pages or so he does the fully quantum treatment of light ( quantization of the field , creation and annihilation operators , etc ) and then revisits the laser . the first 40 pages should cover the " clearly written description " criteria , and - as far as it goes - it is relatively rigorous as well .
let 's assume a typical fermionic mass-term ( interacting leptons and quarks are spin 1/2-particles ) : $$ \tag 1 \bar{\psi}\psi = \bar{\psi}\left ( \frac{1 + \gamma_{5}}{2} + \frac{1 - \gamma_{5}}{2}\right ) \psi = \left| \bar{\psi}\left ( 1 \pm \gamma_{5} \right ) = \left ( ( 1 \mp \gamma_{5} ) \psi\right ) ^{\dagger}\gamma_{0} \right| = $$ $$ =\bar{\psi}_{l}\psi_{r} + \bar{\psi}_{r}\psi_{l} . $$ then let 's assume $su ( 2 ) \otimes u ( 1 ) $ gauge-invariant , realistic theory ( the electroweak part of the sm ) . according to this theory , the left representation $\psi_{l}$ transforms as the doublet part under the gauge transformations , while $\psi_{r}$ transforms as the singlet . so of course , the mass term is not gauge invariant . but if we assume only $u ( 1 ) $ gauge theory , there is not doublets , so the mass term is indeed gauge invariant ( except majorana case , when $\psi = \hat{c} \psi$ , where $\hat{c}$ refers to the charge conjugation ) . this is the reason why we must include ( gauge-invariant ) interaction of the yukawa-type with scalar doublets . for example , i will illustrate my statement by describing the mechanism of appearance of mass of charged leptons into the standard model . we " replace " the mass term $ ( 1 ) $ by $$ l_{int} = -g\bar{\phi}_{l}\varphi \psi_{r} + h.c. $$ here $\varphi = \begin{pmatrix} \varphi_{1} and \varphi_{2} \end{pmatrix}^{t}$ refers to the doublet of the scalar complex field , and $\phi_{l} = \begin{pmatrix} \nu_{l} and \psi_{l}\end{pmatrix}^{t}$ . after using unitary gauge ( \varphi \to $\begin{pmatrix} 0 and \sigma \end{pmatrix}^{t}$ ) and shifting the vacuum ( $\sigma \to \sigma + \eta$ ) we will give the mass term and interaction with higgs boson : $$ l_{\int} = -g\eta ( \bar{\psi}_{l}\psi_{r} + h.c. ) - g\sigma ( \bar{\psi}_{l}\psi_{r} + h.c. ) . $$ so we have the gauge-invariant mass term . but the payment for this is the appearance of yukawa-interaction with massive real scalar field .
john rennie 's answer is good , but i will try to explain intuitively why the symmetry breaking leaves some symmetry unbroken . start with a sphere . you can rotate a sphere in three independent ways—around the x axis , around the y axis , and around the z axis , if you like . all of these are symmetries of the sphere , i.e. , they leave the sphere unchanged . these rotations are called $su ( 2 ) $ [ almost—there is a technicality that i will ignore ] , and saying that an $su ( 2 ) $ gauge theory has three gauge bosons ( which it does ) is the same as saying that a sphere can be rotated in three independent ways . draw a dot somewhere on the sphere . all three of the above rotations will probably move the dot , which means they are not symmetries of the sphere plus dot . but there is still a rotational direction that leaves the dot in place , and there was never any reason to prefer those other axes that do not . so forget about those axes and instead pick the axis of symmetry ( an axis going through the dot and its antipodal point ) , and two other axes perpendicular to that one and each other . the axis of symmetry is the " photon " , and i suppose you could call the other two the w + and w − . this analogy is inaccurate in that the real electroweak gauge group is $su ( 2 ) \times u ( 1 ) $ , not $su ( 2 ) $ , and has four bosons , not three . but a sphere is much easier to visualize , and the basic principle is the same . before the symmetry is broken ( by drawing the dot ) , you can not be sure that any particular boson ( rotational axis ) will remain a symmetry ( remain massless ) , but there will always be some combination of those bosons ( some other axis ) that remains a symmetry , and we call that the photon .
the phenomenon of parallax itself is simply the result of different views of one 's surroundings observed from different locations . for binocular vision , we get a different view of our environment from each eye simultaneously , and the visual cortex of our brains learns very early on how to process the two distinct images into an appearance of a three-dimensional image . ( actual physical interaction with the objects in the environment is also important in " educating " the visual cortex about distances to objects . we generally accomplish this during infancy . ) in the case of astronomical parallax , celestial objects are generally so distant that the two ( or more ) different views must be obtained at different times from different positions of the earth on its orbit . ( the exception is the moon , which is close enough to show parallax in simultaneous observations from well-separated points on the earth . as an example , this is why occultations of stars by the moon are not seen by all observers on the otherwise proper side of the earth . ) to note the presence of astronomical parallax , it is necessary to have very precise measurements of the direction in which the object is seen , relative to a much more distant " background " . the fact that this is not observed with the naked-eye was used until the 18th century as an argument against the motion of the earth about the sun , since ( obviously ) the planets and stars would show parallax if the earth were not simply stationary at the center of the universe . one difficulty in measuring parallax for stars is that , as we now know , the change in observed direction , even at diametrically opposite locations on the earth 's orbit , is at most a bit over one second of arc ( 1/3600 of a degree ) for the closest stars and generally hundredths of a second or less for most of the visible stars . but even to discern that requires being able to compare the observed directions of the stars at different times . in the age before photography , this required having precise charting of the stars on the sky , which was the result of a tremendous effort in the centuries after the first use of telescopes in astronomy . it was not until 1838 that the first sufficiently accurate observations of stellar parallax were accomplished . by comparison , the aberration of starlight due to the earth 's motion is an effect about twenty times larger than the largest stellar parallaxes , so it was became possible to detect that by 1725 .
i ) op is using the period formula $$\tag{1} t~=~2\pi\sqrt{\frac{i}{mgr}} $$ for a compound/physical pendulum ( in the small amplitude limit ) to estimate the gravitational acceleration constant $$\tag{2} g~=~\left ( \frac{2\pi}{t}\right ) ^2 \frac{i}{mr} . $$ here $i$ is the moment of inertia around the pivot point ; $r$ is the distance from cm to the pivot point ; and $m$ is the total mass . ii ) after doing the experiment op finds values for $g$ that are 3-5% too big . ( these results are close enough that op likely did not make any elementary mistakes with units . ) a finite amplitude of $$\tag{3} \theta_0 ~\approx ~25^{\circ}~\approx~ . 44~ {\rm rad}$$ makes the pendulum $$\tag{4} \frac{\theta_0^2}{8}~\approx~ 2\%$$ slower , as compared to the ideal pendulum ( 1 ) , cf . comment by prahar . so correcting for a finite amplitude makes op 's estimates worse , 5-7% too big , as keith thompson points out in a comment above . so the discrepancy is caused by something else . the culprit is likely that it is difficult to get a precise estimate for the moment of inertia $i$ . all the other quantities $t$ , $m$ and $r$ should be fairly easy to measure reliable . so op 's value for $i$ is likely too big . according to steiner 's theorem $$\tag{5} i~=~mr^2+i_0 , $$ where $i_0$ is the moment of inertia around the cm ( and the actual quantity which is poorly known ) . iii ) below follows a suggestion . plot op 's seven data points in an $ ( x , y ) $ diagram with axes $$\tag{6} x~:=~r^2 \quad\text{and}\quad y~:=~r\left ( \frac{t}{2\pi}\right ) ^2 . $$ theoretically , the $ ( x , y ) $ data points should then lie on a straight line $$\tag{7} y~=~ax+b$$ with slope $$\tag{8} a~=~\frac{1}{g}$$ and $y$-intercept $$\tag{9} b=~~\frac{i_0}{gm} . $$ in other words , find the best fitting straight line . this method should hopefully produce a good estimate for $g$ without having to know $i_0$ a priori . ( by the way , notice that we in principle also do not need to know the mass $m$ , cf . the equivalence principle ! ) iv ) finally , as always in experiments , estimate all pertinent uncertainties in the various measurements .
the potential in the schroedinger equation must be single-valued and almost everywhere defined , so to have your problem weel-defined you must first decide which of several solutions is to be taken . apart from that , one does not need an explicit potential . if one calculates solutions numerically it is enough that one can compute the potential numerically at any given $x$ .
the usual procedure involves careful preparation for ultra high vacuum do you reach a good high vacuum ? can all the equipment withstand high temperatures over a long period of time ? if not you might to cool those parts while heating the rest of the chamber . try to remove all materials that might have a high outgassing rate , otherwise ultra high vacuum is never achievable . check for residual gas volumes , e.g. from screw threads . prepare and bake wrap heat tape around metallic parts of vessel , try to get good contact and not much overlap . the tape should not reach very high temperatures when used alone . pump , heat and monitor gas pressure . it should rise a bit , exact numbers depend on the circumstances . do a smell test , if it smells like burnt plastic you forgot something . heat for at least 24 hours , even at elevated temperatures this might take a long time . the longest time a colleague used was 2 weeks . if heating is not going to work alternatively use a sorption pump with either liquid nitrogen or liquid helium .
this comes from the pioneering work of amateur physicist , michael jackson : you can not win , you can not break even , and you can not get out of the game . people keep sayin ' , things are gonna change . but they just look like they are stayin ' the same . -michael jackson , " you can not win " okay , okay . i think this is one of those jokes that starts between graduate students or something by the water cooler , so it does not have an " exact " form . wikiquotes gives the original author as c.p. snow in the following form : zeroth : " you must play the game . " first : " you can not win . " second : " you can not break even . " third : " you can not quit the game . " but that seems somewhat dubious . no one can provide a cite of c.p. snow actually saying or writing it anywhere ; i suspect that this history is a little mythical and that the joke has no real precise statement .
i think the problem is a general tensor analysis one rather semiconductor physics ( gaas ) , since no particular values of any d-symbol is of any importance ? anyway , here is my effort to produce an answer . the piezoelectric coefficient $d_{ijk}=d^{ [ 110 ] }_{ijk}$ is a third rank cartesian tensor which transforms from the reference frame $ [ x_1 , x_2 , x_3 ] $ to $ [ \bar {x}_1 , \bar {x}_2 , \bar {x}_3 ] $ giving $\bar {d}_{pqr}= d^{ [ \bar110 ] }_{pqr}$ according to the general rule : ${d}^{110}_{ijk} =\sigma \bar {d}^{\bar {1}10}_{pqr}a_{pi}a_{qj}a_{prk} $ where $a_{\lambda\mu}$ are the cosines between coordinates in the two frames , and the summation is over repeated indices . in your particular coordinate transformation $ [ - x , y , z ] $ to $ [ x , y , z ] $ which transforms $\bar {d}_{pqr}$ to $d_{ijk}$ you need to bear in mind that $a_{pi}=\delta_{pi}$ etc so you get the general relationship with the $\delta$ symbols multiplying each other in each term of the summation . they are not exponents ( powers ) to base ( -1 ) . the expression you have written does not seem to give the correct relationship between $d^{ [ 110 ] }_{ijk}$ and $d_{pqr}=d^{ [ \bar {1}10 ] }_{pqr}$ . i hope this helps .
maybe you can be a bit more clear in your formulation . unless the soda can changes into something else , do not use the word ' initially ' . are cup and bowl the same in your experiment ? in small scale experiments like this ( unlike the earths atmosphere ) air does not really fall anywhere , and the pressure will be the same everywhere inside the can giving a net force of zero . you do not need it here , but remember that the force from a pressure is calculated by multiplying with the area . f=p*a
the last integral is a surface integral . it is the fluid ( current ) flux crossing the volume surface integrated over the whole surface . it is how much charge is leaving the volume per second . ( $\partial v$ means boundary of v and you sum all local $j_\bot ds$ . )
ok , i will start . definition and list ( filled with mistakes ) . please , comment/add/edit/etc . definition ${lim}_{x\to\infty} p ( x ) = p ( x+t ) , $ for $\forall\ ; t &gt ; 0$ $p ( x ) $ is finite i.e. $p ( x ) = \int p ( x ) dx = $ const . clarifying definitions $p ( x ) :=$ " probability density function of measurable $x \in \re_+$" $p ( x ) :=$ " cumulative distribution function " = $\int_{0}^x p ( x ) \ ; dx$ $a &gt ; 0$ $0 &lt ; f ( x ) &lt ; const $ $g ( x ) \ne 0$ long tailed all exponential distributions : $p = f ( x ) exp ( -ax ) $ all power law distributions : $p = x^{-\alpha}$ , where $\alpha &lt ; -1$ all " stretched exponentials": $p = f ( x ) exp ( -ax^{g ( x ) } ) $ non long tailed uniform distribution : p ( x ) = const all " increasing or equal " distributions : $|p ( x ) | \le |p ( x+a ) | $ for $\forall a$ power law distributions with $p ( x ) = x^\alpha$ , where $-1 \le \alpha \le 0$ ( integral not finite )
i believe that evaporative ionization is the opposite effect of what you are looking for , per the general discussion on ion processes in evaporated droplets . this constrains the range of charge-transfer in droplets to the triboelectric effect , as you mentioned . the kelvin generator , including discussion about charge formation that exceed models , may be more than triboelectric , due to ground effects - which is to say that charge may come from other sources that the system of droplet and atmosphere . the only way i could see a kelvin generator involve electrospray ionization is through corrosion or dissolving of the conductive cups that collect the water . the amsci group suggests that the charge arrives from slowing the water droplets ' fall . their model could align with the idea that ion-transfer from droplet to system occurs via coulomb fission .
you are correct : there is no free charge so $\vec{d}=0$ which means $$ \vec{e}=-\frac{1}{\epsilon_0}\vec{p}=-\frac{k}{\epsilon_0r}\hat{r} $$ but this is for $r_1\leq r\leq r_2$ . inside the shell , $r&lt ; r_1$ , there are no enclosed charges , so $\vec{e}=0$ there . outside the shell , there is also no charge . recall that the total charge for dielectrics can be expressed as $$ q_{tot}=\oint_\mathcal{s}\sigma_b da-\int_\mathcal{v}\rho_b dv = \oint_\mathcal{s}\vec{p}\cdot d\vec{a}-\int_\mathcal{v}\vec{\nabla}\cdot\vec{p} dv $$ where $\rho_b$ and $\sigma_b$ are the volume and surface charge densities of the bound charges . from the divergence theorem , the two terms involving $\vec{p}$ are identical , so $q_{tot}=0$ . thus , your answer should be $$ \vec{e}\left ( r\right ) =\begin{cases}-\frac{k}{\epsilon_0r}\hat{r} and r_1\leq r\leq r_2 \\ 0 and {\rm otherwise}\end{cases} $$
dodo , i am not really sure why you would want to do this . i do not think it will work when unrolled . this is also dangerous and not advisable . if you will go through with it anyway , wear gloves and do it in a well ventilated area . if you remove the case and unroll it , it is just a flat piece of foil . the foil does not make the capacitor as much as it is the foil and the geometry . if the anode foil and cathode foil were still ordered properly after removal , it would still work obviously , but i do not think it would work to the manufactories specifications . if the rod were in direct contact with the layer you put on the rod , yes it would work . to what end you would want that for , i can only guess . cheers .
it is the result of a dependence of the pressure with growing depth , due to the gravitational field ( i.e. . the weight of the water ) . you may do an easy calculation with some simple geometrical form , e.g. a cylinder totally submerged in water , to quickly understand how it works . the force due to pressure in each surface element of the curved wall of the cylinder is proportional to the depth of that element , and has the normal direction to the wall , i.e. towards the axis of the cylinder . after an easy integration in polar coordinates , you can see that the resultant force points upwards . that is because the forces in the upper parts are smaller that the ones near the more deeply submerged part of the cylinder . a surprising conclusion is that a golf ball submerged in a tank of water in the space station , would not go upwards . . . or that the bubbles in a coke in the hands of an astronaut remain where they origin . . . i would love to see that .
i can not find any tabulation . you can do a sellmeier equation fit from visible data and extrapolate it to near-ir . but , the visible index is determined by the uv absorptions , so the extrapolation will only be reliable as long as you are much closer to the uv absorption bands than to the ir absorption bands . since these solvents all have pretty strong high-frequency ir bands ( ch stretch and oh stretch around 3um ) , you probably should not extrapolate past 1um or so . ( that is just a guess . ) i can not really say any more unless i know how accurate you need ( what application ) and what wavelength range you are interested in . you can figure everything out with an ftir , but it can be a bit complicated . addendum : if you calculate the index of refraction in the range 850nm-2.5um by extrapolating from the visible , you will overestimate it . the longer the wavelength , the more severe the overestimation . that is because the effect of a resonance , e.g. the ch stretch resonance , is to increase the refractive index for wavelengths longer than the resonance wavelength , and decrease the refractive index for wavelengths shorter than the resonance wavelength . since your extrapolation from the visible would ignore the ir resonances , it would be an overestimate .
a vibrating shoelace is a poor analogy for a sound wave , because that would be a transverse wave whereas sound is a longitudinal wave . what this means is your example of molecules hitting each other is perfect . a longitudinal wave is described by the variation of the density of the particles . the wikipedia page has some nice animations to help visualize what that means . the wavelength in this context is the distance between two periodic fluctuations in the density of the material . so in the case of the question you linked to , because the sound emitted has a massive wavelength ( and consequently an extremely low frequency ) that means the distance between the density fluctuations are large . so the particle literally moves and collides with another particle and so on , propagating the wave .
the molecules $o_2$ and $n_2$ are symmetric and have no dipole momentum . that is why they can not interact with emw ( at least within dipole approximation ) . one can say that transitions between the oscillator levels in these molecules are forbidden by symmetry in electrodipole approximation . the molecule $co$ consists of two different atoms . the average positions of positive and negative charges are not the same . this molecule is polar .
the ball is deformed while bouncing off . in theory , this can be modelled as an entirely elastic process as a relatively good approximation , however , it actually is not , as some energy is lost in the process and radiated away as heat ( try deforming a ball a few hundred times , it will heat up ) . the process is therefore not entirely elastic , which reduces the kinetic energy of the ball . additionally , a number of other forces affect the ball , listing those mentioned above again for completenes and ordered roughly by the magnitude of the effect : energy lost due to inelasticity of the ball-earth interaction ( ball heats up ) friction of the ball with the air , causing it to slow down friction of the ball with the ground ( "stuck to the ground" ) roughness of ground causing the ball to start spinning or change direction forces stemming from the fact that the earth rotates , although this should mostly affect horizontal velocity momentum transferred to earth
no , there is no need for screens in the movie theaters to be mirrors i.e. specular reflectors . quite on the contrary , it is completely necessary for them not to be mirrors i.e. to be diffuse reflectors . if the screen were a specular reflector , the light would return back into the direction of the projector and would never reach the eyes of the viewers who are not sitting on the line in which the projector is directed . if the screen were a mirror , the viewers would only see themselves and the projector but could not see any magnified versions of the objects that are supposed to be in the movie . in reality , each point of the screen – which is a diffuse reflector – effectively becomes a source of light whose intensity depends on the amount of incident light at this point and this source is located directly in the plane of the screen . so these sources of light are not images ( in the sense of real or virtual images of mirrors or lens ) at all . more precisely , the only image of the real " object " – the object on the screen – is formed in the viewers ' eyes . it is important for the projector to sharply illuminate each point of the screen differently , by the correct intensity of light of the right color . this requires precise optics that chooses the right directions of light rays for each point of the movie between the projector and the screen . on the other hand , each point of the screen is a diffuse reflector and much like real objects in the real world , it emits light to all directions so that all viewers may see it , regardless of the location of their chair .
indeed , the slater-type orbitals ( radial wave functions ) are not orthonormal – they are not even orthogonal to each other . the $\delta_{n , n'}$ kronecker delta symbol does not appear in the inner product and it can not because the $r$-dependent integrand is positively definite and there is no room for cancellation . their not being orthogonal physically means that the orbitals for different $n$ are not mutually exclusive . for a given molecule , one is supposed to use one value of $n$ only .
well the real question should be why is there a °c ( celsius ) . the celsius scale is a " centigrade " scale in that it uniformly divides the temperature range between the boiling point of water , and the freezing point of water into 100 equal parts , and then it arbitrarily calls the freezing point zero °c , and the boiling point becomes 100°c . the kelvin scale is referenced to the triple point of water , not the freezing point , and that temperature is about 0.1°c ( it might be 0.098°c but i am not sure about that ) . quite arbitrarily , it was decided that degrees on the kelvin scale , should be identical in size to celsius degrees , and experimentally the zero on the kelvin scale ( zero kelvins ) is 273.16 celsius degrees below the triple point of water , which makes it also -273.15°c
your intuition is correct : the top loop inductor prevents its current from changing instantaneously , so it starts at zero . qualitatively , the top loop current : starts at zero ( here the inductors block current flow in both loops . ) rises to a max as the top loop uncoupled inductor allows current to flow through it while its source voltage ( the coupled inductor voltage ) simultaneously falls as the bottom loop current rises . decays to zero as the coupled inductor voltage falls to zero . an equivalent circuit model does not contain all 3 inductors in series , but rather two parallel branches ( corresponding to the primary and secondary of the coupled inductor ) , with the " secondary " branch ( corresponding to the top loop ) containing an inductance which blocks initial current flow .
it wont be unboundedly fast e.g. due to the viscosity of the water . in addition , i would like to point out that you will not have the cylinder of water . water will accelerate with gravity as it falls . the density of water is constant . the total flow of the water at different height is constant as well . so the area of the water 's cross-section will decline . for thin stream of water from a kitchen tap , surface tension force is enough to keep the water flowing in one stream , but the thickness of the scream decreases , take a look here . however , as you increase the initial radius of your stream , the water mass ( ~r^2 ) grows while the pressure gradient created by surface tension forces ( ~1/r ) declines . that is why the cylinder will pretty quickly separates into multiple streams , and eventually into droplets . .
since you mention time and x , y , z as dimensions in space , i think you mean the geometrical dimensions and not the unit dimensions . so i answer this question with the above in mind . yes quantity ( with or without units ) can be a dimension in some space . since space is sth related to a definition ( and the associated dimensions of this space ) . one can have for example a dimension of color or a dimension of number of particles ( a unitless quantity ) , in some representation of a physical system . indeed in hamiltonian mechanics the space of the physical system under study ( phase-space ) can have many dimensions ( e . g 12 ) . also popular theories beyond the standard model ( eg string theory ) do in fact assume 11 dimensions . what would be the physical ( or maybe realist ) meaning assigned to these dimensions is another matter .
a few points : power lines are the highest objects in the countryside , they are made with giant grounded metal pylons . lightning wants to go the way of least resistance , so they are nice targets . they ionize the air , this is a strong effect for 220kv+ lines ( they even glow purple in a very dark night , the glow comes from something called corona discharges ) . ionized air is a better conductor than ordinary air . the volage difference between a power line and a storm cloud can be significantly lower , than the difference to the ground . the difference for a 220kv line can for example be ( when the phase angle is right ) the full voltage of $220kv\sqrt{2}$ ( the root comes from effective value vs . amplitude ) . the ground is at $0v$ , as the reference . the lighning has therefore a lower voltage difference it must overwhelm . lightning can kill you even if you are not struck by it . imagine a shockwave where the voltage is spread out around the place of the strike , falling off with distance . so for example if you have one meter difference between your legs and they are in a line with the stikepoint , you have a voltage across your legs because one point is farther away and more voltage was disspated in that point than at the nearer point . one more thing . it is the em influence . a charged cloud attracts a charge of the opposite sign on the ground , this builds up over a large area . when the final discharge occurs the charge that was bound at this location will not have anything holding it so it will move back to where it came from creating currents on the way . this can damage power lines and electrical equipement . i hope that i did not miss anything .
the $\frac{e^2}{4\pi\epsilon_0}\frac{1}{r_0}$ term appears in the potential for the electron motion , as luboš and vijay point out , to keep the whole energy accounting in place so that the nuclear motion can be properly quantized . the key point is that this potential does not involve the electron coordinates , so that as far as the electronic wavefunction is concerned it acts like a constant and therefore does not affect the solution to the electronic eigenvalue equation . if you do include that term , and write the potential energy of the molecule as $$e_p=-\frac{e^2}{4\pi\epsilon_0}\left ( \frac{1}{r_1}+\frac{1}{r_2}-\frac{1}{r_0}\right ) $$ then you can write the potential for the nuclear coordinates directly as $$e_n=\langle\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) |e_p|\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) \rangle$$ where the total wavefunction is split into electronic and nuclear parts as $$\langle \mathbf{r} , \mathbf{r}_1 , \mathbf{r}_2|\psi\rangle=\langle\mathbf{r}|\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) \rangle\langle\mathbf{r}_1 , \mathbf{r}_2|\phi\rangle . $$ the schrödinger equation for the nuclear coordinates is then $$\left ( \frac{\mathbf{p}_1^2}{2m}+\frac{\mathbf{p}_2^2}{2m}+e_n\right ) |\phi\rangle=e|\phi\rangle . $$
the equation $w_\text{net , ext}=k_f-k_i$ is only correct when the only form of energy being transformed is kinetic . if you have other forms of energy that change value , this equation will not work . if you want to look at the system of the two colliding objects , you are correct that $w=0$ ( though for a slightly different reason than what you stated ; the net force is zero , but this does not mean the net work by external forces is zero . ) a more encompassing equation than the one you are using is $$w_\text{net , ext}=\delta e_\text{tot}=\delta k + \delta e_\text{thermal} + \delta u_\text{potential}+\cdots . $$ so , since $w=0$ , the decrease in kinetic energy is accompanied by an increase in other forms of energy , such as thermal energy , acoustic , etc . ( not sure what the " etc . " is actually ) . to more directly address your concern , internal forces absolutely can and do affect kinetic energy , even if the work done by external forces is zero . but just knowing that $w_\text{net , ext}=0$ does not tell you how the internal energies transform ; only that the total sum is constant . ( above i assumed that heat $q$ added to the system is zero . )
there has indeed been some work on relating the geometry of the state space to the limitations of the theory . first , work relating the local state space to the non-locality present in a theory developed by janotta et al . they consider the local state space to be a regular polygon . for a large number of sides , the non-locality tends towards tsirelson 's bound . that is , in the infinite limit , the state space is quantum and so , self-dual . the issue of self-duality of state space has been explored and means that the space of effects is isomorphic to the state space . relating this to the optimality of quantum state space one can think about reversible computation . in order to have a model of quantum computing that encompasses the circuit model and more general state spaces , one can develop reversible computing for all possible gpts . firstly , reversible computations in box-world are trivial as shown by gross et al . this means that reversible dynamics for box-world is limited to permutations and relabelling of data . if we want reversible dynamics on a less trivial level , e.g. the power to map from one bit to another bit , then this leads to self-duality as shown by mueller and ududec . since the self-duality in the previous paragraph indicates a trade-off in non-locality , they also speculate that this reversible computation limits non-locality . so they connect a computational principle to the structure of the state space , very much in the spirit of barrett 's paper .
permittivity is a macroscopic property of matter - it is a consequence of the way material is polarized in the presence of an electric field . the properties of an atomic bond are determined by the atoms participating in the bond , the molecular structure in which they find themselves , and ( to a lesser extent ) the presence of a magnetic and / or electric field that is strong enough to affect the energy states of the orbitals . if i understand your question correctly , you are asking about the influence of material outside of a ( macroscopic ) rod on the inter-atomic bonds . i believe that if there were any effect at all , it would be impossibly hard to measure . immersing a polymer ( especially nylon ) in a liquid can cause significant dimensional changes due to the absorption of water . but permittivity - no , it will not affect the rod .
the objects such as $\hat \phi ( x , y , z , t ) $ in a qft are strictly speaking " operator distributions " . they differ from " ordinary operators " in the same way how distributions differ from functions . only if you integrate such operator distributions over some region with some weight $\rho$ , $$\int d^3 x\ , \hat\phi ( x , y , z , t ) \rho ( x , y , z , t ) =\hat o , $$ you obtain something that is a genuine " operator " . in a free qft , the state vectors may be built as combinations of states in the fock space – an infinite-dimensional harmonic oscillator . but you may also represent them via " wave functional " . much like the wave function in non-relativistic quantum mechanics $\psi ( x , y , z ) $ depends on 3 spatial coordinates , a wave functional depends on a whole function , $\psi [ \phi ( x , y , z ) ] $ . for each allowed configuration of $\phi ( x , y , z ) $ , there is a complex number . yes , one may also integrate over all classical functions $\phi ( x , y , z ) $ . there also exists a dirac delta-like object , the dirac " delta-functional " , and it is usually denoted $\delta$ , $$\int {\mathcal d}\phi ( x , y , z ) f [ \phi ( x , y , z ) ] \delta [ \phi ( x , y , z ) ] = f [ 0 ( x , y , z ) ] $$ i wrote the zero as a function of $x , y , z$ to stress that the argument of $f$ is still a function . the functional integration is a sort of infinite-dimensional integration and the delta-functional is an infinite-dimensional delta-function . one must be careful about these objects , especially if we integrate amplitudes that may have amplitudes and especially if we integrate over curved infinite-dimensional objects such as infinite-dimensional gauge groups etc . – there may be subtleties such as anomalies . yes , the hilbert space of a free qft is still isomorphic to the usual hilbert space : there is a countable basis . but we are talking about the finite-energy excitations only . there are lots of " highly excited states " that are not elements of the fock space – one would need infinite occupation numbers for all one-particle states . physically , such states are inaccessible because the energy can not be infinite . however , when one is changing the energy from one hamiltonian to another ( e . g . by simple operations such as adding the interaction hamiltonian ) , finite-energy states of the former $h_1$ may be infinite-energy states of the latter $h_2$ and vice versa . so one must be careful : the physically relevant finite-energy hilbert space may be obtained from some infinite-occupation-number states in a different , e.g. approximate , hamiltonian . it is still true that the relevant hilbert space is as large as a fock space and it has a countable basis . the " totally inaccessible " states that are too strong deformations have an important example or name – they are " different superselection sectors " . rigor is a strong word . people tried to define a qft rigorously – by aqft , the algebraic/axiomatic quantum field theory . these attempts have largely failed . it does not mean that there is not any " totally set of rules " that qft obeys . instead , it means that it is not helpful to be a nitpicker when it comes to the new issues that arise in qft relatively to more ordinary models of quantum mechanics ; it is neither fully appropriate to think that a qft is " exactly just like a simpler qm model " but it is equally inappropriate to forget that it is formally an object of the same kind . formally , many things proceed exactly in the same way and there are also new issues ( unexpected surprises that contradict a " formal treatment" ) that have some physical explanation and one should understand this explanation . some of these new subtleties are " ir " , connected with long distances , some of them are " uv " , connected with ever shorter distances . the fact that a qft has infinitely many degrees of freedom is both an ir and uv issue . so even if you put a qft into a box , you will not change the fact that you need wave functionals , delta-functionals , and that there are superselection sectors and states inaccessible from the fock space . by the box , you only regulate the ir subtleties but there are still the uv subtleties ( momenta , even in a box , may be arbitrarily large ) . those may be regulated by putting the qft on a lattice . this has some advantages but some limitations , too .
here is how you interpret : the expectation value in a state $|\psi\rangle$ of products of hermitian operators like $x$ and $p$ ( corresponding to observables ) quantifies how correlated ( quantum mechanically entangled ) the two observables are in that state . therefore , the statement $\frac{d}{dt}\langle xp\rangle=0$ in english reads : " the extent to which the two observables ' position ' and ' momentum ' are entangled correlated does not change with time " . now since stationary states are independent of time ( up to a phase ) , it should be pretty clear that the correlation should not change with time ( i.e. . it is fixed ) .
what you are missing is a source that is driving the whole thing . as you wrote it , there is no contradiction : $i_1=i_2=0$ . with a source included , it is no longer true that $i_1^2r_1 = i_2^2 r_2$ . you also need to watch the polarities on your transformer v 's and i 's : remember power in = power out .
the sky is a constantly changing tapestry of interesting sights and events : there is no time better than any other . if you''re interested , now is the best time ! because of our location in the milky way galaxy , summer and winter are the best times for viewing objects within the galaxy : open clusters and nebulae . spring and fall are the best times to view objects outside our galaxy : globular clusters and other galaxies . because of the earth 's rotation , if you stay up late , you can also get a sampling of the next season . look at the autumn galaxies this evening , then stay up past midnight to view the winter clusters and nebulae . superimposed on the " deep sky " are the solar system objects , which operate on their own clock . right now , saturn is disappearing in the west at sunset but venus will soon replace it ; jupiter rises around 10 p.m. and dominates the rest of the night . mars is still far away in the morning sky , but is gradually getting closer .
not quite like in the photo above , which shows more than what the naked eye can see , but yes , absolutely ! our galaxy ( well , the chunk of it visible from these parts ) is a naked-eye object . the fact that your question even exists shows how much time is now spent by people under light-polluted skies . it will not be visible from the city , however . you need to drive an hour ( or two , if you live in a huge urban area ) to the country side , far from city lights . stay outside in full darkness for a few minutes , then look up . there will be a faint " river " of light crossing the sky . that is the milky way . full dark adaptation occurs after 30 minutes of not seeing any source of light , but this is not required for seeing our galaxy . while you are in a dark sky area , also look up the andromeda galaxy , a.k.a. m31 . http://www.physics.ucla.edu/~huffman/m31.html i mean , if you can see m31 with the naked eye , at 2 mil light-years away , then of course you can see milky way , which is basically in our backyard . here 's a light pollution map , not very recent , but still useful : http://www.jshine.net/astronomy/dark_sky/
this is a situation where knowing the history of the terminology can be helpful . the qft/string theory terminology comes from algebraic geometry , where the term moduli space is used for any space whose points correspond to some kind of geometric object . the projective space $\mathbb{p} ( v ) $ , for example , is the moduli space of lines in the vector space $v$ . likewise , a moduli space of instantons is the space of solutions to a set of instanton equations . and the moduli space of complex curves is what you end up integrating over in perturbative string theory after accounting for the gauge symmetries acting on the worldsheet metric . the word ' modulus ' ( plural ' moduli' ) just means ' parameter ' . moduli spaces were originally thought of as spaces of parameters , rather than as spaces of geometric objects ; mathematicians were interested in how the various ways of parameterizing geometric objects were related and eventually realized these parameters were coordinates on a space . string theorists have resurrected this old terminology by using the term ' moduli field ' to refer to a field which parametrizes a moduli space .
there is a classic treatise on " relativity , thermodynamics and cosmology " from r . tolmann from the 1930s - it is still referenced in papers today . this generalises thermodynamics to special relativity and then general relativity . as a simple example the transformation law for temperature is stated as : $t=\sqrt ( 1-v^2/c^2 ) t_0$ when changing to a lorentz moving frame . another example is that " entropy density " $\phi$ is introduced , which is also subject to a lorentz transformation . finally this becomes a scalar with an associated " entropy 4-vector " in gr . the second law is expressed using these constructs by tolmann . there is some discussion in misner , thorne and wheeler too . of course both these texts also include lots of regular general relativity theory which you may not need .
dbrane , aside from " beauty " , the electroweak unification is actually needed for a finite theory of weak interactions . the need for all the fields found in the electroweak theory may be explained step by step , requiring the " tree unitarity " . this is explained e.g. in this book by jiří hořejší: http://www.amazon.com/dp/9810218575/ google books : http://books.google.com/books?id=mnnagd7otlicprintsec=frontcoverhl=cs#v=onepageqf=false the sketch of the algorithm is as follows : beta-decay changes the neutron to a proton , electron , and an antineutrino ; or a down-quark to an up-quark , electron , and an anti-neutrino . this requires a direct four-fermion interaction , originally sketched by fermi in the 1930s , and improved - including the right vector indices and gamma matrices - by gell-mann and feynman in the 1960s . however , this 4-fermion interaction is immediately in trouble . it is non-renormalizable . you may see the problem by noticing that the tree-level probability instantly exceeds 100% when the energies of the four interacting fermions go above hundreds of gev or so . the only way to fix it is to regulate the theory at higher energies , and the only consistent way to regulate a contact interaction is to explain it as an exchange of another particle . the only right particle that can be exchanged to match basic experimental tests is a vector boson . well , they could also exchange a massive scalar but that is not what nature chose for the weak interactions . so there has to be a massive gauge boson , the w boson . one finds out inconsistency in other processes , and has to include the z-bosons as well . one also has to add the partner quarks and leptons - to complete the doublets - otherwise there are problems with other processes ( probabilities of interactions , calculated at the tree level , exceed 100 percent ) . it goes on and on . at the end , one studies the scattering of two longitudinally polarized w-bosons at high energies , and again , it surpasses 100 percent . the only way to subtract the unwanted term is to add new diagrams where the w-bosons exchange a higgs boson . that is how one completes the standard model , including the higgs sector . of course , the final result is physically equivalent to one that assumes the " beautiful " electroweak gauge symmetry to start with . it is a matter of taste which approach is more fundamental and more logical . but it is certainly true that the form of the standard model is not justified just by aesthetic criteria ; it can be justified by the need for it to be consistent , too . by the way , 3 generations of quarks are needed for cp-violation - if this were needed . there is not much other explanation why there are 3 generations . however , the form of the generations is tightly constrained , too - by anomalies . for example , a standard model with quarks and no leptons , or vice versa , would also be inconsistent ( it would suffer from gauge anomalies ) .
with a downwardly $x$-axis , you have the equations of movement : $m_1 \ddot x_1 = m_1g - k_1x_1 + k_2x_2 \tag{1}$ $m_2 \ddot x_2 = m_2g - k_2x_2 \tag{2}$ equilibrium position means that $\ddot x_1 = \ddot x_2 = 0$ , so you have , naming $x_1$ and $x_2$ the equilibrium positions : $0 = m_1g - k_1x_1 + k_2x_2 \tag{3}$ $0 = m_2g - k_2x_2 \tag{4}$ the second equation gives $x_2 = \frac{m_2g}{k_2}$ , and replacing $x_2$ in the first equation gives $x_1 = \frac{ ( m_1 + m_2 ) g}{k_1}$ now , we are interested on oscillations around the equilibrium positions , so we make a change of variables : $ y_1 = x_1 - x_1 , \quad y_2 = x_2 - x_2\tag{5}$ now , we rewrite equations $ ( 1 ) $ and $ ( 2 ) $ using $y_1 , y_2$ : $m_1 \ddot y_1 = - k_1y_1 + k_2y_2 \tag{6}$ $m_2 \ddot y_2 = - k_2y_2 \tag{7}$ we see , that the equilibrium positions correspond to $y_1=y_2=0$ , as wished , and the terms in "$g$" ( gravity ) have disappeared . now , the equation $ ( 7 ) $ has the solution : $y_2 = y_2 ~\cos ( \omega_2 t + \phi_2 ) \tag{8}$ , with $ \omega_2 = \large \sqrt{\frac{k_2}{m_2}}$ knowing $y_2$ , we may search a solution of the equation $ ( 6 ) $ , as : $y_1 = y_1 ~\cos ( \omega_1 t + \phi_1 ) + z ~\cos ( \omega_2 t + \phi_2 ) \tag{9}$ one find : $\omega_1 = \large \sqrt{\frac{k_1}{m_1}}$ and $z = \large \frac{k_2 y_2}{k_1 - m_1 \omega_2^2}$ .
there are three processes to take into account : the warming of ice towards the melting point if it was originally below $0^{\circ} c$ . the melting of ice itself the warming of the resulting water the 1 . and 3 . part is addressed by heat capacity of ice and water respectively and the amount of heat will be directly proportional to temperature difference and weight of the water/ice . the proportionality constant ( actually it also depends on the temperature but not very strongly so let 's just ignore that ) is called specific heat . for water it is about twice as large as that of ice at temperatures around $0^{\circ} c$ . as for the 2 . part , this has to do with latent heat . simply put , this is an amount of heat you need to change phases without changing temperature . less simply put , when warming you are just converting the heat into greater wiggling of water molecules around their stable positions in the crystal thereby increasing their temperature . but at the melting point that heat will instead go into breaking chemical bonds between molecules in the ice lattice . now , latent heat is really big ( you need lots of energy to break those bonds ) . to get a hang on it : you would need the same amount of heat to warm water from $0^{\circ} c$ to $80^{\circ} c$ as you would need to melt the same amount of ice . now , presumably you want your drink cold in the end so that temperature for 3 . will be close to $0^{\circ} c$ and also the ice cubes should be pretty warm ( no use in producing ice cubes of e.g. $-50^{\circ} c$ , right ? ) . this means that these processes will not contribute much cooling . it is fair to say that melting of the ice takes care of everything . note : we can also quickly estimate how much ice you need by neglecting the processes 1 . and 3 . say you are starting with a warm drink of $25^{\circ} c$ and you want to get it to $5^{\circ} c$ . so , reusing the argument about the $80^{\circ} c$ difference being equivalent to a latent heat of the same mass , we see that you need four times less ice than water to get the job done .
if you have ever swum to the bottom of a swimming pool you will know that in water the pressure increases as you go deeper . at a depth of about 10 metres the pressure is twice what it is at the surface , but the water 10 metres down does not burst up to the surface because it is held down by the weight of water above it . in fact the increase of pressure with depth is exactly the weight of water above . exactly the same is true of the atmosphere . the pressure at ground level is 101,325 pa because each square metre of the ground has about 10,329 kg of air above it ( 10329 kg times the acceleration due to gravity 9.81 m/sec$^2$ = 101325 pa ) . if you could magically remove the 100 km or so of atmosphere that is above some patch of air at ground level that air would indeed immediately expand upwards . incidentally , bernoulli 's principle is unrelated to the problem .
huygens and barrow , newton and hooke by v . i arnold . i havent read it , but i am a fan of the author 's writing style . he is a celebrated russian mathematician and also one of the most highly cited russian scientists . road to reality ( which im currently reading ) would also have been a good suggestion but its pretty much all about recent theories , which you are not interested in .
first , we pick a global definitions of " time " in ( i ) . then in ( ii ) we ask , " in my coordinates , do the galaxies have a statistically preferred direction they are moving ? or is the distribution of velocities centered on 0 ? " we demand that in fact the distribution be centered on 0 ( isotropy tells us the distribution is symmetric ) . in this way we pick out the most natural frame to work in . now some people will take the coordinate-invariance of gr a bit too far and say things like " we should not pick reference frames artificially , since all reference frames are equivalent . " but in fact the real universe we live in does have a most natural rest frame , and there is nothing wrong with choosing to work in those coordinates , since it often makes life easier .
you have to break up the domain to get rid of the absolute value . then , do the integral by integrating by parts .
that is a very interesting question . the short of it is that gravity is always attractive . when you imagine a gravitational wave ( gw ) going by , its not like the gravity is pushing and pulling on the object , its like it pulling-less , and pulling-more . by analogy its a lot like the tides . there is a tidal bulge on the side of the earth opposite the moon , not because the moon is pushing that part away from it , but because its just being pulled less than the earth itself .
no , you cannot , since you have not specified the voltages of the two batteries . ignoring converter losses , the relevant quantity is not amp-hours but watt-hours . so let 's say the laptop battery is 18 volts and 2.5 amp-hours , while the ups is 12 volts and 7.5 amp-hours . the energy available from the laptop battery is 18 x 2.5 , or 45 watt-hours . the energy available from the ups battery is 12 x 7.5 , or 90 watt-hours . all else being equal , the ups will provide twice the duration of the laptop battery . all else , of course , is not equal . the laptop battery power goes through a set of dc-dc converters to provide the actual voltages used by the circuits , which have an efficiency less than one . the ups battery power goes through an inverter with its own inefficiency , and the resulting ac goes to the laptop where it is converted to the required internal voltages . consequently , you had expect the overall efficiency of the ups battery to be less than the efficiency of the laptop battery . exactly how much less this is , and its effect on the ratio of the two durations , is not something which can be figured out from first principles .
according to the same wikipedia article you cite , . . . the zero point is determined by placing the thermometer in brine : he used a mixture of ice , water , and ammonium chloride , a salt , at a 1:1:1 ratio . this is a frigorific mixture which stabilizes its temperature automatically : that stable temperature was defined as 0 °f ( −17.78 °c ) . the second point , at 32 degrees , was a mixture of ice and water without the ammonium chloride at a 1:1 ratio . the third point , 96 degrees , was approximately the human body temperature , then called " blood-heat " according to a letter fahrenheit wrote to his friend herman boerhaave , his scale was built on the work of ole rømer , whom he had met earlier . in rømer 's scale , brine freezes at zero , water freezes and melts at 7.5 degrees , body temperature is 22.5 , and water boils at 60 degrees . fahrenheit multiplied each value by four in order to eliminate fractions and increase the granularity of the scale . rømer 's choice of 60$^\circ$ as the boiling point of water makes sense if you consider the fact that rømer was an astronomer , so 60 has special significance . so really it was rømer who pioneered non-decimal-based temperature scales , fahrenheit was just following suit .
the induced metric on a sphere $s^2$ of radius $r$ is given by , $$\mathrm{d}s^2 = g_{\mu\nu}\mathrm{d}x^\mu \mathrm{d}x^\nu = r^2\mathrm{d}\theta^2 + r^2 \sin^2 \theta \ , \mathrm{d}\phi^2$$ we find $\sqrt{g}=r^2 \sin \theta$ , and the surface area must be given by the integral , $$a = \int_{s^2} \mathrm{d}^2 x \ , \sqrt{g}= \int_{0}^{2\pi} \ ! \ ! \mathrm{d}\phi \int_0^\pi \ ! \mathrm{d}\theta \ , ( r^2 \sin \theta ) =4\pi r^2$$ if a net charge $q$ is distributed evenly on the surface of a sphere , the charge density is , $$\sigma = \frac{q}{a} = \frac{q}{4\pi r^2}$$ the integral provided in the question is also an integration over a sphere of radius $a$ , but in more generality for any angle $\theta$ ; we chose $\theta=\pi$ to integrate over all of $s^2$ . otherwise , the integral is nothing more than that . addendum : induced metric how did we find $g_{\mu\nu}$ ? the embedding of a sphere $s^2$ in $\mathbb{r}^3$ is given by , $$x^\mu =\left ( r\cos\theta\sin\phi , r\sin\theta\sin\phi , r\cos\phi\right ) $$ the induced metric is given by the pullback of $\mathbb{r}^3$ onto the sphere , $$g_{ab}=\frac{\partial x^\mu}{\partial \sigma^a} \frac{\partial x^\nu}{\partial \sigma^b}\delta_{\mu\nu} $$ as $\delta_{\mu\nu} = \mathrm{diag} ( 1,1,1 ) $ is the metric of $\mathbb{r}^3$ . the embedding itself is a solution to , $$x^2 +y^2 +z^2 = r^2$$ which is the standard equation of a sphere in $\mathbb{r}^3$ centered at the origin .
the stationary action principle and the euler-lagrange ( el ) equations are very broad and general constructions . the field variables in the variational principle could in principle map into some generic manifold $m$ . on the other hand , euler-poincare ( ep ) equations appear in the special situation where the manifold is a lie group $m=g$ , and the action is left-$g$- invariant . one next uses the exponential map to make the variables lie algebra-valued ( rather than lie group-valued ) . the ep equations reads $$ \left ( \frac{d}{dt}+ {\rm ad}^{\ast}_{\xi}\right ) \frac{\delta \ell}{\delta\xi}~=~0 , $$ where the variables $\xi$ are lie algebra-valued . the lie algebra-valued ep equations are equivalent to the lie group-valued el equations for the same problem . see ref . 1 for further details . the euler ( e ) equations for a rigid body are a special case of the ep equations . references : j.e. marsden and t.s. ratiu , intro to mechanics and symmetry , 2nd eds , 1998 ; section 13.5 .
first , a comment . radiative heat transfer is oftentimes a non-factor for everyday objects encountered here on earth . radiative transfer is important for objects that can not exchange heat conductively or convectively , and for objects whose temperatures differ by a marked amount . that said , the rest of this answer will focus on radiative heat transfer . if every body emits radiation at a given frequency and temperature exactly as well as it absorbs the same radiation , how do objects heat up ? except for objects already in thermal equilibrium , that simply is not the case . consider two parallel flat plates in space . convection and conduction can not happen because of the vacuum of space . we will cover the backsides and edges of the plates with perfect mirrors so the only energy transfer is between the objects , and we will cover the facing sides with a perfectly black material so the objects act like black bodies on those facing sides . per the stefan-boltzmann law plate a radiates energy to plate b at a rate $p_{a\to b} =a\sigma t_a^4$ while plate b radiates energy to plate a at a rate $p_{b\to a} =a\sigma t_b^4$ . the net energy transfer to plate a is $\frac{de_a}{dt} = a\sigma ( t_b^4 - t_a^4 ) $ while the net energy transfer to plate b is $\frac{de_b}{dt} = a\sigma ( t_a^4 - t_b^4 ) = -\frac{de_a}{dt}$ . the energy transfer is zero only if $t_a = t_b$ . otherwise , the outgoing energy from the warmer body exceeds the incoming energy from the cooler body ( and vice versa ) . the warmer transfers heat to the cooler body . the process stops when the two bodies come into thermal equilibrium .
do not forget the context you are working in . you are solving for the phasor voltage across the resistor . when you measure the actual time domain voltage with , say , an oscilloscope , you will see a sinusoid with an amplitude and a phase ( referenced to the source $u_e$ ) . the magnitude of $u_a$ is the amplitude you will measure . the angle ( phase ) of $u_a$ is the phase you will measure . update : in response to the last question : first , we limit $\omega$ to being a real number . now , if you work out $\frac{u_a}{u_e}$ by hand ( something i highly recommend if you have not ) , you should get ( assuming i have not made an error ) : $\frac{u_a}{u_e} = \dfrac{j \omega rc}{1 - ( \omega rc ) ^2 + j3 \omega rc}$ now , you can " see " where this is maximum without calculation . as $\omega$ increases from zero , the real part of the denominator is decreasing and becomes zero when $\omega = \frac{1}{rc}$ . from that point on , the magnitude of the denominator increases faster than the magnitude of the numerator .
there is a lot of ambiguity in the definition of the stress-energy tensor . the stress-energy tensor is a conserved current , and like all conserved currents it is only defined up to a total divergence . i assume this $t_{\mu \nu}$ was calculated using the canonical prescription $ t^\mu_\nu=\frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi^i ) }\partial_\nu \phi^i-\mathcal{l}\delta^\mu_\nu $ ( you seem to be missing the second piece , or you are dealing with a massless field ) . the canonical tensor is not symmetric for fields with spin . essentially , the intrinsic angular momentum is also contributing to t . so you find a term $s^\lambda_{\mu \nu}$ satisfying $\partial_\lambda s^\lambda_{\mu \nu}\approx t_{ [ \mu \nu ] }$ ( s is antisymmetric in its first two indices , and thus has vanishing divergence ) and add it to the canonical tensor . see this worked out in detail here http://en.wikipedia.org/wiki/belinfante%e2%80%93rosenfeld_stress%e2%80%93energy_tensor this procedure might seem a little random , but of course what you really should be doing is obtaining t from $t^{\mu \nu}=\frac{\delta s}{\delta g_{\mu \nu}}$ as in general relativity . this $t$ will always be symmetric , and is in fact the same as the belinfante tensor . however , there is still ambiguity in this procedure . in order to obtain t this way , you have to " covariantize " the theory , promoting the metric to a dynamical field . this covariantization is ambiguous : you may couple the metric to the curvature non-minimally . these couplings vanish in the flat space limit , but can still affect the expression for t . but at least this expression will always be symmetric . hope this helps !
the article you refer to is about the electrolytic splitting of water . a 100% efficient electrolytic cell would require a voltage of about 1.23v to split water , but for various reasons a simple electrolytic cell requires about 1.48v . the difference between the voltages is called the overpotential , and it increases the amount of power needed to split the water because the power required per unit of hydrogen produced is proportional to the cell voltage . the excess power goes into heating the hydrogen and oxygen produced , and in this case it means that simple cells are about 83% efficient at converting electricity into hydrogen . catalysts can be used to increase the efficiency , and indeed platinum based catalysts can be used to reduce the overpotential and make cells with near 100% efficiency . the problem is that platinum is expensive . the result from the stanford team is that a much cheaper nickel based catalyst can achieve the same efficiency as platinum . the paper is here , but note that it is behind a paywall . if the catalyst proves to be stable enough then it will be useful for electrolytic production of hydrogen , but the improvement in efficiency is not going to change the world overnight . it still takes a lot of power to electrolyse water so it is only feasible when cheap electricity is available .
it depends how the charges are distributed in the material , and on the material 's conductance . if you have a metal , the charges of the plates would be mobile and result in a hard to compute distribution . i cannot help you with that . there are probably good approximations to tackle those kind of problems but i am no expert . if the charges are static and equally distributed among the surface , and the material has a relative permittivity ( $\varepsilon_r=1$ ) , you can use coulombs law with respect to infinitesimal parts of the charges and integrate over them . if you suppose that the rectangles have a width of $10cm$ , the force of the top plate on the middle plate could be calculated by $$ \hat{f}_{12} = \frac{1c^2}{4\pi\varepsilon_0}\int_{-25cm}^{25cm}\frac{dx_1}{50cm} \int_{20cm}^{30cm}\frac{dy_2}{10cm} \int_{-15cm}^{15cm} \frac{dx_2}{30cm} \int_{0cm}^{10cm}\frac{dy_2}{10cm} \frac{1}{ ( x_2-x_1 ) ^2+ ( y_2-y_1 ) ^2} \begin{pmatrix}x_2-x_1\\y_2-y_1\end{pmatrix} $$ as you can see , this is already quite complicated with the favourable assumptions we made . this should have an analytical solution but at the moment i am to lazy to do it . wolfram alpha could probably do the separate integrals and you would have to piece them together . you could then compare the result with what you would expect from point charges . ahh , and do not forget that there is the other plate as well . you would need to repeat the integration with opposite sign to obtain the second force and then take the difference for the total force .
an electron 's magnetic field is a dipole field - that is , the field strength is given by source : http://www2.ph.ed.ac.uk/~playfer/emlect4.pdf in this expression , $m$ is the magnetic dipole moment . for an electron , this has the value $$m=-928.476377 × 10^{−26} j/t$$ the magnetic permeability $$\mu_0=4\pi\cdot 10^{-7}\frac{v\cdot s}{ a \cdot m}$$ note - when the spin is up , the magnetic field at the origin points down , because the electron has negative charge . hence the negative sign in the value of $m$
" before gravity stopped holding it together " is the same ( pretty much ) as " so that apparent gravity at the surface is zero " . this means that $$m \omega^2 r = \frac{gm_{moon}m}{r^2}$$ with $g=6.7\cdot 10^{-11}$ , $m_{moon}=7.3\cdot 10^{22} kg$ , $r_{moon}=1740 km$ , we find $$\omega=\sqrt{\frac{gm_{moon}}{r^3}}=0.0092 rev/min = 0.55 rev/hour$$ it is interesting to see that the result has the ratio of mass and the third power of the radius - in other words it depends on the density and not the size . whether the moon consists of loose dust that will just fly apart when you reach this speed is another question - not one you asked . you asked for rev/minute but that is a hard number to grok . note that one lap per two hours is what you would have to do in order to remain " in orbit " at the surface of the moon . interestingly , the apollo 11 command module ( piloted by michael collins , the least known of the three astronauts that formed the crew of apollo 11 ) was orbiting at about 60 nautical miles , and going around once in just about 2 hours . pretty similar . . . and that was no coincidence . also note that this speed would start the moon ripping apart at the equator - where the centrifugal force appears strongest . at other points along the surface , you have a force of gravity pointing towards the center ; the component of gravity that counters the centrifugal force ( normal to the axis of rotation ) scales with $\cos ( \text{latitude} ) $ which is also how the centrifugal force scales . this means that matter will start drifting from higher latitudes towards the equator . i have not even begun addressing the other questions you had - about the impact on earth if the moon was spinning at half that speed ( 4 hours / revolution ) . i guess we would get to see the back of it , and watching the moon at night would be a less peaceful experience - but i can not think of any real physical effects ( tides etc ) on earth that would affect life . chances are great that if the moon is being hit by asteroids to make it spin faster , that this would affect its orbit around the earth : that would impact tides , and that would really wreck coastal ecology and possibly climates ; but i am assuming that the moon would accelerate from glancing impacts with " twin asteroids " coming from opposite directions , leaving no net linear momentum , and no net mass increase / decrease - just make it spin faster . the impact on a moon base would be more substantial . every point on the moon would experience days and nights on a four hour cycle , with the temperatures cycling violently . communication with earth would be disturbed - you would want to place relay transmitters on the poles to maintain contact . and of course gravity would be halved again - instead of 1/6th of the earth 's pull , you would appear to have 1/12th . coriolis forces would be wicked too - although you could drive a golf ball a very long way with such low gravity , you had get a horrible hook / slice , depending on whether you are on the northern or southern hemisphere of the moon ( and whether you are left- or right-handed ) . finally a word on the energy of the moon if it spun that fast . we know that $$e = \frac12 i \omega^2$$ and $$i = 2/5 m r^2 = 8.3\cdot 10^{28} j$$ if you took all the power of the sunlight that hits the earth ( assuming 1 kw/m^2 over the side that is lit ) for 20,000 years - you would have the amount of energy needed to give the moon that kind of energy . i imagine that if a sufficient density of meteorites came near enough to the moon to impart that kind of energy , we would not be worrying on earth about building moon bases . . . footnote about collins ' orbit . it was said that while he was on the far side of the moon , he was the " loneliest human since adam " - because he was literally further from any human beings ( and with no means of contacting them ) than any other human , anywhere on earth ( this was before major tom , of course ) . the nasa log of the flight contains the tidbits needed to reconstruct his orbital speed : hidden by the moon for 47 minutes per orbit ( july 21 , 9:44 am ) altitude of 60 nautical miles orbital velocity was 5329 ft/sec ( july 22 , 12:56 am ) after converting to sensible units ( really - they put a man on the moon with feet and nautical miles ? what are we doing , trying to force si units on a nation that was capable of such a feat , with those units… ) and drawing a little diagram , i convince myself that the fraction of the orbit when the command module was " dark " is computed as $$f = \frac{\pi - 2 \cos^{-1}\left ( \frac{r_m}{r_m + h}\right ) }{2\pi} \approx 0.35$$ and if that took 47 minutes , then the orbit took 121 minutes .
should not pure white light have a unique spectrum no matter what ? white is not a spectral color . it is a perceived color . the human eye has three kinds of color receptors , commonly called red , green , and blue . ( source : the hyperphysics page on " the color-sensitive cones" ) note that there is no receptor for yellow . a spectral yellow light source will trigger both the red and green receptors in a certain way . we see " yellow " even though we do not have yellow receptors . any spectrum of light that triggers the same response will also be seen as " yellow " . computer screen and your tv screen manufacturers depend on this spoofing . those displays only have three kinds of light sources , red , green , and blue . they generate the perception of other colors by emitting a mix of light that triggers the desired response in the human eye . what about white ? white is not a spectral color . there is no point on the spectrum that you could label as " white " . white is a mixture of colors such that our eyes and brain can not distinguish which of red , green , or blue is the winner . just as any mix of colors that trigger our eyes and mind to see " yellow " will be perceived as " yellow " , so will any mixture of colors that trigger the same responses in our eye . aside : there is a mistake in the above image in the labels " bluish purple " and " purplish blue " . that should be " blue-violet " and " violet-blue " ( or possibly " indigo" ) . purple is a beast of a very different color . it is a non-spectral color . the spectrum is just that , a linear range . our eyes do not perceive it as such . we view color as a wheel , with blue circling back to red via the purples .
the link does not work for me either , but it is not obvious that this is a typo ( http://cartech.ides.com/imagedisplay.aspx?e=111imgurl=%2fcarpenterimages%2fa-toolanddie%2f23-ts23-micromelta11%2f05_ts23_3point.gifimgtitle=3-point+bend+test , http://www.matweb.com/search/datasheet.aspx?matguid=638937fc52ca4683bc0c3f18f54f5a24 ) . and it looks like this strength is indeed the highest for an alloy ( http://gofygure.hubpages.com/hub/what-is-the-strongest-metal-the-hardest-metals-known-to-man )
a good source of spectroscopic data is the nist chemistry webbook . it compiles all possible information about polyatomic molecules including their uv/vis and ir spectra . you will find references to corresponding papers and , as i see , the new version includes a java applet that plots the uv spectrum . here is a direct link to tht spectrum .
calculate the determinant of ( 1.2.16 ) . note that these are determinants of $2\times 2$ matrices , as peter kravchuk told you , so $\det ( cm ) =c^2\det ( m ) $ for a constant $c$ . just to be sure , by the word " constant " , i really mean $c$ is a scalar , not a matrix . this $c$ may still depend on $\tau , \sigma$ ( or other variables if there were any ) . there is a coefficient $c^2$ because two columns ( or two rows ) are multiplied by $c$ each and the determinant is multilinear in the rows ( or columns ) . so the determinant of ( 1.2.16 ) is $$\det_{a , b} ( h_{ab} ) = \det_{ab} \left ( \frac{1}{2}\gamma_{ab} \gamma^{cd}h_{cd} \right ) $$ because $\gamma^{cd}h_{cd}/2$ plays the role of the constant $c$ from my previous general identity , the equation above may be simplified to $$\det_{a , b} ( h_{ab} ) = \det_{ab} ( \gamma_{ab} ) \times \left ( \frac{1}{2} \gamma^{cd}h_{cd} \right ) ^2$$ using the symbols such as $h=\det_{ab} ( h_{ab} ) $ and similarly for $\gamma$ , the square root of the minus equation above ( minus because the determinants are negative in a minkowskian signature ) is $$\sqrt{-h} = \sqrt{-\gamma}\times \frac 12 \gamma^{cd}h_{cd} $$ the second power disappeared again . divide ( 1.2.16 ) by the displayed equation i just wrote down . on the right hand side , $ ( 1/2 ) \gamma^{cd}h_{cd}$ will cancel again and you are left with ( 1.2.17 ) . in none of the arguments above , it is important that $c$ may depend on $\tau , \sigma$ . also , it does not matter at all that/whether $h_{ab}$ may be calculated as the induced metric from $\partial_\alpha x^\mu$ etc . why others can derive the second equation immediately those experienced among us see the result immediately because the left hand side of ( 1.2.17 ) is the tensor proportional to $h_{ab}$ with a normalization constant chosen so that the determinant of this left hand side equals minus one ( the minus can not be eliminated because it is given by the minkowski signature ) . in other words , the normalization factor is chosen exactly to " forget " the normalization . similarly , the right hand side of ( 1.2.17 ) is the tensor proportional to $\gamma_{ab}$ with the right normalization factor to makes the determinant of this product equal to minus one . because both $h_{ab}$ and $\gamma_{ab}$ were converted – by adding $ ( -h ) ^{-1/2}$ factors etc . – to matrices whose determinant is equal to minus one , both sides of ( 1.2.17 ) actually encode the information about the matrices $h_{ab }$ and $\gamma_{ab}$ up to their normalization . in other words , ( 1.2.17 ) is equivalent to saying that $h_{ab}$ and $\gamma_{ab}$ are proportional to one another as matrices . the equation ( 1.2.16 ) also says that they are proportional to each other as matrices – with a coefficient that is explicitly given in ( 1.2.16 ) but not in ( 1.2.17 ) – so ( 1.2.17 ) follows from ( 1.2.16 ) ( but not the other way around ) . this paragraph may sound complicated but it is really obvious for those who have a sufficient experience with tensors , matrices , and determinants .
yes , you do need to add in the mass of dark matter if it is present , however on small scales the dark matter is almost uniformly distributed . to see this , consider formation of the solar system from the original dust cloud . if you take some test particle far from the sun and let it fall towards the sun it will accelerate towards the sun , then pass it and head on out again . if every particle in the original dust cloud behaved this way the solar system could never have formed since the dust cloud would simply oscillate about its centre of mass and stay the same overall size . the reason the sun formed is that electrostatic interactions between the dust particles allowed the cloud to dissipate energy as heat and settle towards the centre . now you see why the sun is not full of dark matter . dark matter only interacts by the weak and gravitational forces so the dark matter particles can not dissipate energy and can not settle into the sun . assuming there is dark matter in the solar system it will be oscillating about the sun . in principle weak interactions will eventually dissipate enough energy for the dark matter to become gravitationally bound within the sun , but it is going to take a long time . the average density of matter is astonishingly low . at present the total density is around 5 protons per cubic metre , so the dark matter density is only 1 proton per cubic meter ( and the average baryon density is 1 proton per five cubic metres ! ) . there will be variations in the dark matter density caused by quantum fluctuations during inflation , and indeed these are thought to have been critical in seeding formation of the first galaxies . however on sub-galactic scales the dark matter density fluctuations are so small we can ignore them .
there is a critical current density for every superconductor where the superconductor acts as an ordinary conductor and a voltage difference can be measured between its ends .
the only general requirement on the state function for a single , spinless , quantum particle ( quanton ) in a physically realistic state is that the state function be square integrable , i.e. , the integral of its absolute value squared over all space be finite . non-square integrable state functions are used for many purposes , but they are all idealizations that do not , individually , represent realistic states . if the state function is also to belong to the domain of definition of the hamiltonian , then , in non-relativistic qm , the state function must be spatially differentiable to second order as well . state functions which are square integrable but not second order differentiable do not satisfy the schroedinger equation . but their time evolution is still determined by continuity considerations since the second order differentiable state functions are everywhere dense in the state space , i.e. , hilbert space .
logarithmic series are a very broad topic . generally speaking , many quantities in qcd can be expressed as power series of the form $$x ( s ) = \underbrace{\sum_n x_{0n} ( \alpha_s\ln s ) ^n}_\text{ll terms} + \underbrace{\sum_n x_{1n}\alpha_s ( \alpha_s\ln s ) ^n}_\text{nll terms} + \cdots$$ the kinds of contributions that enter each set of terms depend entirely on what quantity $x$ is being calculated . i can only address this in more detail in the context of bfkl physics . the bfkl equation governs the unintegrated gluon distribution for a hadron , $\mathcal{f}$: $$\mathcal{f} ( \gamma ) = \mathcal{f}^{ ( 0 ) } ( \gamma ) + \frac{\bar{\alpha}_s}{\omega}\chi ( \gamma ) \mathcal{f} ( \gamma ) = \frac{\bar{\alpha}_s}{\omega} [ \underbrace{\chi_0 ( \gamma ) }_\text{ll} + \underbrace{\bar{\alpha}_s\chi_1 ( \gamma ) }_\text{nll} + \cdots ] \mathcal{f} ( \gamma ) $$ where $\gamma$ is the mellin conjugate to the momentum transfer $q$ . the best reference i have been able to find on what is included and excluded from the ll terms is this paper by gavin salam . he identifies three specific effects that contribute to the nll and higher terms : running coupling the running of the strong coupling ( to one-loop order ) is described by $$\bar{\alpha}_s ( q ) = \frac{\bar{\alpha}_s ( q_0 ) }{1 + b\bar{\alpha}_s ( q_0 ) \ln\frac{q^2}{q_0^2}}$$ at ll order , you can just use a constant $\bar{\alpha}_s ( q_0 ) $ for the coupling , but at nll , you have to incorporate the fact that multiple energy scales are involved in the process , and the difference between $\bar{\alpha}_s$ at these multiple scales leads to nll corrections . splitting function part of the bfkl equation involves a splitting function which is associated with gluon branching ( $1\to 2$ ) in the feynman diagrams . the leading term in the splitting function is $\frac{1}{z}$ , where $z$ is the fraction of momentum taken by one of the final-state gluons . that is sufficient for the ll expression , but when additional terms of the splitting function are taken into account , they produce nll and higher contributions . energy scale dependence this is one effect that is more general than just bfkl physics . recall that when we write $\alpha_s\ln s$ we actually mean $\alpha_s\ln\frac{s}{s_0}$ , where $s_0$ is some arbitrary constant . writing the same quantity for different values of $s_0$ involves differences which are at nll and higher order . but of course , those are just the most prominent , specifically identified terms . there are some other , smaller nll corrections , and of course it is largely unknown what contributions come in at nnll and higher order , so it is impossible to provide a complete list .
unpredictability and special relativity can come from the fact that objects that are at a space like separation from us can influence our future like cone . for example , if alpha centauri exploded in a supernova right " now " in our reference frame , we would not know about it for 4 years since that star is 4 light years away from us . so we can not now predict that 4 years from now we will be hit with the supernova blast wave . as @arnoques succintcly put in his comment , to predict an event in our future light cone , the entire past light cone of that event would need to be known and that would include events that are outside our present light cone .
clean dry air lets sunlight through ; dirty moist air scatters it . aerosols ( small air borne particulate contamination ) are more prominent near areas of dense population - due to power plants , cars , fires , . . . these particles form nucleation sites for moisture - and these small water drops become very effective scatterers of sunlight . the humidity is high in the philippines , and it is low in western australia ( perth ) . a map of the nitrogen dioxide concentrations in the earth 's atmosphere ( a proxy for ' man made pollution' ) shows that the region around western australia is quite low in pollution , while a lot of south east asia is quite high ( map from http://www.esa.int - european space agency ) : a map of the particulate pollution ( pm2.5 - particulate matter less than 2.5 micron ) confirms the picture ( credit : aaron van donkelaar , dalhousie university . source at http://www.nasa.gov/images/content/483910main1_global-pm2.5-map-670.jpg ) : although it is not terribly clear , the air in western australia is obviously quite clear - so there will be less " stuff " for light to travel through / scatter off . this is especially noticeable near sunrise/sunset , when the length of the path through the atmosphere is longest . this amplifies the difference . a bit more data to back this up : map of typical humidity distribution in manila ( source : http://weatherspark.com/averages/33313/metro-manila-philippines ) : and for perth ( source : http://weatherspark.com/averages/34080/redcliffe-western-australia ) : these plots show the distribution of the " average daily high and low " values of humidity as a function of date , for both locations . thus , you can see that average high for humidity is lowest on april 23 - at which point it is still 89% . the inner ( darker colored ) band represents the 25 - 75 percentile of the distribution , and the outer ( lighter colored ) band represents the 10-90 percentile . in other word - on april 23 , maximum humidity in manila might be at or below 82% one day in four ; but on august 17 it is above 95% more than half the time . note that the vertical scale on the two plots is different - the minimum values in perth are considerably lower than for manila . . . here is a link to a very interesting and unusual photo sequence of a setting sun showing the phenomenon of the " green flash " . this particular sequence was taken in libya , and the photographer states : the air was so clean and dry that it was difficult to look directly at the sun even when it was only a sliver above the horizon . i have never seen the sky quite like this before . as the sun was going down , you could not look at it at all naked-eye ; even to the very last moment it was too bright . that supports my understanding that dry , clean air == bright sunsets . update in the comments , somebody asked the question : " what is this stuff that is doing the absorbing ? " . as was pointed out , water vapor is not a very good absorber of light in the optical regime - the vibration modes of water molecules are excited in the infrared . however , on page 12 of http://www.learner.org/courses/envsci/unit/pdfs/unit11.pdf we read : air molecules are inefficient scatterers because their sizes are orders of magnitude smaller than the wavelengths of visible radiation ( 0.4 to 0.7 micrometers ) . aerosol particles , by contrast , are efficient scatterers . when relative humidity is high , aerosols absorb water , which causes them to swell and increases their cross-sectional area for scattering , creating haze . without aerosol pollution our visual range would typically be about 200 miles , but haze can reduce visibility significantly this agrees with @whatroughbeast 's observation that haze aerosols are ultimately the " stuff " that scatters the light - a combination of particles in the air ( many of which are man made , and will be present in higher concentrations near densely populated regions - especially ones where coal fired power plants operate ) and humidity which causes the aerosols to increase in size , making them more effective scatterers .
$\mathbf{f}_m ( \mathbf{r}_m ) =\mathbf{f}_{ext} ( \mathbf{r}_m ) +q_m \sum_{i\ne m}q_i\mathbf{n}_{im}/ ( \mathbf{r}_m-\mathbf{r}_i ) ^2$ , is not it ? or you want magnetic forces too ? edit : for magnetic part , you can use the lagrangian from landau-lifshitz textbook ( §65 ) :
if this were computer science , we might say $\psi$ takes a $d$-tuple of reals ( $r$ ) and another real ( $t$ ) and returns a complex number with the attached unit of $l^{-d/2}$ in $d$ dimensions ( with $l$ being the unit of length ) . 1 if you want any more of an interpretation , well then you have already given it : $\psi ( r , t ) $ is the thing such that $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ is the probability of the particle being observed in the region $r$ at time $t$ . you can loosely think of it as a " square root " of a probability distribution . the reason the " square root " interpretation is not quite right , and probably the reason you are not satisfied with the $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ definition , is that any particular instance of $\psi ( r , t ) $ carries extraneous information beyond what is needed to fully specify the physics . in particular , if we have $\psi_1$ describing a situation , then the wavefunction defined by $\psi_2 ( r , t ) = \mathrm{e}^{i\phi} \psi_1 ( r , t ) $ gives identical physics for any real phase $\phi$ . so the return value of the wavefunction itself is not a physical observable -- one always takes a square magnitude or does some other such thing that projects many mathematically distinct functions onto the same physical state . even once you have taken the square magnitude , $\lvert \psi ( r , t ) \rvert^2$ arguably is not directly observable , as all we can measure is $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ ( though admittedly for arbitrary regions $r$ ) . 1 you can check that $-d/2$ is necessarily the exponent . we need some unit such that squaring it and multiplying by the $d$-dimensional volume becomes a probability ( i.e. . is unitless ) . that is , we are solving $x^2 l^d = 1$ , from which we conclude $x = l^{-d/2}$ .
in the simple case of a pure state , the density matrix is the projection $\rho_\psi=\lvert\psi\rangle\langle\psi\rvert$ , for some $\psi\in \mathscr{h}$ ( the hilbert space ) . measuring it on another general state $\rho$ ( i.e. . calculating $\mathrm{tr} [ \rho_\psi\rho ] $ ) gives you the probability of $\psi$ being the " configuration " ( or hilbert space vector ) corresponding to $\rho$ .
just a few pointers for you to explore more on this . check out aharonov 's paper the time symmetric formulation of quantum mechanics : http://arxiv.org/abs/quant-ph/9501011 tony leggett talks about this : http://www.youtube.com/watch?v=igim9uzcumk it is a nice video and quite simple to understand .
1 ) if there is an error $e_j$ , the new states $e_j|0\rangle_l$ and $e_j|1\rangle_l$ are eigenvectors , with eigenvalue $-1$ , of all the stabilizers $s_j$ belonging to some set subset $s_j$ of $s$ . ( the elements of $s_j$ anticommute with $e_j$ ) . this subset $s_j$ identifies uniquely the error $e_j$ . 2 ) $|0\rangle_l$ and $|1\rangle_l$ are eigenvectors , with eigenvalue $1$ , of all the stabilizers $s$ belonging to $s$ ( this is not true for the " components " of $|0\rangle_l$ and $|1\rangle_l$ like , for instance , $|1010101\rangle$ ) . for a stabilizer $s$ , you just calculate $s|0\rangle_l$ and $s|1\rangle_l$ , and you check that the result is $|0\rangle_l$ or $|1\rangle_l$ . for instance : $k^1\left|0\right\rangle_l = ( iiixxxx ) \\\frac{1}{\sqrt{8}} ( \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle + \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle ) = \\ \frac{1}{\sqrt{8}} ( \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle + \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle ) \\ =\left|0\right\rangle_l$ 3 ) $k^4 \left|1010101\right\rangle = iiizzzz |1010101\rangle$ . with $z |0\rangle = |0\rangle$ , and $z |1\rangle = -|1\rangle$ , you get : $k^4 \left|1010101\right\rangle = |1010101\rangle$
i will answer this in 2d , although the 3d case is very similar . modeling the explosion as throwing out a wave of particles with velocity $v_p$ in all directions , if the projectile is initially traveling with translational velocity vector $\mathbf{v}_t$ then the fragments after airburst will be traveling with velocity vectors $$\mathbf{v}=\mathbf{v}_t+\mathbf{s}$$ where $$|\mathbf{s}|=v_p . $$ it is clear that this set of vectors is just the points on a circle of radius $v_p$ centered at $\mathbf{v}_t$ , so choosing $\mathbf{v}_t= ( v_t , 0 ) $ , they are solutions of $$ ( x-v_t ) ^2+y^2=v_p^2 . $$ assuming $v_t&gt ; v_p$ , this takes the form of a circle whose points all satisfy $x&gt ; 0$ , and thus it is visually apparent that this forms a cone-shaped angular distribution . if this is not obvious , one can solve $$ ( x-v_t ) ^2+y^2=v_p^2\\y=\tan ( \theta ) x , $$ differentiate each of the two solutions with respect to $\theta$ , compute their vector norms and then add them to obtain an unnormalized angular distribution function , which can be normalized by dividing by the circle perimeter $2\pi v_p$ to get $$i ( \theta ) =\frac{1}{2\pi} \left ( \sqrt{\frac{v_t \left ( \cos ( 2 \theta ) v_t-2 \cos ^2 ( \theta ) \sqrt{\sec ^2 ( \theta ) v_p^2-\tan ^2 ( \theta ) v_t^2}\right ) +v_p^2}{v_p^2-\sin ^2 ( \theta ) v_t^2}}+\sqrt{\frac{v_t \left ( 2 \cos ^2 ( \theta ) \sqrt{\sec ^2 ( \theta ) v_p^2-\tan ^2 ( \theta ) v_t^2}+\cos ( 2 \theta ) v_t\right ) +v_p^2}{v_p^2-\sin ^2 ( \theta ) v_t^2}}\right ) $$ which has singularities at $$\theta_\pm=\pm\sin ^{-1}\left ( \frac{v_p}{v_t}\right ) . $$ this is why you observe a conical fragment distribution , and the cone half-angle is given by $|\theta_\pm|$ . here is mathematica code to derive the formula i used ( hit Cell &gt; Convert To &gt; StandardForm to get the subscripts to parse correctly ) : the singularities are found by solving for when when the denominator is zero . here is a plot of the angular density for various explosion velocities and projectile speeds : minor notes : the expression $i ( \theta ) $ incorrectly evaluates to nonzero values when $|\theta|&gt ; |\theta_\pm|$ , when in fact it obviously should evaluate to zero ( since there are no fragments outside the cone ) . this error is due to the fact that imaginary values are obtained for the solutions of the circle-line intersection outside this cone angle , which turn into positive real numbers when you take the norm . so to correctly plot it , you need to use $i ( \theta ) $ inside the cone and zero outside the cone .
if you can form the problem in such a way as the is a contribution to the kinetic energy that can not change during the time modeled then you can drop that term . why ? because the euler-lagrange equation is only concerned with differentials and a added or subtracted constant does not affect them . the simplest example of this is writing the problem in a pair of reference frames where the system is at rest in one and in constant linear motion in another . the ke of the center of mass does not affect the outcome .
there is an interesting way to look at christoffel connections with spinor fields . the usual dirac operator is written as $\gamma^\mu\partial_\mu$ . it is interesting to change this to $\partial_\mu ( \gamma^\mu\psi ) $ . this then becomes $$ \partial_\mu ( \gamma^\mu\psi ) ~=~ \gamma^\mu\partial_\mu~+~ ( \partial_\mu\gamma^\mu ) \psi . $$ the anticommutator $\{\gamma^\mu , ~\gamma^\nu\}~=~2g^{\mu\nu}$ and the covariant constancy of the metric gives $\partial_\mu\gamma^\mu~=~\gamma^\mu_{\mu\sigma}\gamma^\sigma$ . so we may then write the dirac operator in this different form as $$ \delta_\nu^\mu\partial_\mu ( \gamma^\nu\psi ) ~=~ \delta^\mu_\nu \gamma^\nu\partial_\mu\psi~+~\delta^\mu_\nu \gamma^\nu_{\mu\sigma}\gamma^\sigma\psi . $$ now if you peel off the kronecker delta you have a covariant derivative of the spinor field . what this means is that in general the clifford algebra $cl ( 3,1 ) $ representation of the dirac matrices is local . the connection coefficient can then be seen as due to transition functions between these representations , so the differential produces connection coefficients .
disclaimer : since this is a homework style problem , i am not going to write all the details . writing down the projectile 's equation , and replacing $\theta$ we arrive at the equation : $$-\frac{x^2}{\frac{4 g ( h-y ) \left ( l^2- ( y-h ) ^2\right ) }{l^2}}-\frac{x ( y-h ) }{\sqrt{l^2- ( y-h ) ^2}}+h=0$$ solving this for $x$ , and simplifying the huge expressions ( perhaps by something like mathematica ) ; we find : $$x\to \frac{ \left ( 2 \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }-2 g ( h-y ) ^2 ( h-l-y ) ( h+l-y ) \right ) }{l^2 \sqrt{- ( h-l-y ) ( h+l-y ) }}$$ squaring : $$\rightarrow x^2=\frac{1}{l^4}4 g ( y-h ) \left ( -2 h \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }+2 y \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }+2 g ( h-y ) ^3 ( h-l-y ) ( h+l-y ) +h l^2 ( h-l-y ) ( h+l-y ) \right ) $$ in general if we have a function of several independent parameters ( i.e. . $f ( x_1 , x_2 , \cdots , x_n ) $ ) , and we want to calculate the error in $f$ , we would write : $$\delta f = \sqrt{\sum_i \left ( \frac{\partial f}{\partial x_i}\delta x_i \right ) ^2}$$ in this case $x^2$ only depens on $y$ , and its error comes from the error in $y$: $$\delta ( x^2 ) = \left| \frac{\partial ( x^2 ) }{\partial y} \delta y\right| = x^2 \left|\frac{ \left ( -g ( h-y ) ^3 \left ( 3 ( h-y ) ^2-l^2\right ) +3 h \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }-3 y \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }+h l^2 \left ( l^2-3 ( h-y ) ^2\right ) \right ) }{ ( h-y ) ( h-l-y ) ( h+l-y ) \left ( g ( h-y ) ^3+h l^2\right ) } \delta y\right|$$ it does not even fit properly ! anyway , putting back all the corresponding numbers , we find : $$\frac{\delta ( x^2 ) }{x^2} \approx 0.52 \delta y$$ which looks like a decent result to me . note , this final equation is only true for values given in this link , namely $h=9.7 \text{cm} , y=17.5 \text{cm} , l=60\text{cm} \ \text{and}\ g=980 \text{cm}\text{ . s}^{-2}$ .
whatever is the nature of the interaction one should take it into account in the hamiltonian only once for each distinct pair of particles : $$ u_\text{int} = \sum_{i&gt ; j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) = \sum_{i&lt ; j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) . $$ usually these sums are merged to make the hamiltonian of the system more symmetric . in that case the total sum should be divided by two : $$ u_\text{int} = \frac{1}{2}\sum_{i\neq j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) . $$ now the total hamiltonian $$ h = \sum_i \frac{p_i^2}{2m} + \frac{1}{2}\sum_{i\neq j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) $$ can be rewritten as a sum of " one-particle " hamiltonians : $$ h = \sum_i\left ( \frac{p_i^2}{2m} + \frac{1}{2}\sum_{j \neq i} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) \right ) . $$ if you mean this when say " put energy into each particle " then , yes , you should put a half of the interaction energy .
gravity is viewed as a force because it is a force . a force $f$ is something that makes objects of mass $m$ accelerate according to $f=ma$ . the moon or the iss orbiting the earth or a falling apple are accelerated by a particular force that is linked to the existence of the earth and we have reserved the technical term " gravity " for it for 3+ centuries . einstein explained this gravitational force , $f=gmm/r^2$ , as a consequence of the curved spacetime around the massive objects . but it is still true that : gravity is an interaction mediated by a field and the field also has an associated particle , exactly like the electromagnetic field . the field that communicates gravity is the metric tensor field $g_{\mu\nu} ( x , y , z , t ) $ . it also defines/disturbs the relationships for distances and geometry in the spacetime but this additional " pretty " interpretation does not matter . it is a field in the very same sense as the electric vector $\vec e ( x , y , z , t ) $ is a field . the metric tensor has a higher number of components but that is just a technical difference . much like the electromagnetic fields may support wave-like solutions , the electromagnetic waves , the metric tensor allows wave-like solutions , the gravitational waves . according to quantum theory , the energy carried by frequency $f$ waves is not continuous . the energy of electromagnetic waves is carried in units , photons , of energy $e=hf$ . the energy of gravitational waves is carried in the units , gravitons , that have energy $e=hf$ . this relationship $e=hf$ is completely universal . in fact , not only " beams " of waves may be interpreted in terms of these particles . even static situations with a force in between may be explained by the action of these particles – photons and gravitons – but they must be virtual , not real , photons and gravitons . again , the situations of electromagnetism and gravity are totally analogous . you ask whether the spacetime is the force field . to some extent yes , but it is more accurate to say that the spacetime geometry , the metric tensor , is the field . concerning your last question , indeed , one may describe the free motion of a probe in the gravitational field by saying that the probe follows the straightest possible trajectories . but where these straightest trajectories lead – and , for example , whether they are periodic in space ( orbits ) – depends on what the gravitational field ( spacetime geometry ) actually is . so instead of thinking about the trajectories as " straight lines " ( which is not good as a universal attitude because the spacetime itself is not " flat " i.e. made of mutually orthogonal straight uniform grids ) , it is more appropriate to think about the trajectories in a coordinate space and they are not straight in general . they are curved and the degree of curvature of these trajectories depends on the metric tensor – the spacetime geometry – the gravitational force field . to summarize , gravity is a fundamental interaction just like the other three . the only differences between gravity and the other three forces are an additional " pretty " interpretation of the gravitational force field and some technicalities such as the higher spin of the messenger particle and non-renormalizability of the effective theory describing this particle .
there is the obvious requirement that the relative humidity at the temperature of the glass surface must be greater than 100% . however there is a kinetic barrier to nucleation of the water droplets on the glass , and this is extremely sensitive to the condition of the glass surface . if you clean the glas with the traditional chromic acid it will nucleate water droplets very easily and the dew will form almost immediately the humidity exceeds 100% . if you put some grease on the surface then polish it off with a cloth ( to leave a very thin film ) this will strongly inhibit nucleation and the dew may not form until you get condensation in the air ( i.e. . a fog ) . so exactly when the dew will form is hard to calculate .
do optical mode phonons interact differently than acoustic ones ? yes . look at the dispersion curve for any material and you can understand this better . let us take silicon for example . you can see that the gradient of the frequency as a function of wave vector ( aka , group velocity ) is what determines the amount of energy that can be carried by phonons . you can see that the optical branches have a much flatter profile than the acoustic branches . therefore , they do not participate heavily in energy transfer and storage ( thermal conductivity and specific heat capacity ) . you can also see that their frequency is much higher than acoustic branches . this means that they interact with em waves of similar frequcencis . this is why , for example , co2 is a greenhouse gas . also , is there a way i can quantify this ? yes . thier interaction is different from acoustic phonons and were first successfully described by einstein where he assumed a dispersion relations at a single frequency , $\omega_0$ , and independent single frequency oscillator at each atom . the density of states is given by $d ( \omega ) = n\delta ( \omega - \omega_0 ) $ the acoustic phonons are described by debye model and they have a liner dispersion relation $\omega= v_s k$ and are responsible for sound wave . the density of states is given by $$d ( \omega ) = \frac{v \omega^2}{2 \pi^2 v_s^3}$$
you need to be a bit careful about the counting of supercharges . in four dimensions , the smallest spinor representation is four-dimensional ( over the real numbers ) , so $\mathcal{n}=1$ supersymmetry has four supercharges . when there are more supercharges , there are simply more states combined into a single ' multiplet ' . for example , in $\mathcal{n}=1$ supersymmetry , a theory with a massless vector boson like a photon necessarily also contains a massless spin-$\frac{1}{2}$ particle , the ' superpartner ' of the photon . in an $\mathcal{n}=2$ theory , a photon has more superpartners : two massless spin-$\frac{1}{2}$ particles , and a massless complex scalar . it is still always true that there are the same number of bosonic and fermionic degrees of freedom . you can find all the details in chapters 2 and 3 of wess and bagger 's classic supersymmetry textbook .
in an inertial reference frame , there most certainly is not zero net force on a planet . there is a force pulling the planet towards the sun , and that is it . there are no other forces on the planet . you are 100% correct : if there really were no net force on the planet , then the planet would be stationary or travel in a straight line at constant velocity . it would not travel in a circle or ellipse . this is newton 's first law . if your tutor disagrees with newton 's first law , i suggest you find a better tutor !
i will try to slightly elaborate on @vladimirkalitvianski answer . from maxwell 's equations , we can derive that the following combination of gauge transformations on $\mathbf{a}$ and $\phi$ leave both $\mathbf{b}$ and $\mathbf{e}$ invariant : \begin{align} and \mathbf{a}'=\mathbf{a}-\mathbf{\nabla} \alpha \\ and \phi'=\phi+\frac{\partial \alpha}{\partial t} \end{align} where $\alpha=\alpha ( \mathbf{x} , t ) $ . this means that all the field configurations of $\mathbf{b}$ and $\mathbf{e}$ related by a gauge transformation are physically equivalent . note that this has nothing to do with the hamiltonian operator in qm . now in qm , we know that a wave function can always be multiplied by a phase factor : $$ \psi'=e^{-iq\alpha}\psi , $$ where $\alpha \neq \alpha ( \mathbf{x} , t ) $ , because the probability of finding the particle at a particular position is unaffected by the above transformation , and also the schrodinger equation and the probability current are unaffected by the above transformation . if we now demand that the above also holds for when $\alpha = \alpha ( \mathbf{x} , t ) $ ( i.e. . a gauge transformation ) , then the schrodinger equation must be made gauge invariant : \begin{equation} i\frac{\partial\psi}{\partial t}=-\frac{1}{2m} ( \mathbf{\nabla}-iq\mathbf{a} ) ^2\psi+ ( v+q\phi ) \psi \end{equation} such that the schrodinger equation is invariant under the simultaneous gauge transformations : \begin{align} and \mathbf{a}'=\mathbf{a}-\mathbf{\nabla} \alpha \\ and \phi'=\phi+\frac{\partial \alpha}{\partial t} \\ and \psi'=e^{-iq\alpha}\psi \tag{1} \end{align} note that we can say that we have adjusted the " normal " hamiltonian by replacing the ordinary ( partial ) derivatives by : \begin{equation} \begin{array}{cc} \displaystyle \mathbf{\nabla} \rightarrow \mathbf{d}\equiv \mathbf{\nabla} - i q \mathbf{a} \ ; , and \displaystyle \frac{\partial}{\partial t} \rightarrow d^0 \equiv\frac{\partial}{\partial t} + iq \end{array} \end{equation} to sum up , by demanding that our theory is invariant under the gauge transformation expressed by equation $ ( 1 ) $ , we are forced to change the hamiltonian operator as we have done above . however , by doing this , the new hamiltonian describes a particle interacting with the potentials $\mathbf{a}$ and $\phi$ . if you are not convinced by this argument , i strongly recommend you to read up on the aharonov–bohm effect ( http://en.wikipedia.org/wiki/aharonov%e2%80%93bohm_effect ) . furthermore , note that we require that a gauge transformation does not affect any observables . this means that we must demand that the probability current is also unaffected . you can show ( although it is quite tedious ) that the current is made gauge invariant by making the replacement : $\displaystyle \mathbf{\nabla} \rightarrow \mathbf{d}$ .
the covariant derivative of $g$ is zero , but it is ordinary derivative is not . you need to know what is $\ddot{x_\mu}$ , \begin{equation} \ddot{x}_{\mu} = \frac{d^2}{dt^2} ( g_{\mu\nu} x^{\nu} ) = g_{\mu\nu} \ddot{x}^{\nu} + \ddot{g}_{\mu\nu} x^{\nu} + 2\dot{g}_{\mu\nu} \dot{x}^{\nu} \ne g_{\mu\nu} \ddot{x}^{\nu} \end{equation} that is the mistake you made in equation ( 1 ) . to get agreement , you need to know what is the meaning of vector $t_{\mu}$ in your equation . i think that is actually the components of the dual of the velocity vector , \begin{equation} t_{\mu} = g_{\mu\nu} \dot{x}^{\nu} \end{equation} notice that $g_{\mu\nu}$ is not inside the time derivative , that is how we lower indices by musical isomorphism . plug in this definition of $t_{\mu}$ and you will get the standard geodesic equation . in fact , equation ( 2 ) is the euler-lagrangian equation for the action , \begin{equation} s = \int d\tau \frac{1}{2} g_{\mu\nu} \dot{x}^{\mu} \dot{x}^{\nu} \end{equation} whose solution gives the distance minimizing curve ( with affine parameterization ) : the geodesic .
$x=x ( y , z ) $ , $y=y ( x , z ) $ , $z= ( x , y ) $ $$dx= ( \frac{\partial x}{\partial y} ) _z dy + ( \frac{\partial x}{\partial z} ) _y dz$$ $$dy= ( \frac{\partial y}{\partial x} ) _z dx + ( \frac{\partial y}{\partial z} ) _x dz$$ $$\therefore dx= ( \frac{\partial x}{\partial y} ) _z [ ( \frac{\partial y}{\partial x} ) _z dx + ( \frac{\partial y}{\partial z} ) _x dz ] + ( \frac{\partial x}{\partial z} ) _y dz$$ $$dx= ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial x} ) _z dx + [ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial z} ) _x + ( \frac{\partial x}{\partial z} ) _y ] dz$$ $$ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial x} ) _z dx + [ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial z} ) _x + ( \frac{\partial x}{\partial z} ) _y ] dz=1dx+0dz$$ $$ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial z} ) _x + ( \frac{\partial x}{\partial z} ) _y=0 $$ using reciprocal relation : $$ ( \frac{\partial x}{\partial y} ) _z = \frac{1}{ ( \frac{\partial y}{\partial x} ) _z} $$ $$\left ( \frac{\partial x}{\partial y} \right ) _{z}\left ( \frac{\partial y}{\partial z} \right ) _{x}\left ( \frac{\partial z}{\partial x} \right ) _{y}=-1$$
well below is given my attempt assuming what i understood from your question . you are correct till $$x=p_c/p_l l$$ that means $$\delta x=p_c/p_l\delta l$$ after that we have to find a relation between $\delta p$ and $\delta x$ before the change and after the volume of the liquid remains constant . $$\pi d^2p-\pi d^2x=\pi d^2 ( p-\delta p ) -\pi d^2 ( x-\delta x ) $$ solve this and you shall get $\delta x=d^2/d^2 \delta p$ that is what i think the answer should be based on what i understood in your question .
a quasi-one-dimensional fermi surface is a fermi surface whose topology is the same as the topology of a surface defined by an equation that only depends on one dimension . in the picture above , taken from this paper , you see that the surface is effectively given by $$|k_x|\leq 1 $$ for the first picture . ( well , it should really be an equality if we talk about the surface itself but i wanted to discuss where the states are located , too . ) it looks like $k_y$ does not really qualitatively matter . that is different from ordinary two- or three-dimensional fermi surfaces given by inequalities such as $$\sqrt{k_x^2+k_y^2+k_z^2}\leq 1$$ because $k_y$ and perhaps $k_z$ play no qualitative role in the quasi-one-dimensional fermi surfaces , one may use various parts of intuition and formalism that are relevant for one-dimensional problems and just add the $y , z$ directions as dummy extra variables that do not affect anything qualitative . i believe – but i may be wrong – that it is confused to talk about " characters of fermi surfaces " . in the context of orbital hybridization , " character " refers to the bonds themselves ( the shape at the level of a single molecule ) , for example $sp^3$ has 25% s-character and 75% p-character . but when one talks about " character " of fermi surfaces , the word " character " just means some more general properties – any properties , unspecified properties , not something particular .
the action $s$ is not a well-known object for the laymen ; however , when one seriously works as a physicist , it becomes as important and natural as the energy $h$ . so the action is probably unintuitive for the inexperienced users - and there is no reason to hide it - but it is important for professional physicists , especially in particle and theoretical physics . the op 's statement that the hamiltonian corresponds to energy is a vacuous tautology because the hamiltonian is a technical synonym for energy . in the same way , one might say that the action intuitively corresponds to wirkung ( a german name ) because it is the same thing , too . because it now has two names , it becomes more natural :- ) and the op could also blame the energy for having " unnatural " units of action per unit time . in other words , the question assumes that energy ( and its unit ) is more fundamental and intuitive than the action ( and its unit ) - so it should not be surprising that using his assumptions , the op may also " deduce " the conclusion that the energy is more fundamental and intuitive than the action . ; - ) but is the assumption = conclusion right ? well , energy is intuitive because it is conserved , and the action is intuitive because it is minimized - so there is no qualitative difference in their importance . of course , the only difference is that non-physicists do not learn to use the action at all . the energy may be imagined as " potatoes " which every can do ; the action is an abstract score on the history that is only useful once we start to derive differential equations out of it - which almost no layman can imagine . if the laymen 's experience with a concept measures whether something is " intuitive " , then the action simply is less intuitive and there is no reason to pretend otherwise . however , physicists learn that it is in some sense more fundamental than the energy . well , the hamiltonian is the key formula defining time evolution in the hamiltonian picture while the action is the key formula to define the evolution in the nicer , covariant , " spacetime " picture , which is why hep physicists use it all the time . what the action is in general otherwise , the main raison d'etre for the action is the principle of least action , http://en.wikipedia.org/wiki/principle_of_least_action which is what everyone should learn if he wants to know anything about the action itself . historically , this principle - and the concept of action - generalized various rules for the light rays that minimize time to get somewhere , and so on . it makes no sense to learn about a quantity without learning about the defining " application " that makes it important in physics . energy is defined so that it is conserved whenever the laws of nature are time-translational symmetric ; and action is defined as whatever is minimized by the history that the system ultimately takes to obey the same laws . the energy is a property of a system at a fixed moment of time - and because it is usually conserved , it has the same values at all moments . on the other hand , the action is not associated with the state of a physical object ; it is associated with a history . there is one point i need to re-emphasize . for particular systems , there may exist particular " defining " formulae for the hamiltonian or the action , such as $e=mv^2/2$ or $s = \int dt ( mv^2/2-kx^2/2 ) $ . however , they are not the most universal and valid definitions of the concepts . these formulae do not explain why they were chosen in this particular way , what they are good for , and how to generalize them in other systems . and one should not be surprised that one may derive the right equations of motion out of these formulae for $h$ or $s$ . instead , the energy is universally defined in such a way that it is conserved as a result of the time-translational symmetry ; and the action is defined in such a way that the condition $\delta s = 0$ ( stationarity of the action ) is equivalent to the equations of motion . these are the general conditions that define the concepts in general and that make them important ; particular formulae for the energy or action are just particular applications of the general rules . in the text above , i was talking about classical i.e. non-quantum physics . in quantum physics , the action does not pick the only allowed history ; instead , one calculates the probability amplitudes as sums over all histories weighted by $\exp ( is/\hbar ) $ which may be easily seen to reduce to the classical predictions in the classical limit . a stationary action of a history means that the nearby histories have a similar phase and they constructively interfere with each other , making the classically allowed history more important than others .
i see two questions here . the first is why self-inductance is not considered when solving faraday 's law problems , and the second is why an emf can ever produce a current in a circuit with non-zero self-inductance . i will answer both of these in turn . 1 . why self-inductance is not considered when solving faraday 's law problems self inductance should be considered , but is left out for simplicity . so for example , if you have a planar circuit with inductance $l$ , resistance $r$ , area $a$ , and there is a magnetic field of strength $b$ normal to the plane of the circuit , then the emf is given by $\mathcal{e}=-l \dot{i} - a \dot{b}$ . this means , for example , that if $\dot{b}$ is constant , then , setting $ir=\mathcal{e}$ , we find $\dot{i} = -\frac{r}{l} i - \frac{a}{l} \dot{b}$ . if the current is $0$ at $t=0$ , then for $t&gt ; 0$ the current is given by $i ( t ) =-\frac{a}{r} \dot{b} \left ( 1-\exp ( \frac{-t}{l/r} ) \right ) $ . at very late times $t \gg \frac{l}{r}$ , the current is $-\frac{a \dot{b}}{r}$ , as you would find by ignoring the inductance . however , at early times , the inductance prevents a sudden jump of the current to this value , so there is a factor of $1-\exp ( \frac{-t}{l/r} ) $ , which causes a smooth increase in the current . 2 . why an emf can ever produce a current in a circuit with non-zero self-inductance . you are worried that emf caused by the circuit is inductance will prevent any current from flowing . consider the planar circuit as in part one , and suppose there is a external emf $v$ applied to the circuit ( and no longer any external magnetic field ) . the easiest way to see that current will flow is by making an analogy with classical mechanics : the current $i$ is analogous to a velocty $v$ ; the resistance is analogous to a drag term , since it represents dissipation ; the inductance is like mass , since the inductance opposes a change in the current the same way a mass opposes a change in velocity ; and the emf $v$ is analogous to a force . now you have no problem believing that if you push on an object in a viscous fluid it will start moving , so you should have no problem believing that a current will start to flow . to analyze the math , all we have to do is replace $-a \dot{b}$ by $v$ in our previous equations , we find the current is $i ( t ) = \frac{v}{r} \left ( 1-\exp ( \frac{-t}{l/r} ) \right ) $ , so as before the current increases smoothly from $0$ to its value $\frac{v}{r}$ at $t=\infty$ .
as you note , this is really a question about cosmology . there is a lot to say , but in the interest of brevity i will just address a part of the question . quantum chromodynamics is a strongly-coupled theory with a very complex vacuum state . the universe is not in the qcd vacuum today , but is pretty darn close to it , since the 2.7k background temperature is way below the mass gap of qcd . at high temperature qcd is a lot simpler -- the quarks and gluons are weakly interacting ( asymptotic freedom ) . the state of high temperature qcd is fairly simple ( though not trivial ) ; it is approximately an ideal gas of relativistic particles . as the universe expanded and cooled adiabatically it passed through a ( first order ) phase transition from the " easier " high-temp state to the " harder " low temp state . just below the phase transition the state was still pretty hot , but as the universe continued to expand adiabatically the state approached the qcd vacuum . of course , this would not have worked if preparing the qcd vacuum were qma hard ; the universe would have gotten stuck is some long-lived metastable state . that apparently did not happen , so we have cosmological evidence that preparing the qcd vacuum is easy , even if jlp have not proved it ( yet ) . you can ask further questions about how the hot initial state was prepared , why it was so homogeneous and isotropic , etc . which would lead us into a discussion about how cosmic inflation started and ended , but that is enough for now .
if $\phi$ is a function of $t$ , then $x$ , for example , is written as $$ x ( t ) =r\cos ( \omega t ) +\ell\sin ( \phi ( t ) ) . $$ applying the chain rule gives $$ \begin{align} \frac{d}{dt}x ( t ) and =\frac{d}{dt}r\cos ( \omega t ) +\frac{d}{d\phi}\ell\sin ( \phi ) \frac{d}{dt}\phi ( t ) \\ and =-r\omega\sin ( \omega t ) +\ell\cos ( \phi ) \dot{\phi} . \end{align} $$
if one calculates it to all orders , it should not . in particular , if the coupling constant is small , the $o ( g^k ) $ accuracy calculation of the cross section obtained in different schemes can differ at most by $o ( g^k ) $ , negligible amounts . however , when one truncates the cross section at some order , the results may really depend on the renormalization scheme by subleading , higher-order terms . physically , cross sections are measurable so a valid theory must predict unequivocal values for it . and a renormalization scheme does not really " change the theory " .
the excesses have looked convincing to many people but they do not look convincing anymore . last october , lux in south dakota presented the results of their superior analysis http://motls.blogspot.com/2013/10/fiat-lux.html?m=1 http://motls.blogspot.com/2013/10/dark-matter-wars-are-over-lux-safely.html?m=1 which safely excluded the theories of wimp dark matter directly suggested by cdms ii si and other experiments . the complete and pure absence of a signal in lux shows almost certainly that the excesses in cdms ii si and other experiments were due to some overlooked background ( non-dark-matter-related ) processes . wimp is still plausible and attractive as a model of dark matter . but there is no evidence one way or another which is why physicists keep on studying it and other models as well , including e.g. those of the sterile neutrino dark matter or axions .
think about this with an example : the sine and cosine functions . they both average individually to zero over an interval . you can multiply those averages and still obtain zero . but if you multiply sin by itself and then average , you get a very distinct non-zero result . when the functions are arbitrary , the average of the product quantifies statistical correlation between the two functions/variables . this correlation gives a rough measure of how causally connected are both of the variables . this correlation information is completely lost when one takes the averages of the functions individually .
new answer what you have done here is just dimensional analysis . but you have gone a little too far . in particular , just because two things have the same dimensions does not mean that they are equal . if you want to expand $f/a$ a little more , you can choose your favorite from \begin{equation} f = \frac{d p}{dt} = \frac{d}{dt} ( m\ , v ) = m\ , \frac{d}{dt} v = m\ , a \end{equation} here , $p$ is the momentum , $m$ is some amount of mass you are keeping track of , $v$ is its velocity , and $a$ the acceleration . now , pressure is really defined as being the force on a unit of area . so if you do not choose an area , you can not define pressure . there might be situations where there is some momentum change in a volume without the matter directly hitting a surfce . for example , with an electromagnetic force . so there could be a net force on a volume . but again , you can not call anything a pressure unless you select an area to measure it . old answer ( from before reading your clarifications ) pressure is the force per unit area applied perpendicular to the surface of an object . ( or at least the area over which you are imagining measuring the pressure . ) if i understand you correctly , you are interested in the pressure on the surface of the tube . is this correct ? but you are referring to the mass flowing past the tube . that is , it is flowing parallel to the surface . so , in your simplified model where the matter is flowing straight , there would be zero pressure . of course , in real life , the matter would probably having some internal motions as well , which means it would bounce off the walls of the tube , exerting pressure . in short , we would need more information to calculate the pressure . for your particular setup , you still have not told us about any interactions between particles of the matter . from what you have told me , i can still assume that the matter particles all have the same velocity ( but are scattered at different positions ) . now , as you know , force is proportional to acceleration . since the velocity is constant , there is no acceleration , so there must be no force , which means there is no pressure .
a spatial fourier transform means a fourier transform in the spatial variable ( $x\rightarrow k$ ) , while a temporal fourier transform is the same transformation , but in terms of the time variable ( $t\rightarrow \omega$ ) . the equation you have written is the ( asymmetric ) temporal fourier transform of $f ( k , t ) $ . the spatial transform looks like some variation of \begin{equation} f ( k , t ) = \frac{1}{2\pi} \int g ( x , t ) e^{i k x} dx \end{equation} where $g ( x , t ) $ is the ( one-dimensional ) van hove function .
for inflation the potential energy of the field dominates the kinetic energy $\dot{\phi} \ll v ( \phi ) $ this limit is referred as slow roll and under such conditions the universe expands quasi exponentially $a ( t ) \propto \exp \left ( h dt\right ) = e^{-n} $ where we define the number of e-folds $n$ as : $dn = -h dt$ so that $n$ is large in the far past and decreases as we go forward in time and as the scale factor $a$ increases . with this we have : $\epsilon = -\frac{\dot{h}}{h^{2}} = \frac{1}{h}\frac{dh}{dn}$ accelerated expansion will only be sustained for a sufficiently long period of time if the second time derivative of $\phi$ is small enough : $|\ddot{\phi}| \ll |3h\dot{\phi}| , |v' ( \phi ) |$ so that the equation of motion for the scalar field is approximately : $3h\dot{\phi} + v' ( \phi ) \simeq 0$ this condition can be expressed in terms of a second dimensionless parameter , defined as : $\eta \cong -\frac{\ddot{\phi}}{h\dot{\phi}} \cong \epsilon + \frac{1}{2\epsilon}\frac{d\epsilon}{dn}$ then $\eta \simeq \frac{1}{8\pi g} \left ( \frac{v'' ( \phi ) }{v ( \phi ) } \right ) $ in the slow regime $\epsilon , |\eta|\ll 1$ , where the last condition ensures that the change of $\epsilon$ per e-fold is small . notice that $\eta$ need not be small for inflation to take place . inflation takes place when $\epsilon &lt ; 1$ , regardless of the value of $\eta$
yes , the photons actually reach you , like rain falling on you , not like watching rain from a distance . when you see a star , photons from the star actually enter your eye . in for example rods of your eye , the photon causes a molecule of retinal to react by change from cis to trans isomer .
the answer by jkl is sufficient but i want to address particularly the why ? . i am so confused . why does it act the way it does ? if one reads a bit about the history of science and physics in particular , it becomes clear that physics at the ultimate end does not answer the ultimate why ? . physics posits laws and uses sophisticated mathematical tools to theorize from axioms , get equations , and check against the data experimentally . it finds how , from axioms one ends with predictions for measurements that validate them . the why ? questions addressed to the axioms has the only answer : because . when one is validating a theory , as for example newton 's gravitational theory , and one hits a disagreement with the data , new axioms and new theoretical tools are developed to explain the why of the disagreement and the new theory validated for the regime of disagreement . special and general relativity are an example . the history of physics has other examples : thermodynamics , developed theoretically to explain bulk behavior , statistical mechanics , out of classical mechanics emerge out of asking how and assuming axioms to contain the why . each theory with its own regime of validity . within a theory a question with a why is answered by proofs of how finally hitting on the axioms . quantum mechanics is the last in the series of exploring the microcosm . the why you are asking hits against the because of its axioms . there are people who continue the exploration , trying new axiomatic theories of how a quantum mechanical theory can emerge from a smaller regime where we are back to classical concepts , and contain the why in axioms for their new theory . they are not successful except with small models . the bulk of theoretical physicists either ignores their efforts or proves that their new theories would violate a basic validated law as , for example , lorentz invariance . and that is the story of why in physics . ultimately the answer is because .
it is not possible that susy particles are hiding in kev or mev range . in particular , there can not be any new charged particles ( and similarly new color-charged particles ) that would be this light because they would be easily pair-produced and easily detected . the first ( february 2012 ) claims by different authors ( the original ones , rupp and van beveren , who made the conjecture ) were refused by the compass collaboration ( which was used as one of the main pieces of " evidence" ) here : http://arxiv.org/abs/1204.2349 compass says that the patterns that attracted the attention or rupp and van beveren are due to $\pi^0$ , $\eta$ , and secondary interactions in the compass spectrometer . rupp and van beveren responded that the compass critique is internally inconsistent . it seems more likely to me that compass is right . the newest russian experimental paper looks strange to me . for example , it never quotes any confidence levels , as far as i can see , and instead says that there are " almost no errors " in their measurement , a claim that it easily refuted by looking at their chaotic wiggly charts . an extended discussion may be found on my blog .
the classically forbidden region coresponds to the region in which $$ t ( x , t ) =e ( t ) -v ( x ) &lt ; 0$$ in this case , you know the potential energy $v ( x ) =\displaystyle\frac{1}{2}m\omega^2x^2$ and the energy of the system is a superposition of $e_{1}$ and $e_{3}$ . this should be enough to allow you to sketch the forbidden region , we call it $\omega$ , and with $\displaystyle\int_{\omega} dx \psi^{*} ( x , t ) \psi ( x , t ) $ probability you are asked for ,
the surface $\mathrm{s}$ does indeed contain charge $q_1$ , and so will have nonvanishing electric flux . however , $\mathrm{s}$ does not contain charges $q_2$ and $q_3$ , so it will have zero total flux due to those charges .
the electromagnetic processes between atoms and molecules in all phases , solid , liquid , gas , depend on what is generically called " van der waals " fields and subsequent forces . it is well known that the atoms/molecules are neutral , nevertheless there exist for all matter dipole and quadrupole and higher order fields which are mainly attractive and form the chemical bonds which is the way neutral atoms and molecules can bind into solids and liquids and interact as gases . these bonds are quantum mechanical , that means that there exist solutions of the schrodinger equation with energy levels from ground state to continuum , one can model them as repeated over all the mass of the solid , liquid and gas . the unfilled energy levels are close to each other in energy and the continuum of n=infinity ( the radial quantum number ) . at the same time the atoms and solids have pure kinetic degrees of freedom : they can vibrate and rotate in solids , they can move in two dimensions in liquids and in all three dimensions in gases . in gases simple scattering of the molecules transfers the kinetic energy of one molecule to the potential energy of another , i.e. raises an electron to a higher level . the electron goes back to its ground state releasing a specific photon , or a cascade of photons , depending on the energy . remember that the higher levels with respect to n , the radial quantum number , are closely packed . these photons are the ones emitted as black body radiation , and they are a continuum because of the 10^23 molecules per mole and the almost continuous energy levels . the temperature is a function of the average kinetic energy in the gas , the higher the temperature the more energetic the kinetic scattering and the higher the average photon energy . in a solid there are also vibrational and rotational kinematic degrees of freedom that are contributing to the average kinetic energy , i.e. temperature . the kinetic energy of the molecules becomes potential energy for an electron in the lattice which then decays to its ground state or through cascades . the logic is the same as for gases and the same holds for liquids that have some extra kinematic degree of freedom with respect to solids . so it is the quantum mechanica behavior of matter at the micro level which is responsible for the black body radiation , and the infrared catastrophy problem of the classical extrapolations was solved . it is the energy levels that make a difference between infinity and well behaved property in the electromagnetic emmissions . thus the average kinetic energy ( proportional to t ) diminishes by turning into electromagnetic emissions through the stepping of energy levels . does that mean in rarefied conditions where the mean-free-path is relatively large , the rate of ir emissions decreases ( while the intensity is still only dependent on the temperature ) ? when the mean free path is large , the temperature is lower , the average kinetic energy of the atoms is lower and thus the photons produced by the transformation of kinetic to exciting electrons to higher potential energy levels and consequent decay to ground energy levels are all lower and will keep getting cooler if the energy is not replenished . i do not know what you mean by the intensity . this is the black body radiation spectrum . as the temperature decreases , the peak of the black-body radiation curve moves to lower intensities and longer wavelengths . the black-body radiation graph is also compared with the classical model of rayleigh and jeans . if you mean gases in low pressure as at the top of the atmosphere etc , one has to study them separately according to the boundary conditions . there can be gases with very high temperatures as in the atmosphere of the sun .
charging , as nibot said , happens because the " balance of charge " is altered . if you have a neutral iss , and you assume that no electrons are added or removed , the iss will stay neutral . this can of course change , and the iss can charge , if electrons are removed or added due to the exposure to ionizing radiations . however , you are right to say that if a metal moves in a magnetic field , you obtain a charge separation , and consequently a voltage . the same concept is used in any dynamo to produce electricity . there was indeed an experiment using this concept : http://en.wikipedia.org/wiki/electrodynamic_tether http://en.wikipedia.org/wiki/tether_satellite it was deployed twice with little success , but it did produce a ( small ) voltage , and therefore create a current .
"2 . where did they even get 2sa+sb=0 from ? " from the assumption that the length of the rope does not change . " why did they determine the change in distance this way ? my first assumption was that if block b moved up 2ft , then block a should move down 2ft ( the rope must " move " 2ft too right ? ) . " no , there is a difference between a movable pulley and a fixed pulley , so a moves down 1 ft . "3 . where did 2va=−vb come from ? " from the same assumption as above . however , the direction of va is shown incorrectly ( or , alternatively , the signs in the formula are wrong ) . "4 the fbd for block a is confusing , why is the friction force fa in the direction of the ropes ? i thought it was block b that is going down ? " it is writen in the statement of the problem that block b went up . " am i the only one who had trouble deducting that the pulley and block a are the same object ? " i do not quite understand what this phrase means exactly and how it is relevant . "5 . look at the final answer , how could vb be negative ? the problem says block b goes up . " see the answer to your item 3 .
please do however remember the following line from the link you yourself have cited : in general , if the behaviour of a system of more than two objects cannot be described by the two-body interactions between all possible pairs , as a first approximation , the deviation is mainly due to a three-body force . hence , it can be seen that , initially , when people were thinking of many-body problems , they encountered terms in the mathematical formulation which they later termed as many body forces . incidentally , they were discovered in strong interactions and were a result of gluon mediation . in the celestial scale hence , you would need to have a similar mediating phenomena/theory to explain any physically valid three body force . also related : n-body forces in classical mechanics
if a crystal has a discrete group of point symmetries then the electronic eigenfunctions will be suitably invariant under that group . formally , the symmetry requires that the eigenfunctions of a hamiltonian with symmetry group $g$ belong to the various representations of that group . in the abstract , a representation of a group $g$ is a vector space ( in this case a subspace of degenerate energy ) $v$ and a " recipe " for unitarily transforming the vectors in $v$ with the transformations from $g$ , in the form of a group homomorphism $r:g\rightarrow u ( v ) $ . if one knows the structure of a group ( in the form of its multiplication table ) then there is a lot that can be said about its possible representations , which are typically denoted by some standard notation ( e . g . $e$ , $a$ , $b$ , etc . ) . the wavefunctions are then labelled by the representation type of the subspace they belong to . to bring this back down to angular momentum , the subspaces with different $l$ are the different representation subspaces . the group in question is the rotation group $\text{so} ( 3 ) $ . it has an infinite family of representations of increasing finite dimension , and the index $l$ that labels them is precisely the angular momentum quantum number of those wavefunctions . in group theoretic terms , then , " having definite angular momentum " simply means " belonging to a suitable representation of $\text{so} ( 3 ) $" . thus a crystal with a point symmetry will have electronic eigenfunctions that do have " definite crystal angular momentum " , in the sense that they belong to a certain representation of the point group . added in response to comment : unfortunately , there is no physical quantity that corresponds to this symmetry . this is due to the general fact that discrete symmetries have no generators . while you can write rotations , for example , in the form $e^{i\mathbf{j}\cdot\hat{\mathbf{n}}\theta}$ , where $\mathbf{j}$ is the generator , this is not really meaningful for discrete symmetries . a good comparison for this is parity : if $\pi$ commutes with $h$ then we say parity is conserved , in the sense that the transformation itself is a constant of the motion . for a more general discrete group $g$ ( instead of $g=\{-1,1\}$ for parity ) then the labels $+$ and $-$ are replaced by the group representation . similarly the labels $l$ and $m$ correspond to the group representation and to the eigenvalues of some particular group transformation . both are conserved under $h$ , but there is no generator .
the light from distant galaxies does undergo a frequency change . it is red shifted , and the amount of red shifting is used to work how fast the distant galaxy is receding and therefore how far away it is . however this is not a damping effect . the light red shifts because the spacetime in between us and the distant galaxy is expanding , so although the light 's energy is conserved it is spread over a larger distance . you ask why light is not damped , but why should it be ? since energy is conserved the light wave can not lose energy unless there is some mechanism to carry the energy away . for a light wave travelling in vacuum there is simply no mechanism by which it can lose energy , so it does not . in your last paragraph i think you are getting mixed up with a different effect . when a terrestrial radio station broadcasts it sends the radio waves out as a half sphere ( the half above the ground ) . as you get further away from the transmitter the field strength of the radio waves decreases as the inverse square of the distance because the energy of the transmitted wave is spread over a larger area . however the total energy of the light wave is conserved . this obviously happens with distant galaxies as well because the more distant a galaxy is the fainter it appears . this is not due to damping , it is just the inverse square law dependance of the field strength .
this is a notation from differential geometry that is unusual in physics ( as far as i am aware ) . the operators $\frac{\partial}{\partial y}$ and $\frac{\partial}{\partial z}$ are basis vectors for vector fields ; see e.g. this planetmath article for an introduction ( in particular where it says " in some sense " :- ) if you do not want to worry about differential geometry , you could write the field in good old-fashioned vector notation as $$ e=\pmatrix{0\\e_y ( t-x ) \\e_z ( t-x ) }\ ; , \quad b=\pmatrix{0\\b_y ( t-x ) \\b_z ( t-x ) }\ ; . $$
it is the curvature of a connection on a principal u ( 1 ) bundle over parameter space . in describing the quantum hall effect , we have a hamiltonian , which depends on a number of parameters $h ( r_1 , r_2 , . . r_n ) $ . suppose we have the system in its ground state . we now vary the parameters adiabatically ( slowly ! ) . as we vary the parameters , we can think of tracing out a curve $ ( r_1 ( \lambda ) , ( r_2 ( \lambda ) . . . ( r_n ( \lambda ) ) $ in parameter space . as we twiddle the parameters we actually evolve the state using the schroedinger equation . if we transport it round a closed curve in parameter space , i.e. we return to our starting parameters , we find that the state picks up a phase factor relative to the starting state . ( phase factors live in $u ( 1 ) $ the group of unit modulus complex numbers ) . the mechanism that allows us to go from a state at one set of parameters to a state at another set is called a connection . in this case the connection is provided by the schroedinger equation . saying that the connection has curvature just means that transport of a state round a closed curve using this connection does not quite get you back to the state you started with ( in fact mathematically the object which defines the curvature is obtained by transport round a little closed parallelogram ) .
another proposal involves putting large pipes into the middle of the oceans ( tropical regions ) and pumping cold water from the depths to the surface . here is one paper that analyzes it . i will try to summarize . this is a very multifaceted idea , so i am going to try to keep to pure physics as well as i can . firstly , what is the engineering involved with this ? in order to pipe cold water from deep down to the surface we would have to pump it against the density gradient . hot water in this temperature range is less dense , so it naturally hangs around the surface . this would take energy and very large pipes . the good news , however , is that the conditions in the ocean make this a little easier in several ways . the construction and placement of mile-long ( or so ) pipes would not be that impractical , as you would have a floating thing holding them up and they would just hang there . the pumping could actually be done by the waves themselves , and it moves the pipe up and down . for this deployment , you would mainly require some one-way valve that allows the water to travel up the pipe but not down . this would not be all that difficult . so we have established we can engineer and build these things that would make the ocean surface cooler . studies also seem to indicate that the capital cost would not be prohibitive . ask it of your engineers and they should be able to make it . next , how would this affect climate ? several ways . the decreased surface temperature of the ocean would cause it to be a large carbon sink . the ocean has already demonstrated a measurable increase in ph and a decrease rate of co2 absorption , which is a natural consequence of increasing concentration . decreasing the temperature would cause it to suck up more . the decreased surface temperature would reduce atmospheric temperature . this is a pretty obvious impact , and the paper i reference notes that this could be the most major immediate effect . i would change the ocean life dramatically . obviously , since phytoplankton are one of the most major photosynthesizers in the world , changing their environment would change the rate of co2 capture by the environment . how ? i have no idea . i am not sure the context or motivations in which researchers propose and discuss such ideas , but i have heard some arguments for geo-engineering proposals in general , of which this proposal is one . for one , the idea is entirely doable as i have argued . it could decrease the temperature of the earth and it could be done with our resources today . if the situation on earth became so perilous to human life , it is likely that politicians would order such a thing . with that established , the motivations become awfully warped . many people argue that quantifying the effect of geo-engineering in the future could motivate current negotiations for emissions mitigation to take the necessary steps . almost all geo-engineering proposals have a temporary effect , so after implemented , they risk a return to an even hotter climate , and possibly deleterious effects due to the solution itself that could be even worse than global warming itself . to add my own personal thinking , it is obvious that we are not reducing emissions today and will not in the conceivable future , so if one believes climate change will significantly impact life , then it is very likely that some form of this geo-engineering will be the future , whether we like it or not . here is a picture : http://www.popsci.com/node/9798 ( illustration credit to graham murdoch of popular science , believed to be fair use )
temperature certainly affects semiconductors . as someone who is not specialized in semiconductors , i can think of at least two microscopic effects at play here . the first is that at lower temperatures there are less phonons ( quantized vibrations of the atomic lattice ) . the effect of phonons is to scatter the electrons and lower the conductance of the semiconductor . thus at lower temperatures the conductance should be increased ( as in the case of metals ) . the second effect is that as the temperature is lowered , the fraction of electrons on high energy states is lowered ( and on low energy states increased ) thus potentially changing the amount of electrons electrons above/below the band gap ( carrier concentration ) . the effect of this is more complex than the effect of phonons .
yeah , the resolution to this question is if you use a billiard ball type interpretation for atoms and light . in vacuum $c$ is always the same , so in between interactions with atoms , light travels at $c$ also . the " effective speed " noted above is a result of the decay time for excited states of atoms . so the light balls move between the atoms at $c$ and are held onto by the atoms for some time $\tau$ before released again . thus light obtains an effective speed in a medium as a result of the finite excitation time for atoms in a medium . if you apply this situation at the interface of a medium and vacuum , the question becomes null . a change to the quanta perspective has these sort of perks . i hope this helps .
you have probably learned about a quantity called " impulse " - try using that to solve the problem . let $j$ be impulse , defined for constant forces as $j = f t$ where $f$ is the force applied and $t$ is the time for which the force is applied . since $f=ma$ , we can substitute this to get : $$\begin{aligned} j and = ft \\ j and = mat \\ j and = m ( at ) \\ j and = m \delta v \\ j and = \delta ( mv ) \\ j and = \delta p \end{aligned}$$ that is , $f t$ is equal to the change in momentum of the cart . since both carts have the same force applied to them for the same amount of time , the momentum change of both carts must be equal . for part ( b ) , consider conservation of energy . the force does a certain work on the cart , which causes the cart to gain kinetic energy . to find the work done on each cart , simply find the increase in kinetic energy of each one . try this part by yourself , and comment if you need further assistance .
i am not sure if this is exactly what you are looking for or perhaps you already know what i am about to say . there is a geometric notion of a twistor spinor ( or conformal killing spinor ) : one which is in the kernel of the penrose operator ( see below ) . then one defines the twistor space as the projectivisation of the space of twistor spinors . doing this for minkowski spacetime recovers the usual twistor space . let $ ( m , g ) $ be a riemannian spin manifold . ( when i say riemannian i include also the case of a metric with indefinite signature . ) let $s$ denote the complex spinor bundle . the spin connection defines a map $$ \nabla : \gamma ( s ) \to \omega^1 ( s ) $$ from spinor fields to one-forms with values in $s$ . now $\omega^1 ( s ) = \gamma ( t^*m \otimes s ) $ and clifford action of one-forms on spinors gives a map $$ \omega^1 ( s ) \to \gamma ( s ) $$ the composition of the previous two maps is the dirac operator . the penrose operator is in some sense the complement of the dirac operator $d$ . the kernel of the clifford map $t^*m \otimes s \to s$ defines a subbundle $w$ , say , of $t^*m \otimes s$ . composing the covariant derivative with the projection $\omega^1 ( s ) \to \gamma ( w ) $ defines the penrose operator $p : \gamma ( s ) \to \gamma ( w ) $: explicitly , $$ p_x \psi = \nabla_x \psi + \frac1n x \cdot d\psi $$ for all vector fields $x$ and spinor fields $\psi$ , and where $n = \dim m$ . ( my clifford algebra conventions are $x^2 = - |x|^2$ . ) notice that the " gamma trace " of the penrose operator vanishes . there is a sizeable literature on twistor spinors mostly in riemannian and lorentzian signatures . this is the work of helga baum and collaborators in berlin . a search for " twistor spinors " in mathscinet should give you many links . one important property of the twistor spinor equation is that it is conformally invariant , whence the twistor spinors of conformally related riemannian spin manifolds correspond in a simple way . since you mention maximally symmetric lorentzian manifolds , this observation might be of use because such spaces are conformally flat , hence you can write down the twistor spinors simply by rescaling the twistor spinors in minkowski spacetime . in riemannian signature ( hence for round spheres and hyperbolic spaces ) this is described in the 1990 humboldt university seminarberichte twistor and killing spinors on riemannian manifolds by baum , friedrich , grunewald and kath , later published by teubner .
edit : i am not sure what specifically you are after . i will explain a little more why it is difficult to give you a number . it is also quite possible that the number you are seeking might not be what you think it is . . . . the decay would be similar to any plasma type reaction for a mercury-vapor gas . there is a delay between absorption and re-emission of light photons and a typical electrical system driving the energy is not going to stop instantly . in fact the dominate feature in the decay rate will probably be due to the slow discharge of the ballast system driving the gas . the common household circuit lighting up a tube usually has a large transformer with a big magnetic field to step up the voltage and resonate the gas to force the plates to conduct through the ionized gas and light up . it takes a considerable amount of time for this electrical system to discharge and while it does it is going to continue to drive energy to the lamp causing it to light and effecting your " decay " rate significantly . here is a 12v lamp driver just for you to see what is involved electrically : note : the large cap ( 0.047uf ) and the transformer are going to resonate the lamp energy much longer than the actual natural decay rate of the gas . in addition the 47uf cap is going to supply power to the circuit for a non-trivial amount of time after the 12v is removed . for comparison here is an 120v 60hz ballast design and you will see there are similar issues . if you want to look at just the gas decay rate then you might have to excite it with a laser for more precise measurements . the states that the energy moves through is usually measured and plotted on a jablonski diagram like this : in your case $\tau=\frac{1}{k}$ where k is equal to ( in the de-excitation case ) $k = k_f + k_i + k_x + k_{et} + …= k_f + k_{nr}$ where $k_f$ is the rate of fluorescence , $k_i$ the rate of internal conversion and vibrational relaxation , $k_x$ the rate of intersystem crossing , $k_{et}$ the rate of inter-molecular energy transfer and $k_{nr}$ is the sum of rates of radiationless de-excitation pathways . and a more detailed model can be measured : the method for measuring a gas properly looks like : here is an example of what the data looks like . much of this information was taken directly from the research documented here : www.jh-inst.cas.cz/~fluorescence/support/lectures/ufch_fluor03.pps‎
you are halfway there , because you already wrote down the hadamard gates . the remaining part , $i-2\left|0\right&gt ; \left&lt ; 0\right|$ ( i negated it from what you have , since this gives a simpler solution ) , is diagonal in the computational basis . write down these diagonal entries and say out loud to yourself what this operator " does " .
the majority of the energy is dissipated in the travel through the air from the cloud to the ground . the energy goes into heating the air and generating the shockwave that we hear as thunder . i can not give you a single definitive refernce for this , but googling " energy dissipation lightning " will find lots of relevant articles . you can understand why this is because the energy dissipated by a current $i$ travelling through a resistance $r$ is given by $w = i^2r$ . in a lightning strike the current is constant , because the charge flowing in one end has to flow out the other end , so the power dissipated is proportional to the resistance . the resistance of air is a lot higher than the resistance of the ground/tree/person or whatever the lightning hits , so the majority of the energy dissipation is in the air . the electrons flowing from the cloud through the lightning bolt end up in the ground , but with an energy only slightly greater than ambient . they will presumably flow into the surrounding area until the potential difference around the point of strike falls to effectively zero . this is likely to be within a few metres , so they would not get anywhere near the earth 's core .
light has a dual nature , one of photons and the other of waves . but energy does not really travel in waves . so what do the wave represent ? let us be clear in our terminology and the domain to which we apply it . our everyday life is lived with classical mechanics and classical electricity and magnetism ( as long as we do not use the net and transistors and the other paraphernalia of modern life ) . classical theories are well developed mathematically and are applicable in the domain where hbar can be considered practically zero . particularly for light maxwell 's equations have been validated in this domain and describe light as a wave . this wave is a propagating changing electric and magnetic field and is a wave in the four dimensional space time , there are peaks and valleys . it displays the classical wave behavior of interference and dispersion . these carry energy , a universal example is simple sunlight . now you use the word photon . the photon is one of the elementary particles of the standard model . this means that we are no longer in the classical domain but in the quantum mechanical domain when we speak of light as a collection of photons . it means that it behaves sometimes as a particle , ( photoelectric effect ) i.e. it has a specific ( x , y , z , t ) value and sometimes as a probability wave , i.e. according to a quantum mechanical wave function the square of which gives the probability of finding a photon at a specific ( x , y , z , t ) which probability has a wave like variation in space because it is the solution of a wave type differential equation describing the dynamics of the situation . note , it is the probability which shows a wave nature , not the " particle " itself . now the wave nature of maxwell 's equation and the wave nature of the probability function are mathematically reconcilable , so that the frequency of the classical wave is the nu in the energy of the photon in e=h*nu . in addition lubos motl has an interestin article about the collective emergence of classical light from photons in his blog , though it needs a background in physics to understand it .
the primary factor that makes asteroids unstable is resonance overlaps with jupiter , which is why there are many gaps in the distribution of asteroid periods . far from resonances , it should not make much difference whether the object in question is interior or exterior to jupiter . sussman and wisdom ( 1988 ) showed that the lyapounov time for pluto is ~20 myr . lasker ( 1988 ) found the typical lyapounov time for the inner planets to be ~5 myr , but considering their much shorter orbital period , i would argue this shows them to be more stable . also , while they are all technically chaotic , they are also bounded by stability regions , like pluto 's 3:2 resonance keeping it from close encounters with neptune . this is best expressed by murray and dermott ( 1999 ) : " the solar system is stable and it is likely that the motions of the planets represent another case of bounded chaos . "
i did some hunting and followed the paper trail to cook et al . ( 1995 ) . in section 4 , they identify a class of stars that brighten aperiodically . they reckon that these " blue bumpers " are be stars : b-type stars that show strong emission lines . then again , this is off one paper and i am really not sure if this is a widely accepted view , but i imagine there would've been more fanfare if this was an exciting new class of object ? i am not massively learned on the subject of be stars , but the current consensus seems to be that they are rotating very rapidly , to the extent that there is a substantial circumstellar disk of material around the equator . be stars are themselves a type of shell star , all of which are known to show aperiodic variations , presumably because of the unstable nature of the system . in fact , the whole class seems to form an observational problem because of their variable nature and light from the star getting mixed up with the disk .
a square has its diagonals at right angles . so , find the forces along each diag . i.e. , m1 and m3 on m5 for one direction and m2 and m4 on m5 for other . you could actually simpy find difference between , " m1 and m3" and " m2 and m4" and the corresponding charges and use them as resultant mass and charge .
they are looking at s-wave solutions , ie zero angular momentum . they are also looking at modes that are pure tensor with respect to the schwarzchild geometry ( ie they set the vector and scalar parts to zero ) . so the angular parts of the perturbation are just proportional to the angular parts of the background metric ( in a sense the angular parts are proportional to the identity , because the angular momentum is zero ) . the interesting parts of the perturbation then are only allowed to involve $r$ and $t$ , and this is the most general form for the perturbation that mixes $r$ and $t$ but leaves the angular parts alone . the instability comes from the fact that there are solutions that are regular at infinity that blow up at the horizon . see the discussion after equation 11 . there are many tricks to find instabilities . the methods used in the gregory-laflamme paper are certainly valid , but their methods are by no means the only nor the most common ones . generally speaking , the question is whether the split into background + perturbations is a good one in the sense that the perturbations remain small . the perturbations might fail to remain small for many reasons--you might have a runaway potential that becomes infinitely negative ( hamiltonian unbounded below ) or at least pushes you away from your chosen background ( tachyon instability ) , you might have negative kinetic energy ( ghost instability ) , you might have negative gradient energy ( gradient instability ) --there are lots of different ways for instabilities to manifest themselves . here , the method is essentially brute force : numerically construct solutions and show explicitly that there are valid solutions to the perturbation equations where the perturbations become infinitely large .
the op has calculated the $\theta _{air}$ correctly . to calculate the angle in the presence of water , we have : $$2\tan{\theta}+ 2\tan{\phi}=0.1$$ using small angle approximation : $$\theta+\phi \approx \frac{1}{20}$$ however , from the snell-decartes law ( as demonstrated by the op ) we have : $$\frac{\sin{\theta}}{\sin\phi} \approx\frac{\theta}{\phi}\approx \frac{4}{3} \\ \rightarrow \phi \approx \frac{3}{4}\theta \\ \rightarrow \frac{7}{4}\theta=\frac{1}{20} \rightarrow \theta=\frac{1}{35}$$ on the other hand , we have $\theta_{\text{air}}=\frac{1}{40}$ ; so the answeer would be : $$\frac{\frac{1}{35}}{\frac{1}{40}}=\frac{8}{7}$$
as several of us have argued beneath your other question here : special relativity version of feynman 's " space-time approach to non-relativistic quantum mechanics " quantum mechanics combined with special relativity needs one to use quantum field theory ( or a theory that is stronger than that ) . if one wants to study relativistic particle in the presence of slits , one may imitate it simply by putting various ( absorbing ) boundary conditions ( for the fields whose excitations are the interfering particles ) at the places where the boundaries of the material reside . the probability waves in space propagate according to the standard free equations , so interference measures the free particles ' propagators . in reality , the only relativistic particles whose interference may be observed in practice at present are photons . the mathematics of interference of individual photons is equivalent to the interference of classical electromagnetic waves - as they studied it since the early 19th century . the probability density is mapped to the energy density of an electromagnetic wave and the $e , b$ fields may be interpreted as the photon 's wave function . the results for the interference patterns are , up to a normalization that can be determined from the total number of particles , identical to the case of the classical electromagnetic waves . so papers for photons exist and the first ones were written in the early 19th century . papers for other particle species do not exist because they are simple sums of the well-known functions that govern the propagator of the corresponding probability waves in the space ; for example , one only needs the simple , free one-particle dirac equation to calculate the propagation of the electrons at any speeds the interference can only be measured for photons whose mathematical description is a very old story . for example , relativistic electrons must have a wavelength comparable to the compton wavelength of the electron , something like $10^{-12}$ meters , or even shorter ( ultrarelativistic electrons ) . the corresponding inteference pattern would probably be too tiny to be seen cheers lm
you have several questions . let 's take them one by one : what is teleported ? in continuous variable ( theoretical ) papers , position and momentum are just convenient denominations for any pair of conjugate continuous variables , in the same way than a qubit can be a 2-level atom , a polarized single photon or a spin-1/2 particle . if a quantum object has continuous unbounded degree of freedom , it behaves like the position ( along a given coordinate axis ) , and there exists a complementary observable which behave like momentum . current experimental realizations include the quadrature of the light , the polarization of a bright beam , collective spin of atomic clouds , etc . therefore , i am not sure that focussing on the position of the particle itself helps in understanding these papers . however , since , as told above , everything is equivalent to a position , the paper should apply to the position of a particle . so in the following , i will assume we are dealing with the position $x$ along the $x$ axis , and $p$ will be the momentum along the same axis . what origin ? the origin does not matter , as long as it is fixed . suppose alice and bob have two different labs , and alice wants to teleport the position and momentum of a particle from her lab to bob 's lab . that means that the initial position of particle a , relative to alice 's apparatus is the same as the final position of particle b relative to bob 's apparatus . in that sense , they can have the " same " position , even if they are in different rooms . of course , if one want to add details one should add that : since we are in the quantum world , " the same position " means that every position measurement would lead to the same measurement statistics . thank to galilean relativity , if alice and bob are moving in respect to each other , the " same momentum " is also to be understood as relative to the movement of each lab . actually , teleporting both position and momentum allows to ' completely ' teleport this degree of freedom , and the statistics of any measurement ( e . g fock-state projection ) will be the same . teleportation procedure i tend to think that continuous variable quantum information is often easier to understand in the heisenberg picture , where the observable operators behave almost like in classical physics initial state alice 's initial particle 's position and momentum are described by the operators $\hat x_a$ and $\hat p_a$ . the entangled pair is described by $\hat x'_a , \hat p'_a$ and $\hat x'_b , \hat p'_b$ . for convenience , i will define $\hat x_\pm=\frac1{\sqrt2} ( \hat x'_a\pm x'_b ) $ and $\hat p_\pm=\frac1{\sqrt2} ( \hat p'_a\pm p'_b ) $ . the entanglement of the pair means that , initially $\hat x_-$ and $\hat p_+$ are both small . it is allowed only if $\hat x_-$ and $\hat p_+$ both have big fluctuations . the bell measurement the bell measurement is a simultaneous measure of $\hat x_m=\hat x_a-\hat x_a'$ and $\hat p_m=\hat p_a+\hat p_a'$ . this measurement is possible because both observables commute . but after the measurement induces a back action on the complementary observables ( $\hat x_a + \hat x_a'$ and $\hat p_a + \hat p_a'$ ) , which become very noisy and cannot be measured later . in particular , the state of the particle a after the measurement does not contain information about the initial state anymore the teleportation itself alice tells bob the values of $x_m$ and $p_m$ , and he moves/accelerates his particle accordingly particle . after this step , we have $$ \hat x_b'^{\text{final}}=\hat x'_b + x_m = \hat x'_b + \hat x_a - \hat x'_a=\hat x_a-\sqrt2 \hat x_- $$ $$ \hat p_b'^{\text{final}}=\hat p'_b + \hat p_m = \hat p'_b + \hat p_a + \hat p'_a=p_a+\sqrt2 \hat p_+ $$ since both $\hat x_-$ and $\hat p_+$ are small , the final observables $\hat x'_b$ and $\hat p'_b$ correspond to the initial observables $\hat x_a$ and $\hat p_a$ . the position and momentum of the first particle have been teleported onto another particle . so if the initial particle was moving ( $\hat p_a\neq0$ ) , then the " target " particle is indeed moving at the end of the process .
yes there is a simple explanation . think of a metallic sphere that is not connected to anything ( i.e. . floating ) , move this conducting sphere close to a positive electrode . now what happens is , the negative charges are induced on the surface of the sphere closest to the electrode ; those negative charges leave positive charges behind . just like figure ( a ) here : the charge that accumulated at the surface because of induction by electrode affects the voltage everywhere . in electrostatics , the method of image is used to include the contribution of induced surface charge to the total voltage everywhere . the only parameter that one needs to describe a floating potential conducting sphere is the initial charge . in the figure above the total charge is zero . if the sphere was initially charged by a positive or negative charge , the induced surface charge will be affected depending on the initial charge magnitude and polarity . now speaking of mathematics , the electric flux is related to the volume charge density by gauss law : in the previous equation , d is the electric flux and rho is the volumetric charge density and q0 is the total charge in the volume . the previous equation basically states that the charge enclosed within a volume generates an electric flux that if integrated on the surface of the enclosing volume gives a constant value equal to the charge enclosed . in conductors it is known that there is no volumetric charge , instead there is only surface charge , so for the conducting sphere case gauss law becomes : the difference between the left hand side and the right hand side is that lhs is evaluated anywhere in space where r ( the radial distance ) > r ( the radius of the sphere ) . rhs is only evaluated at the surface of the sphere where surface charge is located . now in your example , first you assumed zero initial charge ( that is why the right hand side of your condition is zero ) ; second you only care about the part of conductor that faces the electrodes , that is why your surface integral is open rather than closed as gauss law dictates . to confirm this explanation try to plot the surface charge along the edge on which you defined a floating potential . i replicated your model , the voltage everywhere is : i plotted the surface charge density along the floating potential boundary : you see , a negative charge was induced close to the positive electrode leaving an identical positive charge behind ( close to grounded electrode ) , which is what happened in figure ( a ) above . if you integrate the surface charge density along the boundary the result is zero ( there is no initial charge ) . if you used non-zero initial charge , the integration result will be equal to that value . keep in mind the accuracy of the solution depends on the resolution of the mesh , so the value in case of zero charge is never equal to zero exactly because of numerical error . i hope that answered your question
why are you looking for a radial surface . . ? look it as an equipotential surface ( a surface where all points are at same constant electric potential ) as it comes with sphere . hence , you can assume the points a to b as radial to find the potential difference .
here 's coulomb 's law : if you scale everything by $\lambda$ , you get $\frac{1}{\lambda^2}$ in the denominator , but you must also introduce a jacobian for the integral . for a volume charge , the jacobian is $\lambda^3$ , so you are on the surface of a ball of charge and you make the ball bigger ( with the same charge density ) , the e-field increases proportionately . for a surface charge , though , the jacobian is $\lambda^2$ , which cancels the $\frac{1}{\lambda^2}$ in the denominator of coulomb 's law . thus , a 2-d distribution of charge is " scale invariant " . since scaling an infinite sheet leaves it unchanged , scaling changes only the distance from the sheet , and we see that the e-field is independent of distance .
the simple answer : satellites do feel this force , but obviously do not get ripped apart . the tidal forces are simply too small ( for the satellites ' materials ) to actually rip them apart . the why : tidal forces happen because one side of an object feels such a larger huge difference in force than the other side . the magnitude of the force not only has to deal with the size of things pulling on each other , but the distance . even massive things ( like the sun or jupiter ) have relatively little pull when very far away . if you get close , you feel the effects of them much more strongly and the effects increase more quickly ! due to the fact that io is a moon very close to other large bodies , this makes the difference of the force of gravity on one side is very different than that on the other . ( try plugging in correct values for io , jupiter , and the distance between them ; then try calculating the force of jupiter on io as seen from one side of io to the other . ) most man-made satellites around earth are much , much smaller than moons and they are surrounded by very distant or very small things . this makes the difference between the force of gravity on one side of the satellite is almost the same as the force of gravity on the other . this being said , if a man-made satellite were to be in the same situation as io , either being very large or being very close to other big things , it could get ripped to shreds . in short : it is all about what forces are being applied . distant objects apply small forces , close objects apply bigger forces . tidal forces happen when gravity changes wildly between your head and your feet .
as jerry schirmer said , helium-4 is an extremely stable nucleus . what does it mean quantitatively ? it means that it binding energy is very high , namely 28 mev . in other words , helium-4 is 28 mev/$c^2$ lighter than the sum of masses of two free protons and two free neutrons . the best candidates $a=5$ nuclei would have 2 protons and 2 neutrons in the lowest state - i.e. in the same state as they occupy in helium-4 – but the additional 1 proton or 1 neutron would have to be added to a higher shell . but because this higher shell is so much higher in energy than the ground levels , one can not find an $a=5$ nucleus that would be lighter than the sum of the helium mass and one proton ( or one neutron ) . the binding energy would have to be even greater than 28 mev which means that the binding energy per nucleon would have to exceed 28/5=5.6 mev . this is simply too much to ask ; the binding energy you could get for 5 nucleons is simply smaller than 28 mev , so any such object would quickly alpha-decay . i should insert some calculation of the conceivable binding energy for 5 nucleons here except that there is clearly no " analytic " calculation . it is an extremely messy system one would have to describe by nuclear physics ( ill-defined effective theory ) or by qcd ( calculable via lattice qcd , with big computers etc . ) . but let me mention that unlike atoms , where the new valence electrons may always be added and keep the stability , the nuclei are " more neutral " so the attractive force between the helium-4-like " core " of the $a=5$ object and the remaining nucleon is much weaker , sort of dipole-like , and is not enough to produce a new stable bound state . however , what i can say is that this fact about the absence of $a=5$ stable isotopes has important consequences . the big bang nucleosynthesis – first three minutes when nuclei are created – essentially stalls once it reaches helium-4 nuclei . they can not absorb new protons/neutrons to become heavier and instead , the next reaction is the much rarer collision of two helium nuclei . one either has helium-3 plus helium-4 goes to lithium-7 plus positron plus photon ; or beryllium-7 plus photon on the right hand side . lithium-7 may absorb a proton to get back to 2 helium-4 ; beryllium-7 may absorb a neutron to become lithium-7 . these processes are " everything " one may have in empty space . inside stars , one has pressure and temperature which helps to overcome the binding energy and stars may produce heavier elements , too .
does the stars in clusters rotate ? yes . all stars rotate to a greater or lesser extent whether they are in clusters , galaxies or whatever . broadly , move massive stars rotate more ( and are more often in binary systems ) and there is nothing special , as far as i know , about this trend continuing in clusters . does cluster 's stars have moons . if yes do they rotates/orbits or are they " frozen " in space ? this question does not make sense . moons do not orbit stars : they orbit planets . if you mean " do cluster 's stars have planets ? " i am not sure if we know . given that we are currently finding stacks of planets around stars , i there are probably planets around some cluster stars but they are too far away to see ( and will be for some time ) . and those planets will behave just like planets around other stars . they will rotate and follow an orbit . ( i am not sure what you mean by " frozen " in space ? ) what will happen if one of the stars blows up ? the main effect of supernovae in clusters is to expel interstellar gas from the cluster . that is , there are clouds of gas in the cluster at first and the fast-moving gas from a supernova blasts other gas out too . if other stars are very nearby they might be heated a bit but stars are quite hardy things when it comes to being irradiated by supernova . they would not be vapourized , for example . does that structure attracts or repels space objects ? clusters would attract " space objects " by exerting gravity on them . but actually , clusters repel objects in a way . when a star in a cluster approaches a pair of stars in a binary system , the tendency is for one of the three stars to be thrown out . the cluster effectively " radiates " stars over time . also , the above-mentioned expulsion of gas by supernova gradually depletes the cluster of mass and the stars move apart .
according to this article from the nasa web site , in the vicinity of the earth the temperature in the shade is -156c . the boiling point of oxygen at atmospheric pressure is -183c , so if the pressure in your container is at or less than atmospheric pressure the oxygen will not condense . the nasa article does not say what is keeping the temperature at -156c , rather than the 2.7k of the microwave background . possibly it is thermal radiation from the earth .
the key is the coriolis force . the coriolis force is $f_c = -2m\omega \times v $ . here $\omega$ is the rotation of the frame of reference and $v$ is the linear speed of the satellite . if you do the calculations , left as an exercies for the reader , you will get the missing force . in case 2 the coriolis force is 0 , because the velocity $v$ has to be used in the local frame of reference . and there $ v = 0 $ .
the main point is that newtonian gravity fields are conservative . what that means is that it is impossible to have a configuration like the one you drew without there being gravitational fields pointing to the left and to the right in the regions where you want to do the ' horizontal ' transfer . for example , you might try to achieve this on earth by taking the usual uniform gravitational field and locating a very heavy mass just under the foot of the conveyor belt on the left . this will mean , though , that as you move your mass from the foot of that conveyor belt you will be fighting against the attraction of that very same mass , as shown with the red arrows : the net result is that doing both of those horizontal transfers takes work , and in fact it must take exactly the same amount of work as what you have gained from lifting the object in the weaker field . there are , of course , many possible ways to achieve the fields you want , apart from the one in my image , but because all gravitational fields are the sum of attractive forces to a bunch of point masses , and the field of each point mass is conservative , you will always , necessarily , have cross-pointing fields like the one i pictured that will do away with any perpetual motion engine .
you begin it the same way as for mass . $r$ is radost per unit mass , so you can say that : $$ dr = \rho r dv $$ where $r$ is the total radost . you then integrate to get : $$ r = \int_v \rho r dv $$ and apply the divergence theorem like you did for mass . note -- there are several steps between determining $r$ and applying the divergence theorem . but you stated you were unsure where to start and had an example from that point onwards so i left them out .
no , the halbach array is not related to the monopole at all . it is just an special arrangement of magnet . any arrangement of a " dipole " magnet would not result in any monopole . it can be easily understood as follows : if each individual magnet contains no magnetic monopole , i.e. , $\nabla\cdot\mathbf{b}_i=0$ true for whole space , then the resulting magnetic field is $\mathbf{b} = \mathbf{b}_1 + \mathbf{b}_2$ when you put them together . hence the resulting divergence is also $\nabla\cdot\mathbf{b}=0$ . i do not understand what happens to the poles of magnets , or the direction of magnetic flux ? does it change its direction in each next or third element of array ? you should know that magnetic pole is not a very well defined concept as the " boundary " of the magnetic pole is not fixed . the flux line does change the direction across the boundary . but the most important things ( in the first figure ) is that the magnetic field line is continuous and closed , which means that there is no magnetic monopole . all the field lines go out have to come back at the next nearest neighbor . the effect of halbach array is simply to push the closed magnetic field line on one side , hence the resulting magnetic field was enhanced on one side . the diagram 2 looks like a monopole , because there is the same direction of flux all the time ? is it true , or am i misunderstanding something ? no , the flux direction is not in the same direction over the whole region , they are alternating instead . you can think of it by putting two array in diagram one ( flipped one of them ) together . each of them forming a small field line loop . at the center of two up arrows ( $\uparrow$ ) the magnetic field is pointing up . also , at the center of two down arrows ( $\downarrow$ ) , the magnetic field line is pointing down . hence , it is clear that the field line is alternating after each next nearest neighbor . the wiggler of the electron path is the result of the alternating magnetic field . i think there is a problem for the diagram 2 , the oscillating frequency is double of what it is supposed to be . it should be the same as the period of the array .
if the ladder is slipping on the floor as well as the wall , then the point of rotation is where the two normal forces intersect . this comes from the fact that reaction forces must pass through the instant center of motion , or they would do work . in the diagram below forces are red and velocities blue . if the ladder rotated by any other point other than s then there would be a velocity component going through the wall , or the floor . s is the only point that keeps points a and b sliding . this leads to the acceleration vector of the center of mass c to be $$ \begin{aligned} \vec{a}_c and = \begin{pmatrix} \frac{\ell}{2} \omega^2 \sin\theta - \frac{\ell}{2} \dot{\omega} \cos\theta \\ -\frac{\ell}{2} \omega^2 \cos\theta -\frac{\ell}{2} \dot\theta \sin \theta \\ 0 \end{pmatrix} and \vec{\alpha} and = \begin{pmatrix} 0 \\ 0 \\ -\dot\omega \end{pmatrix} \end{aligned}$$ if only gravity is acting , then $$\dot\omega = \frac{m\ , g\ , \frac{\ell}{2}\sin\theta}{i_c+m \left ( \frac{\ell}{2}\right ) ^2} $$
the work is of course equal to the potential energy of the dipole $\vec m$ in the magnetic field , $$ u = -\vec m \cdot \vec b . $$ now , your wording indicates that the magnetic field $\vec b$ is macroscopic so it may be treated as a classical parameter . however , $\vec m$ from a spinning particle is quantum mechanical . for example , for an electron , $$ \vec m = -g_s \mu_b \frac{\vec s}{\hbar} $$ where the spin $g$-factor is $g_s\sim 2.0023$ , $\mu_b=e\hbar/ ( 2m_e ) $ is the bohr magneton , and $\vec s$ is the operator ( or triplet of operators ) of the electron 's angular momentum – that act as $\hbar/2$ times the pauli matrices on the spin-up and spin-down states . so up to a multiplicative constant $\gamma$ , $$ u = \gamma \vec \sigma\cdot \vec b . $$ the work done on the electron is a $q$-number , an operator , which acts on the spin-up and spin-down states . for the up and down states relatively to the direction of the external magnetic field $\vec b$ , the work done has a sharply defined value , an eigenvalue of $u$ . for general linear superpositions , the work done on the electron has a certain probability amplitude to be positive and a certain probability amplitude to be negative . the squared absolute values of these probability amplitudes determine the probabilities that the work will be seen to be one ( positive ) value or the other ( negative ) value , just like everywhere in quantum mechanics . it is however very interesting to consider superpositions of states in this context , especially for $j=1/2$ . for spin-1/2 particles , each superposition of up and down states is equivalent to " up " with respect to a certain axis in space . if we deal with general superpositions , this axis on which the spin is " up " will generally not be aligned with the direction of the magnetic field . if that is the case , the presence of the magnetic field will have the effect of causing " precession " of the axis defining the spin up . the axis at which the particle is polarized " up " will be " turning around " the direction of $\vec b$ like a ninny . this precession results from the time-dependent change of the phases of the wave function – which is opposite for the up and down states , so the relative phase is changing as well – and the fact that the relative phases of the wave functions always matter ( for something ) in quantum mechanics . the classic experiment measuring this spin-dependent magnetic work and exhibiting lots of the quantum properties is called the stern-gerlach experiment . with some probabilities , the particle behaves in one way or another .
the canonical metric on $cp^n$ is the fubini-study metric . the distance between two states $\left| x \right\rangle$ and $\left| y \right\rangle$ is $$\gamma ( x , y ) = \arccos \sqrt{\frac{\left| \left\langle x \middle | y \right\rangle \right|^2}{\left\langle x \middle | x \right\rangle \left\langle y \middle | y \right\rangle}} . $$ the infinitesmal metric is thus : $$ds = \frac{\langle dx | dx \rangle}{\langle x | x \rangle} - \frac{\left | \langle dx | x \rangle \right|^2}{\left | \langle x | x \rangle \right|^2} . $$ notice that for $cp^1$ this reduces to the natural metric on the bloch sphere .
if i am only allowed to use one single word to give an oversimplified intuitive reason for the discreteness in quantum mechanics , i would choose the word ' compactness ' . examples : the finite number of states in a compact region of phase space . see e.g. this phys . se post . the discrete spectrum for lie algebra generators of a compact lie group , e.g. angular momentum operators . see also this phys . se post . on the other hand , the position space $\mathbb{r}^3$ in elementary non-relativistic quantum mechanics is not compact , in agreement that we in principle can find the point particle in any continuous position $\vec{r}\in\mathbb{r}^3$ . see also this phys . se post .
nowadays , rockets use a gimbaled thrust system . the rocket nozzles are gimbaled ( an appliance that allows an object such as a ship 's compass , to remain horizontal even as its support tips ) so they can vector the thrust to direct the rocket . in a gimbaled thrust system , the exhaust nozzle of the rocket can be swivelled from side to side . as the nozzle is moved , the direction of the thrust is changed relative to the center of gravity of the rocket . early rockets had vernier thrusters which uses small rocket engines on either sides , to control the altitude of a rocket . nowadays , they are common in most satellites . in this image , the middle rocket shows the normal flight configuration in which the direction of thrust is along the center line of the rocket and through the center of gravity of the rocket . on the left one , the nozzle has been deflected to the left and the thrust line is now inclined to the center line at a gimbal angle $a$ . as the thrust no longer passes through the center of gravity , a torque is generated about the center of gravity and the nose of the rocket turns to the left . if the nozzle is gimbaled back along the center line , the rocket will move to the left . on the right one , the nozzle has been deflected to the right and the nose is moved to the right . wikipedia says , in spacecraft propulsion , rocket engines are generally mounted on a pair of gimbals to allow a single engine to vector thrust about both the pitch and yaw axes ; or sometimes just one axis is provided per engine . to control roll , twin engines with differential pitch or yaw control signals are used to provide torque about the vehicle 's roll axis . the right and left gimbaling is necessary to direct the rocket to its original path , thereby maintaining its stability . . . this link gives a good explanation regarding the stability of rockets . this essay is also good , but it is somewhat big . . .
the open-source physics engine ode allows you to connect two bodies using any of a number of different joints . one of those joints is the " fixed " joint . it is much more stable , in the physics engine , to represent the two bodies as a single body but maintain two separate geometries for collision purposes . however , ode probably handles collision detection/resolution differently from what you have in mind . it only detects collision after one frame of interpenetration and then constrains the relative velocity of the colliding bodies in such a was as to force them apart on the next time step . that type of constraint is much easier to satisfy for a single rigid body than two , but perhaps you are actually preventing penetration and so need a different technique . the fixed joint simply constrains the two bodies to have zero relative angular velocity and zero relative linear velocity ( and also has an error correction term to eliminate small numerical drift ) . after that , the lcp solver handles the rest .
list of fusion power technologies : http://en.wikipedia.org/wiki/list_of_fusion_power_technologies other more well-designed fusion concepts are not fully tested yet due to lack of funding : http://www.crossfirefusion.com/nuclear-fusion-reactor/crossfire-fusion-reactor.html
this is the diagram you need to draw : the red arrow is the net velocity of the boat - the sum of 25 km/h going due north , and a current of 10 m/s at 60 degrees east of south . ( not drawn to scale ) just do simple math to determine the size of the two green segments - then compute the angle ( heading ) from the arctan of their ratio .
the book derives the equation of continuity , which states that the cross-sectional area times the velocity of a flow is always constant . but nowhere in the derivation does the textbook explicitly assumes that the flow is laminar . so , does the equation hold for turbulent flows too ? that is only a special case of the equation of continuity for situation where density is regarded constant and velocity is constant across the cross-section of the pipe . in general , equation of continuity is $\partial_t \rho + \nabla\cdot ( \rho \mathbf v ) = 0$ and holds true even for turbulent flows if the mass of the fluid is locally conserved .
it means that time is no longer an absolute concept , yes . the time a specific observer experiences in a specific frame of reference , i.e. his proper time depends on the path ( worldline ) he takes through spacetime . in other words , it depends on his state of motion , the way he accelerates . this is the reason for the famous twin paradoxon : the resolution is that both twins move on different worldlines and hence end up having different proper times . this can even be easily understood geometrically : between two points in spacetime , the proper time interval of the shortest path connecting them is the greatest . with this knowledge , the reason for the twin-paradox should be evident from the following picture : clearly , the black lines corresponding to the travelling twin represent a longer part than that of the stationary one . this does not mean , however , that time is not well-defined as a physical quantity . it just means that when you measure it , you may get different results , depending on how you move with respect to somebody else .
waves on strings combine linearly . this means that you can split up a string 's motion into two ( or more ) superimposed waves . the two superimposed waves behave independently , as if the other one was not there . so if you have a standing wave set up on a string , and then you also introduce a travelling pulse , you get something like the following . ( the arrows represent the direction of movement , and the node is marked with a blue dot . ) now to answer your question . i wish i had a way to make the picture animated , but i think you can see it from still pictures . i am going to draw what happens after a short time , when the pulse reaches the node . the standing wave has also moved , and is now swinging back in the other direction . as you can see , the standing wave component still passes through zero at the node , as it always must , but the combined wave ( pulse + standing wave ) does not . because the pulse and the standing wave do not interact , the pulse just passes straight through the node as if it was not there , and the standing wave just keeps waving as if the pulse was not there . note that not interacting is not the same as not interfering . interference happens when two waves get added together and sum to zero , but neither of the two waves is affected by being added in this way , so even when waves interfere , they do not interact .
although i do not know anything about this , using some rough estimates i think i can get the right order of magnitude : volume of graphite in a pencil : $10 cm$ cylinder of $1 mm$ thick = $0.314 mm^3$ ( error : ~factor 2 ) maximum surface a pencil can write : $50 km$ $\times$ $1$ mm = $10 m^2$ ( error : ~factor 5 ) thickness of the graphite layer : volume / surf . area = $31.4$ nanometers size of a ( carbon ) atom : $0.22 nm$ ( error : 10% ) thickness of the layer : $31.4 nm /0.22 nm$ = $142$ carbon atoms so i would say ' about a 100 atoms ' ( or at least more than 10 and less than 1000 ) . i might have been a bit conservative with my error estimates but this seems reasonable .
snell 's law still applies to the curved surface , but you have to measure the angles of incidence and refraction relative to the surface where the light hits . the image is my attempt to show parallel rays of light falling on a curved surface . even though the rays are parallel , the angle of incidence is different for the two rays because it has to be measured relative to the normal at the point the light strikes the surface . hence the angle $i$ is not the same as the angle $i'$ . response to comment : it has become clear from the comments that the problem is that the value of $n$ depends on whether the light is passing from the air to glass or from glass to air . to be precise the two refractive indices are reciprocals of each other i.e. $$ n_{air-glass} = \frac{1}{n_{glass-air}} $$ the refraction of the light ray happens because the speed of light , and therefore the wavelength , changes when the light enters and leaves the glass . the refractive index when a light ray passes from a medium 1 to a medium 2 is : $$ n_{1-2} = \frac{v_1}{v_2} $$ where $v_1$ is the speed of light in medium 1 and $v_2$ is the speed of light in medium 2 . so in our example the refractive index when passing from medium 2 to medium 1 is : $$ n_{2-1} = \frac{v_2}{v_1} $$ i.e. $$ n_{1-2} = \frac{1}{n_{2-1}} $$
( i will assume in my answer that people have read the discussion on the old question , linked to by the op . ) no , it is not like the aether . it is still true that locally , there is no preferred reference frame . you do not even really need to think about spacetime to see what is going on . consider a two-dimensional plane , parametrised by $ ( x , y ) $ , and roll it into a cylinder by identifying $ ( x , y ) \sim ( x + nl , y ) ~\forall~ n$ , where $l$ is some constant . locally , this space is still perfectly isotropic , but globally , the $x$ direction has been picked out by the identification . to see what this means , let 's imagine drawing two straight line segments , each beginning at $ ( x , y ) = ( 0,0 ) $ and ending at $ ( 0 , l ) $ . the first will just be $ ( 0 , t ) ~ , ~ 0\leq t\leq l$ , and the other will be $ ( t , t ) ~ , ~0\leq t\leq l$ ( which ends at a point equivalent to $ ( 0 , l ) $ under the identification , and therefore the same point on the cylinder ) . obviously the length of the first line is just $l$ , but the length of the second line is $\sqrt{2}l$ , by pythagoras . although any small patch of the cylinder is perfectly isotropic , we see here that the rotational symmetry is broken globally by the identification . in spacetime , a similar thing happens , replacing rotational symmetry by boost symmetry . short answer : generally speaking , there is never a twin paradox : in any spacetime , just write down the metric in any coordinate system you like , and calculate the proper times for the two trajectories of interest . this tells you unambiguously which twin is older and which younger .
you can add a gauss-bonnet term $r^2 - 4r^{\mu\nu}r_{\mu\nu} + r^{\mu\nu\rho\sigma}r_{\mu\nu\rho\sigma}$ . it is purely topological .
your assumption is very wrong . gravity is everywhere . or simply , the box has mass . it exerts its very own gravity . if your 1st location has the field , then the 2nd location is also influenced by it unless it is at $\infty$ . you can not just shield anything from gravitational field . so , energy is still conserved when you are moving the box from one place to another . . .
i do not show any contradiction regarding brant 's answer . but , there is a difference between evaporation and boiling . evaporation is a surface phenomena where molecules gain energy through random collisions . but , boiling takes place throughout the entire volume where the molecules change phase throughout the volume of liquid . ok . . now back to the question : there are two things to put our eyes into . . . boiling point of water is $100^oc$ only at normal temperature ( 298 k ) and pressure ( 1 atm ) . as altitude increases , pressure decreases and eventually attains a near-zero kpa in space . this decreases the boiling point of the liquid ( water ) according to $pv=nrt$ . hence in space ( vacuum ) , water definitely boils . . ! ( but , mentioning " outside the gravitational influence " is non-correlated here . . . ) now , triple point of water comes into play . wiki clearly says that - at zero pressure ( which is below the triple point ) , it is impossible for water to remain as liquid . hence the left-over form - " gas " . at pressures below the triple point ( as in outer space ) , solid ice when heated at constant pressure , is converted directly into water vapor in a process called sublimation . temperature in outer space could be 2-3 k . once a good amount of water has boiled from the liquid phase , there would be an isolated area of water molecules which are at the coldest place of their lifetime . they would suddenly freeze and form ice crystals ( similar to a desublimation ) . see this paper for a brief conversation on the topic . . . astronauts have proved this in space . when astronauts urinate in space and release the contents , the urine rapidly boils into vapor , which immediately desublimates or crystallizes directly from the gas to solid phase into tiny urine crystals . urine is not completely water , but you had expect the same process to occur with a glass of water as with astronaut waste . but , it could be achieved simply by using a home-experiment producing snow by desublimation . note : this kind of rapid boiling and freezing would overcome the adhesive forces drastically under such conditions . . . so , no sticking to glass .
a hint $$ dxf ( x ) = f ( x ) +xdf ( x ) $$ $$ xdf ( x ) $$ take the diference and you get $ f ( x ) $ or $ 1 . f ( x ) $
my take is that while the engine is working sucking in air , the pressure in the vicinity of the input falls . the humidity of the surrounding is high and due to the low pressure , that volume is supersaturated in h2o ( water vapor ) , and it forms a local cloud . pv=nrt if the sucked in volume is constant , pressure drop means temperature drop too , conducive to cloud formation given the dust available close to the tarmac . the pulsing would be the result of the vortex john mentions . the pressure will not be uniformly low but will follow the vortex pattern , a wavy pattern which would tend to time the formation of the cloud ( more humidity/less humidity ) .
in your solution you seem to make the assumption that the terminal velocity in the y-direction is zero . this produces the wrong answer . this is how i would solve the problem : first , let 's note that the initial velocity in both x- and y-direction are the same ( due to the $45^{\circ}$ angle ) . let 's call it $v$ . the distance traveled in the x-direction , $d$ , when the ball hit is the ground is given by : $$ d=vt $$ where $t$ is the time of flight . when the ball hits the ground , its velocity in the y-direction will be $-v$ . this means that its velocity has changed by $2v$ ( or rather by $-2v$ ) . hence we also have : $$ 2v=gt $$ substituting for $v$ gives : $$ d=\frac{gt^2}{2} $$ which solved for $t$ gives : $$ t=\sqrt{\frac{2d}{g}}=\sqrt{\frac{2\cdot 180}{9.8}}\approx 6.06\ , \rm{s} $$
well , ya gotta be careful . as you noted , there is an acceptance cone angle . now , consider an idealized case where the fiber is perfectly straight and perfectly cylindrical . then , even for skew rays ( per wikipedia , " ray that does not propagate in a plane that contains both the object point and the optical axis . such rays do not cross the optical axis anywhere , and are not parallel to it" ) you can determine the local angle of incidence when they hit the internal $n_1/n_2$ index boundary , and see that the rays may end up with different vector angles $ ( \theta , \phi ) $ ( with respect to the x and y coordinates ) but basically the same range of angle with respect to the optic axis . all this is a long-winded way of pointing out that light paths have to be reversible , so a ray can not exit at an angle greater than the acceptance angle . however , if you feed a beam with angular spread less than the acceptance angle , it is more than likely that multiple skew bounces , and , more important , curvature in the fiber , will lead to exit angles as large as the acceptance angle .
when you look with an " math eye " at the shape in your first link , you will see that the form is more a part of a sphere , as theory predicts . in second link there is a picture of the parabolic mandrel which was used to form the foil . the pressure will not deform the foil much , just keep it inflated .
what you observe is gibbs ' paradox . the resolution comes about by postulating that the particles are indistinguishable ( and thereby introducing a factor $1/n ! $ ) . then the entropy and the free energy becomes extensive ( in the thermodynamic limit $n , v\to\infty$ , $n/v=\text{const}$ ) .
this point is covered really well by feynmann in vol . 3 of the lectures when he analyzes the finite potential square well . he shows how the curvature of the schroedinger function changes from concave-out to concave-in when the energy goes negative . same thing here .
v1=3 , v2=4.5 , v3=7.5 let total distance be s so average speed =total distance/total time . total distance =s time for 1st half distance = s/2/v1 for second half distance covered = s/2 =distance covered in 2 equal time intervals ( say time interval be t ) =t*4.5+t*7.5=t ( 4.5+7.5 ) =t ( 12 ) so here t={s/2}/12 average speed={s}/ [ {s/2}/3+s/24+s/24 ] =s/{6s/24}=4m/s since 2t was time for second journey
addressing just the last two paragraphs of your question : if $t$ is " non-degenerate " ( at every point as a linear transformation from $t_pm \to t_p^*m$ ) , then modulo some issues with the constraint equations $g$ can be solved from $t$ , at least locally in time , after prescribing it in a compatible way on a space-like hypersurface . see this article by de turck . presumeably the issue with degeneracy has more to do with the method involved in solving the problem , then with it being a critical condition , since the evolution for ricci-flat metrics ( einstein vacuum equations ) is also well-posed . the main difficulty is about the " univocal " part : due to diffeomorphism invariance , the " local " problem for prescribing ricci curvature is under-determined . ( the global problem may have topological constraints ; that i am not sure about . ) so one expects that for the local problem the issue is more likely the overabundance of solutions . on the other hand , the prescription of initial/boundary data may be crucial . roughly speaking the prescribing ricci curvature equation is analogous to the inhomogeneous wave equation with a source term . in which case the hyperbolic nature of the equations shows that for any initial configuration there exists a different evolution . so it may be that this will lead to multiple different metrics compatible with your $t$ ( indeed consider the case where you restrict your manifold to be $m = \mathbb{r}^4$ ; the global nonlinear stability of minkowski space , combined with classical results on classical results on the existence of solutions to the vacuum constraint equations shows that on $m = \mathbb{r}^4$ there are many incongruent ( since for these other solutions the weyl tensor is non-vanishing ) solutions to einstein 's equation when $t = 0$ ; i would expect something similar for the case of prescribed $t$ ) .
when you take the charge distribution to be independent of $\theta$ , and force the problem to be two-dimensional , you are not letting $\delta_\theta \rightarrow 0$ . you are actually letting $\delta_\theta = 2\pi$ . if that is the only simulation you are going to run , the simplest solution is to set $\delta_\theta = 2\pi$ , and have your cells be rings at each value of $r$ and $z$ . if you later want to allow $\theta$ dependence , you can then let $\delta_\theta = 2\pi/n$ , to allow for $n$ cells per $ ( r , z ) $ ring .
i think the issue is we need to separate the ' expected'/obvious quantum effects , from the unexpected ones . for instance , some of your questions refer to quantum mechanics in molecular structure . in the most trivial sense , we could not even have molecular structure or even stable atoms if $\hbar \rightarrow 0$ such that there are no quantum effects . so in a trivial sense , every protein structure is due to quantum mechanics . following up on this . . . i mean , if i do not neglect electron-electron interaction , then pretty much all electrons in a condensed matter system are entangled , are they not ? yes . in the simplest picture of matter , chemists refer to electrons occupying molecular orbitals . this is the hartee-fock approximation in quantum chemistry , and since the wavefunction is written as a slater determinant of the occupied molecular orbitals , in this approximation there is no electron-electron position correlation . obviously in the real world this is not the case . the wavefunction will be correlated to have two electrons further apart on average than in the simplified molecular orbital picture . this means the state is not a product state and there is indeed entanglement of the electron positions . so you are correct on this . ( and as an aside , more advanced multi-electron computational methods beyond hartree-fock can of course be used to account for most of this correlation energy in theoretical calculations . the molecular orbit picture works very well though , which is why this is how it is introduced to students and how chemists colloquially discuss and visualize the quantum mechanics of a molecule . ) however , again this is in some sense the ' trivial ' effects of quantum mechanics , as it is just " chemistry " . but as we get to larger structures , due to interaction with the environment and decoherence , we can increasingly well describe everything with phenomenological parameters and a classical theory . true the phenomenological parameters are due to the quantum mechanics , but this is not the exciting part . the exciting part is when we can not describe the protein interactions with phenomenological models due to the quantum coherence playing an important part at the macroscopic level . yes , it is somewhat arbitrary where we draw this line . but few really thought biology would end up using something so fragile as the coherence of entangled states to actually improve biological function . the better studied example i have seen regarding this is energy transfer in a particular photosynthesis step . whether something is amazing or not is not really a scientific question , but hopefully i clarified enough regarding your question of entanglement of electrons in molecules to see how the use of coherence to improve biological function is at the least unexpected , and hopefully exciting to you as well .
it is a misconception to think that just because the 29 th electron is outside the gaussian surface , it will not have an effect on the electric field inside it . the total flux through the surface is indeed zero , but that does not mean there is no influence : imagine a point charge and draw up a sphere next to it . the electric field goes in on one face and out the other to make a zero flux , but there is definitely a field inside . the situation for copper is slightly more complicated because the electron is in a spherically symmetric s orbital . this does reduce its influence on the inner electrons , not because of gauss 's law but because of a theorem of newton : the electric field due to a spherical shell of charge vanishes inside it . however , the 4s orbital is not simply a spherical shell of charge around the [ ar ] 3d 10 core . even in the most simplistic , hydrogen-based models , the higher s orbitals still have a sizable probability of being inside the core , and have multiple shells of positive probability : source : wikipedia . note that this is a 6s hydrogen orbital with no actual relation to copper ; a 4s orbital has four such spherical shells . because of this , the 4s electron does create a nonzero electric field inside the atomic core , and this does affect the inner electrons . as you have guessed , of course , in real life this is much , much more complicated , and quantum chemists will send you packing if you tell them you need an accurate ab initio description of the whole copper atom all the way to the 1s 2 core . the electrons are highly correlated , and all of them interact strongly with each other , including such lovely job-simplifying features like exchange interactions . quantum mechanically , the bottom line to your question is that it is meaningless to talk about " the 29 th electron , " because all the electrons are indistinguishable and therefore you must consider them all as a whole . ( specifically , " the charge density due to the electron in the 4s orbital " is not a meaningful physical quantity . ) however , the quantumness actually helps . the reason for this is clear if you try to come up with a classical model of a copper atom . even if you preserved the shell structure with the lone 4s electron orbiting well outside them , the inner electrons will be a hugely complicated jumble of electrons whizzing around , and the system will be nowhere near spherically symmetric . while you can still draw a gaussian sphere and get information about the total flux , this does not help at all in describing the electric field at any particular point . in the quantum case , on the other hand , everything is much simpler . you have a bunch of closed shells and a single $s$ electron , and this means that the whole atom is spherically symmetric . electric fields must be radial and uniform , and drawing gaussian spheres does help you calculate the electric field at a given radius .
no . the integral $\int r^2 dv$ . it is not $v$ . $\int dv$=$v$ .
one word : inertia . when you are riding a bike on a level gradient you just need to give it a push to get going , then you can coast for quite a while before friction and air resistance slow you down . the human body does not have wheels that can store kinetic energy , so while running you have to give a good kick to get going , and then another kick to keep going on the next step , and so on . when hills are involved the difference is even more pronounced , since we run downhill the same way we do on the level , by continually pushing ourselves forward ; whereas on a bicycle you can take advantage of the slope and just coast down it . i suspect that raising and lowering your centre of mass is not as inefficient as the other answers have suggested . this is because your legs are springy , so at least to some extent you are just converting energy back and forth between gravitational potential and the spring force in your legs . humans are possibly the most efficient long-distance runners in the animal kingdom . there is a school of thought that says the reason we are bipeds is that we evolved as endurance hunters , chasing our prey until it collapsed from exhaustion rather than trying to outrun it over short distances . whether that is true or not , we probably would not do all that bouncing up and down if there was not a good reason for it . you might ask why , if using wheels is so much more efficient , did not we evolve that instead ? i do not know , but it seems no animal has been able to evolve wheeled locomotion .
a very high energy gamma ray spontaneously pair-produces a particle and anti-particle , the idea being that the gamma ray has enough energy that a decay into matter is feasible . the particle and anti-particle which are created are still very high energy - they have velocities near the speed of light in a vacuum . whenever a particle flies through a substance at a velocity higher than the speed of light in that substance , it emits cherenkov radiation . the typical analogy given is that this is like a sonic boom : in a sonic boom , distinctive waves are produced when something flies through a substance at higher than the speed of sound in that substance ; with cherenkov radiation , the waves are produced by flying through at more than the speed of light . the particle and antiparticle might collide with stuff in the atmosphere , producing high-energy photons ; these high-energy photons can pair-produce again . this way , the cherenkov radiation amplifies ; a small burst of cherenkov light is produced whenever a gamma ray enters the atmosphere . the cherenkov radiation produced by gamma rays has a distinctive pattern that can be detected using a photomultiplier tube . with an array of these detectors , you can observe the shower from several points . then , you work backwards to figure out where the original gamma ray came from and what energy it had ( much easier said than done ! ) .
your problem has two degrees of freedom , acceleration of the top block $a_1$ and acceleration of the bottom block $a_2$ . if the friction force between them is $f$ then the equations of motion are $$ m a_1 = f \\ m a_2 = p - f $$ if the blocks are stuck then $a=a_1=a_2$ and $f$ are unknown . when they slip then $f=\mu m g$ is known , but $a_1$ , $a_2$ are different unknowns . in each case there are two unkowns and two equations . the tansition occurs when friction tries to be $f \ge \mu m g$ which you need to find first .
hawking radiation is a very robust prediction . it comes simply from applying quantum field theory in the curved space-time near the event horizon . it is also part of the synthesis called " black hole thermodynamics " , for which string theory provides an explanation in terms of the statistical mechanics of microstates . in the s-matrix of quantum gravity , if black holes did not evaporate , they had show up as asymptotic states , but they do not . ( there are eternal black holes in anti de sitter space , but they still evaporate , they just do not get to evaporate completely ; the particles produced by the evaporation can not escape to infinity because of the peculiarities of ads geometry , and fall in again . ) so denying the existence of hawking radiation would screw up many other things . you could say that hawking radiation is real but that it falls back in , like in ads space , but there is no reason for it to do so . the paper featured at arxivblog is a " what if " paper which ignores all these problems and proceeds to calculate some of the consequences . you could compare it to an engineering study of one of m.c. escher 's impossible structures : if you ignore the contradictions in its design , maybe you can calculate some of its properties , but it only has recreational value to do so . we do not quite know that a nonevaporating quantum black hole is logically impossible , in the way that we know the impossible staircase is impossible , but in the future a genuine proof may be available . but empirical confirmation of black hole evaporation is rather unlikely . if we could produce mini black holes in colliders , then we had see it , but those models are not especially favored ; they are a " what if " of a different sort , one in which there is at least a consistent fundamental picture behind the hypothesis ( particular braneworld models ) , but it is just one of many possibilities about what happens at the next frontier of physics and those models are not significantly favored . ( these models are also the ones which predict a detectable signature in grb data . ) if we could send a probe to the edge of an astrophysical black hole , maybe the radiation could be detected , but that is a job for interstellar civilizations , if they exist . maybe you could find indirect evidence for hawking evaporation of primordial black holes in the cosmic microwave background . but i do not know how likely that is - again , it would be highly model-dependent .
yes , infrared radiation which is invisible to human eyes but still radiates heat . this is how they make thermal cameras , they are using your body 's infrared radiation which is detected by the camera and forms an image based on visible light .
the main problem about a rigorous solution to such a scattering proplem is that computations are extremely demanding . just imagine you have a wavelength $\lambda$ of some $400$nm to $700$nm for visible light ( from here ) : now , to do physically meaningful simulations , you will need a sub-wavelength lattice which makes any computational cell above , say $10\ , \mu m^3$ not accessible since you have in the order of one million grid points . approximative approaches but of course there can be ways out of it if you are willing to make some approximations which will largely depend on the characteristics of the particles you are looking at . it is best to assume that we only have spherical particles since we can apply mie theory in this case . large particles first of all , let us consider particles which are much larger than the wavelength . then , the radius $r$ times the wave vector $k=2\pi/\lambda$ is much bigger than one , $$kr\gg1$$ which basically means that one observes reflection at a plane interface . you can implement these particles using geometrical optics ( mixed with fresnel reflection if you like ) since nothing really wave-like will happen as in this image ( taken from here ) : small particles second , the particles should be much smaller than the wavelength , $$kr\ll1\ , . $$ then , everything what is observed is a sum of dipolar responses of the particles in the so-called rayleigh-scattering . then , the intensity of light scattered by a single small particle from a beam of unpolarized light of wavelength $\lambda$ and intensity $i_0$ is given by : $$i=i_0 ( 1+\cos^2\theta ) \frac{ ( kr ) ^6}{2 ( kr ) ^2}\left ( \frac{n_p^2-1}{n_p^2+2}\right ) $$ where i have chosen the variables to be consistent with the used terminology and $r$ is the distance to the object , $\theta$ is the scattering angle and $n_p$ is the sphere 's refractive index . here is an image of such a situation with some metal particles also having quadrupolar excitation ( from here ) : a mean field approach - effective permittivity if you have a lot of these small objects , you may use the clausius-mossotti relation which gives you an effective permittivity $\epsilon_p=n_p^2$ depending on the concentration of the particle in some volume : $$\epsilon_{eff} = \epsilon_p + \frac{n\alpha}{1-\frac{n\alpha}{3\epsilon_p}}$$ where $\alpha$ is the polarizability of the sphere , for details see e.g. electromagnetic mixing formulas and applications by sihvola . this would be something like a mean-field approach . you can make some very neat effects using this effective approach since it allows you to calculate a continuous refraction around some particle streams under water . however , if the particles size is in the order of the wavelength , $$kr\approx 1$$ then you may have to take higher multipole moments into account which may be a very demanding task . for much more on the subject i would recommend bohren and huffmanns classic absorption and scattering of light by small particles . sincerely
good question . the rate of temperature increase scales as the power absorbed by the food divided by mass of the food . so to understand your question , you need to understand how power is absorbed . there is a finite amount of power in the microwaves being produced . these microwaves bounce around in the metal cage where you put your food , until they come into contact with the food . ( well , some of them will get absorbed in the metal by imperfect reflection , but let 's ignore that at first . ) once they get absorbed by the food , they turn into heat . because they bounce around until they hit some food , the efficiency of a microwave is pretty high , in the sense that most of the power generated in the form of microwaves goes into heating the food , regardless of how much food you have . so , at lowest-order , increasing the mass will increase the amount of water , but will not increase the amount of power being absorbed by the food . but now , that thing about absorption by the metal comes in . the power absorption will be slightly greater with a lot of food , since the food will be more likely to absorb the microwave before it gets absorbed by the metal . this is a lower-order effect , but it is there . of course , then the issue of skin depth comes in . microwaves only penetrate a certain distance into the food . ( of order an inch , depending on the food . ) so increasing the mass is not really what you want ; you want to increase the mass that is within the skin depth . for example , a wide dish of water that is one inch deep will absorb better than a jug of water with the same volume . this is why you want to split apart chicken breasts when defrosting them , for example . to answer your question , then , the more food you put in , the more efficiently your food will capture the power being produced by the microwave oven . so you will capture the microwave 's power better , but you will still heat slower because you have more mass . but this is not a dominant effect , and you might be better off redistributing your food to maximize the surface area .
some 3d glasses use narrow-band filters rather than polarization , which is quite clever . see http://en.wikipedia.org/wiki/dolby_3d for details . the reason the light looked different through each eye is that the light spectrum is not uniform across the visible wavelength . thus , when different parts of that spectrum are viewed ( through the left or right lenses ) , you get light that is not quite white , and not quite the same .
i ) the question formulation ( v4 ) leaves out some important implicit assumptions$^1$ of theorem 5.2 in ref . 1 . these are , among other things , the following four items . the word topologically equivalent should be replaced by locally topologically equivalent , i.e. in some local neighborhood . the ( vector-valued ) functions $g ( w ) $ and $h ( w ) $ are $o ( w ) $ for $w\to 0$ , where $w= ( u , v ) $ . here $o ( w ) $ refers to the little-o notation . the eigenvalues of the $b$ matrix lie on the imaginary axis . the eigenvalues of the $c$ matrix do not lie on the imaginary axis . ii ) let us first take a step back and put it in context . where did all of this come from ? well , we wanted to analyze the stability of the dynamical system $$ \dot{w}~=~aw + f ( w ) $$ at the point $w=0$ . here $a$ is a constant ( =$w$-independent ) quadratic matrix$^2$ , and the ( vector-valued ) function $f$ is of higher-order , $$f ( w ) ~=~o ( w ) \qquad \text{for} \qquad w~\to~ 0 . $$ we then partially diagonalize the $a$ matrix into sub-blocks $b$ and $c$ with the aforementioned properties ( 3 and 4 ) . after the diagonalization , the $w$-variables are split into $w= ( u , v ) $ , and similarly for the ( vector-valued ) function $f= ( g , h ) $ . iii ) we could furthermore partially diagonalize $c$ into sub-blocks $c_{+}$ and $c_{-}$ depending of whether the eigenvalues have positive or negative real part , respectively . these will correspond to unstable or stable directions , respectively , independently of what the higher-order terms $g$ and $h$ are . ( in a renormalization group jargon , they correspond to so-called relevant or irrelevant deformations , respectively . similarly , the $b$ sub-block corresponds to so-called marginal deformations . ) iv ) the tangent plane $\{w|v=0\}$ at $w=0$ corresponds to marginal directions . we need the higher-order information $f$ to decide if they are stable or unstable directions . if $h\equiv 0$ , we can choose the tangent plane $\{w|v=0\}$ as the center manifold . for general $h$ , the center manifold $c:=\{w|v=v ( u ) \}$ is a deformation of the tangent plane $\{w|v=0\}$ . the tangent plane $\{w|v=0\}$ is only a precise substitute for the center manifold $c$ when $u=0$ . intuitively , the center manifold $c$ by construction becomes curved by encoding the higher-order information $f$ in such a way that points $w\notin c$ in a neighborhood of $0$ are attracted to ( repelled from ) $c$ , for time $t$ going to the future ( coming from the past ) , in accordance with just the linear predictions of the $cw$ term , respectively . in particular , the effects of the $h$ terms in the second equation have already been taking into account in the definition of the curved center manifold $c$ , so that one should now only use the linear equation $$\dot{v} ~ = ~ cv$$ without the $h$-term . the first equation $$\dot{u} ~ =~ bu + g ( u , v ( u ) ) $$ is used to determine the stability for points $w\in c$ on the center manifold $c$ itself in a small neighborhood of $0$ . references : yu . a . kuztnetsov , elements of applied bifurcation theory , 2nd edition , 1998 . -- $^1$ some of the implicit assumptions are explained in the scholarpedia link . $^2$ a finite-dimensional quadratic matrix $a$ is not necessarily diagonalizable . eigenvalues , eigenvectors and eigenspaces should then be understood in the sense of generalized eigenvalues , eigenvectors and eigenspaces .
the momentum flux tensor comes from the momentum equation of navier-stokes equations : $$ \frac{\partial\left ( \rho\mathbf{u}\right ) }{\partial t}+\nabla\cdot\mathbf{p}=0\tag{1} $$ or , using indices ( where it is easier to see that $\mathbf{p}$ is a rank-2 tensor ) : $$ \frac{\partial\left ( \rho u_i\right ) }{\partial t}+\frac{\partial\pi_{ij}}{\partial x_j}=0\tag{2} $$ we can split this tensor into three components : advection of $i$-momentum in the $j$-direction : $\left ( \rho u_i\right ) u_j$ pressure : $p\delta_{ij}$ stress tensor : $\sigma_{ij}$ the first two are rather straight-forward ( but can be elaborated on a little bit more if you need it ) , but the third is a little more complicated . we generally regard the stress tensor as traceless and symmetric : $$ \sigma_{ij}=\mu\left ( \frac{\partial u_j}{\partial x_i}+\frac{\partial u_i}{\partial x_j}-\delta_{ij}\frac{2}{3}\ , \frac{\partial u_i}{\partial x_i}\right ) $$ where $\mu$ is a ( fluid-dependent ) constant , referred to as viscosity . this term describes the deformation of the fluid . edit if we absorb bullets 2 and 3 into the single quantity $\tau_{ij}$ , then equation ( 1 ) becomes $$ \frac{\partial\left ( \rho \mathbf{u}\right ) }{\partial t}+\mathbf{u}\cdot\nabla\rho\mathbf{u}+\nabla\cdot\tau=0\tag{3} $$ the reason we do this is because of the subscript $j$ ( see equation ( 2 ) ) on one of the $u$ 's is identical to the gradient operator , $\nabla$ . we can further describe the left two terms as a single entity most-often called the material derivative : $$ \frac{d}{dt}=\frac{\partial}{\partial t}+\mathbf{u}\cdot\nabla\equiv d_t $$ from the leibniz rule , $$ d_t\rho\mathbf{u}=\mathbf{u}d_t\rho+\rho d_t\mathbf{u} $$ and if we assume that the flow is incompressible , then $d_t\rho=0$ and equation ( 3 ) becomes $$ \rho d_t \mathbf{u}+\nabla\cdot\tau=0\tag{4} $$
i am going to explain roughly what the born rule , following stan liou 's comment . one of the postulates of quantum mechanics relates a mathematical quantity , the wave function ( or state $\psi$ of a hilbert space , $\mathcal{h}$ ) to a measurable entity , the probability of a given event to happen . the idea goes like this : if you want to measure a quantity $a$ , it might be the energy , angular momentum , position , etc . you will take the related ( linear ) operator to this observable $a$ , lets call it $\hat{a}$ . form the postulates of qm we know that when you measure the observable $a$ , we can only read certain values , the eigenvalues of the operator $\hat{a}$ , we will denote them by $a_i$ . what determines which eigenvalue $a_i$ your measure is going to read ? probability , with the following " protocol": at the beginning of the experiment your system was in a state $\psi\in\mathcal{h}$ , that will be a linear combination of the base states ( functions ) composed by the eigenstates of the operator $\hat{a}$ ( those that if you measure them will yield the eigenvalue associated to that eigenstate always , i.e. with probability 1 ) : $$ \psi = \sum_i\alpha_i\phi_i $$ where $\alpha_i\in\mathbb{c}$ denotes the complex amplitude of the eigenstate $\phi_i$ . then the probability of measuring a given $a_i$ ( eigenvalue associated to the eigenstate $\phi_i$ ) in your experiment will be $|\alpha_i|^2$ .
you do not need to use that . you can simply do the cross-contractions by hand . let 's do that . note that i only care about the the $\frac{1}{z^4}$ term to evaluate the central charge . we have \begin{equation} \begin{split} t ( z ) t ( w ) and =\left ( : \partial_z b c ( z ) : - \lambda \partial_z : b c ( z ) : \right ) \left ( : \partial_w b c ( w ) : - \lambda \partial_w : b c ( w ) : \right ) \\ and = : ( \partial_z b ) c ( z ) : : ( \partial_w b ) c ( w ) : - \lambda \partial_z : b c ( z ) : : ( \partial_w b ) c ( w ) : \\ and ~~~~~~~~~~~~~~~~~~~~~~- \lambda : ( \partial_z b ) c ( z ) :\partial_w : b c ( w ) : + \lambda^2 \partial_z : b c ( z ) :\partial_w : b c ( w ) : \end{split} \end{equation} now at each step , we only keep the full contractions to extract the central charge . we then find \begin{equation} \begin{split} t ( z ) t ( w ) and \sim \partial_z \frac{1}{z-w}\partial_w \frac{1}{z-w} - \lambda \partial_z \left ( \frac{1}{z-w} \partial_w \frac{1}{z-w} \right ) \\ and ~~~~~ - \lambda \partial_w \left ( \frac{1}{z-w} \partial_z \frac{1}{z-w} \right ) + \lambda^2 \partial_z \partial_w \frac{1}{ ( z-w ) ^2} \\ and = \frac{-6\lambda^2 + 6 \lambda - 1 }{ ( z-w ) ^4} + \cdots \end{split} \end{equation} we can then read off $$ c = 2 \left ( -6\lambda^2 + 6 \lambda - 1 \right ) = - 3 ( 2 \lambda - 1 ) ^2 + 1 $$
all the work was done during the time your foot was in contact with the ball--those few milliseconds of impulse--identical to momentum--was transferred . without gravity that ball would not travel a certain distance but travel forever . the work done is equal to the ke the ball now possesses . if your foot remained in contact with the ball it would continue to accelerate , and even more work would have been done .
like dmckee says , the potential energy of electrons in an atom does not really compare to the energy of the nucleus . since the nucleus is so tightly packed , and ( in the case of uranium ) contains so many protons , they have a lot of potential energy—it takes a lot of work to " push " them together . the strong force holds protons and neutrons together when they are close enough together . it is sometimes compared to a glue in that sense . however , this force disappears very quickly once the nucleons become separated past a certain distance ( on the order of a femtometer ) , so for a large nucleus like that of uranium , the protons on one end of the nucleus are not " stuck " to the protons on the other side , but there is still plenty of electrostatic force pushing them apart . that is why large nuclei are unstable ; especially those with a higher proton-to-neutron ratio , like u-235 ( or u-236 ) . if you can destabilize that nucleus enough ( you could imagine it stretching out like a football ) , its own electric force will tear it apart , and release a tremendous amount of energy as the fragments accelerate apart . it seems that you have already considered that to a degree , but yes , you do not have to worry about electrons . just think about the protons , because the nucleus is definitely not neutral . it is a bit of a problem if you do not know how close together those nuclear fragments start out , but you could probably estimate it by considering the nuclear radius of uranium , which is about 7 or 8 fm ( it is hard to find an exact figure ) , or by using double the radius of the fragments it creates , which , it would seem , would be paladium nuclei . obviously the protons were not in the same place when they start out , but they were very close together .
it seems they are using the law of cosine http://en.wikipedia.org/wiki/law_of_cosines and $\theta$ is really an angle between $r$ and $r'$ . in this problem you can always choose the coordinate system in the way that the $r$ is directed along the axis z in cartesian coordinates , because only relative distance between points designed by $r$ and $r'$ matters
even in curved spacetime , you can perform a coordinate transformation at any location ( "move to a freely falling frame" ) such that your metric is locally flat , and takes the form \begin{equation} ds^2 = -c^2 dt^2 + dx^2 + dy^2 + dz^2\end{equation} if you consider a null trajectory where $ds^2$ is set to 0 , then the above equation is the statement that " the differential physical distance traveled along the trajectory , as measured by an observer in a freely falling frame at the location in consideration , is equal to the speed of light times the differential time interval measured by that observer . " from einstein 's equivalence principle , this is precisely the way that light must behave .
the statement is simply false as it stands when adopting the standard hilbert space formulation of qm . the true statement is that a self-adjoint operator with pure point spectrum admits a hilbert basis made of eigenvectors . ( it happens in particular , but not only , when either the operator is compact or its resolvent is . ) the proof is not so simple and is a particular case of the general spectral decomposition theorem . a proof can be found in classical books on spectral theory like rudin 's functional analysis book , r and s or prugovecki 's book on mathematical foundation on qm . even the original and always wonderful von neumann 's book contains a proof ( i think the most readable for physicists since there all rigorous spectral theory was invented for the firs time ) . the op 's statement admits another interpretation , in the sense of gelfand 's rigged hilbert space theory . in that case the presence of a continuous part of the spectrum is admitted . that approach is much more intuitive than the pure hilbert space one but , conversely , is extremely more technical mathematically speaking , also because it needs further topological hypotheses to be established than the standard ones of a self-adjoint operator in a hilbert space . i do not know a book where a complete proof appears and i suspect that there is not ( while i am sure that the statement is correct ) . the most complete proof i know can be grasped from several propositions in gelfand vilenkin 's books on generalized functions theory . perhaps the forth volume . however , some statements about some properties of some relevant measures necessary to achieve the final decomposition statement appear without proof therein .
how does this connect in any way to my equations ? you know the distance travelled and you know the proper time , $t ' = \tau = 10y$ . using the timelike invariant interval equation , solve for t : $t^2 = \tau^2 + ( \frac{r}{c} ) ^2 = ( 10y ) ^2 + ( 4.4y ) ^2 = 119.36y^2 \rightarrow t = 10.93y$ $u = \frac{r}{t} = 0.403c$ since you insist on doing it the hard way , here 's one approach : from your first transformation equation : $\dfrac{\delta x}{\delta t'} = \gamma u = \dfrac{4.4ly}{10y} = 0.44c$ from your 4th transformation equation : $\gamma \delta t = \delta t ' + \delta x \frac{\gamma u}{c^2} = 10y + 4.4y ( 0.44 ) = 11.94y$ combining these results : $ ( \gamma u ) ( \gamma \delta t ) = 5.25ly = \gamma^2 u \delta t = \gamma^2 \delta x = \gamma^2 4.4ly$ from which we get : $\gamma = 1.093$ $\delta t = \gamma \delta t ' = 1.093 \cdot 10yr = 10.93y$ $u = \gamma u / \gamma = \dfrac{0.44c}{1.093} = 0.403c$ as i attempted to point out in comments , the invariance of the interval is fundamental and useful here . the lorentz transformations guarantee that : $ ( c\delta t ) ^2 - ( \delta x ) ^2 = ( c\delta t' ) ^2 - ( \delta x' ) ^2$ you know that $\delta x = 4.4ly , \delta x ' = 0$ , and $\delta t ' = 10y$ so you just put in what you know and you get $\delta t$ immediately and the speed $u = \dfrac{\delta x}{\delta t}$ immediately follows . there is no need to find $\gamma$ in this problem
this is very much a quick and dirty answer , i havn't though too much about it . i might update my answer if i find time in the weekend , otherwise i hope others will give more precise answers . $v_{\alpha\beta\gamma\delta}$ transforms reducibly under $su ( 2 ) $ , as tensor products of four spin $\frac 12$ representations . using $\bf\frac 12\otimes\frac 12 = 0 \oplus 1$ and $\bf 1\otimes 1 = 0 \oplus 1 \oplus 2$ , we find that $v_{\alpha\beta\gamma\delta}$ decomposes into these irreducible representations $$\mathbf{\frac 12\otimes\frac 12\otimes\frac 12\otimes\frac 12} = ( 2\times\mathbf{0} ) \oplus ( 3\times \mathbf{1} ) \oplus \mathbf{2} , $$ two singlets , three triplets and a spin 2 ( 5-dimensional representation ) . there is an action of the permutation group $s_4$ on the indices of $v_{\alpha\beta\gamma\delta}$ , for $su ( n ) $ it turns out that decomposing this tensor into irreducible representations of the permutation group also corresponds to irreducible representations of $su ( n ) $ . this can be done rather quickly using young tableau , you can find the details in most representation theory books for physicists ( i do not have a relevant book here and do not remember the details . i might add the answer in the weekend ) . in the case of a tensor product $\bf 1\otimes 1 = 0\oplus 1\oplus 2$ , we get the following decomposition $$ t_{ij} = \delta_{ij}\frac{tr ( t ) }3 + \left ( \frac{t_{ij}-t_{ji}}2\right ) + \left ( \frac{t_{ij}+t_{ji}}2 - \delta_{ij}\frac{tr ( t ) }3\right ) . $$ these three terms transform irreducibly as spin $0$ , spin $1$ and spin $2$ representations of $su ( 2 ) $ , respectively . in terms for the permutation group , these are the trivial , anti-symmetric and trace less symmetric representations , respectively . something similar can be done for $v_{\alpha\beta\gamma\delta}$ , by using young tableau or just by playing around with it .
yes . g2 shows up often , starting with atomic physics ( perhaps racah is the first ; see r . e . behrends , j . dreitlein , c . fronsdal , and b . w . lee , “simple groups and strong interaction symmetries , ” rev . mod . phys . 34 , 1 ( 1962 ) . ) . you will find some refences in my 1976 phys rev paper on cns . physics . gatech . edu/grouptheory/refs . i have whole folder of physics g2 papers , but now i see i did not bother to enter g2 history into www.birdtracks.eu. nobody 's perfect . sorry predrag ( for responses , email to dasgroup [ snail ] gatech . edu , i sometimes look at those . pure accident i saw this question . . . )
i was intending to update my answer to your original question to give you the required additional information . it is just a little technical and takes some time to write down all the technicalities . actually , one just has to write down the the functional : $\int_{s_2} \mathrm{tr} ( \phi d\phi \wedge d\phi ) $ , where , $\phi = \phi_x \sigma_x + \phi_y \sigma_y + \phi_z \sigma_z$ , ( $\sigma_x , \sigma_y , \sigma_z$ are the pauli matrices ) explicitely in his favorite coordinate system to understand its meaning : first observation : taking ito account the constraint defining the two sphere:$\phi_x ^2+\phi_y ^2+\phi_z ^2=1$ , ( in matrix notation , this condition is equivalent to $\phi^2 = i$ ) then the integrand is just the area element of the ( higgs vacuum ) two-sphere . second observation : consider the two form : $\omega = \mathrm{tr} ( \phi d\phi \wedge d\phi ) $ . it is easy to verify that it is closed : $d\omega =\mathrm{tr} ( d\phi \wedge d\phi \wedge d\phi ) = 0$ ( by the antisymmetry of the wedge product ) but it cannot be exact , otherwise the area of the two sphere would have been zero by the stokes theorem . third observation : the variation of this form ( with respect to any perturbation ) is exact : $\delta \omega = d \mathrm{tr} ( \delta \phi \phi d\phi ) $ ( please notice that one needs to use the condition : $\phi^2 = i \rightarrow \phi \delta \phi +\delta \phi \phi = 0 $ ) first conclusion : this form does not change under an infinitesimal variation of the fields . since any continuous map can be built of a series of infinitesimal maps , this form does not change for a continuous deformations of the map $\phi$ . thus it is a topological invariant . second conclusion : please look at the surface area element in spherical coordinates : $\omega = sin\theta d\theta d\phi$ and consider the map $\theta \rightarrow \theta $ , $\phi \rightarrow n \phi $ , clearly this map winds the sphere n times and it is not difficult to verify that the integral is equal to n . this is the reason for the name winding number . in my answer to your first question another family of maps with arbitrary winding numbrers were given .
yes , the $r$-argument really is $r_{ik}:=|r_i-r_k|$ , as he writes two pages earlier at the beginning of " systeme von endlich vielen teilchen " . but then you do not need the force to show the relation , it is just the chain rule , which makes derivatives of $u$ into a two term expression and notice that $u ( r_{ik} ) =u ( |r_i-r_k| ) =u ( |r_k-r_i| ) =u ( r_{ki} ) $ . also , it is not so good , that you write "$\frac{d}{dt}r_{a}\nabla_{a}u_{ab}$" for "$\frac{dr_{a}}{dt}\nabla_{a}u_{ab}$" , because it suggests that you mean "$\frac{d}{dt} ( r_{a}\nabla_{a}u_{ab} ) $" . moreover , the books name is not the autors name . and i would change the title to something readable , and by that i do not mean the problem with the total derivative , but a title which is a sentence , not a formula . e.g. " a problem deriving the energy conservation for radial two particle potentials " .
no . boiling itself does not mean that the water will cook anything . if you have boiling water at 30°c you could touch it ( if we forget that it is at really low pressure ) and nothing would happen . boiling is not what cooks , but temperature . in fact , if you want to purify water at high altitudes , you need to boil water for a longer time because it will be at a lower temperature .
in fluid dynamics , within a isentropic process , the pression is not constant , you have a law like $\frac{p}{\rho^\gamma} = constant$ , or $\frac{p}{t^{\frac{\gamma}{\gamma -1}}} = constant$ here $p$ is the pressure ( called too a static pressure , $\rho$ is the density , $\gamma$ is the ratio of the specific heats of the fluid . because this is an isentropic process , the pressure $p$ can be called " isentropic pressure " . you have a compressible flow equation , which could be written , in the simplest case : $$\frac{v^2}{2} + \frac{\gamma}{\gamma -1}\frac{p}{\rho} = constant = \frac{\gamma}{\gamma -1}\frac{p_0}{\rho_0}$$ here $v$ is the speed of the fluid , $p_0$ is called the " total pressure " ( or stagnation pressure ) , and $\rho_0$ is called the " total density " . at zero velocity $v=0$ , the notions of isentropic pressure and total pressure coincide . idem for density and total density .
even in the " empty " space there are fields . in particular magnetic field is going to bend a trajectory of a charged particle . the earth 's magnetic field is strong enough to trap electrons of energies up to 10 mev and protons with energies up to 100 mev . the charged particles from solar wind and cosmic rays form van allen radiation belt but this magnetic field would also prevent electrons ' fired ' at low earth orbits from escaping . outside the earth magnetosphere the magnetic field is generated by the currents around the sun , and though the fields are smaller ( away from the sun and other magnetic planets ) it still will bend trajectories of the charged particles . so a precision ' shooting ' with charged particles would be very difficult . nevertheless , there is a proposed system of space propulsion , the electric sail , which is somewhat similar to your suggestion . there , positively charged wire is used to deflect protons from the solar wind to produce the thrust .
the sun is not " made of fire " . it is made mostly of hydrogen and helium . its heat and light come from nuclear fusion , a very different process that does not require oxygen . ordinary fire is a chemical reaction ; fusion merges hydrogen nuclei into helium , and produces much more energy . ( other nuclear reactions are possible . ) as for rockets , they carry both fuel and oxygen ( or another oxidizer ) with them ( at least chemical rockets do ; there are other kinds ) . that is the difference between a rocket engine and a jet engine ; jets carry fuel , but get oxygen from the air .
a very general discussion-not specific to a system : the internal energy , $u$ , of a system is a function of state , which means that its value only depends on the thermodynamic variables ( $p , v , t ) $ for example , at a given state ( this means for a given set of values of these variables ) . let us make this more concrete : imagine the system is in a thermodynamic state where the thermodynamic variables have the values ( $p_i , v_i , t_i$ ) ( $i$ stands for initial ) . at these values of the thermodynamic variables the internal energy has a value : internal energy at the initial state $i$: $u ( p_i , t_i , v_i ) $ . you can think of a gas at pressure , volume and temperature condition ( $p_i , v_i , t_i$ ) . now imagine you change the thermodynamic variables to these ones ( $p_f , v_f , t_f$ ) ( $f$ stands for final ) . the internal energy now has a new value internal energy at the initial state $f$: $u ( p_f , t_f , v_f ) $ . in this process you have changed the internal energy of the system by an amount : change in u : $\delta u= u ( p_f , t_f , v_f ) - u ( p_i , t_i , v_i ) $ i hope it is clear to observe that the system could have followed an infinitely large set of $ ( p , v , t ) $-points , along an infinitely large number of different paths in order to go from state $i$ to state $f$ . however , these are not , in any way , influencing by how much $u$ will change , you can take which ever path you please to go from state $i$ to state $f$ . so the system has no memory of the intermediate states . in mathematical terminology , this means that the differential change , $du$ , is a perfect differential and this is stated by the simple mathematical expression $\oint_c du=0$ it is very similar to the gravitational potential of the earth , for example , which tells us that the amount of energy we need to spend to lift an object by 3m , does not depend whether we bring it straight vertically up or we follow some other path .
the answer to your question is , sometimes , but it depends on the source and your hypothesised relationship approximately holds in some cases . in three dimensions , your relationship does not hold . if the sound source is small , then its pressure field outside the source 's hardware will be a general multipole scalar field , i.e. a general superposition of spherical waves each fulfilling helmholtz 's equation $ ( \nabla^2 + k^2 ) \psi = 0$ where $k = 2\pi/\lambda$ is the wavenumber at the frequency in question in the medium in question : $$\psi ( r , \theta , \phi ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu= -\ell}^\ell \psi_{\ell , \nu}\ , j_\ell ( k r ) p^{|\nu|}_\ell ( \cos ( \theta ) ) e^{i\ , \nu\ , \phi}$$ where $ ( r , \theta , \phi ) $ are the spherical co-ordinates of the point in question , $p^{|\nu|}_\ell$ are the associated legendre functions and $j_\ell$ are the spherical bessel functions of the first kind and order $\ell$ . in the farfield , i.e. when $k r \gg \ell$ we have $j_\ell ( k\ , r ) \approx \sin ( k\ , r - \ell \pi/2 ) / r$ so that the wave 's intensity varies like $1/r^2$ in the farfield . therefore , a factor of 10 increase in the distance from the source leads to a factor of 100 decrease in the intensity ( radiated power per unit area ) , which in decibel terms is a loss of $10\log_{10}100 = 20{\rm db}$ . therefore , to match the intensity of a 50db source at 1m distance , you are going to need a 70db source at 10m and a 90db source at 100m . you can get this result imagining the source is like an isotropic radiator with some , say dipole , radiation pattern . there will be an inverse square intensity dependence on dustance . however , suppose your source is somehow cylindrical . maybe it is like a paper cone loudspeaker but the cone is replaced by a very long paper cylinder radiating in and out . if you are near enough to the cylinder that it can be approximated as being infinitely long , then you have essentially gotten yourself a two dimensional problem , the waves are now cylindrical waves : $$\psi ( r , \theta ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu=-\infty}^\infty \psi_{\nu , \ell}\ , j_\nu ( k_\ell r ) e^{i\ , \nu\ , \phi}$$ where now $j_\nu$ is the bessel function of the first kind and order $\nu$ . now in the farfield , $j_\nu ( k_\ell r ) \approx\sqrt{2/\pi}\cos ( k_\ell r -\nu\pi/2 -\pi/4 ) /\sqrt{r}$ so that the wave 's intensity varies like $1/r$ in the farfield and now a factor of ten increase in distance corresponds to a loss of 10db . therefore , to match the intensity of a 50db source at 1m , you would need a 60db source at 10m way or a 70db source at 100m away .
follow the similar tutorial in : http://www.cs.cmu.edu/~baraff/sigcourse/notesd1.pdf http://www.cs.cmu.edu/~baraff/sigcourse/notesd2.pdf to get $$ \boxed{ j=\dfrac{ ( \boldsymbol{\epsilon}+1 ) \ ; \left ( \vec{n}^{\top}\vec{v}_{rel}^{-}\right ) }{\frac{1}{m_{2}}+\frac{1}{m_{1}}+\vec{n}^{\top}\left [ \left ( \frac{\vec{d}_{2}\times\vec{n}}{i_{2}}\right ) \times\vec{d}_{2}+\left ( \frac{\vec{d}_{1}\times\vec{n}}{i_{1}}\right ) \times\vec{d}_{1}\right ] } } $$ where vector/matrix equals to matrix -1 * vector , and × is the vector cross product .
a wavefront ( your signal ) has a fixed amount of energy given to it by the transmitter . whatever happens to the wave once it leaves the transmitter is independent of the transmitter , thus receiving a signal does not drain any additional energy from the transmitter ( though it can drain energy from the wavefront itself ) . edit : as pointed out by @alfred centauri in the comments , the transmitter would be affected if the receiver was in the near field . for amateur radio purposes , the near field ceases to exist well within 200 meters of the transmitter ( for the vast majority of cases ) , thus it is unlikely that anyone " tuning in " to your broadcast would be directly affecting your transmitter .
strictly speaking it is a unit of energy . but using $m=\frac{e}{c^2}$ , you can convert energy into mass . operating , we get $1{\rm\ , ev}/c^2 =1.78\cdot 10^{-36}\rm{\ , kg}$ . ( the $c^2$ is usually ommited . )
the mistake you made is in the way you stated coloumb 's law . it is either $$ \vec{f} = k \frac{q_1 q_2}{r^\color{red}3} \color{red}{\vec{r}} $$ or $$ \vec{f} = k \frac{q_1 q_2}{r^\color{red}2} \color{red}{\hat{r}} $$ but definitely not $$ \vec{f} = k \frac{q_1 q_2}{r^\color{red}3} \color{red}{\hat{r}} $$
if the mass of the penny is negligible comparing to the hammer , the speed of the hammer at the end of the track should be the same in both cases . with the collision in the first scenario , the speed of the penny should be twice of the hammer if the collision is complete elastic . but , for the second move-along scenario , the speed of the penny will be just the same as the hammer at the end of the track .
( 1 ) since $u ( \textbf{r} ) = u ( \textbf{r}+\textbf{r} ) $ , we can expand this part in terms of reciprocal lattice vectors , $u_k ( \textbf{r} ) = \sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_\textbf{k-g}}$ . we can therefore write : \begin{equation} \psi_{\textbf k+\textbf k} = e^{i ( \textbf k + \textbf k ) \cdot \textbf r}\sum_\textbf{g'}{e^{i\textbf{g'}\cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}} = e^{i\textbf k \cdot \textbf r}\sum_\textbf{g'}{e^{i ( \textbf{g'}+\textbf k ) \cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}}=e^{i\textbf k \cdot \textbf r}\sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_{\textbf k-\textbf g}} = \psi_\textbf k \end{equation} where $\textbf g = \textbf k+\textbf g'$ . ( 2 ) you can interpret $\textbf p'$ as being equal to $\textbf p$ . this is true because the real space lattice is periodic ; $\textbf k$ is always equal to $\textbf k + \textbf k$ . ( 3 ) the conserved quantity is $\textbf k$ $\textit mod$ $\textbf k$ . you can see that i used this fact in the answer to ( 2 ) . you can read just about any solid state physics textbooks for complete justification though my personal favorite is ziman 's theory of solids .
i have a strong reason to believe i have found the correct answer to my own question , you may correct me if i am wrong . but this image seems to explain everything about my question in one single hit : these are results from bowmaker and dartnall ( 1980 ) . relevant reference : bowmaker , j.k. , and dartnall , h.j.a. visual pigments of rods and cones in a human retina . journal of physiology , 298 , 1980 , 501-511 . it seems that the l-receptor is actually more active at the very shortest end of wavelengths than it is for just longer than what we can see as visible light . you can see curve of red going up towards the short end of the wavelength axis . the l-receptor ( associated with red ) activation is not a bell-curve over the linear wavelength axis ( as one would expect ) . that would explain the little bit purple-ish blue we see at 400 nm ! so luckily the brain is not freaking out , but the receptors are just a bit strange , probably with the goal to distinguish blue from more blue ( from a functional view of ' evolution' ) . note that it is logical that this is not the case on the right ( longer wavelength ) side of the graph , because there red is accompanied by green closely . thus we can distinguish red from redder by the mixture of green .
in standard newtonian mechanics , acceleration is indeed considered to be an absolute quantity , in that it is not determined relative to any inertial frame of reference ( constant velocity ) . this fact follows directly from the principal that forces are the same everywhere , independent of observer . of course , if you are doing classical mechanics in an accelerating reference frame , then you introduce a fictitious force , and accelerations are not absolute with respect to an " inertial frame " or other accelerating reference frames - though this is less often considered , perhaps . note also that the same statement applies to einstein 's special relativity . ( i do not really understand enough general relativity to comment , but i suspect it says no , and instead considers other more fundamental things , such as space-time geodesics . )
the reason for the asseveration if time $t$ , does not appear in lagrangian $\mathcal{l}$ , then the hamiltonian $\mathcal{h}$ is conserved . this is the energy conservation unless the potential energy depends on velocity . is that , from the definition of the hamiltonian as the legendre transformation , $$\mathcal{h}\equiv\sum_i\dot{q}_i\frac{\partial\mathcal{l}}{\partial\dot{q}_i}-\mathcal{l}\hspace{1in} ( \dagger ) $$ and knowing that for any function in phase space , $f=f ( q_i , p_i , t ) $ , $$\frac{df}{dt}=\frac{\partial{f}}{\partial{q}_1}\frac{d{q_1}}{d{t}}+\ldots+\frac{\partial{f}}{\partial{q}_n}\frac{dq_n}{dt}+\frac{\partial{f}}{\partial{p}_1}\frac{d{p_1}}{d{t}}+\ldots+\frac{\partial{f}}{\partial{p}_n}\frac{dp_n}{dt}+\frac{\partial{f}}{\partial{t}}\\=\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\dot{q}_j+\frac{\partial{f}}{\partial{p}_j}\dot{p}_j\right ) +\frac{\partial{f}}{\partial{t}}\\=\left\{f , \mathcal{h}\right\}+\frac{\partial{f}}{\partial{t}}$$ where $\left\{f , \mathcal{h}\right\}$ is the poisson bracket of $f$ and $\mathcal{h}$ , defined as $$\left\{f , \mathcal{h}\right\}\equiv\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\dot{q}_j+\frac{\partial{f}}{\partial{p}_j}\dot{p}_j\right ) =\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\frac{\partial{\mathcal{h}}}{\partial{p}_j}-\frac{\partial{f}}{\partial{p}_j}\frac{\partial{\mathcal{h}}}{\partial{q}_j}\right ) $$ if $\mathcal{l}=\mathcal{l} ( q_i , p_i ) $ , i.e. the lagrangian does not depend explicitly on time , which in turn means , from the definition $\mathcal{l}\equiv{t}-v$ , that kinetic energy $t$ and potential $v$ does not depend explicitly on time , then $\frac{d\mathcal{h}}{dt}=\left\{\mathcal{h} , \mathcal{h}\right\}=0$ . now , a constant of motion is precisely some function $f$ of phase space that is independent of time , i.e. such that $\frac{df}{dt}=0$ , so in this case the hamiltonian would be conserved . now , from the definition $ ( \dagger ) $ , you may verify that the hamiltonian equals the energy , $$\mathcal{h}\equiv{t}+v$$ only if $v=v ( q_i ) $ alone . so if that is the case , then energy would be conserved . so identify that in your lagrangian and get your conclusions , anyway you can always verify it this way for your particular case .
to answer your question , you should first understand when is a system most stable . firstly it should not have a tendency to move or change state , thus it should be under equilibrium conditions , i.e. the net force should be zero . we know that $$f = - \frac{du}{dx}$$ putting $f=0$ , we get $$\frac{du}{dx}=0 \tag{1}$$ secondly , it should be able to maintain that equilibrium condition by itself . this can be tested by displacing the system by a small distance $dx$ . if the force on the system then becomes opposite to direction of $dx$ , we can say that the system has a tendency to restore back to its original equilibrium position . an example of this would be a ball kept at the bottom of a spherical valley . displace the ball a little towards the right , and the net force on it acts towards left , bringing it back to its original position . you will realise that i just described a stable equilibrium condition . what this proves is that it is the stable equilibrium condition in which the system is most stable . from the above description we have that the small displacement $dx$ and net extra force $df$ should be in opposite directions . $$\frac{df}{dx} &lt ; 0$$ $$-\frac{d^2u}{dx^2}&lt ; 0$$ $$\frac{d^2u}{dx^2}&gt ; 0\tag{2}$$ from $ ( 1 ) $ and $ ( 2 ) $ it is evident that the graph of $u$ should have a minima at stable equilibrium condition , i.e. the potential energy should be minimum when a system attains maximum stability .
notice : pertubative string theory is defined to be the asymptotic perturbation series which are obtained by summing correlators/n-point functions of a 2d superconformal field theory of central charge -15 over all genera and moduli of ( punctured ) riemann surfaces . perturbative quantum field theory is defined to be the asymptotic perturbation series which are obtained by applying the feynman rules to a local lagrangian -- which equivalently , by worldline formalism , means : obtained by summing the correlators/n-point functions of 1d field theories ( of particles ) over all loop orders of feynman graphs . so the two are different . but for any perturbation series one can ask if there is a local non-renormlizable lagrangian such that its feynman-rules reproduce the given perturbation series at sufficiently low energy . if so , one says this lagrangian is the effective field theory of the theory defined by the original perturbation series ( which , if renormalized , is conversely then a " uv-completion " of the given effective field theory ) . now one can ask which effective quantum field theories arise this way as approximations to string perturbation series . it turns out that only rather special ones do . for instance those that arise all look like anomaly-free einstein-yang-mills-dirac theory ( consistent quantum gravity plus gauge fields plus minimally-coupled fermions ) . not like $\phi^4$ , not like the ising model , etc . ( sometimes these days it is forgotten that qft is much more general than the gauge theory plus gravity plus fermions that is seen in what is just the standard model . qft alone has no reason to single out gauge theories coupled to gravity and spinors in the vast space of all possible anomaly-free local lagrangians . ) on the other hand now , within the restricted area of einstein-yang-mills-dirac type theories , it currently seems that by choosing suitable worldsheet cfts one can obtain a large portion of the possible flavors of these theories in the low energy effective approximation . lots of kinds of gauge groups , lots of kinds of particle content , lots of kinds of couplings . there are still constraints as to which such qfts are effective qfts of a string perturbation series , but they are not well understood . ( sometimes people forget what it takes to defined a full 2d cft . it is more than just conformal invariance and modular invariance , and even that is often just checked in low order in those " landscape " surveys . ) in any case , one can come up with heuristic arguments that exclude some einstein-yang-mills-dirac theories as possible candidates for low energy effective quantum field theories approximating a string perturbation series . the space of them has been given a name ( before really being understood , in good tradition . . . ) and that name is , for better or worse , the " swampland " . for this text with more cross-links , see here : http://ncatlab.org/nlab/show/string+theory+faq#relationshipbetweenquantumfieldtheoryandstringtheory
what the definition needs to capture is that a black hole is not ( 1 ) a naked singularity , or ( 2 ) a big bang ( or big crunch ) singularity . we also want the definition to be convenient to work with so that , for example , it is possible to prove no-hair theorems . since we want to exclude naked singularities , it is natural that we require an event horizon . event horizons are by their nature observer-dependent things . for example , if we have a naked singularity , we can always hide its nakedness by picking an observer who is far away from it and accelerating continuously away from it . such an accelerated observer always has an event horizon , even in minkowski space . this example shows that it makes a difference what observer we pick . actually , we can not have a material observer at null infinity , since timelike infinity , not null infinity , is the elephants ' graveyard for material observers . however , the choice of null infinity is the appropriate one because a black hole is supposed to be something that light can not escape from . of course the actual universe is not asymptotically flat , but that does not matter . in practice , all we care about is that the black hole is surrounded by enough empty space so that the notion of light escaping from it is well defined for all practical purposes . there are other possible ways of defining a black hole , e.g. , http://arxiv.org/abs/gr-qc/0508107 .
http://en.wikipedia.org/wiki/containment_building by " nuclear reactor " i take it you mean the containment building that is visible looking at a nuclear reactor site . the pressure vessel which houses the nuclear reactor itself is not quite bell shape , and the containment building itself is not always bell shape either . here is an image from that article . you can browse through wikipedia for actual pictures of examples . i typically refer to these containment buildings being " can " shape or " sphere " shape . the best way to explain the reason for these two extreme shapes is to consider the two extremes from a design perspective . that is : limiting factor is the internal steam pressure in an accident limiting factor is material strength to hold up the containment itself if #1 is the case , the objective is to keep the containment from leaking or blowing apart , and you will make the containment out of steel . steel has good tensile strength and you will also build it spherical . this should probably go without saying , but the strongest structure to hold a pressure inside is a sphere . but that is not always the restricting engineering limit . consider the case that a massive massive volume is needed . large volume , lower maximum pressure . in that case , the ideal material selection will change , and you will objectively use a material that is cheaper and still has good specific compressive strength . trying to build a large volume , lower maximum pressure , containment will lead to a more " can " like shape , where you basically build a cylinder . this lets you build tall , keeping the forces compressive , and still not sacrifice too much in terms of strength against an internal pressure . of course , you still need a roof . generally , containment buildings exhibit a compromise between these two cases , which is what i was intending to demonstrate in that sketch . really , it is incorrect to say they are always dome/bell shape . the shape varies , and varies strongly country to country , and generation to generation , reflecting different engineering design choices .
it seems most experts think the abiotic theory is nonsense see http://www.theoildrum.com/story/2005/11/4/15537/8056 for example dr . jon clarke : . the fact remains that the abiotic theory of petroleum genesis has zero credibility for economically interesting accumulations . 99.9999% of the world 's liquid hydrocarbons are produced by maturation of organic matter derived from organisms . to deny this means you have to come up with good explanations for the following observations . the almost universal association of petroleum with sedimentary rocks . the close link between petroleum reservoirs and source rocks as shown by biomarkers ( the source rocks contain the same organic markers as the petroleum , essentially chemically fingerprinting the two ) . the consistent variation of biomarkers in petroleum in accordance with the history of life on earth ( biomarkers indicative of land plants are found only in devonian and younger rocks , that formed by marine plankton only in neoproterozoic and younger rocks , the oldest oils containing only biomarkers of bacteria ) . the close link between the biomarkers in source rock and depositional environment ( source rocks containing biomarkers of land plants are found only in terrestrial and shallow marine sediments , those indicating marine conditions only in marine sediments , those from hypersaline lakes containing only bacterial biomarkers ) . progressive destruction of oil when heated to over 100 degrees ( precluding formation and/or migration at high temperatures as implied by the abiogenic postulate ) . the generation of petroleum from kerogen on heating in the laboratory ( complete with biomarkers ) , as suggested by the biogenic theory . the strong enrichment in c12 of petroleum indicative of biological fractionation ( no inorganic process can cause anything like the fractionation of light carbon that is seen in petroleum ) . the location of petroleum reservoirs down the hydraulic gradient from the source rocks in many cases ( those which are not are in areas where there is clear evidence of post migration tectonism ) . the almost complete absence of significant petroleum occurrences in igneous and metamorphic rocks ( the rare exceptions discussed below ) . the evidence usually cited in favour of abiogenic petroleum can all be better explained by the biogenic hypothesis e.g. : rare traces of cooked pyrobitumens in igneous rocks ( better explained by reaction with organic rich country rocks , with which the pyrobitumens can usually be tied ) . rare traces of cooked pyrobitumens in metamorphic rocks ( better explained by metamorphism of residual hydrocarbons in the protolith ) . the very rare occurrence of small hydrocarbon accumulations in igneous or metamorphic rocks ( in every case these are adjacent to organic rich sedimentary rocks to which the hydrocarbons can be tied via biomarkers ) . the presence of undoubted mantle derived gases ( such as he and some co2 ) in some natural gas ( there is no reason why gas accumulations must be all from one source , given that some petroleum fields are of mixed provenance it is inevitable that some mantle gas contamination of biogenic hydrocarbons will occur under some circumstances ) . the presence of traces of hydrocarbons in deep wells in crystalline rock ( these can be formed by a range of processes , including metamorphic synthesis by the fischer-tropsch reaction , or from residual organic matter as in 10 ) . traces of hydrocarbon gases in magma volatiles ( in most cases magmas ascend through sedimentary succession , any organic matter present will be thermally cracked and some will be incorporated into the volatile phase , some fischer-tropsch synthesis can also occur ) . traces of hydrocarbon gases at mid ocean ridges ( such traces are not surprising given that the upper mantle has been contaminated with biogenic organic matter through several billion years of subduction , the answer to 14 may be applicable also ) . the geological evidence is utterly against the abiogenic postulate . also abiogenic origin of hydrocarbons : an historical overview by dr . geoffrey lasby abstract : the two theories of abiogenic formation of hydrocarbons , the russian-ukrainian theory of deep , abiotic petroleum origins and thomas gold 's deep gas theory , have been considered in some detail . whilst the russian-ukrainian theorywas portrayed as being scientifically rigorous in contrast to the biogenic theory which was thought to be littered with invalid assumptions , this applies only to the formation of the higher hydrocarbons from methane in the upper mantle . in most other aspects , in particular the influence of the oxidation state of the mantle on the abundance of methane , this rigour is lacking especially when judged against modern criteria as opposed to the level of understanding in the 1950s to 1980s when this theory was at its peak . thomas gold 's theory involves degassing of methane from the mantle and the formation of higher hydrocarbons from methane in the upper layers of the earth 's crust . however , formation of higher hydrocarbons in the upper layers of the earth 's crust occurs only as a result of fischer-tropsch-type reactions in the presence of hydrogen gas but is otherwise not possible on thermodynamic grounds . this theory is therefore invalid . both theories have been overtaken by the increasingly sophisticated understanding of the modes of formation of hydrocarbon deposits in nature .
both rotation and translation . it will execute neither pure rotation nor pure translation
waves are generated by wind . but aside of small local winds , most of the waves you see at the shore are generated by stronger winds far out on the sea from where they start to propagate into all kinds of directions . imagine a point in the middle of the sea where a waves with different directions are created . they will propagate away from this point until they hit a shore , therefore they can only move towards the shore not away from it . this is also why waves always brake towards the shore and not away from it . waves in deep water are more or less circular waves which will go unhindered until the ground is too shallow for the circular motion to go on . see here :
i do not know the problem specifically , but i can easily imagine how 1-y came up . just try to draw a prism using the three points ( 0,0 ) , ( 0,1 ) and ( 1,0 ) on the xy-plane . now try to figure out the linear equation that governs the line between ( 0,1 ) and ( 1,0 ) , and you will see that the line 's equation is $x=1-y$ . by doing a surface integral on surface that is limited by the points i mentioned , and the integral will be ( in the simplest case for calculating the area ) : $$\iint_s ds = \int_0^1dy\int_0^{1-y}dx$$ i hope this helps .
they are two different limits in which two different constants are sent to zero and the resulting limiting theory has different names . however , both of them are limits for dimensionful constants and the analogy is perfect . the properly derived $\hbar\to 0$ limit of a quantum mechanical theory is a classical theory – its classical limit – in the very same sense in which the properly derived $k\to 0$ limit of the laws of statistical mechanics produce the laws of thermodynamics . now , the limit $k\to 0$ and $n\to \infty$ really means the same thing because the implicit assumption in all these limiting procedures is that the macroscopic quantities known from the everyday life are kept finite – and essentially fixed . this is especially the case for the energy and temperature . the thermal energy of $n$ atoms is something like $$ e=3ktn/2 $$ if both $e , t$ are fixed , it is clear that the $k\to 0$ limit is exactly the same thing as the $n\to\infty$ limit . the thermodynamic limit simply means that we are neglecting all effects in a fixed-energy system that are caused by the finiteness of the number of atoms – or , equivalently , the finiteness ( nonzero value ) of the contribution of a single atom to the whole ( which is proportional to $k$ ) . one has the freedom to describe the limit in many ways in the quantum mechanical case , too . we may say that the classical limit appears as $\hbar\to 0$ . but we may also say that the classical limit emerges when $n_j\to \infty$ where $n_j=j_z/\hbar$ , for example . when the angular momentum ( or the action $s$ ) is written as a multiple of $\hbar$ , the dimensionless coefficients ' condition $n_j\to\infty$ is the same thing as $\hbar\to 0$ because their product is fixed . classical physics needs to neglect all effects caused by the situation when the overall quantities ' describing the system are so small that they are comparable to $\hbar$ ( which is the regime where the quantum phenomena become important ) . again , we are comparing two things , so saying that one of them is infinitely larger than the other is the same thing as saying that the other one is infinitely smaller . in both situations , one has many options what $n$ or $n_j$ may exactly be . but in both cases , the limit for the dimensionful constant going to zero , whether it is $k$ or $\hbar$ , is equivalent to some dimensionless numbers ' ( those that measure how much the system is larger relatively to the statistical mechanical or quantum " basic blocks " where the more general theory shows in its full glory ) going to infinity . because the ratio of probabilities of an entropy-changing process and its time reversal goes like $\exp ( ( s_b-s_a ) /k ) $ , we see that for fixed $s_a , s_b$ in macroscopic units , the ratio becomes strictly infinite . so in thermodynamics , i.e. the thermodynamic limit of the statistical considerations , a decreasing entropy is strictly impossible . finally , let me mention that the nonrelativistic limit is analogous to the two limits above , too . we may say that the limit involves $c\to\infty$ which is clearly the same thing as $1/c\to 0$: it plays the same role as $k\to 0$ , for example . however , we may also say that the actual velocities are much smaller than $c$ in the limit , so $\beta=v/c\to 0$ or $1/\beta\to \infty$ . that is analogous to $n\to\infty$ or $n_j\to\infty$ above .
the anti-particle for any particle is obtained by charge c and parity p conjugation . c is the operation that interchanges positive and negative charges and p is the operation that reflects in a mirror . the combined operation of cp must produce a particle of the same mass . this is a theorem of relativistic quantum field theory due to cpt symmetry . this other particle is either the same particle or an antiparticle with opposite charge and/or chirality . some particles such a photons , gluons , z bosons , pions , higgs , graviton etc , do not have anti-particles because they are invariant under the cp transformation . you can say that they are their own anti-particle . this can only happen for particles without electric charge and with no chirality . in principle the qcd color charge is also reversed for an anti-particle . this suggests that a gluon should not be regarded as its own anti-particle , but since colourless states are never seen the distinction cannot really be made in any operational sense . all known particles which are their own anti-particle are bosons , but it is also possible for a fermion to be its own anti-particle if it is a majorana spinor rather than a dirac spinor . no known fermions are of this type ( unless neutrinos are majorana ) but they exist in susy models . observed elementary particles that do have anti-particles include all the quarks and leptons ( except possibly the neutrinos ) and the charged w bosons . any composite particle also has an antiparticle made of the anti-particles of its constituents . this can only be its own anti-particle if all its constituents are ( e . g . a glueball ) , or if it is made of particle/anti-particle combinations as is the case for pions .
the zeroth law posits the existence of temperature by stating that if a is in equilibrium with b and a is in equilibrium with c , then b is in equilibrium with c . we can then assign an intensive property to a , b and c that we call " temperature " . they are in equilibrium == they have the same temperature . as soon as they are not in equilibrium , the zeroth law is silent . thus , as you observed in your question , we cannot derive an ordering of temperatures based on the zeroth law alone . here , the second law comes to the rescue . the formulation i am familiar with states the entropy of a closed system never decreases if we have two objects that are not in thermal equilibrium , then when we bring them into contact we expect heat to flow between them . now according to the second law , if we move heat $\delta q$ from $a$ to $b$ ( at temperatures $t_b$ and $t_b$ respectively ) , the change in entropy is $$\delta s = t_a \delta q - t_b \delta q\\ = \delta q ( t_a - t_b ) $$ now if the entropy of the system cannot decrease , then if $\delta q$ is positive we know that $t_a - t_b$ must be positive . this is where we find the ordering of temperature : heat travels from hotter to cooler until thermal equilibrium is reached . thus when we have two objects in unequal states we can tell which is hotter by looking at the direction in which heat flows between them . that direction is always from hotter to colder - and to prove this you need the second law . there is an amusing ( although somewhat dated - 50 years old this year ) song by the duo of flanders and swann that touches on this topic . see http://youtu.be/vnbivw_1fns
construction of the helicity formula using 3-vector notation the zero component of the pauli lubanski vector $w^0 = \epsilon^{0 ijk}j_{ij}p_k = \epsilon^{ijk}j_{ij}p_k $ the angular momentum genrerators $ j^k = \epsilon^{ijk}j_{ij}$ thus $w^0 = j^k p_k = \vec{j} . \vec{p} $ the orbital angular momentum $ \vec{l} = \vec{x} \times \vec{p}$ is orthogonal to the momentum : $ \vec{l} . \vec{p} = 0$ and since the total angular momentum is the vector sum of the orbirtal and the spin angular momenta $ \vec{j} = \vec{l} + \vec{\sigma}$ thus $w^0 = j^k p_k = \vec{j} . \vec{p} = ( \vec{j-l} ) . \vec{p} = \vec{\sigma} . \vec{p} $ now , since $w^0 = \hat{h} p_0$ and for a massless particle $ p_0 = p$ we obtain : $ \hat{h} = \frac{\vec{\sigma} . \vec{p}}{p_0} = \vec{\sigma} . \hat{p}$ the helicity operator $$\hat{h} = \sigma . \hat{p}$$ where $\sigma$ is the spin operator and $\hat{p}$ is the momentum unit vector is a projection along the axis $\hat{p}$ of a spin operator , thus one might expect it to have for a helicity $\lambda$ the eigenvalues $\lambda$ , $\lambda-1$ , . . . , $-\lambda$ . however , the eigenvectors corresponding to all eigenvalues except $\pm \lambda$ are not physical , because they describe longitudinal polarizations which do not exist in free massless particles . here is an example of the massless spin-1 case ( photon ) . in this case , we may choose the spin operators as : $ \sigma_x = \begin{bmatrix} 0 and 0 and 0\\ 0 and 0 and -i\\ 0 and i and 0 \end{bmatrix}$ $\sigma_y = \begin{bmatrix} 0 and 0 and i\\ 0 and 0 and 0\\ -i and 0 and 0 \end{bmatrix}$ $\sigma_z = \begin{bmatrix} 0 and -i and 0\\ i and 0 and 0\\ 0 and 0 and 0 \end{bmatrix}$ the action of the helicity operator on ( say ) , the electric field in the momentum representation is : $$\hat{h} \vec{e} = i\begin{bmatrix} 0 and -\hat{p}_z and \hat{p}_y\\ \hat{p}_z and 0 and -\hat{p}_x\\ -\hat{p}_x and \hat{p}_x and 0 \end{bmatrix}\begin{bmatrix} e_x\\ e_y\\ e_z \end{bmatrix} = i \hat{p}\times \vec{e}$$ thus : $$\hat{h}^2 \vec{e} = - \hat{p} \times ( \hat{p}\times \vec{e} ) = \vec{e} -\hat{p} ( \hat{p} . \vec{e} ) $$ but , since for a free electromagnetic field : $$\hat{p} . \vec{e} = 0$$ we get : $$\hat{h}^2 = 1$$ , and the only admissible eigenvalues are $\pm 1$
first , i just want to remind readers that it is not true that " more glancing angle always means more reflection " . for p-polarized light , as the angle goes away from the normal , it gets less and less reflective , then at the brewster angle it is not reflective at all , and then beyond the brewster angle it becomes more reflective again : nevertheless , it is certainly true that as the angle approaches perfectly glancing , the reflection approaches 100% . even though the question asks for non-mathematical answers , the math is pretty simple and understandable in my opinion . . . here it is for reference . ( i do not have any non-mathematical answer that is better than other peoples ' . ) the maxwell 's equations boundary conditions say that certain components of the electric and magnetic fields have to be continuous across the boundary . the situation at almost-glancing angle is that the incoming and reflected light waves almost perfectly cancel each other out ( opposite phase , almost-equal magnitude ) , leaving almost no fields on one side of the boundary ; and since there is almost no transmitted light , there is almost no fields on the other side of the boundary too . so everything is continuous , " zero equals zero " . the reason this cannot work at other angles is that two waves cannot destructively interfere unless they point the same direction . ( if two waves have equal and opposite electric fields and equal and opposite magnetic fields , then they have to point the same direction , there is a " right-hand rule " about this . ) at glancing angle , the incident and reflected waves are pointing almost the same direction , so they can destructively interfere . at other angles , the incident and reflected waves are pointing different directions , so they cannot destructively interfere , so there has to be a transmitted wave to make the boundary conditions work . :- )
they are different because in a transmission line we have distributed resistance , capacitance , conductance and inductance ( meaning that each tiny segment of transmission line has its own tiny resistance , capacitance , conductance and inductance ) while in rlc circuits we have lumped resistance , inductance and capacitance . also rlgc does not model a transmission line with the circuit you have shown above but with infinite number of them in series . we know well how to deal with lumped elements and circuits containing them , but dealing with distributed elements and circuits ( e . g transmission lines ) is often much harder and we have to resort to solving maxwell equations directly . so i think not only there is no point in approaching rlc circuits from rlgc model but also it is impractical .
1 ) putting an electron in a loop would measure it is spin . you have to get it to the loop and that involves a change of flux 2 ) your magnetic field went from $\frac{1}{\sqrt{2}}|b_\uparrow\rangle+\frac{1}{\sqrt{2}}|b_\downarrow\rangle \to |b_\uparrow\rangle$ now i do not know how maxwell 's laws work on wavefunctions , but it definitely did not start at 0 . in qm , when we say a state is in a superposition , it is not the classic sort of superposition . you do not vector-add the states ( not classically-vector-add , after all , $|b_\uparrow\rangle$ itself is a vector , but it is orthogonal to $|b_\downarrow\rangle$ ) . rather , you say that it is in both states at the same time . picture a particle with x-coordinate +1 , y-coordinate -1 . it is net coordinate is not 0 . there is no such thing as a net coordinate . it is net coordinate-thingy ( you know what i mean ) can be said to be both at the same time , which is true . so we write it as $ ( 1 , -1 ) $ edit : i asked @davidzaslavsky on chat how to apply maxwell 's laws to a superposition . his logic came from the many-worlds interpretation , and basically it turns out that you just apply the laws to each ket separately and then add them ( according to their relative weightages ) . there may be some qft involved , but this seems correct to both of us . anyways , using this we get that $v=\frac{1}{\sqrt{2}}|d\phi_\uparrow/dt\rangle+\frac{1}{\sqrt{2}}|d\phi_\downarrow/dt\rangle$ . so again , your potential is also a superposition . tying to measure it will collapse it . so , having a wire loop around the electron with a voltmeter measures the potential and prematurely collapses the wavefunction of b . so we get point ( 1 ) from point ( 2 ) .
if you see it staying in one spot you can infer that it is moving directly along your line of sight , as you say . but in the more likely event that you see it move across the sky you can determine that it is moving in a given plane only . unless you have some independent way of judging its size or distance you can not tell any more . this actually happens all the time when a bug flies in front of someone 's camera and they think they have seen an interstellar visitor .
the best single source is at nist . the data is located at the chemistry webbook . once you are at that site click on formula or name under " general searches " . you can not only get all the data you want in a downloadable format , it will even graph it for you enjoy yourself . there is no better way to learn about data than to work with it .
redgrittybrick 's comment is correct . in order to make a roller turn , there has to be force whose vector does not go through the roller 's axis . it needs to avoid the axis in order to exert a turning moment . the only such forces are the frictional ones from the plank and the ground .
the identification goes as follows : $$ \text{kin . mom . }~=~ \text{can . mom . } ~-~\text{charge} \times \text{gauge pot . } $$ $$ \updownarrow $$ $$ m\hat{v}_{\mu} ~=~ \hat{p}_{\mu} - qa_{\mu} ( \hat{x} ) $$ $$ \updownarrow $$ $$ \frac{\hbar}{i} d_{\mu} ~=~ \frac{\hbar}{i}\partial_{\mu} - qa_{\mu} ( x ) $$ $$ \updownarrow $$ $$ d_{\mu} ~=~ \partial_{\mu} -\frac{i}{\hbar} qa_{\mu} ( x ) $$ $$ \updownarrow $$ $$ \text{cov . der . }~=~ \text{par . der . } ~-~\frac{i}{\hbar}\text{charge} \times \text{gauge pot . } $$ the imaginary unit $i$ is needed , e.g. because the derivative is an anti-hermitian operator ( recall the usual integration-by-part proof ) , while the momentum is required to be a hermitian operator in quantum mechanics .
what you need is the euler-bernoulli beam theory . the last three pages of this pdf explain the eigenmodes .
there are effectively two ways that liquid turns to gas : boiling : heat liquid water till it undergoes a phase transition to a gas evaporation : surface water is absorbed by the air since , usually , the air has a lower concentration of water than a damp spot on your clothing . heating the clothing accelerates this process as , at a molecular level , heated water has more kinetic energy and is more likely to be absorbed into the air than " stationary " molecules ( or ones with less kinetic energy ) . the tumbling in the dryer makes the clothing dry evenly .
given that we do not have an answer yet , i will chime in to say that i have not heard this term before despite being a nuclear physicist in grad school . isotopes are atoms with the same number of protons , but different numbers of neutrons . accordingly they have the same chemical properties , but differing atomic masses . ( actually there is a small difference in some chemical properties such as vapor pressures that can be used to extract enriched isotopic samples , but these differences are small enough that the processes tend to be expensive . ) isomers in chemistry are molecules with the same atomic content , but differing structure . the simplest example that comes to mind are the left- and right-handed versions of some large organic molecules . from that we deduce that if we found some isotope that had two stable configurations we would call them " nuclear isomers " by the obvious extension . which leads to the speculation that meta-stable nuclear states might be given that name . take that with a grain of salt . i have no references and no experience with the term in the mouths of practicing scientists .
for questions about resonances and particles the particle data group is the best reference . one can find the whole delta resonance family and remind oneself what each number is standing for , and thus know how to pronounce the symbol . the number in parenthesis is the mass in mev . the superscript is the charge of the particular resonance displayed on the plot , presumably . s , p , d , . . . are by convention the labels of the angular momentum quantum number " l " , and the two numbers are the numerators of the isospin and j quantum number ( j is the total angular momentum quantum number ) . so the first one is read as : delta zero seventeen fifty ( pee three one ) or ( pee three halves one half ) . etc . ( the superscript of parity is missing in your information . ) the bar over a symbol denotes an antiparticle , antidelta ( 1910 ) zero in the second line . i would try and put the charge next to the main symbol , your first option but the other way is clear also . for similar questions the naming scheme for hadrons would be a help in comprehension as well as pronunciation .
okay , so i did some poking around and the 66th-75th editions of the crc handbook of chemistry and physics all have the incorrect atomic mass of cu-63 [ 62.939598 ] , and from 76th edition on they seem to have figured it out . those isotope mass tables are put together from a number of sources , so it is hard ( time consuming ) to tell exactly where the error came from . i did notice however that starting in the 76th edition of the crc , where they get it right , they start citing g . audi and a.h. wapstra , " the 1993 atomic mass evaluation " , nuc . phys . a 565 ( 1993 ) 1-65 . in editions 66-75 , they were citing audi and wapstra 's " the 1983 atomic mass table " which appeared in nuc . phys 432 , 1 ( 1985 ) . now , i looked at the 1993 version , and it has the correct 62.929 . . mass , but i have not been able to find wapstra and audi 's 1983 version of the same table , so i do not know if it was one error by the crc which got carried over year after year , or if 62.939 . . is in fact the value given in that paper . i did find at least one piece of evidence which points to an error by the editors of the crc in the 2nd edition of the encyclopedia of physics [ lerner and trigg , 1991 ] . their table of isotopes lists the correct 62.929 . . . value and also cites wapstra and audi 's 1983 paper . i hope that satisfies everyone 's curiosity , because i do not think i can do any better then that . ; - )
the moon was moving in relation to what ? i had to think about this when you described your experience . because of the distance of the moon compared to objects on the horizon , it should appear to stand still in comparison to everything else closer whizzing past you . but then i thought of your description of " oscillations " and thought " aha ! the window . " if you were travelling exactly west to east , and you were observing the moon through a window exactly facing south ( or north , depending which side of the train you are on and the time of year and where on earth you are , ) it would appear to be standing still in your window . ( neglecting for the moment the comparatively slow apparent progression of the moon through the sky . ) but i reckon your train track was not exactly straight for the duration of your trip and the moon was never exactly perpendicular to your train 's direction . but it was probably quite close to perpendicular . so any slight deviation would cause you to observe the moon travelling towards one edge of the window , and then a slight deviation in the other direction would cause the moon to apparently move towards the other edge of the window . it is simply the winding s-shaped route your train took , heading generally east but never exactly , that caused these oscillations .
at the risk of telling you how to " suck eggs " ( your level in these things is not altogether clear ) , here goes . ingredients : the essential ingredients to this explanation are : a physical " system " which evolves in and whose " events " happen in some space $\mathcal{u}$ ( ordinary euclidean 3-space or minkowsky spacetime , for example ) ; in physics this space is always a linear ( wontedly it is minkowsky spacetime ) space wherein stuff happens : let 's call $\mathcal{u}$ the " scene " of where stuff we want to talk about happens ; a connected lie group $\mathfrak{g}$ which represents the co-ordinate transformations the a system can undergo : in physics these are all linear transformations $\mathcal{u}\to\mathcal{u}$ of the scene $\mathcal{u}$ . wontedly in physics , $\mathfrak{g} = so^+ ( 1,3 ) $ ( the identity connected component of the lorentz group comprising all rotations and boosts of " physical space " , sometimes called the " proper , orthochronous lorentz group " ( proper = unimodular determinant =1 , i.e. " does not invert space " and orthochronous = does not switch the time direction ) or the poincaré group ; a cover of $\mathfrak{g}$ ; this is almost always ( i have never seen it not so ) the universal cover $\tilde{\mathfrak{g}}$ of $\mathfrak{g}$ as explained in my article " lie group homotopy and global topology " on my website here ; a vector space $\mathcal{v}$ which can be , for example , a quantum state space , possibly infinite dimensional and its group $gl ( \mathcal{v} ) $ of linear endomorphisms , i.e. bijetive , linear maps $\phi:\mathcal{v}\to\mathcal{v}$ . informally , $gl ( \mathcal{v} ) $ is the group of invertible matrices acting on $\mathcal{v}$ . most importantly : this space is different from the physical " scene " $\mathcal{u}$ . the scene is $\mathcal{u}$ spacetime all around us , the state space $\mathcal{v}$ is a hilbert space of quantum states . and , actually , although we talk about a " linear " state space $\mathcal{v}$ , we are a bit sloppy : sure , all quantum states are linear superpositions of the basis for $\mathcal{v}$ but they are always of unit magnitude : the probabilities of a measurement 's " collapsing " the state into one of the basis vectors must all sum up to one - " we have to end up in some state " . so , if we are being precise , we take heed that we are actually talking about the unit sphere within $\mathcal{v}$ as the state of quantum states . this state space is very different in character from spacetime , where there is no obligation for 4-positions of events to be unit magnitude ; representations $\rho : \mathfrak{g}\to gl ( \mathcal{v} ) $ , $\tilde{\rho}:\tilde{\mathfrak{g}}\to gl ( \mathcal{v} ) $ of both $\mathfrak{g}$ and its cover $\tilde{\mathfrak{g}}$ , respectively . recall that a representation of a lie group $\mathfrak{g}$ is a homomorphism from from $\mathfrak{g}$ to $gl ( \mathcal{v} ) $ , i.e. a transformation that " respects the group product " so that , given $\gamma , \ , \zeta\in\mathfrak{g}$ , we have $\rho ( \gamma\ , \zeta ) =\rho ( \gamma ) \ , \rho ( \zeta ) $ . and , as discussed above , the linear transformations of the form $\rho ( \gamma ) , \ , \tilde{\rho} ( \tilde{\gamma} ) \in gl ( \mathcal{v} ) $ for $\gamma \in\mathfrak{g}$ and $\tilde{\gamma} \in\tilde{\mathfrak{g}}$ must be unitary so that the transformed quantum states stay normalised . so we can see that $gl ( \mathcal{v} ) $ is very different from $\mathfrak{g}$ or $\tilde{\mathfrak{g}}$: lorentz boosts most assuredly are not unitary ! we say that $\mathfrak{g}$ or $\tilde{\mathfrak{g}}$ " act on the state space $\mathcal{v}$ through the respresentations $\rho$ , $\tilde{\rho}$" . i am using here more of the mathematician 's description of a representation , because here ( i am not always so fussed ) i believe it is clearer than the physicists because we need to take heed that there are two different classes of representations in our discussion : those whereby the group of co-ordinate transformations $\mathcal{g}$ act on the state space $\mathcal{v}$ and those whereby its cover $\tilde{\mathcal{g}}$ acts on $\mathcal{v}$ . baking instructions : wigner 's theorem and why covers are interesting why are we interested in covers at all ? after all , elements of $\tilde{g}$ are not the " physical " co-ordinate transformation . this is where we meet our baking oven for our ingedients : wigner 's theorem . clearly , when our scene $\mathcal{u}$ undergoes a co-ordinate transformation , then the transformations wrought on the quantum state has to preserve inner products in the quantum state space so that the state stays properly normalised . from this assumption alone , i.e. one does not have to assume linearity , wigner proved that the when the scene $\mathcal{u}$ undergoes a " symmetry " ( a lorentz transformation ) , the state space must undergo a " projective homomorphism " $\sigma$ , i.e. if $\gamma , \ , \zeta$ are two lorentz transformations , then the state space transformation corresponding to their product is : $$\sigma ( \gamma\ , \zeta ) = \pm \sigma ( \gamma ) \ , \sigma ( \zeta ) \tag{1}$$ the fact that we do not get exactly a homomorphism is why we are interested in covers : the image of the representation $\tilde{\rho} ( \tilde{\mathfrak{g}} ) $ ( recall that this is a group of unitary operators in $gl ( \mathcal{v} ) $ acting on the state space ) of the cover $\tilde{\mathfrak{g}}$ contains both the transformations that fulfill the genuine $+$-sign homomorphism in ( 1 ) ( which are simply unitary operators in the image $\rho ( \mathfrak{g} ) $ of the co-ordinate transformation group $\mathfrak{g}$ ) and those that flip the sign . so if we allow representations of the cover , we get every possible unitary transformation ( even without an assumption of linearity - this automatically follows ) that can be wrought on the state space $\mathcal{v}$ when the scene $\mathcal{u}$ is transformed . here 's the punchline . quantum states that transform by the transformations belonging to the image $\rho ( \mathfrak{g} ) $ of $\mathfrak{g}$ under the genuine homomorphism $\rho$ are called vectors . quantum states that transform by the transformations belonging to the image $\tilde{\rho} ( \tilde{\mathfrak{g}} ) $ of the cover $\tilde{\mathfrak{g}}$ under the " projective homomorphism " $\tilde{\rho}$ are called spinors . the above can be intuitively thought of as follows : in quantum mechanics , a global phase $e^{i\phi}$ multiplying a system 's quantum state does not affect measurements we make on the system . so quantum systems " do not care whether a homomorphism is genuine or projective " . the universal ( only ) cover of the lorentz group $so ( 1,3 ) $ is the group $sl ( 2 , \ , \mathbb{c} ) $ . so " spinors " transform by a representation of $sl ( 2 , \ , \mathbb{c} ) $ . vectors transform by a representation of $so ( 1,3 ) $ . the word " spinor " can be pretty vague in my experience : it can refer to the transformation in $sl ( 2 , \ , \mathbb{c} ) $ rather than the quantum state that is transformed by it , and people often speak of the unit quaternions as " spinors": roger penrose 's " road to reality " chapter 11 simply defines a spinor as something that takes a negative sign when rotated through $2\ , \pi$ and comes back to its beginning point after a rotation through $4\ , \pi$ . this is actually a pretty good definition , for that is exactly how elements of a representation of $sl ( 2 , \ , \mathbb{c} ) $ act on the state space $\mathcal{v}$ , and is the essential difference between how elements of a representation of $so ( 1,3 ) $ act on state spaces . forget about " quantities with direction " as a vector 's definition : in physics the word " vector " always talks about how something transforms when our scene $\mathcal{u}$ undergoes a symmetry . remember this is pretty near to the word 's literal meaning vehor ( transliterated as vector ) literally means " i am borne " or " i am carried " in latin , so it is all about how the " vector " is borne , either by a transformation in physics or as a pathogen in biology ( the word 's original english meaning ) .
you can not argue with a black object and a white object alone , as i think you partially understand in trying to build your thought experiment . you need a little bit more to define things properly . see whether the following helps . imagine a black object at a temperature $t_0$ and a white object also at $t_0$ inside a perfectly isolating box full of blackbody radiation at some higher temperature $t_1&gt ; t_0$ ( i.e. without the black and white objects , this radiation is in thermodynamic equilibrium ) . to understand exactly what would happen , you would have to describe the " colour " of your objects with emissivity curves that show emissivity as a detailed function of frequency . so your " black " and " white " would need to be defined in much more detail . you would also have to define the surface areas of the two objects and what they are made of ( i.e. define their heat capacities ) . but all of this only effects the dynamics of how the system reaches its final state , i.e. these details only influence how the system evolves . what it evolves to is the same no matter what the details : the box would end up with everything at the same temperature such that the total system energy is , naturally , what it was at the beginning of the thought experiment . " blacker " as opposed to " whiter in this context roughly means " able to interact , per unit surface area , with radiation more swiftly": the blacker object 's temperature will converge to that of the radiation more swiftly than does that of the whiter object , but asymptotically the white object " catches up " . blacker objects absorb more of their incident radiation its true , but they also emit more powerfully than a whiter object at the same temperature . the one concept emissivity describes the transfer in both directions . think of emissivity as being a fractional factor applied to the stefan-boltzmann constant for the surface as well as being the fraction of incident light absorbed by the surface relative to a perfect blackbody radiator . this description is altogether analogous to that of the situation where $t_0&lt ; t_1$ . begin with $t_0=t_1$ , and you have got thermodynamic equilibrium from the beginning . nothing happens , of course . maybe the following will help thinking about what is a really quite a complex question : it would be a fantastic last question for an undergrad thermodynamics exam btw : you can abstract detail away by saying lets define object $a$ to be blacker than object $b$ if , when both objects are made of the same material , are the same size and shape , the temperature of $a$ converges to the final thermodynamic equilibrium temperature more swiftly than that of $b$ when they are both compared in the box-radiation-object thought experiment above . thinking about this now , i am not sure whether the above definition would hold for every beginning temperature of the radiation . maybe there are pairs of surfaces whose relative blackness is different at different beginning temperatures such that $a$ is blacker than $b$ with some beginning temperature whilst the order swaps at a different beginning temperature . i think it is unlikely , but that is probably a different question altogether . by the way , which pub do you drink in ? i might come along . afterword on a heater 's colour : you ask by implication what is the best colour to paint a heater . this is not a simple question and involves the dynamics of the heater system . it is really an engineering question . i suspect in general it is better for them to be blacker rather than whiter . here 's a glimpse of the kind of factors bearing on the situation . if you can say a heater has a constant nett input of $p$ watts , then at steady state that is going to be its output to the room , altogether regardless of its colour . there may be a materials engineering implication here : if you paint the heater whiter , and if its dominant heat transfer to the room is by radiation ( rather than by convection or conduction ) , then it has to raise itself to a higher temperature than it would were it blacker so as to radiate $p$ watts into the room . so its materials might not be as longlasting , and it might be more of a fire hazard than it would be were it blacker . if the heater is the hot water kind , and again if radiative transfer is significant , then the heating system has to run hotter to output power at a given level if the heater is whiter . at a given flow rate and given temperature of heating water , the heat output of heater is lower if it is whiter . you are trying to design the heater to be an " anti-insulator": you want the heat to leak out of the flow circuit in at the heater , not through the lagging on the hot water pipes outside the building channelling the water from the boiler to the heaters . if the hot water pipes leak heat in the same room , then that is no problem . recall the quartic dependence of the stefan boltzmann law . at room temperatures with a low temperature heater ( the hot water kind ) $\sigma\ , t^4$ is likely to be pretty small compared with other heat transfer mechanisms , in contrast to my idealised scenarios above . so the heater 's colour is likely to be pretty irrelevant .
with a delta function potential , the particle is free on either side of the barrier : $$ \psi ( x ) =\begin{cases}\psi_l ( x ) =a_re^{ikx}+a_le^{-ikx} \\ \psi_r ( x ) =b_re^{ikx}+b_le^{-ikx}\end{cases} $$ where $a_i , \ , b_i$ are constants such that $a_r+a_l=b_r+b_l$ ( i.e. . , $\psi ( x ) $ satisfies the continuous function condition ) . but at the barrier we have the issue that $v ( 0 ) =\infty$ . so to resolve this issue , we use schroedinger 's equation and integrate it over some small region $\left [ -\epsilon , \ , \epsilon\right ] $ and then let $\epsilon\to0$: $$ -\frac{\hbar^2}{2m}\int_{-\epsilon}^\epsilon\psi''\ , dx+\int_{-\epsilon}^\epsilon v\psi\ , dx=e\int_{-\epsilon}^\epsilon\psi\ , dx $$ the first term is clearly $d\psi/dx$ evaluated at two points . the last term goes to zero in the limit $\epsilon\to0$ ( recall that $e$ is constant and finite , so that as $\epsilon\to0$ , the width goes to 0 and so does the whole value ) . for the potential term , the delta function has the great property that $$ \int\delta ( x-a ) f ( x ) \ , dx=f ( a ) $$ thus , that middle term becomes $\left . \psi ( x ) \right|_{-\epsilon}^\epsilon$ . we then combine these two to get $$ -\frac{\hbar^2}{2m}\left [ \psi'\left ( +\epsilon\right ) -\psi' ( -\epsilon ) \right ] +\left . \lambda\psi ( x ) \right|_{-\epsilon}^{\epsilon}=0 $$ as $\epsilon\to0$ , we can get the relation you are confused over : $$ \psi'_r ( 0 ) -\psi'_l ( 0 ) =+k\psi ( 0 ) $$
if you have a copy of griffiths , he has a nice discussion of this in the delta function potential section . in summary , if the energy is less than the potential at $-\infty$ and $+\infty$ , then it is a bound state , and the spectrum will be discrete : $$ \psi\left ( x , t\right ) = \sum_n c_n \psi_n\left ( x , t\right ) . $$ otherwise ( if the energy is greater than the potential at $-\infty$ or $+\infty$ ) , it is a scattering state , and the spectrum will be continuous : $$ \psi\left ( x , t\right ) = \int dk \ c\left ( k\right ) \psi_k\left ( x , t\right ) . $$ for a potential like the infinite square well or harmonic oscillator , the potential goes to $+\infty$ at $\pm \infty$ , so there are only bound states . for a free particle ( $v=0$ ) , the energy can never be less than the potential anywhere , so there are only scattering states . for the hydrogen atom , $v\left ( r\right ) = - a / r$ with $a &gt ; 0$ , so there are bound states for $e &lt ; 0$ and scattering states for $e&gt ; 0$ .
energy is associated with work not with force as work energy theorem states . in other words , force can be exerted without generating any work , in such a case whatever exerted the force does not lose or gain any energy because no work was done . in your example , the rubber slope exerted force ( friction force ) but no work was done ( the metal block did not move ) . so there is no exchange of energy although force is exerted . think of the force in this case as energy-free action . the energy of the slope has nothing to do with the gravitational potential energy of the metal block
the roche limit applies when the astronomical body in question is held together by gravity rather than electromagnetic forces . this is the case for bodies with a diameter larger than around 500km . obviously for smaller bodies , like humans , we can get arbitrarily close to the surface , but i suspect this is not what you are asking about . for moons much smaller than the planet they are orbiting , and assuming the moon and the planet have roughly equal densities , the roche limit is about 2.44$r_p$ , where $r_p$ is the radius of the planet . the angle subtended by the planet from the moon is 2 arctan ( 1/2.44 ) or about 45º . so assuming you take the sky to cover 180º , at the roche limit the planet will cover a quarter of the sky ( by width , rather less by area ! ) . if the density of the moon is much greater than the planet the roche limit will be reduced and the planet can look bigger , and likewise if the density of the moon is lower the maximum size of the planet would smaller . however the roche limit varies as the cube root of the density ratio , so you need a big density difference to make much difference to the roche limit . response to comment : if you include the densities the expression of the roche limit is : $$ d = 2.44 r_p \left ( \frac{\rho_p}{\rho_m} \right ) ^{1/3} $$ where $\rho_p$ is the density of the planet and $\rho_m$ is the density of the moon . the average density of jupiter is 1.33kg/m$^3$ and the average density of the moon is 3.35kg/m$^3$ , and substititing these values gives the roche limit as 1.79$r_p$ . using the formula for the angle gives about 58º . you can use the formula to calculate what density ratio is required for the roche limit to fall to $r_p$ , i.e. for the moon to touch the planet 's surface . the required density ratio is about 15 . this could be attained for jupiter if the moon was made of pure osmium ( the densest element ) but this is , to say the least , unlikel ; y to occur in nature .
the uncertainty principle applies to any quantum system , and is way more general than just single particle examples . it is defined for any pair of operators ( physical quantities ) $a$ and $b$ , with the system in a state $|\psi\rangle$ $$ \delta a \ ; \delta b \geq \frac{\hbar}{2} \langle \psi| [ a , b ] |\psi \rangle$$ note : the constant factor ( $\frac{1}{2}$ here ) varies in different derivations , depending on how exactly you define $\delta a$ and $\delta b$ , but the essence is the same . in the case of simple quantum systems , you could take $a$ to be the position operator and $b$ to be the momentum operator . in your case , it seems like you would like to consider the whole nucleus as an effective particle and apply these operators on it is wavefunction/state . sure , you could do that , and you will get an uncertainty relation from that .
you are right , fourier transform spectrometer is just a scanning michelson interferometer . in spectroscopic applications these are just synonims . spectral information is a fourier transform of intensity dependence on path length , thus the name . usually a term wavemeter is used , meaning some interferometric device to measure wavelength , including fts as well as fabry–pérot interferometers .
the bicep2 paper reports a tensor/scalar ratio r=0.20+0.07−0.05 , but then says : this is the value taken before corrections . there exist contributions to the b- mode due to changes in the photon polarization of the cmb while it is traveling before reaching the detector . the dust is the interstellar dust that has to be modeled . subtracting the various dust models and re-deriving the r constraint still results in high significance of detection . for the model which is perhaps the most likely to be close to reality ( ddm2 cross ) the maximum likelihood value shifts to r=0.16+0.06−0.05 which value is proper to compare with planck 's r&lt ; 0.11 ? assuming that planck has corrected for the dust before giving its limit , ( a reasonable assumption since the existence of dust is known ) it is the second value you have to compare with , which is different only by 1 sigma from the bound given by planck , i.e. is consistent . at these errors one does not call measurements definitive . once many standard deviations separate old from new measurements , old ones can be ignored .
i think it is pretty obviously in your last line . so what are the results of the commutator $ [ p_r , p_\theta ] $ and $ [ p_\theta , p_\theta ] $ ? note that $p_\theta$ is not commute with $\theta$ . then you should get the answer .
the answer is 2 . zoom isnt measured in distance , it is more like a ratio of magnification . so it tells you how much you magnified something , ofcourse for lenses the zoom is slightly more technical . a lightyear is a distance , its actually the distance travelled by light in 1 year , so when you see stars 1 ly . away , you see how they appeared 1 year ago .
you can always decompose a motion like this into two parts : ( 1 ) rolling without slipping and ( 2 ) slipping without rolling . what is slipping without rolling ? it means the object moves uniformly in one direction along the surface , with no angular velocity about the object 's own center of mass . for instance , a box that is pushed along the ground can easily slip without rolling . unfortunately , most people seem to assume that you can infer some physically important information from your own notion of what slipping is , without having to define it . i believe this is done to try to connect to intuition , but in the process , things get a lot more nebulous and ill-defined . to me , it is easier to think about this in terms of the object 's rotation--it was never obvious to me that the point in contact with the ground does not have velocity at the instant it touches . i prefer to think instead that an object that rolls without slipping travels 1 circumference along the ground for every for every full rotation it makes . and object that travels more than this distance ( or that does not rotate at all ) is slipping in some way . then , eventually , we can get to the notion that the point in contact during rolling cannot have nonzero velocity through whatever logical or physical arguments necessary . but as is usual in physics , it is not really clear what definition should be considered " fundamental " with other results stemming from it . this emphasizes that physics is not built axiomatically .
wet wood crackles . dry wood does not . water in the wood boils . the steam builds up pressure because it is trapped inside . the wood explodes , releasing the steam and flying pieces .
the mass will play the role in the relaxation time to go from a ballistic regime in the langevin equation to an overdamped regime where only diffusion matters . the bigger the mass , the higher the inertia and therefore the longer the time it takes to reach the overdamped regime . once the overdamped regime is reached or , to phrase it differently , if your time window allows you to only see the overdamped regime in both cases then you will see no difference between the two .
what i have calculated is just a fourier transform of the aperture $h ( x , y ) $: $$f ( \omega_x , \omega_y ) =\int dx\ , dy\ , h ( x , y ) e^{-i ( \omega_xx+\omega_yy ) }$$ ( and i was plotting $|f|^2$ as a function of $\omega_{x , y}$ each changing from -100 to 100 . ) already here one can see that $\omega_{x , y}$ both have a dimension of inverse length . so it is not an angle . now , the actual expression for fraunhofer diffraction is something like : $$u ( x ' , y' ) = \int dx\ , dy\ , h ( x , y ) e^{-i\frac{k}{z} ( xx'+yy' ) }$$ where $x'$ and $y'$ are coordinates on the screen , where you observe the diffraction pattern , $k$ is a wave-vector $k=\frac{2\pi}{\lambda}$ , and $z$ is the distance to the screen . as you can see these formulae are very similar . namely you get the fourier transform by renaming : $$\frac{kx'}{z}\leftrightarrow \omega_x\quad\frac{ky'}{z}\leftrightarrow \omega_y$$ so , in practice $\omega_{x , y}$ denote a position on the screen -- given $\omega_x$ , you get an $x'$ by rescaling : $$x'=\frac{\lambda z}{2\pi}\omega_x$$ finally let us put some numbers . let 's say that $\lambda=600nm$ and $z = 2m$ . then a point with $\omega_x = 50cm^{-1}$ will have a coordinate on the disk $x ' = 1.8cm$ .
refs . 1 and 2 define a canonical transformation ( ct ) $$\tag{1} ( q^i , p_i ) ~\longrightarrow~ ( q^i , p_i ) $$ [ together with choices of hamiltonian $h ( q , p , t ) $ and kamiltonian $k ( q , p , t ) $ ] as satisfying $$ \tag{2} ( p_i\mathrm{d}q^i-h\mathrm{d}t ) - ( p_i\mathrm{d}q^i -k\mathrm{d}t ) ~=~\mathrm{d}f$$ for some generating function $f$ . on the other hand , wikipedia ( march 2014 ) calls a transformation ( 1 ) a canonical transformation ( ct ) if it transforms the hamilton 's eqs . into kamilton 's eqs . a ct ( 2 ) according to the definition of refs . 1 and 2 is a ct according to the definition of wikipedia but not necessarily vice-versa , cf . e.g. this phys . se post . considering op 's second question , it seems that what op is really after is not the notion of a ct per se , but rather the notion of a symplectomorphism $^1$ $f:m\to m$ on a symplectic manifold $ ( m , \omega ) $ , i.e. $$\tag{3} f^{\ast}\omega=\omega . $$ here $\omega$ is the symplectic two-form , which in local darboux/canonical coordinates reads $\omega= \mathrm{d}p_i\wedge \mathrm{d}q^i$ . it is straightforward to see that a symplectomorphism $f:m\to m$ preserves the canonical volume form $$\tag{4} \omega~:=~\frac{1}{n ! }\omega^{\wedge n}$$ in phase space $ ( m , \omega ) $ , i.e. $$\tag{5} f^{\ast}\omega=\omega . $$ references : h . goldstein , classical mechanics , chapter 9 . see text under eq . ( 9.11 ) . l.d. landau and e.m. lifshitz , mechanics , $\s45$ . see text between eqs . ( 45.5-6 ) . -- $^1$ a ct ( 2 ) according to the definition of refs . 1 and 2 is locally a symplectomorphism ( by forgetting about $h$ and $k$ ) .
i would like to know how the string deforms over time as it is pulled from that point ( orange ) . does the end closest to the pulling force move first and straighten up , wrapping around the object ' a ' early on , or does the whole string change shape continuously ? for a better understanding , take a look at this figure ( i have modified the diagram in order to explain things in a better way ) when the string is pulled , the coil of wire ( initially slack ) as shown in region $b$ begins to uncoil . what you will observe is that the portion of the wire in region $a$ begins to move only after the coil of wire in region $b$ gets completely uncoiled ( it remains slack until then ) . try it out with a thread or a chain . this is to show you that the portion of the wire close to the pulling force will move first followed by the other portions of the wire .
the solution to the problem can be found in the nature of the propagator of the dirac equation : it is a matrix with two spinor indices , i.e. $$s_f ( x-y ) =s_f ( x-y ) _{\alpha\beta} . $$ in step 2 , you have treated the propagator as if it was a spinor with a single index , which is not correct . avoiding this , but multiplying the equation by $\gamma^0$ from the left leaves you with the result $$f ( s ) =\gamma^0 s^\dagger \gamma^0=\bar{s} . $$ the last equality sign is consistent with the reference given in one of the comments and with the definition of the adjoint of a matrix discussed in this question .
all points in the observable universe are " connected " in the sense that they can be acted upon by forces that have an infinite range ( gravity and electromagnetism ) . however , points that are outside of our cosmological horizon ( due to the expansion of the universe ) are no longer causally connected with points in our local vicinity , since they are receding from us faster than light . the same is true of points that are inside the event horizon of a black hole .
there is a list on wikipedia . radar guns use an optical doppler effect to measure speed . their acoustic equivalent is used in medicine , where it is called doppler ultrasound and used to measure blood flow or other sorts of motion in the body . animals that use echolocation can use the doppler shift to gain information about the motion of their surroundings . a sonic boom occurs when the doppler shift shifts a frequency to infinity . i guess one can continue to concoct scenarios . i wonder whether , when you drop a cat off a cliff , you can hear the pitch of its screaming drop as it accelerates . ( it is not all that cruel - cats can usually survive a fall at terminal velocity . ) you could use it to determine which way a whale is swimming if you have two boats , both listening to the same whalesong . you could even use the doppler shift to gain information about the position of the whale because both the whale 's position and its velocity contribute to the observed doppler shift at any given place . ( i do not have any information about this actually being done , but it might be an interesting problem to work out the locus of possible whale locations for an observed doppler shift . ) i was also curious about whether the doppler effect gives us information on the motion of the crust that moves during an earthquake . i found this reference which suggests it does .
install virtualbox in your mac create a windows image install iris on your windows image work as usual
you need the clebsch-gordan decomposition , at least in the case $n = 3$ . the reason that we decompose a rank $2$ tensor in the way you describe is that $$\mathbf{1} \otimes \mathbf{1} = \mathbf{2}\oplus \mathbf{1} \oplus \mathbf{0} $$ where the bold numbers denote spin representations . here 's a bit more detail . in quantum physics we are really interested in representations of the lie algebra of $so ( n ) $ namely $\mathfrak{so} ( n ) $ . the most useful case for physical purposes is $n = 3$ , where there is an isomorphism $$\mathfrak{so} ( 3 ) = \mathfrak{su} ( 2 ) $$ the clebsch-gordon result comes from the structure of representations for $\mathfrak{su} ( 2 ) $ . in brief , $\mathfrak{su} ( 2 ) $ has irreps $\mathbf{n}$ for each half-integer $n$ . each irrep has $2n+1$ characteristic labels called weights , evenly spaces between $-n$ and $n$ . physically one interprets these as the component $j_3$ of spin . when you take a tensor product of irreps the weights add up , to give you weights for the tensor product representation . a theorem says that this decomposes into the direct sum of irreps in the only way that uses up all these weights . in case that all sounds absurd , let 's do a concrete example . the tensors you mention are elements of tensor products of the vector representation of $\mathfrak{su} ( 2 ) $ typically denoted $\mathbf{1}$ . we want to prove the result above that $$\mathbf{1} \otimes \mathbf{1} = \mathbf{2}\oplus \mathbf{1} \oplus \mathbf{0} $$ well $\mathbf{1}$ has weights $+1,0 , -1$ so the tensor product will have weights $$-2 , -1 , -1,0,0,0 , +1 , +1 , +2$$ which are all possible ways of adding the weights for $\mathbf{1}$ . now rewrite this list suggestively $$-2 , -1,0 , +1 , +2 , \ \ \ \ \ \ -1,0 , +1 , \ \ \ \ \ \ 0$$ these are just the weights for a $\mathbf{2}$ plus the weights for a $\mathbf{1}$ plus the weights for a $\mathbf{0}$ . now it is not hard to identify $\mathbf{2}$ with the traceless symmetric matrices , $\mathbf{1}$ with the antisymmetric ones and $\mathbf{0}$ with the trace , checking that these all transform correctly under the relevant representations . as an exercise you now have all the tools to prove that $$\mathbf{1} \otimes \mathbf{1} \otimes \mathbf{1} = \mathbf{3}\oplus \mathbf{2} \oplus \mathbf{1} \oplus \mathbf{0}$$ can you identify what these are , in terms of decomposing the rank $3$ tensor ? hint : there exist totally symmetric tracefree tensors , totally antisymmetric tensors , a trace term , and tensors of mixed symmetry . here 's a good reference for the lie algebra stuff . let me know if you need any further details ! p.s. i do not know what one can do for general $n\neq 3$ . the clebsch-gordan niceness is a specific property of $\mathfrak{su} ( 2 ) $ so i expect it becomes quite messy . perhaps somebody else has some expertise here ?
this is your circuit : the current that comes from the source , when reaches the point that must choose it is way , sees no difference between the two paths ( symmetry ) , so half of it flows through one way and the other part flows in the second way . it means that , $i_1=i_2$ , so the potential difference across yellow resistors is the same . it means that the potential of point $\mathbf{a}$ is equal to potential of point $\mathbf{b}$ : $$i_1=i_2\to v_a=4.4-i_1r \text{ , }v_b=4.4-i_2r\to v_a=v_b$$ so there is not any potential difference across the blue resistor , and the current through it is 0 , and it can be omitted from the circuit without any change in the behavior of the circuit .
yes , there is a very good reason why leptons and quarks mix . it would be shocking if they did not mix . just to avoid confusions , we are not saying that leptons and quarks mix with each other : they do not because leptons are eigenstates of color or baryon charge with different eigenvalues than quarks which are also eigenstates : this difference prevents mixing between leptons and quarks because the mass terms would violate the charge conservation ( for color and/or baryon charge - the former would be an internal inconsistency ; the latter would be consistent internally but it would imply that the proton would decay rapidly ) . however , leptons mix with leptons and quarks mix with quarks . the theory - whether you mean quantum field theory , string theory , or anything else that may be found in the future - produces several generations of quarks . let 's talk about upper quarks , $u_1 , u_2 , u_3$ - normally called $u , c , t$ . for each of them , the quantum field theory produces a 4-component dirac spinor . it is actually more correct to decompose it into 2-component spinors , but let us use the 4-component formalism here . if those three dirac spinors have some kinetic terms , they may always be diagonalized so that the kinetic terms have the form i$$\sum_i \overline{u_i}\partial^\mu \gamma_\mu u_i$$ there could be some general matrix $i_{ij}$ multiplying $\overline{u_i}$ and $u_j$ but by a proper transformation of the $u$ fields , it can be set to the identity . however , once you bring the kinetic terms into this form - by choosing the right combinations - there is not much freedom left . generally , the underlying quantum field theory or string theory or whatever will also produce arbitrarily bilinear interactions of the form $$-\sum_{i , j} m_{ij}\overline{u_i} u_j$$ the matrix $m_{ij}$ may be chosen hermitean because the antihermitean part would cancel , anyway , as the product of two $u$ 's is hermitian - it gets complex-conjugated under the exchange of $i , j$ . but aside from the hermiticity , $m_{ij}$ is a completely unknown and arbitrary matrix . there is no reason for it to be diagonal ; even if it is a generic hermitian matrix with off-diagonal terms , it satisfies all symmetries and consistency conditions we normally require . in particular , it conserves the electroweak su ( 2 ) isospin , u ( 1 ) hypercharge , su ( 3 ) color , the angular momentum ( only spin in this case ) , and the rest of the lorentz symmetry ( and also the baryon number ) . it is not surprising because from the viewpoint of the values of all these charges , the three generations of the quarks are indistinguishable , so coupling $\overline{u_1}$ with $u_2$ is as good as coupling it with $u_1$ . now , the kinetic term is still invariant under su ( 3 ) rotations of the three quark flavors . you may use this surviving symmetry to diagonalize the matrix $m_{ij}$ . in the right basis , it is diagonal . however , you will not be able to diagonalize the lower-type quark mass matrix at the same moment . a similar mass matrix exists for the down-type quarks . however , the standard model has an su ( 2 ) electroweak isospin symmetry which is very important , at any energy scale . for a chosen triplet of fields $u_1 , u_2 , u_3$ of the up-type quarks for which the mass matrix was diagonal , we have their su ( 2 ) partners - obtained by applying an su ( 2 ) generator - $d_1 , d_2 , d_3$ . however , the mass matrix for these three down-type quarks , which is $m^d_{ij}$ , another hermitian matrix , can be and probably will be non-diagonal once again . right now , we do not have any symmetry left . so it means that the mass of the three down-type quarks is expressed by a $3\times 3$ hermitian matrix that is not necessarily diagonal , and there is no useful residual symmetry left that would allow us to diagonalize the matrix . so we must accept it as a fact : the three mass eigenstates among the down-type quarks will be different from the su ( 2 ) partners of the three mass eigenstates among the three up-type quarks . you can not do anything about it . the ckm matrix is what relates these two bases . the case of the leptons is analogous if one includes the masses for the neutrinos . it would be shocking to find the matrix to be diagonal in the same basis because there is no real " need " - such as symmetry or renormalizability or anomaly cancellation - that would dictate that the off-diagonal elements of the mass matrix have to vanish . because it does not have to happen , it probably will not happen - this principle is known as gell-mann 's totalitarian ( or anarchic ) principle . all terms that are allowed by the symmetries and consistency will occur with nonzero coefficients - most likely with coefficients that are " natural " i.e. comparable to one ( or the typical mass scale ) . in particular , this is also true for the off-diagonal elements of the mass matrices relating several generations of particles that are otherwise equivalent when it comes to their conserved charges . so the mixing generally occurs . nature seems to satisfy the anarchic principle , at least with some nonzero coefficients - not necessarily comparable to one . so it follows that it produces two pretty much random hermitian matrices for the up-type quarks and down-type quarks , and the bases in which these two matrices can be diagonalized will not have the property that one basis may be obtained by su ( 2 ) -rotating the other one . that is why the ckm matrix is not equal to the identity and one needs to discuss the mixing . similarly for the pmns matrix for the leptons . " guessing " that the matrices should be diagonal is as unnatural as guessing that the wealth of bill gates is exactly 56,789,012,345 dollars . there is no reason why a " nice " guess like that should be true , so it will probably not be true . the same thing holds for the guess that the off-diagonal entries of the mass matrices vanish .
synchrotron radiation can be coherent and incoherent . coherent sr arises when electrons are grouped into short bunches so that the entire bunch emits sr as a whole . quantum mechanically , in coherent sr the photon emission from different electrons in a bunch sum up at the amplitudes level and constructively interfere . in the incoherent sr they sum up at the level of intensity , and there is no interference . incoherent sr does not care how electrons are distributed along the ring , while the coherent sr is obviously boosted up in the presence of strong bunching . so , the more homogeneously you distribute the electrons , the less the effect of coherent sr will be and the less overall sr you will have . now let 's look at the incoherent sr . theoretically , you are right : if we managed to create the absolute homogeneous charge distribution along the ring , we would ( classically ) have no sr at all because charge distribution does not change in time . the point is that this is not feasible experimentally , at least for the accelerators and the beams we have . that would require putting electrons in a well-defined quantum state of the radial motion and a well-defined angular quantum number m for the azimuthal dependence , and the accelerator technology is very far from that . however , there is another thing which mimics that situation closely . people have managed recently to put freely propagating electrons in states with well-defined orbital angular momentum ( m as high as 75 ) , see this paper in science for details , and they really see the annular distribution for the electron density . for such a state there exists a reference frame where the electron does not move along the z axis but just rotated as a whole in the transverse plane ( with some radial distribution ) around the symmetry axis . this rotation is not driven by any force , it is just the peculiar superposition of plane waves that creates this steady pattern . so in this case you can say that the electron indeed circulates but does not emit any sr .
so i wonder how do we explain the levorotatory and the dextrorotatory in atom level pov ? it is not easy to predict whether solution of given molecule will be levorotatory or dextrorotatory ; this will depend in a complicated way on the structure of the molecule and also on the frequency of light in question . there are models to calculate indices of refraction and absorption for left and right-polarized light for given frequency and molecular structure , but they are quite involved to explain here . the important idea in these is the observation that most often optically active molecules have three-dimensional structure of nuclei that is not congruent with its mirror image ( we cannot take molecule and by rotating and translating it superimpose it on its mirror image ) . optically active molecules have more spatially separated parts ( at least 4 different atoms to allow for this mirror asymmetry ) and an important role in the calculations of indices of refraction has the way these parts interact .
it was not a black hole because the density was not sufficiently high . the density was lower than what is needed for a black hole because the volume was larger . the volume was larger because the atoms ( mostly hydrogen ) were kept away from each other by the pressure produced by the fusion processes . once the fusion processes stop , this source of repulsion between the atoms disappears , the volume shrinks , the density goes up , and the black hole threshold may be surpassed .
let 's assume that we are considering the effective acceleration due to gravity felt by an object at rest relative the the earth 's surface at lattitude $\phi$ . since the object is at rest , $\dot{\vec r} = 0$ , and the coriolis term vanishes . with the $z$-axis along the axis of rotation , we can write $$ \vec\omega = \omega \hat z $$ on the other hand , the position vector is given by $\vec r = r\hat r$ . it follows that the centrifugal force is $$ \vec f_\mathrm{cf} =-\vec\omega\times ( \vec\omega\times \vec r ) = -\omega^2r \ , \hat z\times ( \hat z \times \hat r ) =-\omega^2 r\ , ( ( \hat z \cdot\hat r ) \hat z - \hat r ) $$ since it is the radial component of the centrifugal term contributes to the component of the effective acceleration due to gravity perpendicular to the surface of the earth , we take the dot product of this expression with $\hat r$ to obtain this component ; $$ \vec f_\mathrm{cf}\cdot \hat r = \omega^2 r ( 1-\cos^2\theta ) = \omega^2r\sin^2\theta $$ where $\theta$ is the spherical polar angle measured from the positive $z$ axis . since the polar angle $\theta$ and lattitude $\phi$ are related by $\theta = \pi/2-\phi$ , this result can also be written as $$ \vec f_\mathrm{cf}\cdot \hat r = \omega^2 r\cos^2\phi $$ the perpendicular component of the effective acceleration due to gravity is therefore $$ g_0 = g-\omega^2 r\cos^2\phi $$ i am a bit baffled by the expression you say one is supposed to get . for one thing , the dimensions do not seem to work out since $x$ is dimensionless , but is being equated to something with dimensions of acceleration . however , if one were to take the result i just derived , then notice that it implies $$ \frac{g-g_0}{g} = \frac{\omega^2 r \cos^2\phi}{g_0+\omega^2r\cos^2\phi} = \frac{x \cos^2\phi}{1+x\cos^2\phi} \approx ( x-x^2\cos^2\phi ) \cos^2\phi $$ where in the last equality we have assumed that $x$ is small , namely that the spin of the earth is slow . this is as close as i can seem to get to the desired expression .
in physics " nothing " is generally taken to be the lowest energy state of a theory . we would not normally use the word " nothing " but instead describe the lowest energy state as the " vacuum " . i can not think of an intuitive way to describe the qm vacuum because all the obvious analogies have " something " instead of nothing " nothing " , so i will do my best but you may still find the idea hard to grasp . that is not just you - everybody finds it hard to grasp . start with the classical description of an electric field ( maxwell 's equations ) . it is not too hard to image an electric field as a field filling space . you can even feel the field : for example if you put your hand near an old style tv screen you can feel the static electricity . you can imagine turning down the electric field until it disappears completely , in which case you are left with the vacuum i.e. nothing . now imagine the same field , but this time we are using the quantum description of the field ( quantum electrodynamics instead of maxell 's equations ) . at the classical level the field is approximately the same as the description maxwell 's equations give , but now we have fluctuations in the field due to the energy-time uncertainty principle . just as before , imagine turning down the electric field until it disappears . unlike the classical description , the ( average ) electric field may disappear but the fluctuations do not . this means the quantum vacuum is different from the classical vacuum because it contains the fluctuations even after you have turned the field down to zero . the key point is that when i say " turn the field down " i mean reduce the energy to the lowest it will go i.e. you can not make the energy of the electric field any lower . by definition this is what we call the " vacuum " even though it is not empty ( i.e. . it contains the fluctuations ) . it is not possible to make the vacuum any emptier because the fluctuations are always present and you can not remove them .
yes , he did . there is a press release at ucsb that acknowledges it as his second win : http://www.ia.ucsb.edu/pa/display.aspx?pkey=3161 the reason for this is , presumably , that the committee is considering giving him the bigger "2014 fundamental physics prize . " in essence , the physics frontiers prize is a nomination for that . ( this is based on the description on the press release , and past reading . )
what is a local lagrangian density ? a classical field theory on minkowski space $\mathbb r^{d , 1}$ is specified by a space $\mathcal c$ of field configurations $\phi:\mathbb r^{d , 1}\to t$ , and an action functional $s:\mathcal c\to\mathbb r$ . the set $t$ is called the target space of the theory , and is often a vector space . if there exists a function $l:\mathcal c\times\mathbb r\to \mathbb r$ for which \begin{align} s [ \phi ] = \int_{\mathbb r} dt\ , l [ \phi ] ( t ) , \end{align} then we call $l$ a lagrangian for the theory . if , further , there exists a function $\tilde l$ such that \begin{align} l [ \phi ] ( t ) = \int_{\mathbb r^d} d^d\mathbf x \ , \tilde l [ \phi ] ( t , \mathbf x ) \end{align} then we call $\tilde l$ a langrangian density for the theory . finally , if there exists a positive integer $n$ and a function $\mathscr l$ such that \begin{align} \tilde l [ \phi ] ( t , \mathbf x ) = \mathscr l ( t , \mathbf x , \phi ( t , \mathbf x ) , \partial\phi ( t , \mathbf x ) , \dots , \partial^n\phi ( t , \mathbf x ) ) \end{align} then we say that the lagrangian density is local . in other words , the lagrangian density is local provided its value at a given spacetime point depends only on that point , the value of the field at that point , and a finite number of its derivatives at that same point . an example of a non-local lagrangian density . consider $t = \mathbb r$ , namely a theory of a single real scalar field . let $\mathbf a\in\mathbb r^d$ be given , and define a lagrangian density by \begin{align} \tilde l [ \phi ] ( t , \mathbf x ) = \phi ( t , \mathbf x ) + \phi ( t , \mathbf x+\mathbf a ) . \end{align} this lagrangian density is not local because the value of the lagrangian at a given point $ ( t , \mathbf x ) $ depends on the value of the field at that point and on the value of the field at the point $ ( t , \mathbf x+\mathbf a ) $ . if we were to taylor expand the second term $\phi ( t , \mathbf a ) $ about $\mathbf x$ , then we would see that the lagrangian density depends on an infinite number of derivatives of the field , thus violating the definition of a local lagrangian density . what is the issue with theories with non-local lagrangian densities ? i am no expert on this , so i will divert to another user . i will say , however , that people do study theories with non-local lagrangian densities in practice , so there is nothing a priori " wrong " with them , but they might generically exhibit some pathology that you might prefer not to have . perhaps most relevant , though , if you are taking qft from a high energy theorist for example , is that the lagrangian density of the standard model is local , so there is no need to consider non-local beasts if one is studying the standard model .
i do not believe the coriolis force has much effect on a tsunami because it does only affect moving masses . coriolis force in fact is not a force but a movement pattern looking as though a force were involved . it is a result of inertia " driving " the moving masses towards a constant direction in space and at the same time the earth 's rotation taking place . however , while a tsunami travels across the globe there is little water moving , instead what actually is moving is its energy . by contrast , in hurricanes there is actually a huge amount of air moving which is affected by coriolis force . update : raskolnikov supplied us with sources that suggest coriolis force has an effect which i do not question . however , i think it is negligible , although the wave recordings on the images presented to us show a curved trajectory . deepak vaid suggests this is due to ocean currents which i find not very convincing as they move at negligible speeds compared to that one the tsunami moved at . i think the curvature of the trajectory we see on the images is an illusion due to the map projection just as in the image below showing the geodesic ( straight line ) between japan and chile : this is the aforementioned image mapped onto a sphere . http://www.youtube.com/watch?v=yodfmhn4alq the noaa map in gnomonic projection
1 . x-ray diffraction takes pictures of fourier space as briefly described on pg . 34 of " introduction to solid state physics " 7th edition by kittel , the scattering amplitude for an arbitrarily-shaped object is $$f ( \mathbf{k}_i , \mathbf{k}_o ) =\widehat{n} ( \mathbf{k}_i-\mathbf{k}_o ) $$ where $\mathbf{k}_i$ is the incident wavevector , $\mathbf{k}_o$ is the outgoing ( or scattered ) wavevector , and $\widehat{n}$ is the fourier transform of the material 's scattering density $n$ . since it is often electrons which scatter light , $n$ is often assumed to be the electron density as a function of location in the material . this is a surprisingly simple equation ; to illustrate it is visual meaning , imagine an object is placed inside a hollow sphere whose walls are lined with photographic paper , and through a small hole the object is bombarded with x-rays of fixed wavevector $\mathbf{k}_i$ , which are scattered by the object and strike the interior of the imaging sphere , forming an image . what is this image ? imagine a bubble of radius $|\mathbf{k}_i|$ centered at $\mathbf{k}_i$ in fourier space . the image formed on the photographic paper is the image of $\widehat{n}$ on the surface of that bubble , a fact which is simple to deduce by examining the set of points of the form $\mathbf{k}_i-\mathbf{k}_o$ for a fixed value of $\mathbf{k}_i$ , and noting that $|\mathbf{k}_i|=|\mathbf{k}_o|$ . that is what x-ray diffraction examines ; it literally takes a bubble-shaped slice of an object in fourier space . this bubble is sometimes call the " ewald bubble " , and it is what diffraction takes a picture of . 2 . fourier space of a crystal now , let 's insert the fact that we are looking at a crystal . a very simple approximation of a crystal is a 3d dirac comb ( ie , a dirac delta placed at each unit cell ) , whereupon $$n ( \mathbf{r} ) \approx \mbox{diraccomb} ( \mathbf{ar} ) $$ where $\mathbf{a}$ is the inverse of the $3\times3$ matrix whose columns are the 3 lattice basis vectors for the crystal . one then fourier transforms to obtain $$\widehat{n} ( \mathbf{k} ) \approx\mbox{det} ( a ) ^{-1}\mbox{diraccomb}\left ( \frac{\mathbf{a}^{-1}\mathbf{k}}{2\pi}\right ) $$ which essentially tells you that the fourier transform of the lattice $n$ is also another lattice . this lattice , residing in fourier space , is often referred to as the " reciprocal lattice " of a crystal . 3 . crystal diffraction now let 's combine the previous two results . if a reciprocal lattice point of $\widehat{n}$ happens to reside on the surface of the ewald bubble , then light will be diffracted in that direction in real space . but how do we image the rest of reciprocal space , not just the points that happen to lie on that one bubble-shaped slice ? suppose we rotate the crystal through euler angles $\alpha , \beta , \gamma$ . then $n ( \mathbf{r} ) $ will become $n ( \mathbf{r} ( \alpha , \beta , \gamma ) \cdot\mathbf{r} ) $ where $\mathbf{r} ( \alpha , \beta , \gamma ) $ is the rotation matrix for the crystal rotation . since $$\mathbf{r} ( \alpha , \beta , \gamma ) ^{-1}=\mathbf{r} ( -\gamma , -\beta , -\alpha ) , $$ we see that $\widehat{n} ( \mathbf{r} ) $ becomes $\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{r} ) $ ( rotations are unitary so the determinant is 1 ) , and hence rotation of a crystal in real space correspond to a reversed rotation of the crystal 's reciprocal space . thus , by rotating the crystal and imaging it after each rotation , we can sweep out a spherical region of radius $2|\mathbf{k}|$ in reciprocal space . visually , we are rotating the ewald bubble , whose surface is attached to the origin , to sweep out a spherical region whose radius is the diameter of the imaging bubble ( which is $2|\mathbf{k}|$ ) . 4 . powder diffraction with those preliminaries out of the way , powder diffraction is quite simple . a powder is a large number of very small crystals oriented in random directions . if there are a large number of crystals all oriented randomly ( isotropic fine powder ) then we can average over all orientations to get $$\widehat{n}_{avg} ( \mathbf{k} ) \propto\int_0^{2\pi}d\alpha\int_0^{\pi}d\beta\mbox{sin} ( \beta ) \int_0^{2\pi}d\gamma\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{k} ) $$ but you do not need to worry about integrating it , because it is visually obvious that each delta function located at a reciprocal lattice point $\mathbf{g}$ will be " rotationally smeared " out to form a series of concentric bubbles of radius $|\mathbf{g}|$ centered at the origin . where these concentric bubbles intersect the ewald bubble , diffraction will occur in that direction . and it is a simple fact of geometry that when one bubble intersects another , the intersection region is a circular ring common to both . as a result , there will be ring-shaped regions on the ewald bubble where constructive interference can occur , and thus , the sample will emit cone-shaped beams of light . and that is how the cone-shaped beams of light in the picture above come to fruition .
just to clarify the text : integrating ( 5 ) gives ( 7 ) ( the very unstable conditions ) integrating ( 3 ) gives ( 8 ) ( moderately unstable to marginally unstable ) integrating ( 2 ) gives ( 9 ) ( marginally to moderately stable ) integrating ( 6 ) gives ( 10 ) ( very stable ) . equations ( 1 ) and ( 4 ) are definitions that are used . so for example , to get ( 9 ) : $$\phi_m = \frac{k z}{u_*}\frac{du}{dz} = 1 + 5\zeta = 1 + 5\frac{z}{l}$$ which is setting the definition given by ( 1 ) to the empirical correlation given by ( 6 ) and substituting in ( 4 ) for $\zeta$ . rearranging gives : $$\frac{du}{dz} = \frac{u_*}{kz} + 5 \frac{u_*}{kl}$$ integrating from $z_0$ to $\xi$ ( a dummy variable of integration that will take the value $z$ when done ) : $$u ( z ) = \frac{u_*}{k}\ln \xi |^{z}_{z_0} + 5 \frac{u_* \xi}{l} |^{z}_{z_0}$$ which simplifies to : $$u ( z ) = \frac{u_*}{k}\left ( \ln \frac{z}{z_0} + 5 \frac{z}{l}\right ) $$ then you would use ( 4 ) to convert that last term back into $\zeta$ which gives ( 9 ) . a similar process is used for the rest .
no , you have a differential equation to solve : $$m\ , \frac{{\rm d} v}{{\rm d} t} = m\ , \frac{{\rm d} x}{{\rm d} t} \frac{{\rm d} v}{{\rm d} x} = m\ , v\ , \frac{{\rm d} v}{{\rm d} x}=-\beta \ , v$$ where $\beta = 200$ . since we are interested in speed $v$ against distance $x$ , we choose the form $v\ , \mathrm{d}_x v$ for the acceleration . so we are left with : $$m\ , \frac{{\rm d} v}{{\rm d} x} = - \beta$$ which i believe will tell you what you need . it looks as though you may still a bit shaky with underlying meanings of derivatives and integrals : are you taking a calculus course at the moment ?
as long as the flying object stays in orbit round the earth you have not changed the net mass or centre of mass of the earth . we have succeeded in reducing the net mass of the earth by the mass of the voyager probes and the like , but this is insignificant compared to the mass of meteoric dust the earth accumulates every day ( allegedly 100-300 tonnes per day ! ) . so launching satellites and planes taking off is not going to directly affect earth 's orbit . i suppose it slightly changes the ( gravitational ) shape of the earth and might change forces from other objects , but this effect is likely to be immeasurably small .
in terms of the equations that you have presented equn 1 ) is correct . the problem is with 2 ) and especially 3 ) . the reason is that this object does not have a constant velocity during motion . so you will need to either remember or calculate the formulae for an object with an initial velocity and moving under acceleration . it is true that these formulae will also contain time t terms , but there are ways to eliminate the t term , which might occur once you see the corrected formula for motion under acceleration . so to begin with the velocity of the object at time t is the initial velocity plus a term from the ( constant ) acceleration applied for time t . also i am not sure how much calculus you remember . equation 2 ) would be more accurate as $a=dv/dt$ . with some basic calculus manipulations you can derive the equations . if you dont quite remember , then take a guess and try to confirm with some calculus you do remember or directly .
space_cadet mentioned already work about deriving spacetime as a smooth lorentzian manifold from more " fundamental " concepts , there are a lot of others -like causal sets , but the motivation for the question was : the reason for my interest in this regards one of the mysteries of quantum mechanics , that of quantum entanglement and action at distance . i wondered whether , if space is imagined as having a topology that arises from a notion of neighbourhood at a fine level , then quantum entanglement might be a result of a ' short circuit ' in the connection lattice . i am not convinced that such an explanation is possible or warranted , the reason for this is the reeh-schlieder theorem from quantum field theory ( i write " not convinced " because there is some subjectivity allowed , because the following paragraph describes an aspect of axiomatic quantum field theory which may become obsolete in the future with the development of a more complete theory ) : it describes " action at a distance " in a mathematically precise way . according to the reeh-schlieder theorem there are correlations in the vacuum state between measurements at an arbitrary distance . the point is : the proof of the reeh-schlieder theorem is independent of any axiom describing causality , showing that quantum entanglement effects do not violate einstein causality , and do not depend on the precise notion of causality . therefore a change in spacetime topology in order to explain quantum entanglement effects will not work . discussions of the notion of quantum entanglement often conflate the notion of entanglement as " an action at a distance " and einstein causality - these are two different things , and the first does not violate the second .
note that $\partial v=s$ , so that $$\tag{1} c~=~\partial s~=~\partial^2v~=~\emptyset$$ is the empty set . ( topologically , the boundary of a boundary is empty , or equivalently , the boundary operator $\partial^2=0$ squares to zero . ) on the other hand , the circulation $$\tag{2} \gamma~=~\oint_{c=\emptyset}\vec{a}\cdot d\vec{r}~=~0$$ along the empty curve $c=\emptyset$ vanishes identically for any vector field $\vec{a}$ . in particular , one can not conclude from ( 2 ) that the magnetic potential $\vec{a}$ should be a gradient field .
the simple answer is that the angle between the front fork and the vertical causes the force from the ground to create a moment about the axis of rotation that turns the wheel in that direction . this has nothing to do with actually riding the bike , and it will happen even if the bike is stationary . basically , if you project the axis of ( steering ) rotation all the way through the wheel , top to bottom , it will not be coincident with the point of contact with the ground . when the bike leans over , the upward ( normal ) force from the ground is not in the same plane as the axis of rotation , which causes a moment about that axis . when the bike begins to turn , the frictional component of the contact force will cause the force to go back into the same plane as the axis of rotation , which causes the wheel to hold its position steady .
as far as i can see this is just going to be the uncertainty of the mean of your dataset . to determine this you must know the uncertainty in the individual data points . for the simple case where you can consider the uncertainty in the data to be constant for all points then the the uncertainty of the mean is $$ u_c=\frac{u}{\sqrt n}$$ for the case where the errors are different then $$ u_c=\frac{\sqrt{\sum u_i^2}}{n} $$ the general formula for combining uncertainties is $$ u_c^2 = \sum \left ( \frac{\partial f}{\partial x_i}\right ) ^2 u_i^2 $$ however , i would also point out there are several flaws with your approach . firstly , if the peak is asymmetric the centroid will not be aligned with the peak position . similarly , the method treats all points with equal weighting when determining peak position . therefore , the result can be effected by noise far from the peak which likely has no relevance to the actual uncertainty of the peak position . see this http://terpconnect.umd.edu/~toh/spectrum/peakfindingandmeasurement.htm for a better approach to peak finding .
well for the human body , to calculate the impact force is not as simple as you think , because we are soft objects and have a large cross section area ( of course depends on which part of your body you land ) . the involved factors are : velocity upon impact : with initial speed $v_0=0$ and s the distance of the fall , then $$v=2gs$$ rate of deceleration : with $d$: deceleration distance : $$a=\frac{v^2}{2d}$$ g-force : with g : gravitational acceleration : $$g = \frac{a}{g} $$ force of impact : w : weight $$f=wg$$ duration of fall ( redundant here ) : $$t=\sqrt{\frac{2s}{g}}$$ impact pressure : a : cross-section of impact $$p = \frac{f}{a} $$ force withstand based on stress absorption ( specific to object denoted $\sigma$ ) :$$f_{abs}=\sigma a$$ numerical estimate for your example : simplification : to simplify the deal with force withstand of your body , we can consider landing on a surface that deflects $0.3 m$ upon impact , so : $$v=\sqrt{2*9.8*1}\approx 4.43 m/s$$ $$a=\frac{4.43^2}{2*0.3}\approx 32.6 m/s^2$$ $$g=\frac{32.6}{9.8}\approx 3.33$$ $$f=10*9.8*3.33=326.34 n$$ much lower than the impact force you had in mind ! now if you fall on your hip , the impact pressure is ( considering a hip of $a=0.025 m^2$ ) : $$p=\frac{326.34}{0.025}=13053 pa$$ note we were working with $m=10 kg$
mostly because they are heavy . rocks erode putting their constituents into solution , the heavy stuff settles out in river/sea beds , and metals are heavy . for many metals hydrothermal process are more important . super hot water deep in the earth dissolves the rock containing the minerals , it moves along cracks in the rock and cools depositing the salt and metals as lines in the rock . in an asteroid with no geological process the metals are found in their raw state having cooled directly from the original ball of primeval gas
it is not so much as one single particle will be seen with different masses as it is that that one type of particle will be seen as having multiple different masses when it is detected multiple times . for example if the extra dimension is like a rolled up microscopic cylinder , the particle can have an infinite number of discrete masses starting from the mass which has no extra kinetic energy in the rolled up cylindrical direction , to masses that have 1 , 2 , . . . units of momentum in the extra rolled up cylindrical direction . these momenta are quantized since only whole wavelengths of the particles wavefunction are allowed around the cylindrical dimension and each unit of extra momentum around the cylinder will be seen as additional rest mass . these are called the kaluza-klein tower of excitations of the basic particle . kaluza-klein theory was developed in the 1920s in an attempt to unify gravitation ( general relativity ) and electromagnetism ( maxwells equations ) . by the way , according to this source : you may also be interested to know that the original 1921 theory has evolved into today 's string theory , as both share the idea of using multiple extra space dimensions to describe the world . the most advanced version of strings is known as m-theory , which utilizes an 11-dimensional spacetime having seven compactified extra space dimensions – a far cry from kaluza 's original single extra dimension ! by the way , kaluza originally came up with his idea in 1919 and communicated it to einstein in hope that the great scientist would recommend it for publication . but einstein , who expressed great admiration for kaluza 's idea , sat on it for two years before recommending it . i am sure this did not sit well with kaluza .
in minkowski spacetime the one way light travel time to a galaxy at proper distance $\chi$ is just : $$ t = \frac{\chi}{c} $$ so : $$ \chi = ct $$ as you say . however in an frw universe the travel time is given by a different equation so the proper distance is not simply $ct$ . let 's assume all motion is in the $x$ direction , so the metric simplifies to : $$ c^2ds^2 = -c^2dt^2 + a^2 ( t ) dx^2 \tag{1} $$ we will take our position to be $ ( 0 , 0 ) $ and the galaxy to be at $ ( 0 , \chi ) $ , and we will adopt the usual convention that $a = 1$ at the current time . to get the proper distance we integrate $ds$ , and since $dt = 0$ and $a = 1$ the proper distance is just : $$ \delta s = \int_0^\chi dx = \chi $$ now let 's calculate the time it takes the light beam to get from the galaxy back to us ( i.e. . one half of the journey ) . light travels on a null geodesic so $ds = 0$ and putting this into the metric ( 1 ) and rearranging we get : $$ \frac{dx}{dt} = \frac{c}{a ( t ) } $$ if the universe is static $a ( t ) = 1$ for all $t$ , and we get $x = ct$ so you would be correct that the proper distance is equal to half the total travel time times $c$ . but then with $a = 1$ we just have minkowski spacetime so that is hardly surprising . to calculate the trajectory of the light we need to assume a form for $a ( t ) $ so let 's make the approximation : $$ a ( t ) = 1 + ht $$ where $h$ is the current value of the hubble constant . then we get : $$ \frac{dx}{dt} = \frac{c}{1 + ht} $$ and this integrates to give us : $$ \chi = \frac{c}{h} \log ( h\tau + 1 ) $$ or rearranging this to get the travel time : $$ \tau = \frac{exp ( \frac{\chi h}{c} ) -1}{h} \tag{2} $$ so the proper distance $\chi$ is not simply the travel time times $c$ . just to reassure ourselves that we get the correct result in the limit of $h \rightarrow 0$ , i.e. minkowski spacetime , note that for small $h$: $$ exp ( \frac{\chi h}{c} ) \approx 1 + \frac{\chi h}{c} $$ put this back into equation ( 2 ) and we get : $$ \tau \approx \frac{\chi}{c} $$ which is where we came in .
this is the usual argument for explaining retarded time - consider a charge moving with a constant velocity along a straight line . if the charge suddenly comes to a halt , there will be a change in the electric field due to the acceleration . but this change in the electric field is not communicated instantaneously through the whole universe , that is prohibited by special relativity and the finite travel time of light . therefore one has to infer that observers that are closer to the charge " see " the change in the field earlier than observers farther away . what that means for an observer too far away to see the change is this - if you are calculating a current/some other property dependent on the velocity of the particle , you have information that is old . the particle has already stopped moving , but you just have not seen it as yet . so to get the physically accurate picture in the frame of the particle , you have to use retarded time , because otherwise you will infer that the particle is still moving at a constant velocity now . but you do not possess that information . you only know that it was moving at time $t - \frac{r}{c}$ earlier , so all calculations about the state of the system must be carried out with respect to that time . so to answer your questions : $t_r$ is a time variable because it still depends on t $$t_r = t - \frac{r}{c}$$ which is clearly dependent on $t$ . it is not that you have two time axes , you are just evaluating your system at a finite point in the past ( i.e. . , $t_r \leq t$ always ) .
i agree with jwenting but in some sense , i feel that he is not answering the question : why there is no " combined $\alpha$ plus $\beta$ decay in which a nucleus emits e.g. a helium atom ? well , let me start with the $\beta$-decay . nuclei randomly - after some typical time , but unpredictably - may emit an electron because a neutron inside the nuclei may decay via $$ n\to p+e^- +\bar\nu$$ which may be reduced to a more microscopic decay of a down-quark , $$ d\to u + e^- +\bar\nu . $$ this interaction , mediated by a virtual w-boson , is why a nucleus - with neutrons - may sometimes randomly emit an electron . so the $\beta$-decay is due to the weak nuclear force . on the other hand , the $\alpha$-decay is due to the strong nuclear force : the nucleus literally breaks into pieces , with a very stable combination of 2 protons and 2 neutrons appearing as one of the pieces ( helium nucleus ) . the two processes above are independent , and each of them can kind of be reduced to a single elementary interaction whose origin is different . this independence and different origin is why the " combined " decay , with an emission of both electron ( or two electrons ) and a helium nucleus , is extremely unlikely . such an emission of a whole atom ( which is electrically neutral but it is surely not " nothing " ! ) could only occur if several of the elementary decay interactions would occur at almost the same time which is extremely unlikely .
i have noticed that none of these answers actually answer the question . the simplest explanation of string theory i can think of : particles we currently consider " point particles " ( electrons , quarks , photons , etc . ) are actually tiny pieces of string with each a characteristic vibration . they interact in a sort of harmony that results in/manifests as the physical laws we observe . if anyone with more knowledge in the field can correct me , i ask for improvements . this is just how i personally explain it to people who ask , and i would hate to give out false information .
the short answer , as karsus ren says , is that it is electromagnetic induction that generates the electricity : the relative movement of a conductor and a magnetic field . as vladimir kalitvianski points out , strictly speaking , windmills do not generate electricity - wind turbines generate electricity , and windmills mill grain . still , it is become reasonably common to refer to wind turbines as windmills . as crowley says , a wind turbine takes the wind 's horizontal motion , turns it into rotary motion , which in some ( but not all ) turbines then goes through a gearbox . the motion then ( depending on the nacelle design ) either : a ) rotates either a conducting coil ( typically copper ) through a magnetic field ; or b ) rotates magnets around a coil ; either way , this generates electricity through electromagnetic induction . the magnets may be permanent magnets ( typically a neodymium alloy ) or electromagnets . the theoretical limit on how much of the wind 's energy can be captured by the blades is called the betz limit , and is an efficiency of ~59.3% ( 16/27 ) . the danish wind-turbine manufacturer vestas has some very accessible stuff on their e-learning site at http://www.vestaselearning.com/ it might seem banal at times , but do not be fooled - there is a lot of decent material in there .
you were onto it in the comments , so i might be late to offer anything new here . the pressure is irrelevant in this problem ; it is a trick , i guess . a reversible adiabatic process is one in which there is no heat flow in or out of the gas , so all of the work done in the expansion/compression goes into the temperature change . just calculate the change in energy like you were saying ( $u_2-u_1$ ) and that is the work done ( on it , not by it ) ! good job figuring it out ! also , as with all physics problems , make sure that the negative signs match your intuition . i can not always keep $-w$ and the like straight on paper , especially since i often mix up whether $w$ represents the work done by the gas or the work done on the gas . i am pretty sure it is the former . so , if the gas expands , it does work , and $w&gt ; 0$ . if the gas contracts , work is done on it , so the work the gas does is negative .
no , a shared atmosphere between body and moon is not possible . for a natural satellite to remain , the orbit must be very stable , because those satellites exist for billions of years . even the tiniest bit of atmosphere ( a few molecules ) would cause a tiny drag . however , drag adds up , so over a long time period , even a heavy object ( such as the moon ) would be dragged down due to drag and ultimately collide with the body it is rotating around . a balloon needs a quite significant atmosphere to be used . present balloons can reach up to 30–35 km altitude . since atmospheric density ( in the heterosphere ) drops off exponentially with elevation , balloons would get gigantic even to reach a little bit higher . reaching an elevation where the atmosphere has negligible density is , in a balloon , impossible . one can however , in theory , try to go as high as possible with a balloon , and then use other methods ( such as rockets ) from there , thus bypassing the densest part of the atmosphere and save a lot of fuel . edit : one more way to look at it : if a satellite would have enough gravitational pull to pull up an observer in a balloon , it would certainly pull up the atmosphere ; therefore the satellite would be in the atmosphere , which is impossible . therefore , a a satellite can never have enough gravitational pull to pull up an observer inside the atmosphere .
this question of the week article describes the tradeoffs of aircraft propeller blades - number , angle , shape , length etc . the final paragraph gives a good summary and relate directly to your question about why the blades do not cover the entire circle . it turns out that they do cover as much as is needed for a given engine to transfer the available energy to the air , which is i guess why you see everything from simple 2-blade props to jet-turbines . we are now left with the final two options , increasing the blade chord or the number of blades . both have the effect of increasing the solidity of the propeller disk . solidity simply refers to the area of the propeller disk occupied by solid componenets ( the blades ) versus that area open to the air flow . as solidity increases , a propeller can transfer more power to the air . while increasing the blade chord is the easier option , it is less efficient because the aspect ratio of the blades is decreased resulting in some loss of aerodynamic efficiency . thus , increasing the number of blades is the most attractive approach . as the power of engines increased over the years , aircraft designers adopted increasingly more propeller blades . once they ran out of room on the propeller hub , designers adopted twin contra-rotating propellers on the same engine . two good examples are the tu-95 bomber and tu-114 airliner . these russian aircraft were equipped with the most powerful turboprop engines ever built , and both designs feature a total of eight propeller blades per engine .
the only way is to figure out some tricky atom structure which will work at higher temperatures . check out existing examples : http://en.wikipedia.org/wiki/bscco http://en.wikipedia.org/wiki/high-temperature_superconductivity#examples you ( probably ) can not make a superconductor by passing current through non-superconductor , but i do not say it is impossible . from the it side , if one would be able to create precise model for superconductivity in any atom structure and then bruteforce different structures , then it might be possible to invent something new . but this is insanely large piece of work .
for a plane wave , all cartesian field component space and time dependencies are $exp ( i\ , \vec{k}\ , \vec{x}-i\ , \omega\ , t ) $: time derivatives $f\mapsto \mathrm{d}_t f$ become $f\mapsto\ , -i\ , \omega\ , f$ and vector curls $\vec{f}\mapsto\nabla\times\vec{f}$ become $\vec{f}\mapsto i\ , \vec{k}\times\vec{f}$ . so now we recallampère 's law in time-harmonic form : $$\begin{array}{lclcl} and \nabla\times h and = and \vec{j}+\partial_t\vec{d}\\\rightarrow and \vec{e} and = and \frac{i}{\sigma -i\ , \omega\ , \epsilon}\vec{k}\times\vec{h}\end{array}\tag{1}$$ so you see from this that $\vec{e} , \ , \vec{k} , \ , \vec{h}$ , in that order , form a right handed triple of mutually orthogonal vectors . on the incidence side of the boundary , assumed normal to $\vec{k}$ , there are two such triples : an incident wave with electric field $\vec{e}^+$ and a reflected wave $\vec{e}^-$ ( this one has wavevector $-\vec{k}$ . on the transmission side there is only one triple with electric field $e^t$ . now we simply equate electric fields at the boundary ( they are all in the same direction , so we only need signed scalars henceforth ) , since tangential $\vec{e}$ must be continuous across it : $$e^+ + e^- = e^t\tag{2}$$ and use ( 1 ) to equate the magnetic fields at the boundary so as to fulfill the boundary condition that $\vec{h}$ must be continuous across the boundary . we note that the magnetic fields too are all in the same direction ( orthogonal to the $\vec{e}$s ) , so signed scalars work here too : $$\frac{-1}{\omega\ , \epsilon_0} ( e^+ - e^- ) = \frac{i}{\sigma -i\ , \omega\ , \epsilon}\ , e^t\tag{3}$$ note the minus sign on the lhs between the two $e$s : $e^+ - e^-$: this is because $\vec{k}$ is in the opposite direction for the reflected wave . you should now be able to find $e^-$ and $e^t$ in terms of $e^+$ from ( 2 ) and ( 3 ) and then use maxwell 's relationship $\epsilon = n^2$ to simplify .
[ quote ] if i understood correctly what i have been taught so far , in qft one must find some way to quantize the fields obeying the field equation in question . [ \quote ] this is correct . in this particular case you start with the lagrangian that has schrodinger 's equation as its eom . the following turns out to be the correct one : $$\mathcal{l} = i\psi^*\partial_{t}\psi - \frac{1}{2m} ( \partial_{j}\psi^* ) ( \partial_j \psi ) . $$ ( the $j$ 's are summed over ) . now to quantize , we can proceed in the way you suggested , i.e. impose ccr on the field $\psi$ and $\psi^*$ and then express them in terms of the creation and annihilation operators . here it turns out that you can define the creation and annihilation operators to be the fourier transform of the fields $\psi$ and $\psi^{\dagger}$ , since you can show that the fourier transforms obey the same commutation relations as the creation and annihilation operators need to , i.e. $$ [ \hat{\psi} ( \mathbf{k} ) , \hat{\psi}^{\dagger} ( \mathbf{k'} ) ] = ( 2\pi ) ^3\delta ( \mathbf{k}-\mathbf{k'} ) . $$ though one thing i am unsure why your professor did was summation instead of integration , since creation and annihilation operators usually depend on a continuous degree of freedom $k$ and thus need to be integrated over . that is , if i was to take that approach , i could postulate that $$\psi ( \mathbf{x} ) = \int \frac{d^3\mathbf{k}}{ ( 2\pi ) ^3} e^{i\mathbf{k . x}}a_{\mathbf{k}} , $$ and from there show that $\psi$ and $\psi^{\dagger}$ obey the correct commutation relations if i impose the commutation relations for the $a$ 's .
how the tap work ? and how we can apply equation of continuity to the water flow when we turn the knob and when we cover the tap with thumb the tap works by changing the minimum cross-sectional area of the flow . for a given pressure difference ( upstream pressure minus downstream pressure ) flow rate is a function of minimum cross-sectional area . using your thumb would do the same thing . you can stop the flow with your thumb if you are strong relative to the force of the flow . http://people.uncw.edu/lugo/mcp/diff_eq/deproj/orifice/orifice.htm where am getting wrong with my understanding of the hydraulic analogy . probably you are misunderstanding the equation of continuity . the equation of continuity only means that the mass flow rate in equals the mass flow rate out . it does not mean that the flow in and the flow out never change . flow rate in and flow rate out can change simultanteously . your statement " on the other hand removing pipe 2 will not change water flow [ rate ] " is incorrect . removing pipe 2 will make a big difference in total flow if it is large in cross section compare to pipe 1 . it will make a small difference in total flow if it is small in cross section compare to pipe 1 . your statement " when we decrease the area of the mouth of tap by our thumb the amount of water flowing out remains same " is also incorrect . instead , the flow rate approaches zero as you make the cross-sectional area of the unblocked portion of the mouth small .
to understand this explanation , you need to understand fourier decomposition of the electromagnetic field . in any homogeneous medium , any electromagnetic field can be thought of as a linear superposition of plane waves , all in different directions . because they run in different directions , the phase delays they undergo in propagating from , say , your aperture to another , parallel plane are all different . therefore the wavefront gets " scrambled " owing to these direction-dependent phase delays . this interference between the different plane wave components of the electromagnetic field is what we commonly call " diffraction " . i further explain this idea , as well as draw some diagrams in this answer here as well as this one here . so with this introduction in mind , let 's look at your paragraph . for simplicity , assume only one transverse direction and one axial ( in the direction of propagation ) direction . let 's also assume scalar optics , i.e. that the electromagnetic field is well represented by the behaviour of one of its cartesian components , so that we can do fourier optics on scalar field . so we have a uniformly lit aperture of width $a$ . its transverse profile is therefore the function ${\rm rect} ( 2 x/a ) $ where ${\rm rect} ( x ) = 1 ; \ , |x| \leq 1$ and ${\rm rect} ( x ) = 0 ; \ , |x| &gt ; 1$ . we take a fourier transform to find the superposition weights of each plane wave component , because each such component has a transverse variation $\exp ( i\ , k_x\ , x ) $ where $k_x$ is the fourier transform variable with units of reciprocal length . the fresnel distance is , as the paragraph says , simply the axial distance needed for this spread to double the beam width . so it is a rough measure of how quickly the light spreads . so this is how the " divergence " arises from diffraction , i.e. the interference between an optical field 's plane wave components as they propagate . also $\sin\theta = k_x/k$ where $k = 2\pi/lambda$ defines the angle that this plane wave component makes with the axial direction . we take the fourier transform , we find that there is a spread of $k_x$ values such that the plane wave components most skewed to the axial direction make an angle without direction of roughly $\lambda/a$ . so , owing to these skewed components , the field 's energy spreads out . the beam width diverges slowly at first and then , after an axial distance of several fresnel distances , the divergence speeds up so that the propagation becomes well modelled by the cone of rays diverging from the centre of the aperture . indeed if you plot contours of constant intensity , they are hyperbolas which begin at right angles to the aperture but bend so that their asymptotes are the cone defined by ray theory . the fresnel distance defines how far the " knee " of the hyperbol is from the aperture . for your question : is it not that the validity holds when all objects are comfortably larger , and not smaller , than the wavelength of light ? this is in general right , but it breaks down near focuses and in situations like this where we are near and aperture and if the aperture is comparable to the light wavelength . in this case you should be able to understand from the fourier analysis the reciprocal relationship between the aperture width and the angular spread .
planck did not know bose-einstein statistics at the time around 1900 . with the existence of minimal unit , or quantization $e=hf$ , in mind , he derived the planck 's law which describe the black body radiation . two decades late , after the establishment of the bose-einstein statistics , then it is known that plank 's law is a special case of bose-einstein distribution by simple using $e=hf$ .
one way to study this case is through the numerical analysis of diffraction , as described in my other answer to you . you can also do this pretty much as you describe through huygens 's principle or as feynman describes in his popular qed book . if you set up an equation to describe what you have said , you will see that the amplitude at a point with transverse co-ordinate $x$ on a screen at an axial distance $d$ from the plane with the knife edge is : $$\psi ( x ) \approx\int\limits_0^\infty\exp\left ( i\ , k\ , \sqrt{ ( x-x ) ^2+d^2}\right ) \ , {\rm d}\ , x\qquad ( 1 ) $$ where the line of sources runs from $x=0$ to $w$ ( the width of the bright region ) , where we can take $w\to+\infty$ if we like . we have neglected the dependence of the magnitude of the contribution from each source on the distance $\sqrt{ ( x-x ) ^2+d^2}$ . this is because we now invoke an idea from the method of stationary phase , whereby only contributions from the integrand in the neighbourhood of the point $x=x$ where the integrand 's phase is stationary will be important . thus for $x\approx 0$ we can assume $|x-x|\ll d$ and so : $$\psi ( x ) \approx\int\limits_0^w\exp\left ( i\ , k\ , \frac{ ( x-x ) ^2}{2\ , d}\right ) \ , {\rm d}\ , x\qquad ( 2 ) $$ an integral which can be done in closed form : $$\begin{array}{lcl}\psi ( x ) and \approx and \sqrt{\frac{2\ , d}{k}}\int\limits_{\sqrt{\frac{k}{2\ , d}} ( x-w ) }^{\sqrt{\frac{k}{2\ , d}} x} e^{i\ , u^2}\ , {\rm d}\ , u \\ and = and \sqrt{\frac{d}{2\ , k}} e^{i\frac{\pi}{4}} \sqrt{\pi} \left ( {\rm erf}\left ( e^{3\ , i\frac{\pi}{4}}\sqrt{\frac{k}{2\ , d}} ( x-w ) \right ) -{\rm erf}\left ( e^{3\ , i\frac{\pi}{4}}\sqrt{\frac{k}{2\ , d}}\ , x\right ) \right ) \\ and = and \sqrt{\frac{d}{2\ , k}} \left ( c\left ( \sqrt{\frac{k}{2\ , d}} x\right ) + i\ , s\left ( \sqrt{\frac{k}{2\ , d}}x\right ) -\right . \\ and and \qquad\left . \left ( c\left ( \sqrt{\frac{k}{2\ , d}} ( x-w ) \right ) + i\ , s\left ( \sqrt{\frac{k}{2\ , d}} ( x-w ) \right ) \right ) \right ) \end{array}\qquad ( 3 ) $$ where : $$\begin{array}{lcl} c ( s ) and = and \int\limits_0^s\ , \cos ( u^2 ) \ , {\rm d}\ , u\\ s ( s ) and = and \int\limits_0^s\ , \sin ( u^2 ) \ , {\rm d}\ , u\\ \end{array}\qquad ( 4 ) $$ where $c ( s ) $ and $s ( s ) $ are called the fresnel integrals . if i plot the squared magnitude of this function ( related to the fresnel integrals ) in normalised units when $k=d=1$ and $l\to\infty$ ( noting $c ( \infty ) =s ( \infty ) = -1/2$ ) for $x\in [ -10,20 ] $ i get the following plot : which i believe is exactly your plot with a shrunken horizontal axis ( yours is likely mine with the transformation $x_s = 2\ , \pi\ , x_r$ where $x_s$ is satwik 's $x$-co-ordinate and $x_r$ rod 's ) . footnote : one of the loveliest curves from eighteenth and nineteenth century mathematics is the cornu spiral , which is a special case of the euler spiral . $\psi ( x ) $ in ( 3 ) traces a path in the complex plane parameterised by $x$ , which turns out to be the arclength $s$ of the spiral path in $\mathbb{c}$ such that : $$\begin{array}{lcl}x and = and {\rm re} ( \psi ( s ) ) \propto c ( s ) + \frac{1}{2}\\ y and = and {\rm im} ( \psi ( s ) ) \propto s ( s ) + \frac{1}{2}\end{array}\qquad ( 4 ) $$ and i plot the normalised and shifted path $z = c ( s ) + i\ , s ( s ) $ i get the lovely spiral below . the curly bits spiral all the way in to $\pm ( 1+i ) /2$ as $s\to\infty$ . the shifting and then taking magnitude squared explains why the intensity plot above is not symmetric about $x=0$ , oscillating as $x\to\infty$ and dwindling monotonically as $x\to-\infty$ .
edit 1: i think i just understood you question : you are actually trying to calculate some sort if “internal” inductance , i.e. the contribution to the inductance of only the field inside the conductor . when calculating the flux , you have to choose a closed path over which you would want the electromotive force , and then integrate the magnetic flux over the surface limited by this path . normally the path would be the whole electrical circuit , but since you are only interested in the contribution of the internal field , you chose the return path along the edge of the wire , which is fine . now you have to choose the forward path . the forward path should be along the lines of current . the problem is that , different lines of current give different fluxes . then you can calculate the flux as a function of where , in the conductor 's cross-section , you take the forward path . but since you are using the low-frequency approximation ( no skin effect , then uniform current density ) , you can just average the forward-path dependence over the whole cross-section . then you get the missing factor two . a somewhat different argument is given in this old bulletin of the bureau of standards : the author instead weights individual flux lines as per the fraction of the conductor they enclose . this gives the same factor two . edit 2: as requested , a few clarifications . by “integrate the magnetic flux” i really mean “calculate the magnetic flux” . i used “integrate” because the calculation involves an integral : $$ \phi = \int_a \mathbf{b}\cdot\mathbf{n}\ ; \mathrm{d} a $$ where $\mathbf{n}$ is the unit normal to the surface . it is not exactly the same as “integrate the magnetic field” because of the dot product with $\mathbf{n}$ . i talked about “forward path” and “return path” because , if it is not an antenna ( as the low-frequency approximation suggests ) , a wire is usually part of a transmission line which consists of at least two conductors . assume for example that you use a pair of wires to connect a source to a load , like in the figure below ( i hope everyone can see box drawing characters ) : where the arrows represent the electric current . i assume the wire you are interested in is the top one , which i called “forward path” . the bottom wire , which i called “return path” , brings the current back to the source . taken together , these two wires form a loop and the current will make some magnetic flux through the loop . then , if you try to change the current , some electromotive force will appear because of this flux , and you will be able to model this as the effect of an inductor along the transmission line , as below : this is the self inductance of the transmission line , and is what i first thought you where trying to calculate . the self inductance of a bare wire is somewhat ill-defined . well , it is defined , but with some assumptions about the surface over which to integrate the flux , and it scales as $l\log\frac{l}{r}$ , which makes it is value per unit length diverge logarithmically when considering an arbitrarily long wire , as pointed out by zassounotsukushi and mmc . once you add the second wire , the surface over which you have to integrate the flux is clearly defined , and the inductance of the line scales like $l\log\frac{d}{r}$ , where $d$ is the distance between the wires . no more logarithmic divergence with respect to $l$ . on the other hand , it depends logarithmically on the distance between the wires , therefore you cannot just assume that the return path is just far enough to be ignored . btw , the return path is not necessarily a wire , it could be , e.g. , a ground plane . for the particular calculation you are doing ( only the contribution of the field inside the conductor ) , you use a very narrow loop where the return path is replaced by a line along the edge of the conductor , in order to enclose only the internal field . original answer below , which is somewhat bogus , as i thought you where after the total self-inductance ( including external field ) per unit length of an infinite wire . the comments of georg refer to this original version . you cannot assign an inductance to a long wire alone : you have to consider the whole circuit . the current carried by the wire has to come back in some way , and you need to know how far from your wire is the way back . assume for a moment that the wire is actually the inner conductor of a coaxial cable . you can easily calculate the linear inductance of the cable as a function of the inner an outer conductor radii . now make the outer radius go to infinity and you have a diverging self-inductance ! this means that in practice you can never assume that the way back is “far enough” to ignore it .
if you close both switches 1 and 2 and redraw the circuit , it will look like this so bulbs a , b and c are connected in parallel . the current will flow in all the bulbs , all of them will be working
a linear operator $a : d ( a ) \to {\cal h}$ with $d ( a ) \subset {\cal h}$ a subspace and ${\cal h}$ a hilbert space ( a normed space could be enough ) , is said to be bounded if : $$\sup_{\psi \in d ( a ) \: , ||\psi|| \neq 0} \frac{||a\psi||}{||\psi||} &lt ; +\infty\: . $$ in this case the lhs is indicated by $||a||$ and it is called the norm of $a$ . notice that , therefore , boundedness , is not referred to the set of values $a\psi$ , which is always unbounded if $a\neq 0$ , as $||a\lambda\psi|| = |\lambda|\: ||a\psi||$ for $\psi \in d ( a ) $ and $\lambda$ can be chosen arbitrarily large still satisfying $\lambda \psi \in d ( a ) $ since $d ( a ) $ is an subspace . it is possible to prove that $a : d ( a ) \to {\cal h}$ is bounded if and only if , for every $\psi_0 \in d ( a ) $: $$\lim_{\psi \to \psi_0} a\psi = a \psi_0\: . $$ another remarkable result is that a self-adjoint operator is bounded if and only if its domain is the whole hilbert space . regarding $a= \frac{d}{dx}$ , first of all you should define its domain to discuss boundedness . an important domain is the space ${\cal s} ( \mathbb r ) $ of schwartz functions since , if $-id/dx$ is defined thereon , it turns out hermitian and it admits only one self-adjoint extension that it is nothing but the momentum operator . $d/dx$ on ${\cal s} ( \mathbb r ) $ is unbounded . the shortest way to prove it is passing to fourier transform . fourier transform is unitary so that it transforms ( un ) bounded operators into ( un ) bounded operators . ${\cal s} ( \mathbb r ) $ is invariant under fourier transform , and $d/dx$ is transformed to the multiplicative operator $ik$ i henceforth denote by $\hat a$ . so we end up with studying boundedness of the operator : $$ ( \hat a \hat{\psi} ) ( k ) = ik \hat{\psi} ( k ) \: , \quad \hat\psi \in {\cal s} ( \mathbb r ) \: . $$ fix $\hat\psi_0 \in {\cal s} ( \mathbb r ) $ with $||\hat\psi_0||=1$ assuming that $\hat\psi_{0}$ vanishes outside $ [ 0,1 ] $ ( there is always such function as $c_0^\infty ( \mathbb r ) \subset {\cal s} ( \mathbb r ) $ and there is a function of the first space supported in every compact set in $\mathbb r$ ) , and consider the class of functions $$\hat\psi_n ( k ) := \hat \psi_{0} ( k- n ) $$ obviously , $\hat\psi_n \in {\cal s} ( \mathbb r ) $ and translational invariance of the integral implies $||\hat\psi_n||=||\hat\psi_0||=1$ . next , notice that : $$\frac{||\hat a\hat\psi_n||^2}{||\hat\psi_n||^2} = \int_{ [ n , n+1 ] } |x|^2 |\hat\psi_{0} ( k-n ) |^2 dk \geq \int_{ [ n , n+1 ] } n^2 |\hat\psi_{0} ( k-n ) |^2 dk$$ $$ = n^2 \int_{ [ 0,1 ] } |\hat\psi_{0} ( k ) |^2 dk = n^2\: . $$ we conclude that : $$\sup_{\hat{\psi} \in {\cal s} ( \mathbb r ) \: , ||\hat{\psi}||\neq 0} \frac{||\hat a\hat\psi||}{||\hat\psi||} \geq n \quad \forall n\in \mathbb n $$ so $\hat a$ is unbounded and $a$ is consequently .
the comment on this page http://chemistry.about.com/od/photogalleries/ig/nuclear-tests-photo-gallery/operation-teapot-test.htm http://chemistry.about.com/b/2011/04/19/nuclear-explosion-lines-spikes.htm says : sounding rockets or smoke flares may be launched just before a device explodes so that their vapor trails may be used to record the passage of the otherwise invisible shock wave . to learn about every detail of these tests , contact your nearest fbi agent .
there is such a diagram--- it is the flux of energy in the linearized gravitational field ( if you use full gr , you get complications with defining the energy ) . unlike the electromagnetic case , where the electric field carry the bulk of the energy and the momentum of the charge carriers is negligible , in the gravitational case , it is the opposite . you can also imagine electromagnetic circuits in which you accelerate very massive spheres which are very lightly charged , and use these as current carriers , and in this case , the momentum of the current carriers will not be negligible . edit : to clarify , there are gravitational fields created by moving water which surround the pipe , like the electric and magnetic fields surround the current-carrying wire . there is an energy flow in these gravitational fields , which carries energy , just like the poynting flux does . these effects are negligible for ordinary materials at ordinary density . nearly all the energy flux ( all but the tiny negligible fraction in the gravitational field ) is carried by the water in the pipe , but the momentum in the water is not analogous to the poynting vector , it is analogous to the electron momentum , which also carries a small amount of energy in a current carrying wire .
the total escape velocity is about 11,200 m/s ( approximately 7 miles/sec . ) in a direction roughly tangential to the earth 's surface . at 30 degrees north latitude ( e . g . , somewhere in southern texas ) the eath 's spin would contribute about 400 m/s to the tangential velocity , so actual speed , relative to the earth 's surface at said launch point would be about 10,800 m/s , if the load was launched in a roughly tangential direction , eastward ; closer to the 11,200 m/s if you went straight up with it . the height of the mountain will serve a good purpose in getting you up to where the air is thinner when you leave the track ; but , otherwise has negligible on how fast you need to go . the main problem with this is that 11,200 m/s , at 14000 feet , is about mach 24 ; roughly 13 times the muzzle velocity of a 30-06 high-powered rifle . heating , shredding , melting , ablation , and all sorts of bad stuff occur at these speeds , even in thin air . studies have been done on this , to engineer a ground-power-assisted launch . the idea is to get the load up to a very high speed --but not high enough to shred or melt it-- and let the rocket engines take over from there . i believe i read about a track hypothesized for somewhere in the vast wastelands of texas .
i am not totally sure i know what the first questions mean , so let me start with the last one . the wavefunction of a particle is a function of time as well as position . a lot of quantum mechanics is concerned with wavefunctions corresponding to " stationary states . " you might think that such wavefunctions would have no time-dependence , but that is not the case . a stationary state corresponds to a particle with a certain amount of energy $e$ , and the wavefunction has time dependence of the form $e^{-i\omega t}$ , where $\omega=e/\hbar$ . people often leave out that time dependence , just to confuse you . so when people refer to a wavefunction of the form $e^{ikx}$ , they really mean $e^{ikx}e^{-i\omega t}$ , or $e^{i ( kx-\omega t ) }$ . this corresponds to a wave moving to the right . in particular , a point of any given " phase " moves to the right at speed $\omega/k$ . for instance , the point $kx-\omega t=0$ is a point where the wavefunction is purely real . this point moves along according to the rule $x= ( \omega/k ) t$ . another way to think about it : a particle has definite momentum if it is an eigenfunction of the momentum operator : $$ \hat p\psi = p\psi . $$ here $\hat p=-i\hbar ( \partial/\partial x ) $ is the momentum operator ( specifically its $x$ component ) , and the number $p$ is the eigenvalue . for a particle with such a wavefunction , a measurement of momentum is guaranteed to give the value $p$ . the wavefunction $\psi ( x ) =e^{ikx}$ obeys this rule : $$ \hat pe^{ikx}=-i\hbar{\partial ( e^{ikx} ) \over\partial x}=\hbar ke^{ikx}=\hbar k \psi , $$ so $p=\hbar k$ . so a particle with positive $k$ will be observed to have positive momentum . now back to the first couple of questions . when you solve for the wavefunction of a particle in a delta function potential with negative energy , you find that the solutions are of the form $e^{-k|x|}$ for some $k$ . these wavefunctions approach zero for large $|x|$ . so physically , the particle has essentially zero probability of being far away from the origin . that is what " bound state " means . on the other hand , with positive energy , there are solutions that look like $e^{\pm ikx}$ . these wavefunctions do not die away to zero at large distances : $|\psi|^2$ remains constant as $|x|\to\infty$ . so these particles have significant probability to be anywhere .
we have to be careful in stating exactly what we are going to allow ourselves to assume here . we need some sort of principle of relativity -- that the laws are the same for both observers . but we do not want to assume anything else a priori , right ? for instance , we do not want to assume at first that rulers have the same length for both observers -- we need to prove that . let 's work in one dimension for simplicity . suppose that observer b is moving at constant velocity v relative to observer a . suppose some object is moving along with some speed $u_b$ as measured by b . we want to show that the speed as measured by observer a is $u_a=u_b+v$ . consider the position of the object at two different times , separated by a small amount $dt$ . since time is absolute ( all observers use the same $dt$ ) , what we want to show is equivalent to $$ dx_a = dx_b+v\ , dt $$ ( multiplying the original equation through by $dt$ ) . here $dx_a$ means $x_a ( t+dt ) -x_a ( t ) $ , that is , the change in the position of the object at the two times , as measured by a , and similarly for b . here 's a useful fact : if both observers measure the distance between two points at an instant of time $t$ , they must get the same answer . the reason is symmetry . if the two disagreed , then one would have to get a bigger answer than the other . but for a measurement of this sort , there is nothing to break the symmetry between a and b -- that is , we can just change the sign of $v$ , and consider b to be stationary and a to be moving , and that should not affect the answer . i think that is enough to get us there . suppose that observer b sets of a firecracker at his location at time $t$ , and another at time $t+dt$ . the two observers must agree on the distance from b to the object at the time the first firecracker went off , and they must agree on the distance from b to the observer at the time the second firecracker went off . the difference between these these two numbers is $dx_b$ . but the difference between these two numbers is also $dx_a-v\ , dt$ , since observer a knows that observer b traveled a distance $v\ , dt$ during that time interval . the conclusion follows .
in gilmore 's book : http://www.amazon.com/single-particle-detection-measurement-gilmore/dp/0850667550 chapter 5 is on solid state ionization and it is use in detectors . that was hanging around the lab when i was working in particle physics . i would give that one a try . it is a bit old these days but the principles are there .
first , this question has nothing to do with matlab . besides , if you think there is an error in your code implementation , this is most certainly the wrong stackexchange . that said , the error is a physical one . you are confusing the distribution of velocity vectors $\vec{v}$ , which is a 3d gaussian ( by that i mean a function from $\mathbb{r}^3$ to $\mathbb{r}$ that is the product of gaussians in three cartesian coordinates ) with the 1d distribution of those vectors ' magnitudes . the former had better be symmetric about 0 - do you expect that gas in thermal equilibrium is moving in one particular direction ? this is in fact exactly what you plot ( well , actually you plot a 1d slice of the 3d domain , essentially the distribution of $x$-velocities ) . you need a $v^2$ factor in the distribution ( and a few modified constants ) to get the asymmetric thing you are looking for . this is the same $v^2$ ( or $r^2$ ) that arises , for instance , when converting a spherically symmetric integrand from cartesian to spherical coordinates . when you do that , by the way , do not try to plug in negative values - they do not make physical sense , since again it is a distribution of magnitudes . in terms of the notation in that wikipedia article , the distribution of vectors is $f_\mathbf{v}$ , which is $f_v^3$ , and you are plotting $f_v$ . what you want to plot is what the article calls unsubscripted $f$ ( well , one of the many distinct things it calls $f$ ) : $$ f ( v ) = \sqrt{\left ( \frac{m}{2\pi kt}\right ) ^3} 4\pi v^2 \exp\left ( \frac{-mv^2}{2kt}\right ) . $$
the aufbau principle is not rigorous because it is based upon the approximation that the electron-electron interaction can be averaged into a mean field . this is called the hartree-foch or self consistent field method . the centrally symmetric mean field results in a set of atomic orbitals that you can populate 2 electrons at a time . the trouble is that the electron correlations mix up the atomic orbitals so that distinct atomic orbitals no longer exist . instead you have a single wavefunction that describes all the electrons and does not factor into parts for each electron . for example this is explicitly done in the configuration interaction technique for improving the accuracy of hartree-foch calculations .
it is same if radius of axle is same for both . torque is defined as moment of force about axis of rotation as : $$\tau=\vec r\times \vec f$$ $\vec r$ is the position vector of point of application of force from axis of rotation . in your case it it $\tau=rf$ where $r$ is radius of axle of wheel .
because the largest earthquakes are caused by the motion of faults in the earth 's lithosphere , the upper limit to earthquake magnitude is going to be related to the contact area between tectonic plates , not the size of the entire earth . some geophysicists believe the magnitude 9.5 earthquake in chile was likely close to the maximum size possible . largest earthquakes in the world since 1900
in general air resistance makes the optimal angle lower than 45$^{\circ}$ . however i do not know of any rigorous way to show this without sitting down at the computer . the argument usually used is that in the prescence of air resistance you want to minimise the energy lost to the air by reducing the time of flight , and this means choosing a lower trajectory . the reduction in range by using a lower angle is balanced out by the greater average speed of the projectile . if you google for " optimum projectile angle air resistance " or something like this you will find lots of articles going into this question .
all we are doing is using a set of units where certain quantities happen to take convenient numerical values . for example , in the si system we might measure lengths in meters and time intervals in seconds . in those units we have $c = 3 \times 10^8 \text{m}/\text{s}$ . but you could just as well measure all your distances in terms of some new unit , let 's call it a " finglonger " , that is equal to $2.5 \times 10^6 \text{m}$ , and time intervals in a new unit , we will call it the " zoidberg " , that is equal to $8.33 \times 10^{-3} \text{s}$ . then the speed of light in terms of your new units is $$ c = 3 \times 10^{8} \text{m}/\text{s} = 1 \frac{\text{finglonger}}{\text{zoidberg}} ~ . $$ the units are still there -- they have not been " deleted " -- but we usually just make a mental note of the fact and do not bother writing them .
here is a link that allows you to simulate the field lines and equipotentials of both point charges and charged plates : http://www.falstad.com/emstatic/ perhaps you will find what you are looking for here .
look up the catenary curve properties and notice that the weight of the cable causes a reaction in both $x$ and $y$ axis as you noted . the vertical components of the reactions sum up to the weight of the cable , and the horizontal components are such as the tension being tangent to the shape of the cable . now what you " feel " when you move the support is the inertial force of moving the cable which i suppose it too small for you to feel . if you try with a heavy steel cable , and you accelerate in the same order of magnitude as gravity then you will feel inertial resistance . notice that you also feel an effective stiffness as you pull on the cable as the sag decreases and the incident angle creates higher horizontal reactions . in addition you might have elastic deformation also decreasing the above stiffness by $1/k_{eff} = 1/k_{elastic} + 1/k_{geometric}$ .
the person hits the ground at the same speed in both scenarios . once you are in the air , you fall towards the ground with a constant acceleration of about 10 m/s^2 . everything falls the same way - rocks , cannonballs , people - regardless of size or somersaulting . there may be some small effects from air resistance , but not enough to be noticeable . the sommersaulter may also land with a different orientation so that his center of mass falls a further distance . then he could hit the ground with slightly higher speed . but basically , everything falls the same way . check out this youtube video of a feather and hammer falling simultaneously on the moon , which shows that heavy and light objects fall the same way absent air . also see this youtube video of the tv show mythbusters dropped a bullet straight down and firing one from a gun . the bullets fall in the same amount of time , regardless of their horizontal speed .
if you assume that your body is a uniform , thin rigid rod . one end of the rod is pivoted ( aka your feet ) during the fall . then one simply recalls that the angular velocity $\omega$ of rotation of your body is related to the tangential velocity $v$ of a point a distance $r$ from the pivot by \begin{align} v = \omega r \end{align} now , if you have height $h$ , then your center of mass will be a distance $h/2$ from your feet , and your head will be a distance $h$ from your feet , so the ratio $v_\mathrm{head}/v_\mathrm{com}$ of the tangential velocity of your head to that of your center of mass is \begin{align} \frac{v_{\mathrm{head}}}{v_{\mathrm{com}}} = \frac{\omega h}{\omega ( h/2 ) } = 2 . \end{align} in other words , the tangential velocity of your head will be twice as large as that of your center of mass . the intuition behind this is simply that for a given change in the angular position of your body , your head moves twice as far as your center of mass . note that in the real world , the assumptions above are not really correct , but they do give you an idea of the basic physics in a rough approximation .
if you only take into account expansion of space , then there is no way to violate causality , because information still cannot travel faster than light . if two galaxies drift apart faster than light , it is not even possible for signals emitted from one galaxy to reach the other . they have crossed each other 's hubble radius .
to change the past you require a closed timelike curve . stephen hawking proved that closed timelike curves cannot be created in a finite system without using exotic matter . i think the proof was in his paper on the chronology protection conjecture but i do not have access to the paper at the moment . this far we have a reliable grasp on whether causality can be violated , but from here things get speculative . it seems likely that no infinite structures exist , if only because the universe has not existed for an infinite time ( though if there was a big bounce we could be wrong about this ) . the big question is whether exotic matter exists . the trouble is that there is no proof that exotic matter either exists or does not exist . it is like negative mass - there is nothing to stop you plugging a negative value for mass into the gr equations , but that does not mean it is a physically meaningful thing to do . we have never observed exotic matter , but that does not necessarily mean it does not exist . the chronology protection conjecture that i mentioned above is the closest we have to a mathematical approach to constraining causality violations , but it is just a conjecture and has not been proven - though it has not been disproven either . at the moment we simply do not understand the physics well enough to give a definitive answer .
in a fission reactor , we can talk about the temperature of the fuel , and we can also talk about the temperature of the neutrons . temperature of the neutrons has abundant qualifiers on it . neutrons in a fission reactor are really in a transient state - moving from genesis to absorption again . they are generated at extremely high energies , and absorbed at much lower energies on average . we do not often measure the temperature of neutrons in the same units as the fuel . the fuel is in kelvin $k$ , whereas the neutrons are most often stated in electron-volts , most commonly $mev$ . going between the two is not a problem . there is no theoretical difficulty in applying the same relationship between particle energy and temperature as we do for gases ( $ {\tfrac {1}{2}}m\overline {v^{2}}={\tfrac {3}{2}}k_{{\mathrm {b}}}t $ ) . in fact , for light water reactors ( lwrs ) , the neutrons are in direct momentum exchange with protons that exist in the moderator . they are even of similar masses . in the lower part of the neutron energy spectrum , in fact , the neutrons are relatively well thermalized to the moderator temperature , but it depends on the reactor design . here 's one illustration of that : you can see that there is a " thermal flux " hump . this exists around the temperature of the reactor moderator . but we can not say it is the same . even though the protons ( in water ) and neutrons are about the same mass , the neutron energy distribution is shifted toward the higher energies , because they downscatter from higher energies to begin with , and they are preferentially absorbed by the fuel at lower energies . two mechanisms for temperature dependence in commercial lwrs , the reactivity is related to temperature via two major interplays : moderator temperature affects that above thermal peak of neutrons , and the location of the peak affects reactivity the fuel has absorption peaks between the fast and thermal energies , and the fuel temperature affects the efficiency of these peaks through a more complicated effect i should note that our reactors will come to equilibrium given enough time ( provided we build them stable ) . so if you change reactivity , then ultimately that will result in a change in the fission reaction rate . but practically , the operators control the reaction rate with engineered methods ( control rods ) , and that is set by regulatory concerns . so if you change the moderator temperature , you will change the reaction rate . as i have described , this is due to mechanisms that vaguely fit the intuition you had . changing temperature literally changes the temperature of the neutrons . this matters for the reaction because neutrons are absorbed by the fuel at lower energies much faster than at higher energies . things are a little bit more wonky than this , because a higher reaction rate produces more heat , which heats up the reactor more - creating a feedback loop . but we can still control the temperature of the moderator/coolant that we pump into the reactor . the role of fuel temperature is much more complicated . this is the resonance absorption peak doppler broadening effect . basically , there are lots of very very narrow absorption peaks in the epithermal range ( in the graph ) . as the fuel heats up , these " smear " out into larger energy ranges . with the peaks less sharp , there are fewer " windows " though the neutrons can sneak through the downscattering gauntlet . the doppler effect is actually the dominant effect for the reactor control . it is honestly the main thing that we rely on for keeping the reactors stable , since it is clearly a negative feedback , and it acts very quickly ( since most the the fission heat is deposited directly in the fuel ) . so let me conclude by saying that #1 has a lot of mechanistic qualities that resemble the intuition you had about temperature ( in the conventional sense ) affecting reaction rate . on the other hand , #2 is more important for operating reactors , and it is qualitatively quite differently from the fusion temperature and reaction rate interdependence .
the electrons ( torn from one surface ( electrode ) through , say , electron field emission , just move in vacuum in electric field to the other surface ( electrode ) .
normally i recommend beginners to sr always use the lorentz transformations to work through problems , but actually in this case using the lorentz transformations is algebraically a bit fiddly , and there is an easier way to do it . we can use the fact that the proper time is lorentz invarient . for any pair of events $ ( t_1 , x_1 ) $ and $ ( t_2 , x_2 ) $ we calculate the change in proper time using : $$ c^2\delta \tau^2 = c^2\delta t^2 - \delta x^2 $$ where $\delta t = t_2 - t_1$ and $\delta x = x_2 - x_1$ . the invariance of the proper time means that all observers must agree on the value of $\delta\tau$ . so we will calculate $\delta\tau$ in both $s_1$ and $s_2$ and equate them . this diagram shows how we do it : start in $s_1$ . in this frame we see observer 2 moving past at velocity $v$ . we will choose our origin so observer 2 passes one end of the snake at $ ( 0 , 0 ) $ , which means they pass the other end of the snake at $ ( d/v , d ) $ - we get the time $t = d/v$ simply by dividing the length of the snake $d$ by the velocity $v$ . the proper time is : $$ c^2\delta \tau^2 = c^2 \frac{d^2}{v^2} - d^2 \tag{1} $$ now look at things in frame $s_2$ . in this frame observer 2 is stationary and sees the snake flashing past him at velocity $v$ . the time the snake takes to pass is $d'/v$ , where $d'$ is the contracted length that we are trying to calculate . again we choose our origin so the head of the snake passes at $ ( 0 , 0 ) $ , and that means the tail of the snake passes at $ ( d'/v , 0 ) $ . this time the proper time is : $$ c^2\delta \tau^2 = c^2 \frac{d'^2}{v^2} \tag{2} $$ since the value of $c^2\delta\tau^2$ must be the same in both equations ( 1 ) and ( 2 ) we can set the right hand side of these two equations equal : $$ c^2 \frac{d^2}{v^2} - d^2 = c^2 \frac{d'^2}{v^2} $$ the rest is just algebra . we tidy the expression up by rearranging it as : $$ d^2 \left ( \frac{c^2}{v^2} - 1\right ) = d'^2 \frac{c^2}{v^2} $$ and dividing both sides by $c^2/v^2$ gives : $$ d^2 \left ( 1 - \frac{v^2}{c^2}\right ) = d'^2 $$ or : $$\begin{align} d ' and = d \sqrt{1 - \frac{v^2}{c^2}} \\ and = \frac{d}{\gamma} \end{align}$$
i think adam 's answer is excellent , and i would rather make this a comment but can not as i just signed up in order to answer . while i agree with most of what adam said , there are cases where large-n works well for $n=1$ . the case i am familiar with is the large-n expansion for the " spin ices " dy$_2$ti$_2$o$_7$ and ho$_2$ti$_2$o$_7$ , which have ising spins on a pyrochlore lattice . the best reference for this is sergei isakov 's phd thesis ( university of stockholm 2004 ) , specifically section 4.3 and subsection 4.8.2 . it is noted in that thesis that we should not expect large-n to work for pyrochlores below $n=3$ ; the expansion is singular at $n=2$ owing to order-by-disorder . extensive checking against monte carlo simulations and experiment verify that the expansion is good for $n=1$ ( references given in the thesis ) , but we do not know why this is . i found myself needing to prove the equivalence of large-n and mft at high temperatures . i apologise if it is bad etiquette to reference one 's own work , but i provide a mathematical proof in section 2.4 of my master 's thesis which can be found here ( perimeter institute 2011 ) . i believe the main working difference between large-n and mft is that mft always contains a non-zero critical temperature $t_c$ , but that $t_c\sim 1/n$ with $n$ the number of degrees of freedom at each lattice site . this means that large-n , with $n\rightarrow\infty$ , has $t_c=0$ to zeroth order in the $1/n$ expansion ( at which one generally works ) .
there are many different software programs that will do what you want , but planetarium software is very complex and takes a long time to learn before you can use it at its best . i am a technical writer and software support person for starry night software and , as part of my job , write a weekly article for space . com which is almost always illustrated using starry night software . i have been using various versions of starry night for over a decade and can generally get it to do what i want for my illustrations , though i sometimes need to fudge things a bit with other software . i doubt whether you will ever find one program which will do everything you want , so my suggestion is to do what i did : find a program that does most of it , and then work with that program until you can get the best from it .
you misunderstood the classification i believe . let 's take an example . in class d and 1d , the classification tells you there are two possible vacua ( you understood this apparently ) . this is the famous $\mathbb{z}_{2}$ ensemble in the classification . next the classification tells you also that : at the boundary between the two gapped vacua , a majorana mode emerges . more precisely , an evanescent , localised mode emerges at the boundary between the two vacua , since they are gapped . for superconductors this emergent mode is a majorana one , thanks to the particle-hole symmetry . the construction is topological in essence : you map the problem of your differential equation to a group language , you recognise some properties ( the cartan class for instance , above the d one ) which allow you to classify your problem ( using equivalence relation , the more complicated part being to choose relevant criterion ) . then these classes have additional properties , like the boundary term discussed above . to be a little bit more clear , it is topological because you want to understand how the local solutions to your problem are gluing to some other local ones in order to make global ones . think about the möbius stripe that is locally differentiable , but not globally ( check out fiber bundle also , this is the mathematical object describing the property you are looking for ) . for the class d in 1d , you can find the two solutions in both vacua , but you can not glue them continuously without making a majorana , continuity here means making a global continuous solution , or wave-function . nevertheless , the topological property tells you that something strange exists , it will never give you the wave-function . you can think about group classifications in quantum mechanics : the groups of your molecule/lattice-cell tell you which interaction terms exist , they never give you their strength ( or the energy band splitting if you prefer to see the problem that way ) . a microscopic calculation is always required to get microscopic details . now , there are some tricks : since the characteristic length inside a superconductor is the coherence length $\xi=\hbar v_{f}/\delta$ ( $\delta$ gap parameter , $v_{f}$ fermi velocity ) , you can think as a decaying wave-function as $\psi_{\text{maj . }}\sim e^{-x/\xi}$ at the $x=0$ interface . this is cheating because this is not a wave-function , but it nevertheless gives the correct estimate for the real wave function which should be something like $\psi_{\text{maj . }}\sim e^{-x/\xi}\sin k_{f}x$ for instance ( $k_{f}$ fermi wave-vector ) . next , some limitations of the classification : it works for non-interacting systems only ( the coulomb interaction is not taken into account for instance ) it works for pure clean system only ( sometimes this issue is not trivial : for ( so-called topological ) $p$-wave superconductor the superconductivity itself is destroyed by disorder ; so perhaps the topological properties are conserved below the gap , but the gap vanishes due to disorder . . . ) so in practise it can never be applied to condensed matter problem . some people infer the topological property to avoid discussing these points . i think these points are the most relevant one , though .
i think perhaps what you are missing is in the " skipping through the commutator " part . do you understand where we get this equation ( try computing it yourself , if not ) : $$a_{-}a_{+} = \frac{1}{2 \hbar m \omega} [ p^{2} + ( m\omega x ) ^{2} ] - \frac{i}{2\hbar} [ x , p ] $$ now , the canonical commutator , i am sure you noticed ( as it is boxed on the same page in griffiths ) is $ [ x , p ] = ih$ . insert this into the above equation and note that we now have : $$a_{-}a_{+} = \frac{1}{2 \hbar m \omega} [ p^{2} + ( m\omega x ) ^{2} ] + \frac{1}{2}$$ all you need to do from there recognize the first term as $\frac{1}{h\omega}h$ . looking at the original equation , we factored $ [ p^{2}+ ( m\omega x ) ^{2} ] $ , so we can replace this with a+a− . under this , could not we just say that $h=\frac{1}{2} ( a_{+}a_{−} ) $ careful here . . . remeber that $p$ and $x$ in this expression ( and in the hamiltonian generally ) are operators , not scalars . this is why our " intuitive guesses " of $a_{\pm}$ are not exact factors of $ [ p^{2}+ ( m\omega x ) ^{2} ] $ , and why the canonical commutator above is important . edit : i just noticed that griffiths does include this intermediate step in computing $a_{-}a_{+}$: $$a_{-}a_{+} = \frac{1}{2 \hbar m \omega} [ p^{2} + ( m\omega x ) ^{2}-im\omega ( xp-px ) ] $$ notice that if $x$ and $p$ were scalars , the rightmost term would be 0 , and your intuition about $a_{-}$ and $a_{+}$ being " factors " would be correct . once you realize they are operators , however , it is obvious that we need to substitute $ [ x , p ] = xp-px = ih$ .
you might want to calculate poynting vector ( which corresponds to intensity [ w/m$^2$ ] of electromagnetic waves ) : $$\vec{s} = \frac{1}{\mu_0} \vec{e} \times \vec{b} , $$ which for plane waves amounts to $$\langle s \rangle = \frac{c b_0^2}{2 \mu_0} . $$
you conjecture is correct . one can relate the 2d ising model with the bond correlated percolation model . the details are in the paper percolation , clusters , and phase transitions in spin models . the basic idea is to consider interacting ( nearest neighbor ) spins as forming a bond with a certain probability . one can then show that the partition function of the ising model is related to the generating function of the bond-correlated percolation model . the above paper demonstrates that the bond-correlated percolation model has the same critical temperature and critical exponents as the 2d ising model . however , the values of $t_c$ and the critical exponents seem to be dependent on exactly how one defines a bond . see section iii . a . 1 in universality classes in nonequilibrium lattice systems ( or arxiv version ) . nonetheless your intuitive picture that there would be spanning clusters below $t_c$ and no such clusters above $t_c$ remains valid . edit 21 may 2012 i found a pedagogical paper that discusses this issue .
if you do not already have an estimate of where you are pointing , the only other option i know of is wcsfixer . there also used to be the pittsbugh wcs correction service , but it seems to be defunct now . these tools only work with fits files , so your first step would be converting whatever format you have into a fits file . the fits website has a fits viewer page that also has a list of tools that can convert formats , although converting from fits to something else is more commonly supported . a note on terminology : wcs stands for " world coordinate system " , the standard used for metadata in fits files that lets software transform between pixel coordinates and sky coordinates . if you have good wcs values , tools such as ds9 will let you find the coordinates of objects in a fits image interactively . if you already have an idea where you are pointing , there are more options . the simplest conceptually is to find the field were you think you are in skyview or google sky and see if you can match up objects . if you can , any of the several professional data reduction toolchains have the tools that can be used to generate wcs metadata . for example , the iraf data reduction environment has a few different options . the learning curve is steep , but not insurmountable for someone computer savvy with some time . daophot and focas ( now incorporated into iraf ? ) are other options . if you already have an approximate wcs solution , there are several other tools that will help refine the solution . the astromatic .nettoolchain is one example . ( i believe astrometry .netuses astromatic .netsoftware " under the sheets , " while wcsfixer uses iraf . ) the algorithms that can determine pointing without an initial guess generally use triangles of stars in the field , because the angles in a triangle are the unchanged by rotation and scale . however , these algorithms can work poorly in highly distorted images . see campusano et al . 1994 and tabur 2007 for more details . another tool of interest is ds9 's catalog overlay feature , but its more useful for confirming that your wcs is correct than it is at doing something about it if you find that it is not .
i think you are referring to asteroid 2005 yu55 which is making an approach on november 8 . this article on nasa jpl 's asteroid watch site gives some details including : the asteroid 's surface is darker than charcoal at optical wavelengths . amateur astronomers who want to get a glimpse at yu55 will need a telescope with an aperture of 6 inches ( 15 centimeters ) or larger . which to me implies that it would not be visible with the naked eye . it is estimated size is 1,300 feet ( 400 meters ) . jpl has also posted an hour-long video here discussing the asteroid . another nasa site , the near earth object program , has better technical information which may help locate the asteroid . some excerpts : . . . the object will reach a visual brightness of 11th magnitude and should be easily visible to observers in the northern and southern hemispheres . the closest approach to earth and the moon will be respectively 0.00217 au and 0.00160 au on 2011 november 8 at 23:28 and november 9 at 07:13 ut . . . . the best time for new ground-based optical and infrared observations will be late in the day on november 8 , after 21:00 hours ut from the eastern atlantic and western africa zone . to get the exact coordinates , you can try nasa 's solar system dynamics site . the page for asteroid 2005 yu55 has orbital elements with a link to generate ephemeris .
this sort of structured surface for friction reduction is an area of current research in phyiscs/engineering so its good to hear someone is actually using it and they work . generally these surface are designed with indentations rather than protusions , more like a golf ball surface , precisely to reduce wear . similarly changing the size/shape of the features will have a strong effect . a more conventional approach would be to use some sort of lubricant between the surfaces such as oil or water to reduce friction , but i guess there may be reasons why you are not doing that anyway . if you want something more exotic , anything that reduces the apparent load should help . think repelling magnets or firing air through holes in one surface . unfortunately i expect these more " cool " ideas are unlikely to very useful in practice .
yes , with a relatively inexpensive solar filter in front of the telescope almost any telescope can be used . the solar filter can be based on bopet ( trade name " mylar" ) . to view flares and prominences a much more expensive ( on the order of usd 1000 for a 8" reflector ) hydrogen-alpha filter is needed .
because a constant vector ( like the translation vector ) is annihilated by the differential operator $\partial^\mu$ .
let $v^{\mu}=\frac{dx^\mu}{d\lambda}$ . then , $\epsilon=-v_\mu v^\mu$ . to see the conservation of this quantity along the geodesic we have to look at the covariant derivative along the curve , that is $$v^\nu\nabla_\nu\epsilon=-v^\nu\nabla_\nu ( v_\mu v^\mu ) =-v^\nu v^\mu\nabla_\nu v_\mu -v^\nu v_\mu\nabla_\nu v^\mu . $$ but metric compatibility implies that $\nabla_\nu v_\mu=g_{\mu \sigma}\nabla_\nu v^\sigma$ . also using the geodesic equation $v^\nu \nabla_\nu v^\mu=0$ , we find $$v^\nu\nabla_\nu\epsilon=0 , $$ which shows that $\epsilon$ is a conserved quantity .
i think lhs of eqn 2.7 is normalized , meaning $\frac{1}{z}\int\mathcal{d}\phi \mathcal{o} exp\left ( -s_{e}\left [ \phi\right ] \right ) $ evaluated on $\mathcal{m}_{n}$ if you put $\mathcal{o} =1$ , you get 1 . but $z$ itself is proportional to the correlation function of the two primary fields ref : http://arxiv.org/abs/hep-th/0405152 sect iiia hope this is useful
quantum field theory is a general framework . there are many different kinds of field theories , the most well-known of which are qed ( quantum electrodynamics ) and qcd ( quantum chromodynamics ) . qed has been shown to agree incredibly well with experiment . the anomalous magnetic dipole moment of an electron , as computed using qed , agrees with experiment to 10 significant figures , the best agreement between theory and experiment of any physical theory in history . the large coupling constant of the strong force makes qcd much more difficult to deal with theoretically , and confinement makes it difficult probe experimentally .
yes , but one first has to generalize the classical 2-point brachistochrone problem $a \to b$ where the initial speed $v_a$ traditionally is zero , to the case where the initial speed $v_a$ may be non-zero but fixed . the solution to this initial speed brachistochrone problem ( assuming no friction ) is still a cycloid . now consider the 3-points brachistochrone problem $a \to b\to c$ with initial speed $v_a$ . the speed $v_b$ is given by energy conservation alone . thus the two segments $a \to b$ and $b \to c$ are completely decoupled , and they can be optimized as two independent 2-point brachistochrone problems with initial speeds $v_a$ and $v_b$ , respectively , leading to two corresponding cycloids $a \to b$ and $b \to c$ .
the assertion is based on the assumption that you either have only ‘small’ increases in temperature ( and hence small increases in entropy , think of all the $ds$ and $dt$ you encounter in standard thermodynamics ) or that your system is sufficiently homogenous that the change in entropy is a continous function of the change in temperature . this obviously breaks down if your molecules start to break up .
they trust the method used by gps for geodesy , where the claim is they can go down to picosecond and cm accuracy if necessary ( for military use ) . gps error analysis takes even general relativity into account . in gps signal propagation ( pdf ) the systematic errors of a simple gps setup are given , but the opera experiment has more sophisticated use of four satellites . there has been no analysis of the gps systematics further in the paper published in the archive , that is why i say they trust it . among other corrections , gps corrects back to the velocity of light in vacuum . the meter is defined as a fraction of the velocity of light in vacuum ( the second is defined by the caesium clock at normal temperature and pressure ) . in my opinion , it is possible that some of the very sophisticated corrections of gps values might be systematically off , with an end result of effectively redefining the meter . this would not show up in navigation or geodesy because the lengths probed by the opera experiment are very large ( 732&nbsp ; km ) and the errors very small , 20&nbsp ; cm . a tiny systematic offset of what a meter is for gps would not show up in the normal world use of it , but it would show up in measuring the neutrino speed with this method .
yes , of course , symplectic groups describe generalized situations that reveal the uncertainty principle . the reason for the relationship is that the symplectic groups are defined by preserving an antisymmetric bilinear invariant , $$ m a m^t = a $$ where $m$ is a matrix included into the symplectic group is the equation holds and $a$ is a non-singular antisymmetric matrix . where does the uncertainty principle enter ? it enters because $a$ may be understood to be the commutator ( or poisson bracket ) of the basic coordinates $x_i , p_i$ on the phase space . if we summarize $n$ coordinates $x_i$ and $n$ coordinates $p_i$ into a $2n$-dimensional space with coordinates $q_m$ , their commutators are $$ [ q_m , q_n ] = a_{mn} $$ with an antisymmetric matrix $a$ . consequently , the symplectic transformations may be defined as the group of all linear transformations mixing $x_i , p_i$ , the coordinates of the phase space , that preserve the commutator i.e. all the uncertainty relations between the coordinates $q_m$ . curved , nonlinear generalizations of these spaces are known as " symplectic manifolds " and nonlinear generalizations of the symplectic transformations above are known as " canonical transformations " . i think it does not make sense to talk about this relationship too much beyond the comments above because the relationship is in no way " equivalence " . one may say lots of things about related concepts but they are in no way a canonical answer to your question – they do not follow just from the idea of the " relationship " itself . i just wanted to make sure that a relationship between mathematical structures on both sides , especially the antisymmetric matrix , certainly exists .
the forces are never balanced , as there is only ever one force - gravity . the key is to remember newton 's second law : $f = ma$ . force and acceleration are paired , not force and velocity . knowing just an object 's current velocity tells you nothing about what forces are acting on it . there are two ways to see how the velocity goes to zero . either the initial impulse ( instantaneous transfer of momentum ) is depleted by a force applied over time , $$ m \delta v = \int f \ \mathrm{d}t , $$ or the initial kinetic energy is depleted by a force applied over a distance , $$ \frac{1}{2} m \delta ( v^2 ) = \int f \ \mathrm{d}x . $$ in both cases , the force of gravity is acting continuously to slowly cancel the initial velocity , and there is nothing that turns off this force at the apex of the trajectory .
in the specific case of the hamiltonian the non-locality arises because the time evolution depends on values of the field which are arbitrary far away . in the one dimensional case we have \begin{equation} i \frac{\partial}{\partial t}\ , \psi ~~~=~~~ \tilde{h}\ , \psi ~~=\ \sqrt{~m^2+\mathbf{\tilde{p}}^2_x~}\ \psi\ =\ \nonumber \end{equation} \begin{equation} \sqrt{ ~m^2-\partial_x^2~}~~ \psi ~~=~~ \frac{m}{x} k_1\left ( mx\right ) ~*~ \psi ~~~~~~~ \end{equation} ( we used $\hbar=c=1$ ) . in the last term $*$ denotes a convolution , in this case with a bessel k function . it is clear that this instantaneous dependency violates the speed of light restriction . see also my stackexchange answer here : $\nabla$ and non-locality in simple relativistic model of quantum mechanics now in the general case the value of $\psi ( x ) $ will depend on $\psi ( y ) $ at other locations in the past an it will depend on other fields such as $a^\mu ( y ) $ at other locations in the past . mathematically these dependencies stem from " taylor-expanded series " of differential operators but as long as you do not violate the speed of light restriction then this is perfectly fine . hans
surface coating of an integrating sphere is optimized for low losses . this white coating ( barium sulfate or ptfe /teflon ) acts like an ideal lambertian scatterer . all light is scattered ( ok , not 100% , but a very high percentage like 99,5% . see ressources ) it is emitted in the hemisphere following the cosine law : perpendicular to the surface it is highest . intensity decresease follows a cosine law . first generation stray light ( blue in op 's picture ) shows this light cone . imagine this cone at the corner of a cube : some light will hit a wall again and suffers tiny losses . detector port in cubic geometry hat a lower propability to to be hit with the ray of highest energy . with a sphere however all surface normal vectors point to its center . remember , that these rays " carry more energy " according to lambert 's cosine law . it will have lower losses than a measurement head with a cube geometry . a spherical geometry reduces the necessary number of stray events . resources labsphere spectralon data sheet : 99,5% hemispherical reflectance value : so 0,5% loss . spherical geometry is more expensive than cube geometry : stellarnet there also is a cylindrical geometry : ilx lightwave manuals
i can not claim any experimental experience in this area ( fortunately :- ) but i thought it was interesting enough to be worth a bit of googling . the results suggest there is a difference between shells and bombs . there is an extensive collection of eye witness accounts of ww2 at http://www.bbc.co.uk/history/ww2peopleswar/categories/, and searching this suggests that falling bombs make little if any sound . i could not find any of the eye witness accounts that mentioned a whistling sound . however if you google for stories from , for example , the current troubles in syria there are lots of reports of the whistling sounds shells make . chapter 5 of the art of noises describes the stereotypical whistling sound falling in tone , and as this dates from the years before hollywood it is presumably relatively uncontaminated . the author attributes this to fact that the shell velocity is highest immediately after firing and falls during flight due to air resistance . it is probably relevant that shells are generally fired at greater than the speed of sound so you would not hear them approaching . you had only hear them after they passed you , and of course the sound of those shells would be red shifted .
this appears to be related to the decomposition of a totally symmetric tensor into traceless parts , which is a fairly involved process . the general equation is $$\mathcal{c} q_{a_1 a_2\cdots a_s} = \sum_{k=0}^{ [ \frac{s}{2} ] } ( -1 ) ^s \frac{\binom{s}{k} \binom{s}{2k}}{ \binom{2s}{2k}} \delta_{ ( a_1 a_2} \cdots \delta_{a_{2k-1} a_{2k}} q_{a_{2k+1}\cdots a_s ) }{}^{c_1} {}_{c_1} {}^{c_2}{}_{c_2} {}^{\cdots c_k}{}_{\cdots c_k} , $$ where $ [ \cdot ] $ denotes the integer part , einstein summation is implied and $q_{ ( a_1 a_2 \cdots a_s ) } \equiv \frac{1}{s ! } \sum_{\sigma\in s_s} q_{a_{\sigma ( 1 ) } a_{\sigma ( 2 ) } a_{\sigma ( 3 ) } \cdots a_{\sigma ( s ) }}$ . for the quadrupole moment it is $\mathcal{c}q_{ab} = q_{ab} - \frac{1}{3} q^c{}_c \delta_{ab}$ , for the octupole $\mathcal{c}q_{abc} = q_{abc} - \frac{1}{5} ( q^d{}_{dc}\delta_{ab} + q^d{}_{da} \delta_{bc}+ q^d{}_{db}\delta_{ac} ) $ ; these yield the factors in your question . an indication ( perhaps proof , although i am not certain about this at the moment ) that the traceless part of a totally symmetric tensor is an irreducible representation is easy to see if one uses the hook formula in dimension 3 . a totally symmetric tensor of rank $s$ has $\frac{1}{2} ( s+1 ) ( s+2 ) $ degrees of freedom and the traceless one has the latter minus the number of ways to obtain the traces , $\binom{s}{2}$ , which yields $2s+1$ . this is the dimension of the irreducible representation of the algebra of so ( 3 ) with spin $s$ . a full proof of this statement is in maggiore gravitational waves - theory and experiments . reference : f.a.e. pirani lectures on general relativity 1965 .
the apparent contradiction with the second kepler law which you refer to is due to incompatible assumptions of the models in question . both kepler laws and lagrangian points can be derived from newtonian laws of motion and the law of universal gravitation under very specific assumptions about which objects can be neglected and which must be taken into account . in order to derive kepler laws one assumes the universe with only two bodies orbiting around their barycenter . in order to derive lagrangian points one assumes the universe with three bodies with one of them having mass negligible compared to the other two . for example , calculating soho 's orbit around the sun using kepler laws amounts to assuming earth 's influence on soho is negligible . this would give soho higher orbital velocity , but since earth 's influence is in fact very significant it would disagree with observations completely . on the other hand , employing lagrange solutions to the restricted three body problem assumes that the sun 's and earth 's influence is significant . in both cases the influence of the moon , jupiter and other planets and objects of the solar system is assumed to be negligible . thus both solutions produce approximations to the real world with the first being very inaccurate and entirely useless while the second having some useful relation to the observed reality . as for the derivation of lagrange points , you will find it here and in most books on orbital mechanics , e.g. a.e. roy 's orbital motion .
this is probably related to the derivation of de-broglie wavelength . . . since photon has wave-particle duality , we could equate planck 's quantum theory ( wave nature ) which gives the expression for energy of a wave of frequency $\nu$ , ( $e=h\nu$ ) with einstein 's mass-energy equivalence ( particle nature ) which gives relativistic energy for photon ( $e=mc^2$ ) $$mc^2=h\nu$$ the resultant mass gives the relativistic mass for a moving photon ( since photon has zero rest mass )
this is a good challenge ! here is maybe a solution : ordering a 1 meter long si rod with the correct doping level . it seems they are already able to make 2 mm diameter rods of 1 meter long out of pure silicon . ( http://www.goodfellow.com/catalogue/gfcat4j.php?ewd_token=5iqzqhxaluzqcqfmm0hg7c5opn6zxsn=o76p2nzwunxfcv3bozryzd6ivaflft )
the lasing mode ( stimulated emission ) may have nothing to do with the direction of the pump laser . for instance , flashlamp pumped lasers are pumped from the side , e.g. a ruby laser . stimulated emission occurs in the same direction as the stimulating photons -- that refers to another photon in the laser mode , not in the pump . this begs the question of how the lasing gets started . we usually say it is started by ' vacuum ' , or the quantum fluctuations of the field , which get exponentially amplified once the process gets going . so , in general , there is no preference between clockwise and counterclockwise modes in a ring cavity , and so the system can be unstable by switching between the modes . in most lasers , people take great pains to make sure the system has a preferred mode . ring lasers need a way to break the symmetry , which is usually an optical diode ( a faraday isolator ) . as an interesting note , another way to break the symmetry is to have a non-planar ring oscillator and polarization optics ( it is quite a clever solution ! ) .
in your question , i see 3 different context , where considering gravitational forces : a ) 2 point-like objects b ) 1 point-like object and one extended spherical symmetric object ( not too dense ) c ) a auto-gravitating extended spherical symmetric object ( not too dense ) a ) if you take 2 point-like objects , and take the limit $r \rightarrow 0$ , in fact , at some value of $r &gt ; 0$ , you create a black hole , because the ratio $\frac{energy}{radius}$ cannot excess a constant value $\sim \frac{1}{g}$ ( in $c=1$ units ) . note that mass is a kind of energy . so you do not have a problem with $r=0$ , because you create a black hole before . b ) if you consider a problem of a point-like object and a extended spherical symmetric object like earth ( not too dense ) , a theorem states that a object at distance $r$ only feels the gravitational force of masses inside the sphere of radius $r$ . that is , for instance , if the point-like object is inside the earth at radius $r &lt ; r_{earth}$ , it feels only the gravitational force of masses inside the sphere of radius $r$ . if we suppose a constant density $$\rho = \frac{m_{earth}}{4/3 \pi r_{earth}^3}$$ , then the force will be $$ f ( r ) = \frac {g m m ( r ) }{r^2} = \frac {g m ( \rho ~4/3 \pi r^3 ) }{r^2}$$ so , you have a linear force : $$f ( r ) \sim r$$ so , when $r\rightarrow 0$ , nothing bad , about gravitation , appears . ( of course , temperature and pression increase very much . . . ) if the spherical object is very dense , it is an other story , because you have a black hole , and you may have a " singularity": it is thought that something very bad happens to objects reaching the singularity ( tidal forces , roasted , etc . . ) . but you are here in the context of general relativity . c ) the last problem is an auto-gravitating extended spherical symmetric object . i will just give this reference of corse , as usual , if the object is too dense , you need general relativity , black holes , etc . . .
the universe is made up of one hundred billion galaxies each with between tens of millions of stars to hundreds of trillions of stars . so we have quite a few stars . it used to be somewhat unknown whether or not stars had planets and if so how many . recently a satellite called kepler was designed to look for evidence of planets and found that many stars have planets . since it is easier to see bigger planets , we have not seen that many the size of the earth , but that does not mean that they do not exist . in fact a few have been detected about the same size of the earth . the conventional wisdom is becoming solidified that many/most stars have some planets around them , which means that they are " solar systems . " as for habitability , that is the next question that people are working on - determining atmospheric content of planets . this is quite a bit harder of course . as for life or similar life on those planets that is an area of open research known as astrobiology . further reading : galaxies , kepler , astrobiology .
$$ ( \psi^\dagger \gamma^0 \psi ) ^* = \psi^\dagger \gamma^0 \psi$$ because $\gamma^0$ is hermitian . also , $$ \begin{align} ( \psi^\dagger i \gamma^0 \gamma^\mu \partial_\mu \psi ) ^* and = -i \partial_\mu\psi^\dagger \gamma^{\mu\dagger} \gamma^0 \psi\\ and = -i \partial_\mu\psi^\dagger ( \gamma^0 \gamma^\mu \gamma^0 ) \gamma^0 \psi\\ and = -i \partial_\mu\psi^\dagger \gamma^0 \gamma^\mu \psi\\ and = i \psi^\dagger \gamma^0 \gamma^\mu \partial_\mu\psi + \mathrm{surface\ , \ , term}\\ \end{align} $$ for the second line i used $\gamma^{\mu\dagger} = \gamma^0 \gamma^\mu \gamma^0$ and for the last line i integrated by parts . i think your question hinges on this part , because the last " index " we sum over is the spacetime index $x^\mu$ , i.e. , integration . it is the same reason why the quantum mechanical momentum operator $p = i \tfrac{\partial}{\partial x}$ is hermitian . edit : something i glossed over is that the spinors are also grassmann numbers , so care has to be taken . in particular , this means that the components of the spinors satisfy $$ ( \psi_i \phi_j ) ^* = \phi_j^* \psi_i^*$$ ( more about that here ) . one already interchanges the objects when taking the hermitian conjugate by the rules of matrix algebra , and there is a temptation to want to introduce a minus sign because they are grassmann numbers , but this would be redundant . borrowing from the linked math . se answer : $$ \begin{align} ( \eta\xi ) ^* and = [ ( a+ib ) ( c+id ) ] ^*\\ and = ( ac-bd+ibc+iad ) ^*\\ and =ca-db-icb-ida\\ and = ( c-id ) ( a-ib ) =\xi^*\eta^* \end{align} $$
the fact that the radiation will fall off at $\frac{1}{r}$ will break the set of conditions required for the enveloping metric to stay asymptotically flat i am not sure this is right . there are various definitions of asymptotic flatness . older definitions were written in terms of coordinates , newer ones in terms of conformal transformations . the original motivation , as described in ch . 11 of wald , was to accomplish for gr what had already been done for e and m . in e and m in sr , the coordinate-based requirements given by wald are that the fields fall off like $1/r^2$ at $i^0$ , but only like $1/r$ at $\mathscr{i}^+$ . this is clearly designed to allow radiation . the definition of asymptotic flatness in wald is actually framed in a pretty restrictive context . he first gives a definition that is purely for a vacuum spacetime ( not electrovac ) , and then remarks that the definition carries over automatically to a spacetime in which there is a vacuum in some open neighborhood of the boundary . obviously it should be possible to extend this to a case in which the matter fields fall off fast enough , but it looks like he just wants to avoid making the already technical discussion even more technical . but the definition of asymptotic flatness for vacuum spacetimes definitely allows for spacetimes with gravitational radiation , since the adm energy , which is only defined in asymptotically flat spacetimes , includes the energy of gravitational radiation at null infinity . ( this could probably be checked explicitly by power-counting . for an asymptotically flat spacetime , the metric differs from minkowski by $o ( 1/v ) $ , where $v$ is an affine parameter defined in the lightlike direction . ) as further confirmation that these spacetimes with hawking radiation are asymptotically flat , you can find penrose diagrams for them . for example , there is one in figure 2.41 in penrose , cycles of time .
i will not get into theoretical details -- luboš ad marek did that better than i am able to . let me give an example instead : suppose that we need to calculate this integral : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}$ here $y_{lm}$ -- are spherical harmonics and we integrate over the sphere $d\omega=\sin\theta d\theta d\phi$ . this kind of integrals appear over and over in , say , spectroscopy problems . let us calculate it for $m_1=m_2=m_3=0$: $\int d\omega ( y_{30} ) ^*y_{20}y_{10} = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\int d\omega \cos\theta\ , ( 1-3\cos^2\theta ) ( 3\cos\theta-5\cos^3\theta ) =$ $ = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\cdot 2\pi \int d\theta\ , \left ( 3\cos^2\theta\sin\theta-14\cos^4\theta\sin\theta+15\cos^6\theta\sin\theta\right ) =\frac{3}{2}\sqrt{\frac{3}{35\pi}}$ hard work , huh ? the problem is that we usually need to evaluate this for all values of $m_i$ . that is 7*5*3 = 105 integrals . so instead of doing all of them we got to exploit their symmetry . and that is exactly where the wigner-eckart theorem is useful : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3} = \langle l=3 , m_1| y_{2m_2} | l=1 , m_3\rangle = c_{m_1m_2m_3}^{3\ , 2\ , 1} ( 3||y_2||1 ) $ $c_{m_1m_2m_3}^{j_1j_2j_3}$ -- are the clebsch-gordan coefficients $ ( 3||y_2||1 ) $ -- is the reduced matrix element which we can derive from our expression for $m_1=m_2=m_3=0$: $\frac{3}{2}\sqrt{\frac{3}{35\pi}} = c_{0\ , 0\ , 0}^{3\ , 2\ , 1} ( 3||y_2||1 ) \quad \rightarrow \quad ( 3||y_2||1 ) =\frac{1}{2}\sqrt{\frac{3}{\pi}}$ so the final answer for our integral is : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}=\sqrt{\frac{3}{4\pi}}c_{m_1m_2m_3}^{3\ , 2\ , 1}$ it is reduced to calculation of the clebsch-gordan coefficient and there are a lot of , tables , programs , reduction and summation formulae to work with them .
since op 's problem looks like a homework assignment , we will only provide op with a series of hints rather than a full solution . i ) the lagrangian $l~=~\int \ ! dx ~{\cal l}$ is the spatial integral of the lagrangian density ${\cal l}$ . let us call the position field $q ( x , t ) $ and the corresponding velocity field $v ( x , t ) $ . the lagrangian $l [ q ( \cdot , t ) , v ( \cdot , t ) ] $ is a functional of the position and the velocity fields , cf . e.g. this phys . se answer . the momentum field is the functional derivative $$ \tag{1} p ( x , t ) ~:=~\frac{\delta l [ q ( \cdot , t ) , v ( \cdot , t ) ] }{\delta v ( x , t ) } . $$ an infinitesimal variation $\delta l$ of the lagrangian functional $l$ is given by $$\tag{2} \delta l~=~\int \ ! dx~\left ( \frac{\delta l}{\delta q}\delta q+\frac{\delta l}{\delta v}\delta v \right ) . $$ the exercise basically asks op to derive a functional version of noether 's theorem , which he probably knows for ordinary point mechanics . ii ) let $$\tag{3} \delta q~=~\varepsilon y$$ be an infinitesimal variation of the position field , where $\varepsilon$ is an infinitesimal constant , and where $y$ is the generator . correspondingly , the velocity field transforms as $$\tag{4}\delta v~=~\varepsilon \dot{y} . $$ ( the transformation ( 3 ) is a so-called vertical transformation . in general , one could also allow horizontal contributions from variation of $x$ and $t$ . ) iii ) assume that the transformation ( 2 ) is a so-called quasi-symmetry $$\tag{5} \delta\left ( \left . l \right|_{v=\dot{q}}\right ) ~=~ \varepsilon\frac{d}{dt} \left ( \left . f\right|_{v=\dot{q}}\right ) , $$ where $f [ q ( \cdot , t ) , v ( \cdot , t ) ] $ is some functional . ( if $f=0$ is zero , then the quasi-symmetry becomes a symmetry of the lagrangian $l$ . ) iv ) the ( temporal component of the ) bare noether current $j^0$ for a vertical transformation ( 3 ) is simply the momentum field $p$ times the vertical generator $y$ , $$\tag{6} j^0 ~:=~p y . $$ the bare noether charge $q^0$ is the spatial integral $$\tag{7} q^0 ~:=~ \int \ ! dx~j^0 . $$ the full noether charge $q$ is improved with ( minus ) the $f$-functional , $$\tag{8} q ~:=~q^0-f . $$ formula ( 8 ) corresponds to op 's last formula ( v1 ) . noether 's theorem states that $q$ is a conserved quantity on-shell , i.e. if the equation of motion $$\tag{9} \left . \frac{d}{dt} \left ( \frac{\delta l}{\delta v}\right|_{v=\dot{q}}\right ) ~=~\left . \frac{\delta l}{\delta q}\right|_{v=\dot{q}} $$ is satisfied . the proof in the functional setting is very similar to the usual proof given in ordinary point mechanics .
you may always add the numbers in front of the units , and if the units are the same , one could argue that the addition satisfies the rules of dimensional analysis . however , it still does not imply that it is meaningful to sum the temperatures . in other words , it does not mean that these sums of numbers have natural physical interpretations . if one adds them , he should add the absolute temperatures ( in kelvins ) because in that case , one is basically adding " energies per degree of freedom " , and it makes sense to add energies . adding numbers in front of " celsius degrees " , i.e. non-absolute temperatures , is physically meaningless , unless one is computing an average of a sort . this is a point that famously drove richard feynman up the wall . read judging books by their covers and search for " temperature " . he was really mad about a textbook that wanted to force children to add numbers by asking them to calculate the " total temperature " , a physically meaningless concept . it only makes sense to add figures with the units of " celsius degrees " if these quantities are inteprreted as temperature differences , not temperatures . as a unit of temperature different , one celsius degree is exactly the same thing as one kelvin . if you interpolate or extrapolate a function of the temperature , $f ( t ) $ , you do it as you would do it for any other function , ignoring the information that the independent variable is the temperature . results of simplest extrapolation/interpolation techniques will not depend on the units of temperatures you used .
your professor is right . capacitors k2 and k3 are not parallel and then in series with capacitor k1 , because the vertical line that is separating k1 on left and k2 and k3 on right is not an equipotential line . that is , potentials on the left side of k2 and on the left side of k3 are not the same ! you actually have upper half of k1 and k2 in series and lower half of k1 and k3 in series , all together parallel . interesting note : only if you have put a metal plate between k1 on the right and k2 and k3 on the left , your procedure would be right !
poincaré invariance is a fundamental requirement of relativistic ( quantum ) physics . in particular if $u_g : {\cal h} \to {\cal h}$ represents the ( non-necessarily linear ) action of a poincaré transformation $g$ on ( normalized ) vectors $\psi$ of the hilbert space $\cal h$ of the considered system , transition probabilities have to be preserved : $$|\langle u_g ( \psi ) | u_g ( \phi ) \rangle|^2 = |\langle \psi | \phi \rangle|^2 \quad \forall \psi , \phi \in {\cal h}\: , ||\psi||=||\phi||=1\: . \quad ( 1 ) $$ a celebrated theorem due to wigner establishes that a ( bijective ) map $u : {\cal h} \to {\cal h}$ verifying ( 1 ) must necessarily be linear and unitary or anti linear and anti unitary , depending on the physical nature of the transformation . concerning representations of poincaré group $\cal p$ , by definition they have to satisfy , in addition to ( 1 ) , ${\cal p} \ni g \mapsto u_g$ with $u_g u_h = u_{g\cdot h}$ ( $^*$ ) and $u_e = id$ , where $\cdot$ is the group product in $\cal p$ , just in view of the definition of group representation . in principle each $u_g$ has to be unitary or anti unitary . if $g\in \cal p$ belongs to the proper orthochronous subgroup ${\cal p}_+^\uparrow$ , it can always be decomposed as $g= h\cdot h$ where $h$ still belongs to the same subgroup . therefore $u_g= u_{h} u_{h}$ , thus $u_g$ must be unitary ( even if $u_h$ is anti unitary , as the composition of a pair of anti unitary operator is always unitary ) . we conclude that the orthochronous poincaré group ${\cal p}_+^\uparrow$ ( and consequently the orthochronous lorentz group $so ( 1,3 ) ^\uparrow$ ) can only be represented by unitary operators in quantum theories , when the action of the group is on states . non unitary representations arise dropping the last requirement . for instance dealing with dirac or weyl spinors . ( $^*$ ) actually a phase could take place , since states are represented by normalized vectors up to phases : $u_gu_h = e^{i\alpha ( g , h ) }u_{g\cdot h}$ , however it does not change the result of the subsequent reasoning . as a matter of fact , it is possible to prove that continuous ( projective ) unitary representations of ${\cal p}_+^\uparrow$ are not affected by such phases , differently form representations of galileo 's group where those phases play a crucial role .
first of all , the vev of $\phi$ scales like a positive power of $\mu$ which has the units of mass . so all the effects of the symmetry breaking scale like a positive power of the mass scale $\mu$ . at energies $e$ satisfying $e\gg \mu$ , i.e. much higher than $\mu$ , the value of $\mu$ itself as well as vev and other things may simply be neglected relatively to $e$: all the corrections from the symmetry breaking go to zero , relatively speaking . in the uv , i.e. at short distances , the dimensionless couplings ( and interactions ) such as $\lambda$ are much more important than the positive-mass-dimension dimensionful couplings such as $\mu$ . it is still true that the high-energy theory was not exactly symmetric in the treatment above ; it was just approximately symmetric . however , in fact , sometimes it is exactly symmetric under the sign flip of $\phi$ . that is because the value of $\mu$ runs , as in the renormalization group , and in many interesting theories , $\mu$ actually switches the sign at energies exceeding some critical energy scale $e_0$ . so the high energy theory may have a single symmetry-preserving minimum of the potential and the symmetry breaking may be a result of the flow to low energies . it is legitimate you want to know the higher-loop corrections to the higgs potential but the subtlety you should appreciate is that all these calculations proceed relatively to some renormalization ( mass ) scale . if the scale is low , the behavior of the theory at much higher energies may only be deduced by the ( inverted ) rg flows . however , the deduction of the short-distance theory from its long-distance effective field theory approximation is never exact ; it is the high-energy theory that is the legitimate starting point and the low-energy physics is its consequence .
actually what you have to transform to define a symmetry for a classical or quantum system are the dynamical variables describing the system and appearing in the action rather than the metric ( moreover time reversal could need a further complex conjugation ) . in any cases here you are thinking of a discrete symmetries . noether theorem instead implies the existence of dynamically conserved quantities provided the symmetries of the action are continuous : there is a dynamically conserved quantity for each continuous ( differentiable actually ) one-parameter group of symmetries of the action . passing to quantum systems ( fields in particular ) , dynamically conserved quantities may arise also for discrete symmetries , provided they are described by simultaneously unitary and self-adjoint operators . parity operator can be taken of this type , but time reversal one cannot ( if the hamiltonian is bounded below as is physically necessary for the stability of the system ) , as it is an anti unitary operator ( there are the only two possibilities permitted by kadison-wigner theorem ) .
an orbit is stable because of conservation of angular momentum . suppose we start with an object in an exactly circular orbit and slow it down slightly . that means it is moving at less than orbital velocity so it starts to fall inwards . however as its distance to the sun decreases the tangential component of its velocity has to increase to conserve angular momentum . so as the object nears the sun it moves faster and faster , and at its closest approach to the sun it is moving at well above orbital velocity so it starts to move outwards again . you end up with an elliptical orbit : ( this diagram shamelessly cribbed from google images ) it is actually very difficult to get something orbiting a star to fall into it , because you have to reduce the tangential velocity to zero . at the distance of the earth from the sun the orbital velocity is 108,000 km/h . you would have to slow the earth by this amount to make it fall into the sun , and fortunately no meteorite is likely to do that . on a side note , nasa recently sent the messenger spaceship to study mercury , and getting the ship to mercury was hard because of the need to shed all that orbital velocity . even though mercury is a lot closer to the sun than the earth is you can not just fall there . messenger had to use several gravity assists to shed enough speed to allow it to orbit mercury .
i think you are exactly right . in fact the method you used of imagining a second moving car is exactly what you would do if you were trying to find the pressure as a function of position in the original problem with the wall . the technique is called the method of images .
update to address new questions . the answer to this question is no . at least if you take the question purely formally . only theories such as classical field theory , quantum field theory and continuum mechanics are field theories ( you generally recognize them by having continuous degrees of freedom ; also they usually have the word field in the title :- ) ) . but physically , lots of different theories may be equivalent , or may be approximations of some other theory , so there are many connections among them ( this is the point i was trying to illustrate , but maybe i overemphasized it ) . difference between qm and qft is essentially the same as between classical mechanics and classical field theory . in the mechanics you have just a few particles ( or more generally , small number of degrees of freedom ) , while fields have an infinite number of degrees of freedom . naturally , field theories are a lot harder than the corresponding mechanics . but there is a connection i already mentioned : you can see what happens when you let the number of particles grow arbitrarily large . this system will then essentially behave as a field theory . so in a sense , we can say that field theory is a large $n$ ( number of degrees of freedom ) limit of the corresponding mechanical theory . of course , this view is very simplified , but i do not want to get too technical here . field theory is a theory that studies fields . now what is a field ? i suppose everyone should be familiar with at least some of them , e.g. gravitational or electromagnetic ( em ) field . now , how do you recognize that object is a field ? well , essentially , you look at how complicated the object is . to make this more precise : main objects of study of classical mechanics are point particles . all you need to keep track of them is just few parameters ( position , velocity ) . on the other hand , consider the em field : you need to keep track of the data ( electric and magnetic field vector ) in every point of the universe , so there is infinitely many parameters of this system ! this is what i meant by system being large : you need a lot of data to describe it . now , it might seem that something is amiss . you do need a lot of data to describe real objects ( just think of how many atoms there are in the grain of sand ) . so are ordinary objects fields ? yes and no , both answers are correct depending on your point of view . if you consider a massive object as essentially being described by few parameters ( like center of mass velocity and moment of inertia ) and completely ignore all information about atoms then it is clearly not a field . nevertheless , at the microscopic level , atoms wiggle around and even the grain of sand really is as complicated object as any em field ( not to mention that atoms themselves produce em field ) , so it is certainly correct to call them that . now let us see where our definition of field takes us . let 's talk about quantum mechanics for a while . what about two quantum particles ? is it a field ? well , clearly not . what about three ? still not . and what if we keep adding particles so that there will be a huge number of them ? well , it turns out that we will get a quantum field ! this is precisely the correspondence between e.g. photons and quantum em field . you can either look at em field as being described by vector of electric and magnetic field at every point as in the classical case , or you can instead reorganize your data so that you keep track of what kind of photons you have . it is useful to carry both pictures in head and use the more appropriate one . there is also a subject of continuum mechanics . there you can also start with particles ( describing atoms in some real object , e.g. water ) and because there are so many of them , you can again reorganize your data , consider the object as being essentially continuous ( which real objects surely are at least unless you look at them with a microscope ) , and instead describe them by parameters such as pressure and temperature at every point . to summarize : the field theory is essentially about dealing with large objects . however , when we are looking at the problem with particle hat on , we usually do not say it is a field . for instance , when describing real objects as consisting of atoms , we are usually talking about statistical mechanics , or condensed matter physics . only when we move to the realm of continuum mechanics , we say that there are fields . there is much more to be said on the topic but this post got already too long so i will stop here . if you have any questions , ask away !
i suppose that " periodic movement " need not be oscillatory . for example , consider an object that moves right for one minute , stops for one minute and repeats . this would be periodic movement in the sense of occuring at regular intervals . and , while oscillation implies back and forth movement ( if we are still talking about movement ) , it need not be periodic .
to formalize dushya 's comment as an answer : since the kilogram is an arbitrary , man-made unit , the actual numerical value of the proton mass in kilograms is meaningless ( i.e. . it is as good as its value in pounds , ounces , stones , solar masses , $\textrm{mt}/c^2$ , etc . ) . the true fundamental constants of nature are dimensionless : they have the same value in every unit system . thus dimensional constants like $c$ , $\hbar$ , $g$ , and indeed $m_p$ and $m_e$ , are not very meaningful and can be set to $1$ with a judicious choice of units ( which is done quite often ) . true fundamental constants are often ratios of dimensional quantities such as the fine structure constant , $$\alpha=\frac{e^2/4\pi\epsilon_0}{\hbar c} , $$ which quantifies how strong , on a quantum scale , the electromagnetic interaction is . in terms of mass , the constants you had like to predict are things like the ratio $m_p/m_e\approx 1800$ , and so on . given that , the formula you have found is just a fluke : a consequence of the fact that we chose as our basic unit of mass the mass of a cube of water whose sides measure one hundred-millionth of a quarter of a meridian . edit , given the long comment thread : @fred , let me try and rephrase this a bit to see if i can bring out the arbitrariness we are talking about well up to the surface . the real number you have discovered is the inverse of the one you posted : $$\frac{10^{26}}{\sum_{m=1}^\infty \frac{1}{ ( m^2+1 ) _{2m}}}\approx 5.978638 \times 10^{26} , $$ which appears to approximate within experimental error the number of protons and neutrons that will fit - at sea level and at " room " temperature - a cubical box about yea big in side containing that particular common chemical that you find in drinking fountains , kitchen sinks , lakes , and even falling out of the sky ( on earth ) rather often .   since the proton really is quite fundamental , any stabilizing influence of the fractalness needs to account for the size of the earth , its predominant climate a hundred years ago , the abundance of water in it , and the detailed chemical state of the brains of a number of mainly french gentlemen that sat down a while ago to try and make unit systems ( which are always arbitrary ) at least simple to work with .
note that : \begin{equation} \begin{aligned} \langle j | a \rangle and = \langle j |\left ( \sum\limits_{i} a_i |i \rangle \right ) \\ and = \langle j| \left ( a_1 |1 \rangle + a_2 |2 \rangle + a_3 |3 \rangle + \cdots + a_j |j \rangle + \cdots \right ) \\ and = a_1 \underbrace{\langle j | 1 \rangle}_{=\delta_{j1} = 0} + a_2 \underbrace{\langle j | 2 \rangle}_{=\delta_{j2} = 0} + a_3 \underbrace{\langle j | 3 \rangle}_{=\delta_{j3} = 0} + \cdots + a_j \underbrace{\langle j | j \rangle}_{=\delta_{jj} = 1} + \cdots \\ and = a_j \end{aligned} \end{equation}
an answer that professor lenny susskind gave to a non-physicist audience at stanford in june 2012 went along these lines ( by memory and some very short notes i took ) : the charge on an electron is$\ \alpha \ \approx \ 1/137\ $which means that 99% of the electron is just the bare electron while about 1% of the time it is an electron plus a virtual photon . whereas the charge on a magnetic monopole would be $1/\alpha \ \approx \ 137$ so the magnetic monopole would have about 100 constituents on average - like lots of photons , current etc . thus the magnetic monopole would be a composite particle and very heavy due to all the strong fields and constituents . do not blame any errors on lenny , it could be my mistaken memory/notes . i think that the dimensionless number $\alpha$ is a reasonable stand-in for electric charge since it is the coupling constant used to calculate connections between electrons and photons . similarly , $1/\alpha$ would be the coupling between magnetic monopoles and photons .
what you wrote down is the quotient of the 2-dimensional translation group by a discrete subgroup . but by far not every closed manifold arises as a quotient of groups this way . one should be aware that the term " compactification " in physics is used not so much to refer to what in mathematics is called compactification of non-compact spaces . for instace one-point compactification in the mathematical sense turns the real line line into the circle . ( however it also turns the plane into the 2-sphere , not into the torus . ) instead , what is meant by " compactification " in physics is that you just choose a closed ( and hence compact ) manifold $q$ , then choose spacetime $x$ to be a $q$-fiber bundle over space base space ( often assumed to be just a product $x = q \times y$ ) , and then describe the kaluza-klein mechanism for passing from physics on $x = q \times y$ to effective physics on just $y$ . in particular for calabi-yau " compactifications " you just choose $q$ to be a calabi-yau manifold , and then consider the kaluza-klein mechanism on spacetimes which are $q$-fiber bundles . you do not actually obtain these spacetimes as compactifications of non-compact spacetimes in the sense of mathematics . ( well one could consider that problem , but this is not what is generally meant by " compactification " in physics . )
the non-relativistic expression for the wave operators $$ ω±=lim{_{t→∓∞}}u ( t ) {_{full}}u{_{0}} ( t ) , $$ needs revision in field-theoretic situations since usually the free and interacting fields act in different hilbert spaces . an early example is given in " asymptotic conditions and infrared divergences in quantum electrodynamics " by p . p . kulish and l . d . faddeev . theoretical and mathematical physics 1970 , volume 4 , issue 2 , pp 745-757 . thus i expect that there is no simple answer to this question .
i am not sure if i know the correct answer ( as i am a student my self ) , but i will try ( and if i am wrong , someone please correct me ) . the first thing that took me some time to figure out is what they mean by adjoint representation . in georgi 's book he defines the adjoint representation of a generator as : \begin{equation} [ t_i ] _{jk} \equiv -if_{ijk} \end{equation} which is equivalent to the adjoint representation of a lie algebra . however , when discussing monopole , they actually mean the adjoint representation of a lie group . this means that $\phi$ takes values in the lie algebra ( the vector space formed by the generators ) and can be expressed in terms of the generators in an arbitrary representation : \begin{equation} \phi = \phi^a t^a \end{equation} where $t^a$ denote the generators in an arbitrary representation ( and there is an implicit sum over repeated indices ) . now , let us look at the simplest example , which is the bosonic part of the $\mathrm{su ( 2 ) }$ gauge invariant georgi-glashow model : \begin{equation} \mathcal{l}=\frac{1}{8} \mathrm{tr} ( f_{\mu \nu} f^{\mu \nu} ) - \frac{1}{4} \mathrm{tr} ( d_\mu \phi d^\mu \phi ) - \frac{\lambda}{4} ( 1-\phi^a \phi^a ) ^2 \end{equation} we can write the kinetic and potential energy , $t$ and $v$ , as : \begin{equation} t=\int \left ( - \frac{1}{4} \mathrm{tr} ( f_{0i}f_{0i} ) - \frac{1}{4} \mathrm{tr} ( d_0 \phi d_0 \phi ) \right ) \mathrm{d^3}x \end{equation} and : \begin{equation} v=\int \left ( - \frac{1}{8} \mathrm{tr} ( f_{ij}f_{ij} ) - \frac{1}{4} \mathrm{tr} ( d_i \phi d_i \phi ) + \frac{\lambda}{4} ( 1-\phi^a \phi^a ) ^2 \right ) \mathrm{d^3}x \end{equation} where we used $l= \int \mathcal{l} \ ; \mathrm{d^3}x = t-v$ . in order to get finite energy solutions we have to impose boundary conditions such that the total energy of the model vanished at spatial infinity . it should be clear that one of the requirements to ensure that the energy vanishes is : \begin{equation} \phi^a \phi^a =1 \end{equation} this implies that the higgs vacuum corresponds to an infinite amount of degenerate vacuum values lying on the surface of a unit two-sphere in field space , which we will denote by $s^2_1$ . furthermore , by imposing the aforementioned finite energy boundary condition , this gives rise to the following map : \begin{equation} \phi : s^2_\infty \mapsto s^2_1 \end{equation} where $s^2_\infty$ denotes the two-sphere associated with spatial infinity ( in 3 dimensions ) . this is in fact the definition of the winding number ( or degree ) between two two-dimensional spheres and is therefore classified by $\pi_2 ( s^2 ) =\mathbb{z}$ ( and it is in theory possible to construct topological solitons ) . now , if $\phi$ was in the fundamental representation , then i do not think it is possible to construct these topological solitons .
physics should not depend on the system of units you choose to solve any problem , otherwise physics would in europe will be different than physics in the us , which is stupid ! the answer to your question depends on the initial definition of units you use in your problem . you start with si units , you end with joules . you start with cgs , you end with ergs . you start with natural units , you end with ev .
but the magnetic field of the electron cannot couple to its own spin ? or can it ? how do i explain the energy spit in this reference frame ? in classical em theory , the common explanation of the ls term from the frame of the electron is not very convincing , because this frame is non-inertial and there are potentially all kinds of non-inertial forces which were not discussed . explaining this in the frame of nucleus seems as an easier task . if we imagine charged rotating and orbiting ball in a central electric field of the nucleus , this central electric field affects the orbital motion of the ball ; the ball accelerates , which changes its own electric and magnetic field . these fields of the ball then could influence the rotation of the ball . i do not know if it gives something close to the ls term , but it seems possible .
your error comes in when you change the sign of $d\vec{x}$ to $-d\vec{x}$ in part 2 . the differential element $d\vec{x}$ always points in the direction from the lower bound of the integral ( $x_2$ in this case ) to the upper bound ( $x_1$ ) . since you switched the bounds of the integral in part 2 , you already switch the direction . by including a negative sign , you end up switching the direction again and thus have a integral equivalent to that of part 1 ) . below is an explicity calculation done in regards to some sign confusion when calculating work . let us first consider the work done by gravity ( $\vec{f}=-mg\hat{x}$ ) on an object moving from the point $\vec{x_1}=x_1\hat{x}$ to $\vec{x_2}=x_2\hat{x}$ where $x_2&gt ; x_1$ . this is $w_1 = \int_{x_1}^{x_2} \vec{f} \cdot d\vec{x} = \int_{x_1}^{x_2} f ( -\hat{x} ) \cdot ( \hat{x} ) dx = -f \int_{x_1}^{x_2} dx = -f ( x_2 - x_1 ) $ where $f=mg$ thus we see $w_1$ is negative , which makes physical sense since the work done by gravity as the object moves up counteracts the motion of the object . now , let us consider the work done as the object moves down . this is $w_2 = \int_{x_1}^{x_2} \vec{f} \cdot d\vec{x} = \int_{x_1}^{x_2} f ( -\hat{x} ) \cdot ( -\hat{x} ) dx = f \int_{x_1}^{x_2} dx = f ( x_2 - x_1 ) $ note that here the differential element $d\vec{x}$ is now equal to $dx ( -\hat{x} ) $ ( i.e. . it is pointing down ) . also note , that i could also flip my integral bounds such that they go from $x_2$ to $x_1$ , but in this case the sign of the differential element would be negative and thus the direction of the differential element would be positive ( i.e. . $d\vec{x} = -dx ( -\hat{x} ) = dx\hat{x}$ ) . from this we see that $w_2$ is positive , again making physical sense since the work done by gravity is now in the direction of the object 's motion . adding together $w_1$ and $w_2$ we see that the total work $w=w_1 + w_2$ done by gravity on the object is zero .
the radius of a non-rotating black hole is $$r_s = \frac{2gm}{c^2} \tag{1}$$ where $m$ is the mass , $g$ is newton 's constant , and $c$ is the speed of light . this is the distance from the center of the black hole to the event horizon . the event horizon is the surface that traps light and objects , it separates inside and outside the black hole . anything that passes inside the event horizon can never escape , even light . this is why it is said that the escape velocity at the surface of a black hole is $c$ . it is also the case that any object with $r &lt ; r_s$ is a black hole . but since the mass should be roughly proportional to the volume , and the volume is proportional to $r^3$ , anything heavy enough will form a black hole . therefore from ( 1 ) the proper answer to your question is : because they are very massive , not because they are small .
i can not say i know any that do it . a small refrigerator modified with clear panels and post who tip acts as the seed-point for nucleation - this is the basic concept of the lab setup a for ice crystal growth . i think your objectives need to be stated better . " large " could be an imaginative thing wherein a real implementation will demand you grow on a substrate . this then creates its own problems regarding purity of the crystal and morphology . i am on mobile , so pardon the lack of references .
you state that the source produces a pulse per second . a hum , on the other hand , is a sound with a frequency of several hundreds of cycles per second . if that source is particularly violent it may be able to get a sound going in a very good resonator . ( example : i have a guitar , and i have noticed that when i sneeze and the guitar happens to lie right next to me sometimes one or two strings are audibly triggered . ) anyway , to have a chance the resonator must be one that is very responsive . that is , the resonator must be shaped ideally for one particular frequency , so that all of the energy that it does aborb goes to that one frequency . the cube that you describe is definitely a very poor resonator . i think you would have a hard time getting the sound from any source to resonate in it at all . for comparison , bottles tend to have a narrow resonance response . you blow over the top of an open bottle , and when you hit the right angle of blowing just over it , and with the right airflow , you get a resonance in that bottle . hum that same note to the bottle and your humming gets amplified . that amplification shows the resonator 's responsiveness . summerising , to have a chance of producing some effect you need a resonator that is very , very responsive .
from the dutch national science quiz 2006 ( my translation ) : question 14: you put a duvet cover together with smaller laundry in the washing machine . why , at the end of the program , all smaller laundry has twisted itself in the duvet cover ? due to the left-and-right cycling of the drum washing machines predominantly cycle one way . to loosen the laundry , the drum sometimes abruptly turns the other way . due to this opposite movement , suddenly a few liters of water bumps very forcefully into the laundry , and therefore the opening of the duvet cover will come to lie completely open . smaller laundry falls in one piece at a time . as soon as the machine goes into centrifuge mode , the smaller laundry pieces are being pushed in further . it is very difficult for laundry that is in the duvet cover to get out . nb : the false answers were ( my translation ) : because small laundry pieces are more sensitive to water vortices than large items due to the area ( size ) difference between the duvet cover and smaller laundry i would like to add , from personal experience , that it is very rare that all smaller laundry ends up in the cover . sometimes half of it will get in there , and sometimes just the odd sock . but perhaps that is just me and my machine .
we should probably distinguish between a particle being " point-like " and a particle being " structure-less " . in classical mechanics we talk of " point-like " particles , objects with no extension . it is the case that in general relativity any " point-like " mass would be inside of its event horizon and so would be a black hole . in quantum-mechanics even a " structure-less " particle - a particle with no consitituent parts - is wave-like and has extension , though not a fixed size , and it can never be come exactly point-like since that would take an infinite amount of energy . i do not believe it to be the case , therefore , that quantum-mechanically all particles are black holes in any sense .
the centre of mass is the point at which our collection of objects will balance if we put a pivot there . let 's call this point $\bf r$ . the vector joining the point $i$ to $\bf r$ is simply $\bf r_i - \bf r$ . the force acting at this point is $m_i \bf g$ , so the torque at the point $i$ is : $$ \bf t_i = m_i \bf g \times ( \bf r_i - \bf r ) $$ the total torque must sum to zero , because that is how we define the centre of mass , so : $$ \sum m_i \bf g \times ( \bf r_i - \bf r ) = 0$$ and the cross product is distributive over addition so we can take it outside the sum : $$ \bf g \times \sum m_i \bf ( \bf r_i - \bf r ) = 0$$ and this can ony be satisfied if : $$ \sum m_i \bf ( \bf r_i - \bf r ) = 0$$
an oscillator is usually characterized by its quality factor q . this is a dimensionless parameter which measures how " good " of an oscillator it is . it also relates to the quantity you are interested in - a linear , damped oscillator will exhibit a lorentzian peaked response in the frequency domain . the bandwidth of the resonance ( points where the response is decreased to 50% ) is given by $\delta f = \frac{f_0}{q}$ . the quality factor can also be related to the damping coefficient - for more info check out wikipedia : http://en.wikipedia.org/wiki/q_factor
the $u ( p , s ) $ and $v ( p , s ) $ describe the spin of the particle and antiparticle , while the $ \psi $ are the fields . a dirac field can then be expanded in terms of these objects , \begin{equation} \psi = \int \frac{ \ , d^3p }{ ( 2\pi ) ^3 } \frac{1}{ \sqrt{ 2e _p }} \sum _s \left ( a _p ^s u ( p , s ) e ^{ - i p \cdot x } + b _p ^{s \ , \dagger} v ( p , s ) e ^{ i p \cdot x } \right ) \end{equation} you may wonder why $ u $ and $ v $ are even needed since a complex scalar field can just be expanded as , \begin{equation} \phi = \int \frac{ \ , d^3p }{ ( 2\pi ) ^3 } \frac{1}{ \sqrt{ 2e _p }} \sum _s \left ( a _p ^s e ^{ - i p \cdot x } + b _p ^{s \ , \dagger} e ^{ i p \cdot x } \right ) \end{equation} the reason is a scalar field does not carry any spin and so can just be described by an ordinary function ( think quantum mechanics where we need two component spinors to describe particle spin ) . on the other hand , a dirac fermion is actually made up of two particles - a left chiral fermion and a right chiral fermion . each of these two fields can be either spin up or spin down and hence requires 2 degrees of freedom . in total it needs $ 2 \times 2 =4$ components to describe it .
there are no bound states for this potential , for any positive $\alpha$ . all energies $e&gt ; 0$ are allowed , and all of those are unbound states , in which $\phi\sim e^{ikr}$ for large $r$ , with $e=k^2/2$ . the easiest way to prove that there are no states with $e&lt ; 0$ is to show that the expectation values of both kinetic energy and potential energy , $\langle t\rangle$ and $\langle v\rangle$ , are positive for any valid wavefunction . for an energy eigenfucntion , $e=\langle e\rangle=\langle t\rangle+\langle v\rangle&gt ; 0$ . the fact that these are not bound states -- that is , that they go as $e^{ikr}$ rather than as a decaying function of $r$ at large distances -- is most easily seen from the fact that , at sufficiently large $r$ , the potential term is always negligible in comparison to the right-hand side . so at large $r$ we have $-\phi&#39 ; &#39 ; /2=e\phi$ .
firstly , is that correct ? yes your intuitive understanding for this part of the coriolis effect is correct . the second part , that is , why wind in the east direction is deflected south , is a bit trickier , and involves the use of centripetal force . this is given by the equation : $f = \frac{mv^2}{r}$ if we re-arrange the above equation , we can find $r$ in terms of $v$ , and we arrive at : $r = \frac{mv^2}{f}$ this tells us that as velocity increases , the radius required to maintain the orbit also increases . now let 's apply this concept to winds on the earth . if we feel no wind on the earth , then the air in the atmosphere is travelling at the same velocity as the earth . the earth is naturally spinning towards the east . in the case of an additional eastward wind felt on the earth , this wind has effectively increased its velocity , and therefore the above equation tells us that the radius of orbit must increase as well . radius in this case is the distance , measured perpendicularly of the earth 's axis , between the axis and the wind . in order for the radius to increase , the wind moves southwards , where the radius is larger . similarly , wind moving in the west direction , is moving in the direction opposite of that to the earth , and therefore its velocity is decreased . consequently this wind moves towards the north , where the radius is less . the above image shows what happens . the wind moving east begins to expand its radius , thus moving outwards . gravity pulls it back , and the wind moves south , in order to maintain the larger radius required for its increased velocity .
moshe is answer gets the main point across . if you are interested in trying to learn something more detailed , here 's a thought about where you might start . ( i am not even close to expert on these matters , but this is something that i have found readable and worthwhile . ) there is an interesting thread of literature including http://arxiv.org/abs/hep-th/9309145 by susskind and http://arxiv.org/abs/hep-th/9612146 by horowitz and polchinski , along with other papers that those might lead you to . the idea is that free strings have a certain entropy , in the sense that for a given mass , there are many different combinations of oscillator modes you can turn on to find a string state of that mass . the string energy is proportional to its length , and a typical string of a given energy looks roughly like a random walk of a particular length . once you turn on a coupling , at some point some of these states will become black holes , because their energy is contained in a region smaller than their schwarzschild radius . for a given mass , a black hole state also has some entropy . there are various consistency checks you can do on the way these things scale , to see that it is sensible to talk about a sort of smooth transition between string states and black hole states if you fix the energy and vary the coupling . there are more technical and more precise papers in the literature on black hole entropy and microstate counting , but for a non-expert like me this particular thread of literature seems interesting because it relies on parametric scaling laws that are pretty readily comprehensible , and paints a relatively clear picture of the physics .
the area under the curve in the pv-diagram is the integral $$ \int p \ ; \mathrm dv = \int \frac fa a\ ; \mathrm ds=\int f \ ; \mathrm ds \equiv w $$ by definition of pressure as force per area and ( infinitesimal ) volume as area times distance . this is the mechanical work done by the system on the environment in case of expansion or by the environment on the system in case of compression , which differ by sign . it is called external to emphasize the interaction with the environment .
verified it with fea , it is correct . also , taking into account the " rocking " effect requires the piece-wise description : $$ j\ddot{\theta} + \bigg\lbrace\begin{matrix} -m g r \sin ( \alpha + \theta ) and \theta &lt ; 0 \\ m g r \sin ( \alpha - \theta ) and \theta \geq 0 \end{matrix} = m a r \cos ( \alpha - \theta ) $$
this is a heavy question , that contains many topics in it that are worthy of their own questions , so i am not going to give a complete answer . i am relying mainly on this excellent review paper by nayak , simon , stern , freedman and das sarma . the first part can be skipped by anyone already familiar with anyons . abelian and non-abelian anyons anyons are emergent quasiparticles in two dimensional systems that have exchange statistics which are neither fermionic nor bosonic . a system that contains anyonic quasiparticles has a ground state that is separated by a gap from the rest of the spectrum . we can move the quasiparticles around adiabatically , and as long as the energy we put in the system is lower than the gap we will not excite it and it will remain in the ground state . this is partly why we say the system is topologically protected by the gap . the simpler case is when the system contains abelian anyons , in which case the ground state is non-degenerate ( i.e. . one dimensional ) . when two quasiparticles are adiabatically exchanged we know the system cannot leave the ground state , so the only thing that can happen is that the ground state wavefunction is multiplied by a phase $e^{i \theta}$ . if these were just fermions or bosons than we would have $\theta=\pi$ or $\theta=0$ respectively , but for anyons $\theta$ can have other values . the more interesting case is non-abelian anyons where the ground state is degenerate ( so it is in fact a ground space ) . in this case the exchange of quasiparticles can have a more complicated effect on the ground space than just a phase , most generally such an exchange applies a unitary matrix $u$ on the ground space ( the name ' non-abelian ' comes from the fact that these matrices do not in general commute with each other ) . the quantum dimension so we know that the ground space of a system with non-abelian anyons is degenerate , but what can we say about its dimension ? we expect that the more quasiparticles we have in the system , the larger the dimension will be . indeed it turns out that for $m$ quasiparticles , the dimension of the ground space for large $m$ is roughly $\sim d_a^{m-2}$ where $d_a$ is a number that depends on $a$ - the type of the quasiparticles in the system . this scaling law is reminiscent of the scaling of the dimension of a tensor product of multiple hilbert spaces of dimension $d_a$ , and for this reason $d_a$ is called the quantum dimension of a quasiparticle of type $a$ . you can think of it as the asymptotic degeneracy per particle . for abelian anyons we have a one-dimensional ground space no matter how many quasiparticles are in the system , so for them $d_a=1$ . although we used the analogy to a tensor product of hilbert spaces , note that in that case the dimension of each hilbert space is an integer , while the quantum dimension is in general not an integer . this is an important property of non-abelian anyons that differentiates them from just a set of particles with local hilbert spaces - the ground space of non-abelian anyons is highly nonlocal . more details on anyons and the quantum dimension can be found in the review paper cited above . the quantum dimension can be generalized to other systems with topological properties , maintaining the same intuitive meaning of asymptotic degeneracy per particle . it is in general very hard to calculate the quantum dimension , and there is only a handful of papers that do ( most of them cited in the paper by kitaev and preskill that inspired this question ) . relation to entanglement i can also try and give a handwaving argument for why the quantum dimension would be related to entanglement . first of all , the fact that the entanglement entropy of a bounded region depends only on the length of the boundary $l$ and not on the area of the region is very clearly explained in this paper by srednicki , which is also cited by kitaev and preskill . basically it says that the entanglement entropy can be calculated by tracing out the bounded region , or by tracing out everything outside the bounded region , and the two approaches will yield the same result . this means the entanglement has to depend only on features that both regions have in common , and this rules out the area of the regions and leaves only the boundary between them . now for a system with no topological order the entanglement would go to zero when the size of bounded region goes to zero . however for a topological system there is intrinsic entanglement in the ground space which yields the constant term $-\gamma$ in the entanglement . the maximal entanglement entropy a system with dimension $d$ has with its environment is $\log d$ , so in an analogous manner the topological entanglement is $\gamma=\log d$ where $d$ is the quantum dimension . again this last argument relies heavily on handwaving so if anyone can improve it please do . i hope this answers at least the main concerns in the question , and i welcome any criticism .
a few things : 1 ) just because an observer crossing the event horizon does not necessarily feel ill effects at the time of crossing the horizon , it does not mean that they will not inevitably end up at the singularity , where there will be plenty of ill effects--all timelike curves that cross the horizon end up at the singularity in a finite amount of proper time . for a particle falling into a non-spinning black hole , it is actually the same amount of proper time that it would take to fall into a newtonian point mass . 2 ) you have to be very careful about what you mean by ' horizon ' in the case of a black hole that eventually evaporates . there are several definitions of ' horizon ' , and depending on how you resolve the singularity , and upon how the hole evaporates these different definitions can differ in meaning--the most common difference is the apparent horizon-a ' point at which , for this given time , you can not go back ' , and the event horizon--'the point at which , you must end up at the singularity ' . it might be possible that your evaporating black hole spacetime may have an apparent horizon but no event horizon , for instance . in that case , the whole paradox goes away . 3 ) a careful answer of this requires the careful drawing of a penrose-carter diagram of the relevant spacetime . if you managed to tweak it somehow so that you fell in , blasted your rockets for long enough to outlive the recontraction of the horizon , the short answer is that you would not receive all of the information about all of the future , just that determined by the " null past " of the horizon--you would find out about all of the lightlike and timelike rays that fell into the horizon , but not those that would head toward the spot where the horizon used to be at times later than when the horizon was there .
gotcha covered : $$f_1=g{m_e m\over ( 1.5r_e ) ^2}\mathbf {-t}$$ -t from upwards force of rope . $$f_2=g{m_em\over ( 2.5r_e ) ^2}\mathbf {+t}$$ +t from downwards force of rope . then since the rope is not stretching , $$f_1~=~f_2$$ $$2t~=~g{m_e m\over ( 1.5r_e ) ^2}-g{m_em\over ( 2.5r_e ) ^2}~=~{64\over225}mg$$ $$\therefore~t~=~{32\over225}mg$$
plenty ! the cuo$_2$ planes of cuprate superconductors are perhaps the most famous example , but in fact a lot of layered oxides are going to coordinate in this fashion . the lieb lattice is essentially a two-dimensional counterpart of the " perovskite " structure , which is ubiquitous in nature .
it is not very clear to me if you are asking about energy or momentum . you should also ask about a specific interaction process as there are many , this is required especially to answer your last , quantitative , question . however , generally speaking , a $\gamma$ photon cannot give some of its energy to anything else : it is all or nothing . even in the compton scattering , in which you get a less energetic photon , the initial photon is destroyed . the momentum must be conserved as well , so yes : when a photon hits another particle this is accelerated , you can even generate some measurable pressure with a very intense radiation !
in a vacuum , the dc resistance to the flow of current is infinite . there is nothing contained within the vacuum that can conduct electricity .
when physicists say that a quantum field $\phi ( x ) $ is real-valued , they are usually referring to feynman 's path integral formulation of quantum field theory , which is equivalent to schwinger 's operator formulation . the values of a field $\phi ( x ) $ in the path integral formulations are numbers . e.g. : if the numbers are real , we say that the field $\phi ( x ) $ is real-valued . ( such a field $\phi ( x ) $ typically corresponds to a hermitian field operator $\hat{\phi} ( x ) $ in the operator formalism . ) if the numbers are complex , we say that the field $\phi ( x ) $ is complex-valued . if the numbers are grassmann-odd , we say that the field $\phi ( x ) $ is grassmann-odd . ( the numbers in this case are so-called supernumbers . see also this phys . se post . )
update - answer edited to be consistent with the latest version of the question . the different definitions you mentioned are not definitions . in fact , what you are describing are different representations of the lorentz algebra . representation theory plays a very important role in physics . as far as the lie algebra are concerned , the generators $l_{\mu\nu}$ are simply some operators with some defined commutation properties . the choices $l_{\mu\nu} = j_{\mu\nu} , s_{\mu\nu}$ and $m_{\mu\nu}$ are different realizations or representations of the same algebra . here , i am defining \begin{align} \left ( j_{\mu\nu} \right ) _{ab} and = - i \left ( \eta_{\mu a} \eta_{\nu b} - \eta_{\mu b} \eta_{\nu a} \right ) \\ \left ( s_{\mu\nu}\right ) _{ab} and = \frac{i}{4} [ \gamma_\mu , \gamma_\nu ] _{ab} \\ m_{\mu\nu} and = i \left ( x_\mu \partial_\nu + x_\nu \partial_\mu \right ) \end{align} another possible representation is the trivial one , where $l_{\mu\nu}=0$ . why is it important to have these different representations ? in physics , one has several different fields ( denoting particles ) . we know that these fields must transform in some way under the lorentz group ( among other things ) . the question then is , how do fields transform under the lorentz group ? the answer is simple . we pick different representations of the lorentz algebra , and then define the fields to transform under that representation ! for example objects transforming under the trivial representation are called scalars . objects transforming under $s_{\mu\nu}$ are called spinors . objects transforming under $j_{\mu\nu}$ are called vectors . one can come up with other representations as well , but these ones are the most common . what about $m_{\mu\nu}$ you ask ? the objects i described above are actually how non-fields transform ( for lack of a better term . i am simply referring to objects with no space-time dependence ) . on the other hand , in physics , we care about fields . in order to describe these guys , one needs to define not only the transformation of their components but also the space time dependences . this is done by including the $m_{\mu\nu}$ representation to all the definitions described above . we then have fields transforming under the trivial representation $l_{\mu\nu}= 0 + m_{\mu\nu}$ are called scalar fields . fields transforming under $s_{\mu\nu} + m_{\mu\nu} $ are called spinor fields . fields transforming under $j_{\mu\nu} + m_{\mu\nu}$ are called vector fields . mathematically , nothing makes these representations any more fundamental than the others . however , most of the particles in nature can be grouped into scalars ( higgs , pion ) , spinors ( quarks , leptons ) and vectors ( photon , w-boson , z-boson ) . thus , the above representations are often all that one talks about . as far as i know , clifford algebras are used only in constructing spinor representations of the lorentz algebra . there maybe some obscure context in some other part of physics where this pops up , but i have not seen it . of course , i am no expert in all of physics , so do not take my word for it . others might have a different perspective of this . finally , just to be explicit about how fields transform ( as requested ) i mention it here . a general field $\phi_a ( x ) $ transforms under a lorentz transformation as $$ \phi_a ( x ) \to \sum_b \left [ \exp \left ( \frac{i}{2} \omega^{\mu\nu} l_{\mu\nu} \right ) \right ] _{ab} \phi_b ( x ) $$ where $l_{\mu\nu}$ is the representation corresponding to the type of field $\phi_a ( x ) $ and $\omega^{\mu\nu}$ is the parameter of the lorentz transformation . for example , if $\phi_a ( x ) $ is a spinor , then $$ \phi_a ( x ) \to \sum_b \left [ \exp \left ( \frac{i}{2} \omega^{\mu\nu} \left ( s_{\mu\nu} + m_{\mu\nu} \right ) \right ) \right ] _{ab} \phi_b ( x ) $$
the lhc was envisioned as a " discovery " machine , a multipurpose one . the higgs gets the press but the expectations is that new physics will become accessible with the higher energy available for center of mass collisions . the z was discovered in the sps the proton antiproton previous generation collider . the previous machine in the same tunnel as the lhc , lep , an electron positron collider was needed to establish with great accuracy the parameters of z itself and the standard model . in general leptonic collisions probe elementary interactions with many less assumptions than proton proton or proton antiproton machines . this is because one is throwing balls of three quarks with their gluons at each other and measures the debris , in order to study the interactions . new physics , because of the high energy , will appear , but will be in a complexity unprecedented up to now . hopefully the next generation will be a lepton machine that will allow to establish the appropriate models unequivocally . now on the question of one off detectors : cern is practically using all the accelerators built up to now as increasing energy stages to feed the end machine , the lhc . nothing is wasted . in addition a lot of experiments are approved and running in beam lines that are not in the mainstream but may prove valuable or have unexpected theoretical repercussions . thus one expects that the lhc will open the window to the new physics that is tantalizing us , strings and unification of all forces at the moment , and the next generation machines will be leptonic ones to allow accurate measurements of parameters and decide between models .
there are a lot of questions here , and i am more sure about the answers to some of them than i am about others . but nevertheless , i will give it a go . the first and most important question is what would the temperatures be like . this is the one i am least sure about . the reason i am unsure is that it is highly dependent on a lot of things that would probably be quite different on a tidally locked earth . these include the greenhouse effect ( which is highly dependent on water vapour ) , albedo ( which depends on how many clouds there are , as well as on ice ) , and most importantly on the wind speeds . if there was no heat transport from the hot side to the cold side then we could expect the temperatures to be similar to the temperature range on the moon - around -150&nbsp ; &deg ; c on the cold side , and more than 100&nbsp ; &deg ; c on the hot side . however , if the planet has an atmosphere then it will transport heat from the hot side to the cold side , and this might make the temperature difference much more moderate . this could only work if the rate of heat transport ( and hence the wind speeds ) were quite substantial , because if it gets too cold on the cold side then the atmosphere will start to freeze . this will leave less air to transport heat , leading to more freezing , and so on in a feedback loop that would result in an airless world . this is probably the most likely outcome of a tidally locked earth-like planet - but it is not so interesting from the point of view of fiction , so i will focus on how an inhabitable world might be plausible . a high enough heat transfer rate might be possible if the hot side were very hot , perhaps because of a large greenhouse effect in addition to permanently facing the sun . i am not sure whether this would mean the hot side would have to be so hot that the ocean would boil , but i suspect so . in this case we would expect to find most of the water in the form of ice on the cold side - but perhaps some of it could be found in a liquid state near the boundary . one would hope there could be some kind of water cycle - i am imagining something like glaciers being continually melted by the warm air blowing in from the hot side , with the meltwater flowing in huge rivers ( or even flowing seas ) to the hot side , where it evaporates and cycles back around to fall as snow on the cold side . next we need to consider what the air flow patterns would be like . the situation on the present-day earth is complicated , but , oversimplifying a bit , the basic principle is that air heats up at the equator , then rises , then travels towards the poles at high altitudes , then cools ( due to emitting thermal radiation into space ) , then sinks again and travels back toward the equator . so the prevailing winds at ground level tend to blow towards the equator . on a tidally locked earth i would expect a more extreme version of this phenomenon . the flow patters in the middle of the hot side would probably be crazily chaotic - there would be a lot of energy there to drive storms , especially if there was a water cycle providing moisture - but in the region near the boundary i would expect a fairly stable convection cell , with hot air blowing to the cold side at high altitude , sinking , and blowing back to the hot side at low altitude . so a person standing on the surface near the boundary would experience very strong prevailing winds towards the sunlit side . exactly how strong these winds would be , i can not say , and it is also very hard to say how much cloud cover or rainfall there would be . however , in general a higher temperature difference means more energy goes into weather systems , and on this world the temperature difference would be bigger than on earth , so we had generally expect the atmosphere 's dynamics to be stronger and more violent . something to note is that heat can be transferred as latent heat rather than simply hotter and cooler air . water absorbs heat when it evaporates and releases it when it condenses . so if there is a strong water cycle , with lots of cool liquid water flowing towards the hot side and lots of warm water vapour being transported by the atmosphere toward the cold side , then you can have a higher heat transport without needing such strong winds . in this scenario , someone on the surface would experience a prevailing wind towards the sunlit side , but looking up at the sky they would see clouds moving in the opposite direction . an extra complication on earth is the coriolis force , which causes the prevailing surface winds to blow around the earth instead of just from pole to equator , and also allows hurricanes to form . it also causes the jet streams . but a tidally locked earth would hardly be rotating , so there would not be a coriolis force and these phenomena would not happen - the prevailing winds near the boundary would blow directly towards the sun , and rotating hurricane-type storm systems would be extremely rare if they existed at all . the next question is about whether there would be a distinct northern and southern ocean . i do not think this would be the case . this idea comes from the fact that the earth 's rotation causes the oceans to bulge out around the equator - but it also causes all the rock to bulge out , too , which is why we do not have just a big ocean around the equator . i guess you can imagine that if the earth somehow stopped spinning then the oceans would flow towards the poles , but the rock would move much more slowly , so that there would be a period of time where there would be oceans near the poles but land around the equator . but over longer geological time scales the rocky part of the earth would change its shape as well , and you had end up with roughly evenly distributed oceans again . the same goes for the question about the sun causing a permanent tidal bulge in the oceans . it would , but it would also bulge the shape of the planet , so you would not necessarily have all the ocean on the sides facing toward and away from the sun . i think the fact that the water would tend to freeze on the cold side would have a much bigger effect on water distribution than anything else . of course , the moon would still cause tides , if there was a moon . next is the magnetic field - i do not feel qualified to answer this one . i have always wanted to understand how earth 's magnetic field is generated , but i have never found a good explanation , so i do not know how plausible it is to imagine it working without the earth 's rotation . the importance of the magnetic field is that prevents the atmosphere from being slowly blown away by the solar wind , so it would be needed for this planet 's atmosphere to persist in the long term . i am also not sure how to answer the question about the moon . it seems hard to imagine how the moon could be in orbit yet the earth still be tidally locked to the sun , but i am no expert . another potential big difference is plate tectonics . if the oceans are frozen on the cold side then this will put a lot of weight on the plates there , which will probably change the dynamics of the whole system - but unfortunately i am not currently able to imagine what the result would be . plate tectonics are quite important for recycling elements over very long time scales . the water cycle on this planet would tend to transport nutrients to the hot side and deposit them there , so you had need geological activity to recycle them in the long term . one more really important thing we have to think about is photosynthesis . it will be very difficult for plants to persist on this planet , because the hot side will be very very hot , and the cold side has no light . so probably there would be much less life on this planet than there is on earth , with most of it existing towards the edges of the hot side . ( the prevailing winds and water currents here will be from the cold side , so these could keep things relatively cool even though the sun is always in the sky . ) if the cloud cover is 100% then this will also make photosynthesis harder due to reducing the available light . this is important because it might mean there would not be enough oxygen to support human life , so i guess in designing this fictional world there is a tradeoff between having a strong water cycle to transport lots of heat , versus having it weak enough that enough light reaches parts of the surface . you mention the possibility of life on the dark side . this is not impossible of course , but you had have to consider the question of what it would eat . most of the life on the sea floor , for example , eats goo composed of dead stuff that drips down from the surface - so its power source is ultimately sunlight , which is converted into chemical free energy by photosynthetic plankton . so while there could be some life on the dark side ( feeding off geothermal energy from hydrothermal vents under the ice , for example ) there could not be very much of an ecosystem there unless there was a constant source of chemical energy being transported somehow from the light side . but it is hard to imagine how this could happen without making the composition of the atmosphere very different from earth 's . i hope this provides some food for thought . it is of course all very speculative , and there may well be important things that i have not thought of , or where i have got something wrong due to lack of expertise - so caveat emptor .
quadrupole magnet quadrupole magnets are mostly used for beam focusing . http://en.wikipedia.org/wiki/quadrupole_magnet
lubos hit the key point : you need to calculate the polarization of an atom/molecule so that is the starting point . as you may already know , when a dielectric is subjected to the impinging e-field of an em wave , there are dipoles generated that contribute to the total internal field . the resultant field for most materials is given by $ ( \epsilon−\epsilon_0 ) e=p$ . however , classically , the polarization will depend on the relative displacement between the electron cloud and the nucleus and this displacement can be calculated by thinking of the electron as a harmonic oscillator . that is , the electron cloud will oscillate about the nucleus . there are three terms that must come into play to describe the displacement of the electron . the electron cloud bound to the nucleus must have some sort of restoring force : $−mω_0^2x$ where $ω_0$ is the resonate frequency and m is the mass of the electron . the impinging em wave will exert a time varying force , say $cosωt$ , on the electrons : $ee ( t ) = eecosωt$ where $ω$ is the driving frequency . for a gas , atoms are far enough apart that we can “ignore” interactions between them . however , for atoms and molecules in close proximity , one cannot ignore these interactions which behave as “frictional” type forces . that is , the electron oscillators will dissipation some of their energy as heat . therefore , there must be some type of velocity term : $mβ\frac{dx}{dt}$ where $β$ is a damping constant . if we now stuff all of this into newton’s second law , we have an equation for the electron displacement : $$m\frac{d^2 x}{dt^2} = −mω_0^2x - mβ\frac{dx}{dt} + ee_0cosωt$$ physically , we expect that the electron will oscillate at the same frequency as the impinging em wave , so that the equation above has the solution $x ( t ) = acosωt$ . substituting this assumed solution and solving for the amplitude , we get $$x ( t ) = \frac{ee ( t ) }{m ( ω_0^2-ω^2+iβ ω ) }$$ the electric polarization is the density of dipole moments : $p ( t ) = ex ( t ) n $ where $n =$ number of dipoles . solving for the electric permittivity using $ ( ϵ−ϵ_0 ) e=p$ , $$ϵ=ϵ_0 + \frac{p ( t ) }{e ( t ) } = ϵ_0 + \frac{ne^2}{m ( ω_0^2-ω^2+iβω ) }$$ the way the electric permittivity is related to the index of refraction is as follows : most materials are nonmagnetic at optical frequencies ( the relative permeability is very close to one ) . so to a good approximation , the index of refraction depends only on the relative permittivity $ϵ_r : n^2 ( ω ) = ϵ_r = \frac{ϵ}{ϵ_0}$ . therefore , the dispersion relationship from a damped harmonic oscillator view point looks like $$n^2 ( ω ) = ϵ_r = \frac{ϵ}{ϵ_0} = 1 + \frac{ne^2}{m ϵ_0} \frac{1}{ω_0^2-ω^2+iβω}$$ you can see that the index of refraction is frequency dependent . note that this is valid for only a single resonate frequency ; a given substance has several such resonate frequencies and this equation will need to be modified but the quantum mechanical solution looks very similar to the one above . i will not explain this since i think that i have answered your question .
i would like to add a little to lubos 's answer : first a historical note : this is what einstein proposed as a way of understanding quantum mechanics in 1919 or thereabouts , in the paper " do gravitational fields play a role in the composition of the elementary particles ? " einstein was of the opinion that a complicated enough classical theory , like general relativity , would lead continuous waves to collapse into standard-size soliton-like particles and these particles he felt might then bang around along the wave in such a way to reproduce quantum mechanics . this idea reappears several times in the literature , but it demonstrably does not work . a field theory , like gr , is a classical theory , and it therefore is local hidden variables ( the variables are not even hidden in this case ) . this is ruled out by bell 's theorem--- the correlations in quantum mechanics do not allow local fields to carry the data that determines the experimental outcome , not without conspiracy ( superdeterminism ) or nonlocal equations ( faster than light changes in the variables ) . neither works in a straightforward field theory like gr . secondly , gr is not as badly understood as all that , although it is not as well understood as one would like , mostly because numerical methods are in their infancy , and one 's intuition must come laboriously from analyzing exact solutions when these are available . the example i gave of particles oscillating into and out of an extremal black hole is not really new ( do not give me too much credit ) , the new thing there is the holographic interpretation , namely that the coming-out is an ordinary coming out event in this universe . the oscillations of particles into and out of a near-extremal black hole were appreciated in the 1960s , but each oscillation takes you to a disconnected branch classically , because crossing a horizon takes an infinite amount of t-time . this is not possible quantum mechanically , since this disconnected maximally extended thing is not compatible with unitarity . the nice thing about the in-out solution for geodesics in the extremal reissner nordstrom is that if you replace the test particle with a little charged black hole , you can make nonrelativistic oscillations if both black holes are near extremal . the external field of the two black holes does not have a full merger , the little black hole , now not considered as a test particle , but as a solution to gr proper , just smears out on the horizon , then bounces back . i did not calculate this in detail yet , but it can be solved completely with an analysis along the lines of atiyah and hitchin in their famous paper on slow soliton scattering ( the atiyah hitchin space ) , except here , unlike the other case , i am not optimistic there will be a simple geometrical solution , rather one has to bite the bullet and trace the bouncing behavior in the solution either by numerical integration or solving for the near-static phase-space geometry of the two extremal black holes . causalities and ctc 's the basic idea you are giving is that perhaps hidden variables plus closed time-like curves can reproduce bell inequality violations . i will give some sentences about why this is extremely unlikely . quantum mechanics has entangled wavefunctions . what this means is that the wavefunction for k particles is in 3k dimensional space , not in 3 dimensional space . the growth in dimensions means that quantum mechanics packs a stronger computational punch than classical mechanics , and you can not simulate quantum mechanics of k-particles with less than exponentially much classical information . this is why quantum computation works in pure quantum mechanics . so the structure of quantum mechanics is exponentially big and has the entanglements that violate bell 's inequality . if you wish to reproduce this from something like gr , you need gross nonlocality and some way of reproducing nonlocality . so if you have a pair of electrons that bind to an atom ( so that their spins anti-align ) , and then you knock out the nucleus , and do bell measurements on the two outgoing electrons , you need to reproduce the nonlocal correlations from ctc 's in gr . this means that the electron needs to have ctc 's " inside " which go back in time and magically alter the attributes of the other electron . this only became required once you put them together in an atom , and let the photons radiate , and during this process the two point electrons did not necessarily come close to each other ( assuming they are classical and described in space ) . how do ctc 's help correlate them ? to make this work , you would have to go all the way back in time to where the two electrons were created from the inflaton field , and correlate them back then . this type of back-and-forth in time description is utterly conspiratorial , and very unconvincing . there is also no shred of a hint that this will reproduce anything like qm , it is just not ruled out , because you are postulating little tiny internal back-in-time paths on all electrons , something we have no evidence for . there are no real ctc 's in physical exterior solutions of gr . the ctc 's in the intepretation i gave of oscillations into and out of extremal black holes are unphysical--- they are only closed in time because of the wrongness of the classical picture of the horizon . the ctc 's in the interior of a kerr solution can only occur when you wind around the ring singularity , and then it should be possible to unwrap the interior so that it has a pure-causal description , simply by including the winding number of your path around the ring . i do not know the interior kerr well enough to see how to do this , and this must work in any number of dimensions , not just 4 , so i hesitate to say it is what happens , but there must be a reconciliation of causality and kerr interior , because you can set up fields at the horizon of kerr , and let them traverse the interior , and the evolution equation should not have additional constraints , as come from ctcs . all in all , the form of the two theories , gr and qm , is completely different , the descriptions are of a different computational complexity , and the causality notion is totally different in the two schemes , so it is implausible in the highest degree that gr can explain qm . what is more , today we have a good quantum version of gr , string theory , which subsumes and extends the classical theory , so that it is a mistake to pretend that this progress does not exist , and to work as if we were living in 1926 . within string theory , you give a full accounting of all gr effects on flat and ads backgrounds in principle , from an ordinary unitary quantum theory . this quantum gr means that we know how gr and qm are reconciled ( in perturbations to flat and ads backgrounds ) , and the classical limit where gr is reproduced is just not quantum , it is an ordinary classical field theory .
after a lot more searching , i have found the answer to my question ! :d below is a summary of the information i found . there is no specific webpage i can link to because i relied on sources who quoted other sources which no longer exist , but maybe this information can be useful to someone else someday . most of what i learned comes from professor lou bloomfield who currently teaches physics at the university of virginia . edit : none of this is quoted material : all information posted below has been completely reworded , and the analogies ( aside from the guitar string ) are mine . when surrounded by normal matter , a light wave 's electric field will cause electrons to jiggle at a rate equal to the frequency of the light wave : the electric component of the light wave will alternately attract and repel charged particles . when electrons in a material transparent to a certain frequency are excited by a light wave of that frequency , this takes energy away from the light wave . but surprisingly , no photons are absorbed : since the material is transparent to the frequency of the wave , there is no higher orbital which matches exactly the energy level an individual photon would impart to an electron . this means the energy transfer can not involve a real particle interaction . so what happens ? instead of absorbing one or more photons , the electrons enter a virtual quantum state : a temporary excitation that does not exactly match one of the states that the electron can occupy . this is very much like vibrating a guitar string by aiming sound at the string . if the sound you aim at the string matches a frequency that the string can vibrate at , it will cause the string to vibrate . if the sound you use is the wrong frequency , the string will wiggle a little bit as though trying to vibrate , then stop when the sound passes . that is what happens to the electrons : they borrow energy from the light wave , wiggle a little , and then return the energy . a virtual quantum state is very limited in duration , and does not count as a particle interaction . the light wave and the electron remain unentangled and continue to act as probability waves . the electron can only play with the light wave 's energy for a brief period before returning it . the characteristics of the light wave remain unchanged because there was no real particle interaction . so the light does not ricochet off of atoms , nor does it get emitted in the usual sense by the electrons which play with it . even though the interactions are all virtual , electrons are matter and they take time to jiggle . as this happens over and over and over again , it slows the progress of the wave . you might think of this like a kind of friction which acts against the progress of the wave . consider a car whose wheels turn at a constant speed , and imagine it encounters a series of large bumps that slow it down slightly . the speedometer is based on the wheel rotation , so it would say the car has not changed speed at all : it is just as fast as it was on flat terrain . the car will , however , cover less ground per time interval because some of the wheel-turning is used to surmount the humps . these humps are akin to the process of electrons temporarily borrowing energy from the light wave . so is the light wave truly slowed , or is the light still moving at c and only its progress is slowed ? this is not actually a well-formed question , and for all practical purposes the answer does not matter . however , i find it easier to think about it as slowing the wave 's progress . this means the characteristic that " light moves at speed c in all reference frames " still holds true , which makes it much easier for me to reason about relativistic effects . additionally , i was incorrect about different frequencies slowing by the same amount : lower frequencies are slowed less than higher frequencies . when the frequency is lower , even though the wave has less energy , the electrons will need to jiggle over a wider area ( they are pulled for a longer period , then pushed for a longer period ) . since the electrons remain bound to their atoms in this interaction , they can not be pulled out of the atom by a virtual excitation . so the slower the frequency is , the " more virtual " the excitation must be , and the less time the electrons have to play with the light . is this information useful ? if so , is there a way i could make it more accessible ? just curious , as i am very new to se .
have you read about the construction and operation of bubble chambers ? they are large vessels ( with at least one transparent side ) and work by rapidly reducing the pressure in the contained fluid so that it is superheated for a time . doing that requires plumbing ; both for filling the chamber with working fluid , and for arranging the pressure drop . as the working fluid is often cryogenic it was also probably kept in circulation so that it could be periodically re-cooled , calling for more plumbing . as the working fluid is chosen to be transparent except where bubbles nucleate , you can see through to whatever occupies the far side chamber . ( the figure is a negative image so the light background of the image represents a dark background space . ) i would not be surprised if those neat , uniform circles all in a line represent the plumbing in the vessel , and the rest of the structure in the background the panels and welds of the chamber .
let me try to answer . for your first question the statement is that you can work with either ${\mathbb p}^2$ or ${\mathbb p}^1\times {\mathbb p}^1$ - the moduli space is the same . more generally , if $s$ is any surface which contains ${\mathbb a}^2$ as an open subset and $d_{\infty}$ is the divisor at $\infty$ then $bun_g ( s , d_{\infty} ) $ is independent of $s$ . for the second question : it is true that ${\mathfrak q}={\mathcal m}_{g , p}$ ( for $p$ being the borel subgroup and $g=sl ( n ) $ ) but it is not true that $q={\mathcal qm}_{g , p}$ . the point is that the quasi-maps ' space ${\mathcal qm}_{g , p}$ is defined for any $g$ and it is singular ; for $g=sl ( n ) $ ( and only in that case ) it has a nice resolution of singularities which is given by the laumon space . if you are interested to know more , you can read my 2006 icm talk ( "spaces of quasi-maps into the flag varieties and their applications" ) - the above questions are discussed there .
you can indeed test two quantum states for being equal , but the results are not 100% guaranteed accurate : you measure the eigenvalue of the swap operator ( which swaps the two quantum states ) . if they are equal , then you have a 100% chance of getting the +1 eigenvalue . if they are orthogonal , then you have a 50% chance of getting either the +1 and -1 eigenvalue . this test ( a ) destroys the quantum state if you test it against a state that it is not equal to and ( b ) only yields the correct answer half the time if the answer is " no " . these two drawbacks mean that you cannot use it to clone . however , this is still a very useful test as a subroutine in designing some quantum algorithms . i do not know whether anybody has proved a theorem saying that you cannot test equality better than the swap test , but it is definitely true , as the op speculates , that there is no perfect test for equality of quantum states .
i will assume in this answer that " drag " means tension . you are asked to find the tension in the chain as it is rotating . this is independent of the link size , so long as the links are not a significant fraction of the circumference . if you have a hoop of mass density per unit length $\rho$ and circumference c ( so that $\rho c = m$ where m is the total mass ) , rotating with rotational velocity $\omega$ , the centripetal force on a segment of length l is the mass times the rotational velocity squared times the radius , or $$ f_c = \rho l w^2 {c\over 2\pi} $$ if the chain is at tension t , the two endpoints of the segment pull in with a total force of $$ {tl\over c} $$ setting the two forces equal , the l drops out ( as it must ) and gives the tension : $$ t = ( \rho c ) \omega^2 {c\over 2\pi} = m \omega^2 {c\over 2\pi} $$ or $\omega= 30 {1\over s}$ , $m= . 4 \mathrm{kg}$ , $c = 1.2 m$ , this is about 68n .
you can gain some intuition from looking at the density distribution function in momentum space which for the $|bcs\rangle$ is given by $n_k=v^{2}_k$ . in the bcs limit one finds approximately the filled fermi sphere , while in the bec limit $n_k\sim 1/ ( 1+ [ ka ] ^2 ) ^2$ which is proportional to the square of the fourier transform of the dimer wave function . for this reason in the bec limit the state $|bcs\rangle$ describes a condensate of dimers . you can find a little bit more about this question in http://arxiv.org/abs/0706.3360
once the battery is disconnected , the charge on the capacitor plates is stuck where it is and has no path to go anywhere else . since the charge remains on the plates , there is an electric field between the plates . and because there is a electric field between the plates there must be a voltage difference between them . we know the voltage was equal to the battery voltage when the battery was connected . and since it does not change when the battery is disconnected , it must still be equal to the battery voltage afterwards .
your question comes down to whether the em absorption is a resonant process or not , where resonant means it corresponds to the energy of some excitation of the water molecule . the answer is that it is not a resonant process . microwave ovens operate at 2.45ghz but the lowest energy transitions of water molecules are rotational transitions , which have energies in the 100ghz to 1thz range . the energy of the photons in a microwave oven are too low for any resonant absorption . google for details of the rotational spectrum of water . i found examples here and here . the em radiation from the oven makes dipolar molecules within it line up with the electric field . as the field oscillates the water molecules change direction ( at 2.45ghz ) . in liquid water the molecules interact strongly and exchange energy with each other , so the energy of the flipping motion gets transferred to translational energy of the water molecules i.e. heat . because this is not a resonant process changing the microwave energy by small amounts ( up to an order of magnitude ) will not make a lot of difference to the heating . as some of the comments have mentioned , this process is called dielectric heating .
i would recommend that you stick to one frame , otherwise you will have to make coordinate transformations to make your equations consistent and that is significantly more work than it is worth . also , be careful that if the inclined plane accelerates ( which it will if i understand your setup correctly ) then you have to introduce so-called fictitious forces to solve the problem correctly in the incline 's ( non-intertial ) frame , and that is also , in this case , probably more trouble than it is worth in my opinion .
you should visit the astronomical league site . they have several observational programs , ranging from naked eye to detailed telescopic spotting programs . i enjoyed the double star series myself . http://www.astroleague.org/
start from the beginning . why constraint relations ? why are they there ? let me emphasize : let 's take origin at top pulley which is at rest . note that length of top rope is constant : $a+b=k\implies a''+b''=0 \implies a''=-b''$ also length of second rope is constant : $ ( c-b ) + ( d-b ) =k\implies c''+d''=2b''$ note that $d$ is a constant as the top pulley and ground is rest : $c''=2b''$ hence , $c''=-2a''$ as stated in comments . also , everything we have done is futile and the block $m_2$ will hit the ground very quickly .
running the rges in reverse should be valid so long as you do not integrate over a scale where degrees of freedom enter/leave the theory . if you integrated out the electrons in qed , you had have irrevocably lost that information in your low energy description of interacting photons . you had see some non-renormalizable theory with interacting corrections to pure em but rg evolving to the uv would not tell you what that would be . just like rg evolving qed to the uv keeps you unaware of the strong or the weak sector physics . on the other hand , so long as you have not crossed any characteristic scale in your theory , the theory at the scales you have integrated out should be the same as the theory at the scale you are currently at . so you should be able to go back to where you came from . to summarize , so long as you do not integrate out some characteristic scale , you can keep going back and forth .
i have just found this interesting paper by thomas breuer from 1995: https://homepages.fhv.at/tb/cms/?download=tbphilsc.pdf the paper seems to prove that for a system that includes observer himself there are quantum states which are indistinguishable by the observer however technical means he employs , while he can measure any such states in the brains of other people . the paper claims this proves that not only quantum mecanics is not universal theory that is applicable to all objects in the universe , but that no such universal theory can exist . it also follows that in the world that the observer himself observes there is hidden information in his brain which cannot be extracted and read by any means even with help of other people ( while the same information can be easily extracted from the brains of other people ) . i do not know however how to interpret it regarding the wavefunction . does it mean the observer 's wavefunction indeterminate , singular or inexistent ? update . and this this paper says it all . there will be subjective decoherence once the observer wants to measure himself . so he will see himself in a mixed state while others in the same situation will be observed as if they were in coherent state . note that this position is often taken for quantum mechanics . according to many interpretations , as for example the one of bohr , or the one of london and bauer ( 1939 ) and wigner ( 1961 , 1963 ) , or even perhaps19 the one of von neumann ( 1932 ) , the “true” observer ( or his mind ) cannot be described by quantum mechanics . these authors say that if quantum mechanics is universally valid at all , then it is so only in the relative sense that every observer can perhaps apply it to any selected part of the world , except himself . it supposedly applies to schroedinger’s cat , wigner’s friend and wigner himself under the condition that they lose their status of observer and are observed by something or somebody else .
the string that the eggs hang from is allowed to move in the same direction as the eggs when swung perpendicular . this is not the case when you swing in parallel . if you hold the top string still and swing the egg perpendicular you will see that almost none of that energy transfers to the other egg and the egg you pushed will continue to swing . basically the main string only has one degree of freedom and moving parallel is not the direction that the string moves .
i am not entirely sure what you are trying to ask , but i think it is this : when is the schrodinger equation ( or a similar differential equation ) separable ? what conditions must the potential function satisfy ? the short answer is that the schrodinger equation is separable when the potential is independent of time ( though there maybe time independent potentials that also work ) . a differential equation of two independent variables is separable if the equation can be algebraically manipulated such that only one type of variable appears on each side of the equation . in the case of a partial differential equation ( i.e. . the schrodinger equation ) dependent variable can be written as a product of functions of the two independent variables ; that is $$ \psi ( x , t ) = \rho ( x ) \phi ( t ) $$ if we apply the schrodinger equation to this " guess " and assume $v$ is independent of time we find ( after a few steps ) : $$ -\frac{\hbar^2}{2m} \frac{d^2\rho}{dx^2} = ( e-v ) \rho $$ for and $e=$ constant . note that this equation is an ordinary differential equation though we started with a partial differential equation . more importantly , since $\rho ( x ) $ is independent of time , and therefore so is this entire equation . hence , it is called the time independent schrodinger equation . this is essentially a shortened version of the derivation provided in griffith 's book .
you can define quantum mechanics on a cantor set , but in order for it to be nontrivial , it needs to be a levy quantum mechanics , not a gaussian quantum mechanics , in that it will be the quantum analog of a levy process , not a brownian motion , as the ordinary schrodinger equation is . to define schrodinger quantum mechanics , you take the continuum limit of a nearest neighbor amplitude random walk . to do this , i will first remind you of the standard imaginary time map between random walks and quantum mechanical systems . when you have a stochastic process on a discrete space in discrete time , you have a transition operator : $$ \rho_j ( t+1 ) = \sum_i \rho_i ( t ) k_{i\rightarrow j} $$ where $k_{i\rightarrow j} = k_{ij} $ is a stochastic matrix : $$ \sum_j k_{ij} = 1 $$ these stochastic matrices generically have a stationary distribution , which i will call $\rho^0$ . i will assume that this stationary distribution obeys detailed balance , or in mathemtical jargon , that it is the " reversing measure " for k : $$ \rho^0_i k_{ij} = \rho^0_j k_{ji}$$ this says that the transitions between states i and j balance in equilibrium separately from any other transitions . the stationary distribution for random walk on a graph obeys detailed balance and it is ${1\over d ( i ) }$ where d is the degree of the vertex . when you take the continuous time limit , you make k equal to the identity plus an infinitesimal transition rate , and the stochastic equation becomes : $$ {d\over dt} \rho_j = \sum_i \rho_i r_{ij} $$ and you still have a stationary distribution $\rho0$ for the continuous time case . now you can define a symmetric h from the continuous time random process : $$ h_{ij} = {1\over \sqrt{\rho^0_i}} r_{ij} \sqrt{\rho^0_j} $$ and the detailed balance condition gives you symmetry of h . you then can define the imaginary time continuation as a standard quantum mechanical unitary time evolution , generated by this hamiltonian . this is the most abstract form of wick continuation . if you do this process on a random walk whose limit is a brownian motion , you get ordinary schrodinger quantum mechanics . if you do the same process on a random walk which takes steps of size s according to a distribution : $$ p ( s ) \propto {1\over s^{1+\alpha}}$$ where $0&lt ; \alpha&lt ; 2$ , you get levy quantum mechanics . so to define quantum mechanics on a cantor set , all you need is an appropriate stochastic motion . the ordinary brownian motion fails to have a limit , it just stays still on the cantor set--- it ends up fully localized . but the levy process generalizes just fine . the cantor set can be defined as all base 3 numbers with digits which are all 0 or 2 . a discrete approximation is truncating this at n digits . define a random walk on this graph by toggling a digit between 0 to 2 at digit position k with a rate which goes as : $$ e^{-ak} $$ where $a&gt ; 0$ . if you take the limit of continuous time , timesteps of size $\epsilon$ , and $a= {a\over \epsilon}$ , you get a hop which is a power law in size ( since it is an exponetial distribution on exponentially shrinking sizes , and this is a powerlaw in the size ) , and the continuum limit is levy quantum mechanics restricted to the cantor set . this is related to the question of localizing dirac fermions , since the |k| dispersion relation is levy . you do not localize levy particles with a local potential , unlike normal schrodinger particles . this was the subject of this question : how to localize the massless fermions in dirac materials ? .
the photons and gravitons involved in static fields are not causal , they do not propagate along light cones . they are acausal things in a feynman framework . any gauge charge is visible outside the black hole , this is because gauge fields are determined by a gauss law at infinity . a good classical picture is that a charged black hole has a charge-per-unit-area on the horizon , which is considered as a gr version of an charged plate , the charge per unit area is the electric field density on the horizon , while the horizon always carries mass per unit area away from extremality , and this is by the surface gravity of the black hole . these classical picture do not refer to particles , only to fields . the duality between particle and field description is subtle , and should not be used for casual arguments like this .
first , one inevitably gets the same solutions if he solves the problem in the slab 's rest frame , and then lorentz-transforms the result to the frame where the slab is moving ; or if one solves the problem directly in the frame where the slab is moving . the reason is that maxwell 's equations are covariant under the lorentz transformations . so if they are satisfied in one frame , they will be satisfied in any frame related by boosts , too . however , we must properly transform all the magnetizations and material relations etc . and add the corresponding moving sources which will be the main subtlety in the text below . in your particular problem , one may say some generic statements about the magnetic ( and electric ) fields without much thinking . for example , if $\vec m$ is in the $x$-direction , it means that the electrons may be thought of to spin in the $yz$-plane . take a surface of the slab parallel to the $yz$-plane - i.e. one face that belongs to a $x=x_0$ plane . it is pretty clear that there will inevitably be a component of the magnetic field $\vec b$ in the $x$-direction near the external side of the surface . if one boosts the $b_x$ magnetic field in the $z$-direction , there will inevitably be a nonzero electric field in the $y$-direction , $e_y$ . in the frame where the slab is moving , we seem to have no electric sources $\rho$ of the $\mbox{div}\ , \vec d=\rho$ gauss 's law and no right-hand side of the maxwell-faraday equation , $\nabla\times \vec e = -\partial \vec b / \partial t$ . so because there are no electric sources , you would think that the electric field should vanish . however , this is a flawed argument because the form of maxwell 's equations we are using here are only " maxwell 's equations for materials at rest " . in particular , the gauss 's law is optimized for $\vec d$ which we are imagining to be given by $\epsilon \vec e$ , and is " purely electric " . however , for a moving material , there should be an extra term of the type $\vec v\times \vec m$ included in $\vec d$ . because the latter has a nonzero $y$-component in the moving frame , there will be a nonzero $e_y$ in this frame , too . the precise form of maxwell 's equations in a moving medium may be confusing and unfamiliar so i think it may be a good idea to try to transform the local physics to the rest frame of any material , whenever needed , and perhaps lorentz-transform back . whenever subtleties would occur , one would have to revisit the derivation of the " macroscopic maxwell 's equations " ( for materials ) and redo it with the possibility of moving materials . microscopic maxwell 's equations alternatively , you could always try to use the microscopic maxwell 's equations which include the gauss 's law in the form $\vec \nabla\cdot \vec e = \rho / \epsilon_0$ . but in this form , $\rho$ includes not only free charges but also the " microscopic charges " related to the material . because the slab has nonzero values of $j_y$ and $j_z$ ( currents inside the material ) - recall that the electrons are kind of rotating in the $yz$-plane ( to produce the magnetic $x$-field ) , it is also true that when we boost the system in the $z$ direction , the corresponding multiple of $j_z$ will produce a nonzero value of $\rho$ ( microscopic charge density ) . this will be the source of the $e_y$ field discussed above . in particular , $j_z$ will be proportional to $ [ \delta ( y-y_1 ) -\delta ( y-y_2 ) ] $ in the slab 's frame which means that there will be $\rho \sim [ \delta ( y-y_1 ) -\delta ( y-y_2 ) ] $ in the frame where the slab is moving . it is this $\rho$ that will induce a nonzero value of $e_y$ right outside the material ( in the frame where the slab is moving ) .
the mass of the ring is wrong . the ring ends up at an angle , so its total width is not $dx$ but $\frac{dx}{sin\theta}$ you made what i believe was a typo when you wrote $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \left ( r^2 - x^2 \right ) \text{d}x$$ because based on what you wrote further down , you intended to write $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \sqrt{\left ( r^2 - x^2 \right ) }\text{d}x$$ this problem is much better done in polar coordinates - instead of $x$ , use $\theta$ . but the above is the basic reason why you went wrong . in essence , $sin\theta=\frac{r}{r}$ so you could write $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \frac{r}{sin\theta} \ \text{d}x \\ = \frac{m}{4\pi r^2}\cdot 2\pi \frac{r}{\frac{r}{r}} \ \text{d}x\\ = \frac{m}{4\pi r^2}\cdot 2\pi r \ \text{d}x\\ = \frac{m}{2 r} \ \text{d}x$$ now we can substitute this into the integral : $$i = \int_{-r}^{r} \frac{m}{2 r} \cdot \left ( r^2 - x^2\right ) \ \text{d}x \\ = \frac{m}{2r}\left [ {2r^3-\frac23 r^3}\right ] \\ = \frac23 m r^2$$
electricity ( and electromagnetic fields in general ) will invariably follow the " easiest " path . the entire point of a faraday cage is to provide the electricity from whatever discharge you are facing with an " easy " path that does not involve the person inside the cage . holes will not change a thing . why would the electricity even try to get through the holes if there is so much conducting chicken wire for it to go through ? electricity loves chicken wire , it hates air . so if there is chicken wire , it just goes through it . nonetheless it is perfectly possible to get electric shock inside a faraday cage . just install an electric socket inside the cage , linked to the electric grid outside , and then do something stupid like cram your fingers into the socket . . . ( mark the the frequency of any external discharge will not change a thing . electricity prefers conducting material to isolators like air and people , so the faraday cage will shield you from all external discharges . )
a few comments : 1 ) we do not know what happens with objects of volume smaller than $\ell_{p}^{3}$ , where $\ell_{p}$ is the planck length , which is approximately $10^{-35}$ m . the known laws of physics do not hold for anything that small . 2 ) general relativity , in its classical glory , predicts that objects will collapse down to a shape with zero volume . a spinning black hole will take the shape of a ring , with a radius determined by the angular momentum and mass of the black hole . since it is virtually impossible to tune down the angular momentum of physical objects to exactly zero , we will expect most singularities to be ring-shaped , perhaps having a $\ell_{p}$ thickness . 3 ) the singularity is not expected to be observable to outside observers , so most astrophysicists will instead talk about the size of the black hole horizon , which is a macroscopic size .
part 1: as described in the text book , $\psi$ is a linear combination of its eigenfunctions . $a_n$ is just a scalar multiple of that state , which basically tells you how much goes into that state , its the probability amplitude of that state . therefore the probability to be in state $\psi_n$ is equal to $\|a_n\|^2 = a_n a^*_n$ . part 2: it should be clear now that the summation of all $\|a_n\|^2$ values should give you 1 , i.e. the probability of the system being in one of its eigenstates is exactly 1 ( its surely in one of its states ! ) .
forget the friction part , unless you are always peeling rubber or screeching the brakes . the acceleration is the combined sum of forces - air drag , engine thrust at the wheels , slope of the road , etc . , divided by mass ( weight ) . you are right . those things change all the time , so the acceleration and speed are always changing . the cruise control is a feedback system that tries to adjust the throttle to stay at one speed , but only approximately . cars are designed with a property called driveability , which basically means they are not too jerky in acceleration , so they do try to limit the 3rd derivative . the 2nd derivative , acceleration , is limited by engine power and the amount of available tire friction . forget the higher derivatives . you can take as many derivatives as you want . it is a continuous system and infinitely differentiable .
you may consider every excited state of an atom $\psi_n$ as another particle . then their number may be infinite even in this simple example . the higher energy of the system , the higher $n$ . in qft each particle has occupation numbers . during reactions some occupation numbers decrease , some other increase . factually qft equations are the balance equations governing the occupation numbers while interactions .
the deal here is that you have to be very careful about what quantity you are interested in and what you have . assumptions you have measurements $f_i$ of the fraction of stuff ( does not really matter what ) during time interval $i$ . you may also have measurements of $o_i$ of the total output of the medium in which stuff makes up a fraction . alternately you may only know $\bar{o}$ the average output or the total output over the entire time range $\mathcal{o}$ . note that for $n$ measurements at uniform spacing $\mathcal{o} = n * \bar{o}$ . case 1: you want to know " how much stuff " totaled over several time periods and you have both fractional values and outputs for every period . this is as good as it gets . you need to add up the daily stuff . the daily amount of stuff is $s_i = f_i o_i$ . that is just the definition of $f_i$ . so assuming you have the daily outputs you get : $$ \mathcal{s} = \sum_i s_i = \sum_i f_i o_i \quad . $$ case 2: you want to know " how much stuff " totaled over several time periods , but you have the total output or average output without periodic output values . the best you can do is $$\mathcal{s} \approx \sum_{i=1}^n f_i \bar{o} = \mathcal{o} \frac{1}{n}\sum_{i=1}^n f_i = \bar{o} \bar{f}$$ where $\bar{f}$ is defined by this relation as the mean fraction . in this case it makes sense to take a mean of the fractional reading , but only because you do not have enough data to get the right answer . this will be approximately correct if ( 1 ) the $o_i$s have small variance or ( 2 ) the $f_i$s have a small variance or ( 3 ) you have a lot of data and the $f_i$s and $o_i$s are uncorrelated . case 3: you want to show that the fraction is uniformly above or below some limit . then you just need the $f_i$s , and computing their mean and variance is fine as long as ( 1 ) the answer is a long way from the limit and ( 2 ) the $f_i$ are uncorrelated with the $o_i$s . case 4: you want to show that the stuff is uniformly above or below some limit . then you need the daily values to do it right . in the absence of the daily output you can fall back on the average output again , but it will only be reliable if all three conditions listed for case 2 apply . side note : you were concerned in the comments that 2 million parts per million does not make sense . and you are partially right . you can not have a fractional concentration of 2 million ppm , but there are cases when that value is meaningful . you probably already understand this . people are happy to talk about 200% in some cases , but " percent " is literally " per one hundred " . two hundred pars per hundred is the number 2 and it makes sense in cases involving changes but not in cases involve the fraction of people who qualify for something .
yeah , definitely . one example is an inelastic collision , where both masses will have the same velocity after colliding . in this case , let 's say a bullet of mass $m$ and speed $v_0$ hits a stationary rock of mass $m$ and they stick together and move with a final speed $v$ . intuitively , you can already tell that their velocity must be in the same direction as the bullet 's initial velocity . but let 's make it explicit : from conservation of momentum , we have that $$mv_0=mv+mv$$ $$mv_0= ( m+m ) v$$ since mass must always be positive , $v$ must have the same sign as $v_0$ , which means that the bullet moves in the same direction as before-keeps moving forward ( although at a slower speed ) -and is not ' reflected ' by the rock . edit ( to respond to additional question ) . under what conditions does this happen ? essentially , the bullet would have to stick ' into ' the rock .
i think the answer is it depends on distance ( relative to the size of your system ) . another well known example of a boson which is comprised of fermionic components is the helium-4 atom , which has integer spin ( both the nucleus and the neutral atom itself ) . fermionic or bosonic behavior of a composite particle ( or system ) is only seen at large ( compared to size of the system ) distance . at proximity , where spatial structure begins to be important , a composite particle ( or system ) behaves according to its constituent makeup . for example , two atoms of helium-4 cannot share the same space if it is comparable in size to that of the inner structure of the helium atom itself ( ~10−10 m ) —despite bosonic properties of the helium-4 atoms . thus , liquid helium has finite density comparable to the density of ordinary liquid matter . ( taken from here . ) i think this provides a concrete example of what you were asking . hopefully this helps .
depends on what you mean by ' central force ' . if your central force is of the form ${\vec f} = f ( r ) {\hat r}$ ( the force points radially inward/outward and its magnitude depends only on the distance from the center ) , then it is easy to show that $\phi = - \int dr f ( r ) $ is a potential field for the force and generates the force . this is usually what i see people mean when they say " central force . " if , however , you just mean that the force points radially inward/outward , but can depend on the other coordinates , then you have ${\vec f} = f ( r , \theta , \phi ) {\hat r}$ , and you are going to run into problems finding the potential , because you need $f = - \frac{\partial v}{\partial r}$ , but you will also need to have $\frac{\partial v}{\partial \theta} = \frac{\partial v}{\partial \phi} = 0$ to kill the non-radial components , and this will lead to contradictions . it is logical that a field of this form is gong to be nonconservative , because if the force is greater at $\theta = 0$ than it is at $\theta = \pi/2$ , then you can do net work around a closed curve by moving outward from $r_{1}$ to $r_{2}$ at $\theta = 0$ ( positive work ) , then staying at $r_{2}$ constant , going from $\theta =0 $ to $\theta = \pi/2$ ( zero work--radial force ) , going back to $r_{1}$ ( less work than the first step ) , and returning to $\theta = 0$ ( zero work ) .
i will just try to answer 1 ) . my take is that , at least in some sense , there is no violation of the uncertainty principle , as the droplet 's coordinate and momentum are not defined very well : the droplet interacts with its own wave , so , for example , momentum is distributed between the droplet and its wave , and it is difficult to define the droplet 's coordinate , as the droplet influences its surroundings through its wave as well . you may tell me that the droplet is classical , so its coordinate and momentum should be determined independently of its wave . while such approach is certainly legitimate , when we are talking about the uncertainty principle , we should make an apples-to-apples comparison with microscopic interference , in which case we do not divide the particle and , say , its electromagnetic field . in quantum mechanics we tend to assume that wherever and whenever we detect some influence of the particle , we measure its coordinate with some precision , although , e.g. , the effective radius of the coulomb interaction is infinite . by the way , one of the reasons the couder experiment seems interesting to me , it shows that some traditional mantras about quantum interference , while relatively consistent , may look absurd when applied to the pretty similar case of the couder experiment .
some obscure thermal conductivity effect might heat the air inside to double the temperature , thereby doubling its pressure , but if the air expands and the pita does not let new gas in , it will not be able to expand more than a factor of 2 before the pressure is the same as outside . the expansion observed is a factor of a ten or more , so it is gas production . i think that the explanation is production of steam , and the critical temperature is close to the boiling point of water , give or take the effects of solutes in water trapped in the pita . steam production gives as much expansion as you want , and also , when i cut the pita open , i notice that it is full of steam . ( i do not think its such a terrible question either )
generally taking note of these relationships is a precursor to either ( a ) applying an approximation or ( b ) using a purturbative or series solution . in case ( a ) what qualifies is completely a matter of your sensitivity to error . if you are going to throw out terms $\mathcal{o} ( c^{\pm 2} ) $ and require a 1% approximation then $c$ had better differ from 1 by a factor slightly larger than 10 . in case ( b ) there may be hard limits on the value of $c$ for which the mathematical approach you are using converges . you must understand the analysis that follow or you are sunk . once convergence is guaranteed we come back to the situation in case ( a ) : you look at the residual after you are done summing as many orders as you are going to work and compare that to your desired sensitivity . as a rule of thumb , factors less than 10 rarely qualify for " much [ greater|less ] than " .
since this is homework , we are not supposed to give you the answer . but one mistake you made is in your formula for the magnitude of $r$ - the inner square root needed to be squared . so the length of $r$ is simply the square root of the sum of the squares of the $i$ , $j$ and $k$ lengths . good luck . . .
the reason you are not finding scale invariance in your equation is that you have inserted a factor of $\lambda^2$ in the first two terms , which as far as i can see has no reason to be there . i am guessing you put it in because of your uncertainty as to how to dilate derivatives . transforming a derivative is nothing complicated , though . you simply make the transformation on the variable that is being differentiated with respect to , treat it as a differential in the denominator , and simplify . for example : $$\frac{\partial}{\partial x_i} \to \frac{\partial}{\partial ( \lambda x_i ) } = \frac{\partial}{\lambda\partial x_i} = \lambda^{-1}\frac{\partial}{\partial x_i}$$ because $\mathrm{d} ( \lambda x_i ) = \lambda\mathrm{d}x_i$ . when you do this , in each of the first two terms you get $\lambda^{-1}$ from the transformation of the field and $\lambda^{-2}$ from the transformation of the derivative , for an overall scaling factor of $\lambda^{-3}$ . and in the last term , you get $\lambda^{-3}$ from the fields alone . all those factors of $\lambda^{-3}$ then cancel out . the reason dimensionless couplings are preferred ( in certain applications ) is that they do not set any scale for the theory . if your coupling $g$ had a dimension of $l^{-1}$ , for example , then you could establish a characteristic length scale for that theory as $g^{-1}$ . but being scale invariant means that there should be no preferred length scale ; you can change any scale $x \to \lambda x$ without altering the theory . if you have a dimensionful coupling , that is obviously not the case , because you can distinguish between $x$ and $\lambda x$ by measuring them in units of $g^{-1}$ . you can get around that by having $g$ change under a scale transformation as well , but then it is no longer a coupling constant , which makes things more complicated .
the internal energy of a black hole is just its mass . you can measure the mass of a black hole by its gravitational effects on outside bodies , and then extrapolate an equivalent energy using $e=mc^{2}$ . an isolated neutron star also has a well-defined total mass , ( the adm mass ) , which can be used to define an internal energy . in the case of a neutron star , since it has x numbers of neutrons , and baryon number is conserved in gravitational collapse , you can even divide the total energy by the number of baryons to get a ' total energy per baryon . '
as an orbit goes around the earth , your challenge if you launch vertically is that once you reach the desired height you will then need to accelerate sideways to orbital velocity . for an l2 orbit , this is around 1km/s with respect to the earth . so you need to carry all that fuel up with you , to then burn it . that is a vast amount of mass wasted in the initial launch , so realistically it is not going to happen . in reality , the launch profiles used give you orbital velocity and height as efficiently as possible , to maximise payload .
some good things to remember for basic circuits are that parallel pathways have the effect of increasing the area available to current flow , and because of this always lower resistance -- at least in the basic circuits i know about . another thing i remember is adding inverses always amounts to the product over the sum : ( 1/r1 + 1/r2 + 1/r3 + . . . ) ^-1 = r1r2r3 . . . /r1+r2+r3 . . . } this is a relationship seen in many areas of physics , most notably ( imo ) for the reduced mass of a system , which can make many calculations much simpler . since other people have basically already answered your question ( the angle reveals the length ) , i thought i would give my two cents , i hope it moves your studying along a bit faster . also , unless i missed something too , i think the lower length should be 5 pi/3 . if the path from a to b is a circle , then depending on how you write your fractions , the sum should be 6 pi / 3 or 2 pi . does this also suggest how , simple as it may be , one of my suggestions might speed things up ?
let $q$ denote the set of all possible configurations of the system ( the configuration manifold ) . consider a point $q_0\in q$ . for the sake of conceptual clarity , and to make contact with physics notation , let 's work in some local coordinate patch around $q_0$ . suppose that $q_0$ represents the position of the system under consideration at time $t_0$ . at a given time $t$ later , the system will be at some position say $q ( t ) $ that is determined by the evolution equations ( the euler-lagrange equations if we are doing lagrangian mechanics ) , and the quantity \begin{align} q ( t ) - q ( t_0 ) = q ( t ) - q_0 \end{align} would be the displacement of the system after a time $t$ . suppose , instead we consider some other curve $\gamma ( s ) $ in the configuration space which starts at the point $t_0$ ; \begin{align} \gamma ( s_0 ) = q_0 , \end{align} and suppose that we compute the displacement \begin{align} \gamma ( s ) - \gamma ( s_0 ) = \gamma ( s ) - q_0 \end{align} that would result from moving along this other curve of our choosing . we call this displacement the virtual displacement after a time $t$ corresponding to moving along the curve $\gamma$ . it is called virtual because it is the displacement in the position of the system that would occur if the system were to move along the curve $\gamma$ of our choosing -- a " virtual " curve as opposed to the " real " curve along which the system travels according to the lagrangian evolution of the system . note . i used the parameter $s$ for the curve $\gamma$ instead of $t$ to emphasize that moving along that curve does not correspond to time-evolution . now what about virtual " infinitesimal " displacements ? well , recall that the term " infinitesimal " in physics essentially always refers to " first order " approximations , see , e.g. this se post : rigorous underpinnings of infinitesimals in physics so when we are discussing a virtual infinitesimal displacement , what we have in mind is taking the virtual displacement $\gamma ( s ) - q_0$ , taylor expanding it to first order in $s$ , and extracting only the first order term . let 's do this : \begin{align} \gamma ( s ) - q_0 = \gamma ( s_0 ) + \dot\gamma ( s_0 ) t + o ( s^2 ) - q_0 \end{align} using the fact that $\gamma ( s_0 ) = q_0$ , we see that the taylor expansion of the virtual displacement is \begin{align} \gamma ( s ) - q_0 = \dot\gamma ( s_0 ) t + o ( s^2 ) , \end{align} and now we notice that to first order in $s$ , the size of the virtual displacement is controlled by the coefficient of $s$ , namely $\dot\gamma ( s ) $ . in other words , virtual infinitesimal displacements ( meaning we just keep the first order contribution in $s$ ) , are determined by the velocity vector of the chosen " virtual curve " at $s_0$ . but if you have taken a differential geometry course , then you know that velocities of curves on a manifold are simply tangent vectors to that manifold ! so virtual infinitesimal displacements can be associated with tangent vectors to the configuration manifold . the intuition to keep in mind here as that a virtual displacement just tells us how far we would get away from a certain point on the manifold if we were to travel on a certain curve of our choosing that may not coincide with the actual motion of the system determined by time evolution . the " infinitesimal " part and identifying this part with tangent vectors comes simply from considering what happens only to first order .
the static air pressure seen by the aircraft does not change with the aircraft 's velocity . your confusion is from a common misinterpretation of bernoulli 's principle . it is not true that a fluid 's pressure will decrease simply by virtue of flowing faster . after all , this violates the idea that physics should be the same in all inertial frames . here is a simple counterexample to the typical interpretation of the bernoulli principle . consider a tube of infinite length and uniform diameter with some gas sitting in it . now consider various coordinate systems with a velocity in the direction of the tube . in these different coordinate systems , the velocity of the gas will be different , but we expect the force on the walls of the tube due to the fluid 's pressure to be the same in all cases . ( the tube is not going to rupture simply because of a choice of coordinate system ! ) instead , bernoulli 's principle says that , in a given flow ( say , along a streamline ) , a local increase in velocity is associated with local decrease in pressure . the canonical example is fluid flow through a tube with a constriction ( a venturi ) . quoting from the wikipedia article for bernoulli 's principle : bernoulli 's principle can be derived from the principle of conservation of energy . this states that , in a steady flow , the sum of all forms of mechanical energy in a fluid along a streamline is the same at all points on that streamline . this requires that the sum of kinetic energy and potential energy remain constant . . . . if a fluid is flowing horizontally and along a section of a streamline , where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure ; and if its speed decreases , it can only be because it has moved from a region of lower pressure to a region of higher pressure . consequently , within a fluid flowing horizontally , the highest speed occurs where the pressure is lowest , and the lowest speed occurs where the pressure is highest . note the emphasis on relative changes occurring on a streamline . the specific flaw in your argument is here : now the fluid is accelerated along a streamline , and hence the static pressure should drop according to the relation given above . in the wind tunnel , something has to do work to accelerate the air to the wind tunnel velocity , adding energy to the flow , which violates the conservation of energy assumption in bernoulli 's principle .
let 's take your last question first . let the stress tensor at a point ( x , y , z ) in the fluid be given as $\sigma$ . you can pick a cartesian basis $\{ e_1 , e_2 , e_3 \}$ and express the components of the tensor in that basis $$ \begin{bmatrix} \sigma_{xx} and \sigma_{xy} and \sigma_{xz} \\ \sigma_{xy} and \sigma_{yy} and \sigma_{yz} \\ \sigma_{xz} and \sigma_{yz} and \sigma_{zz} \end{bmatrix} $$ the normal stresses are simply $\sigma_{xx} , \sigma_{yy}$ and $\sigma_{zz}$ . it is important to realize that these stresses will have different values in another basis . clearly , you can not attach too much physical significance to things that are basis dependent . however , it is a theorem of continuum mechanics that you can always find at least one basis in which the off-diagonal ( shear terms ) are zero . in this basis , the tensor components are $$ \begin{bmatrix} \sigma_{1} and 0 and 0 \\ 0 and \sigma_{2} and 0 \\ 0 and 0 and \sigma_{3} \end{bmatrix} $$ these numbers have actual physical significance . $\max ( {\sigma_1 , \sigma_2 , \sigma_3} ) $ is the largest principal normal stress at the point . similarly , $\min ( {\sigma_1 , \sigma_2 , \sigma_3} ) $ is the smallest normal stress at that point . it is not too hard to realize that $\sigma_1 , \sigma_2 , \sigma_3$ are the eigenvalues of the stress tensor . on the other hand , the pressure is ( -1/3 ) times the trace of the stress tensor , i.e. $$ p = -\frac{1}{3} \sigma_{jj} $$ the trace is an invariant of the stress tensor , so if you take the sum of the diagonals of the stress tensor in any basis you will get the same value . mathematically , $$ tr ( [ \beta ] [ \sigma_{ij} ] [ \beta ] ^t ) = tr ( \sigma_{ij} ) $$ so you see that the pressure and the normal stress are very different entities indeed . in particular , the pressure is isotropic - it has no preferred direction . now consider a state of pure shear in a fluid . to keep matters simple , we will assume planar flow and ignore out of plane components . the stress tensor for pure shear in our standard basis looks like this $$ \begin{bmatrix} 0 and \tau \\ \tau and 0 \\ \end{bmatrix} $$ looks like the normal stresses are zero , right ? not so fast . as this is a symmetric real tensor , you can always find another basis in which you have normal stress components ! in fact , if you solve the eigenvalue problem setting $\det ( \sigma-\lambda i ) =0$ , you get principal normal stresses of $\pm \tau$ . so , in a coordinate system with basis vectors $e'_1 = \{ ( \frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} ) \}$ and $e'_2 = \{ ( -\frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} ) \}$ rather than $\{ ( 1,0 ) , ( 0,1 ) \}$ , you get a stress tensor from a situation of " pure " shear that looks like this $$ \begin{bmatrix} \tau and 0 \\ 0 and -\tau \\ \end{bmatrix} $$ you can easily verify this by carrying out the change of basis yourself . so what looks like " pure shear " in one basis is biaxial normal stresses in another basis . since the signs are different , you have both tensile and compressive normal stresses in your fluid .
it is not necessarily true . for a zero potential $v_2$ you have $p_2=0$ , whereas if $v_1$ is a rectangular pit , in general , $p_1&gt ; 0$ .
in addition to fresnel equations , and in response to your question regarding the " . . . relation between the amplitude of the transmitted/reflected rays and the original ray": $$t_{\parallel}=\frac{2n_{1}\cos\theta_{i}}{n_{2}\cos\theta_{i}+n_{1}\cos\theta_{t}}a_{\parallel}$$ $$t_{\perp}=\frac{2n_{1}\cos\theta_{i}}{n_{1}\cos\theta_{i}+n_{2}\cos\theta_{t}}a_{\perp}$$ $$r_{\parallel}=\frac{n_{2}\cos\theta_{i}-n_{1}\cos\theta_{t}}{n_{2}\cos\theta_{i}+n_{1}\cos\theta_{t}}a_{\parallel}$$ $$r_{\perp}=\frac{n_{1}\cos\theta_{i}-n_{2}\cos\theta_{t}}{n_{1}\cos\theta_{i}+n_{2}\cos\theta_{t}}a_{\perp}$$ where $a_{\parallel}$ and $a_{\perp}$ is the parallel and perpendicular component of the amplitude of the electric field for the incident wave , respectively . accordingly for the $t$ ( transmitted wave ) and $r$ ( reflected wave ) . i think the notation is straightforward to understand . this set of equations are also called fresnel equations ( there are three or four representations ) .
the opposite sign of the shifts is due to the conservation of the location of the center-of-mass or , equivalently , momentum conservation . at least for the simple system of 2 bodies , the animation on the page is being observed from the inertial frame of the center of mass . one may check this fact by seeing that the trajectories are periodic ( ellipses ) even if the masses are comparable and the initial velocities and locations are generic ( but allowing a bound state ) . in the center-of-mass frame , the total momentum of both bodies is zero . so if one of them moves in one direction , the other is moving in the opposite direction . their trajectories are really ellipses that are similar to each other ( one obtains one from another by scaling , multiplication by a negative number ) . it follows that if the apsis of one body is on one side from the center of mass , the apsis of the other body must be on the other side .
if your rate is $\omega_p$ given in rad/s then the period is $\frac{2 \pi}{\omega_p}$ with units of seconds .
the brownian motion $x ( t ) $ is non-differentiable , so a particular trajectory $x ( t ) $ can not extremize an action $s$ which would be a functional of $x ( t ) $ and its derivative , $\dot x ( t ) $ , because the derivative is not even well-defined and any expression of the type $\int [ \dot x ( t ) ] ^2 dt$ , the usual kinetic term in the action , diverges . ( see e.g. middle of page 2 of this paper to see the statement that there is no lagrangian , too . the paper does its best to construct something that is " as close as possible " to the normal lagrangian formulation . ) however , when you mention field theory , it is interesting to point out that the typical trajectories $x ( t ) $ that contribute to feynman 's path integral computation of ordinary quantum mechanics do resemble the brownian trajectories very closely . but the amount of zigzag motion is determined by the uncertainty principle and planck 's constant , not by adjustable collisions with the molecules of a liquid etc . there are many other differences in the physical interpretation , too .
the experiment is correct . well , what is really correct is the final interpretation of the experiment . radiation gets redshifted in the gravitational fields . it is one of the well-known consequences of general relativity . it is been known theoretical since 1915 or so and directly and safely experimentally verified in the 1960s . all the " magic " terminology on the page is only meant to express the idea that the energy and the total relativistic mass must scale in the same way because they are related by $e=mc^2$ , the well-known identity in special relativity . thanks to this identity , the conversion of matter into energy and vice versa is not really " magical " in any sense . it may be done . when a particle ( electron ) and an antiparticle ( positron ) annihilate , they are indeed converted to a pair of photons . the gravitational potential may be used to accelerate the electron and positron against each other , for example , and the extra kinetic energy means that the annihilation will result in higher-frequency photons when it is done at a lower altitude , deeper in the gravitational field . because the energy conservation law says that it does not really matter whether we annihilate the electron-positron pair at a higher altitude , or we do so at a lower altitude , and transfer the photons back to the higher altitude , it follows that the photon has to lose some of its energy ( the extra energy from the additional kinetic energy of the electron and positron ) when it is flying up i.e. escaping from the gravitational field . that is the gravitational red shift . the energy , total mass , as well as the frequency get reduced by the factor $\sqrt{g_{00} ( a ) /g_{00} ( b ) }$ between the points a and b .
( 1 ) the first law is written in form of differentials themselves , so i think there may be no escape from using differential equations . ( 2 ) the way most commonly the first law is written is , $du=dq-dw_{\text{work done by the system}}$ . here dw is work done by the system . however , in subjects other than physics , more important quantity is the work done by the experimenter . ( this is quite common in chemistry ) as the process in thermodynamics are most " quasistatic" ( http://en.wikipedia.org/wiki/quasistatic_process ) , the container/piston is always in equilibrium . so , $\vec{f_{ext}}=-\vec{f_{int}}$ ( they are equal and opposite ) , then we have , $dw_{\text{by the system}}=-dw_{\text{on the system}}$ . and so the first law can be written as : $du=dq+dw_{\text{work done on the system}}$ . here dw is work done on the system . as the two $dw$s have different meanings we will not a different answer . ( 3 ) the internal energy of a gas is state variable/state function ( http://en.wikipedia.org/wiki/functions_of_state ) . $u$ depends only on the final and initial states of the system and not on what process was used to get from initial to the final state . so we can use a constant volume process to get $du$ which then can be used in any process without any modification .
your calculations are correct , provided the cylinder is indeed ohmic . the constant $e$ you are getting is the difference in electric field between both terminals . as for the current flowing from inside to outside , as you said the cross sectional area will be different , and so will the length . the length $l=r_b-r_a$ , but the cross sectional area is not uniform , because at the beginning of the wire ( the interior ) , $a=2\pi r_al$ , and at the " end " of the wire ( the exterior ) , $a=2\pi r_bl$ . so you will have to treat each portion of the wire as its own infinitesimal resistor $dr$ , and the total resistance is the series combination of them : $$dr=\rho\frac{dl}{2\pi ll}$$ $$r=\int_{r_a}^{r_b}\rho\frac{1}{2\pi ll}dl$$
just in view of the double universal covering provided by $su ( 2 ) $ , $so ( 3 ) $ must a quotient of $su ( 2 ) $ with respect to a central discrete normal subgroup with two elements . this is consequence of a general property of universal covering lie groups : if $\pi : \tilde{g} \to g$ is the universal covering lie-group homomorphism , the kernel $h$ of $\pi$ is a discrete normal central subgroup of the universal covering $\tilde{g}$ of $g= \tilde{g}/h$ , and $h$ is isomorphic to the fundamental group of $g$ , i.e. $\pi_1 ( g ) $ ( wich , for lie groups , is abelian ) . one element of that subgroup must be $i$ ( since a group includes the neutral element ) . the other , $j$ , must verify $jj=i$ and thus $j=j^{-1}= j^\dagger$ . by direct inspection one sees that in $su ( 2 ) $ it is only possible for $j= -i$ . so $so ( 3 ) = su ( 2 ) /\{i , -i\}$ . notice that $\{i , -i\} = \{e^{i4\pi \vec{n}\cdot \vec{\sigma}/2 } , e^{i2\pi \vec{n}\cdot \vec{\sigma}/2 }\}$ stays in the center of $su ( 2 ) $ , namely the elements of this subgroup commute with all of the elements of $su ( 2 ) $ . moreover $\{i , -i\}=: \mathbb z_2$ is just the first homotopy group of $so ( 3 ) $ as it must be in view of the general statement i quoted above . a unitary representations of $so ( 3 ) $ is also a representation of $su ( 2 ) $ through the projection lie group homomorphism $\pi : su ( 2 ) \to su ( 2 ) /\{i , -i\} = so ( 3 ) $ . so , studying unitary reps of $su ( 2 ) $ covers the whole class of unitary reps of $so ( 3 ) $ . let us study those reps . consider a unitary representation $u$ of $su ( 2 ) $ in the hilbert space $h$ . the central subgroup $\{i , -i\}$ must be represented by $u ( i ) = i_h$ and $u ( -i ) = j_h$ , but $j_hj_h= i_h$ so , as before , $j_h= j_h^{-1}= j_h^\dagger$ . as $j_h$ is unitary and self-adjoint simultaneously , its spectrum has to be included in $\mathbb r \cap \{\lambda \in \mathbb c \:|\: |\lambda|=1\}$ . so ( a ) it is made of $\pm 1$ at most and ( b ) the spectrum is a pure point spectrum and so only proper eigenspeces arise in its spectral decomposition . if $-1$ is not present in the spectrum , the only eigenvalue is $1$ and thus $u ( -i ) = i_h$ . if only the eigenvalue $-1$ is present , instead , $u ( -i ) = -i_h$ . if the representation is irreducible $\pm 1$ cannot be simultaneously eigenvalues . otherwise $h$ would be split into the orthogonal direct sum of eigenspaces $h_{+1}\oplus h_{-1}$ . as $u ( -1 ) =j_h$ commutes with all $u ( g ) $ ( because $-i$ is in the center of $su ( 2 ) $ and $u$ is a representation ) , $h_{+1}$ and $h_{-1}$ would be invariant subspaces for all the representation and it is forbidden as $u$ is irreducible . we conclude that , if $u$ is an irreducible unitary representation of $su ( 2 ) $ , the discrete normal subgroup $\{i , -i\}$ can only be represented by either $\{i_h\}$ or $\{i_h , -i_h\}$ . moreover : since $so ( 3 ) = su ( 2 ) /\{i , -i\}$ , in the former case $u$ is also a representation of $so ( 3 ) $ . it means that $i = e^{i 4\pi \vec{n}\cdot \vec{\sigma} }$ and $e^{i 2\pi \vec{n}\cdot \vec{\sigma}/2 } = -i$ are both transformed into $i_h$ by $u$ . in the latter case , instead , $u$ is not a true representation of $so ( 3 ) $ , just in view of a sign appearing after $2\pi$ , because $e^{i 2\pi \vec{n}\cdot \vec{\sigma}/2 } = -i$ is transformed into $-i_h$ and only $i = e^{i 4\pi \vec{n}\cdot \vec{\sigma}/2 }$ is transformed into $i$ by $u$ .
apart from a few special cases the way we measure the size of a particle is to scatter other particles off it . the hyperphysics site has a nice introduction to this - the article talks about measuring the size of nuclei by scattering , but this applies to any object . the scattering from a point like particle is different to a particle with a finite radius , and from the difference in the scattering patterns we can measure the radius . the measurement can only be done if the de broglie wavelength of the particles you are scattering is smaller than the size of your target . since $\lambda = h/p$ this means really small objects can only be measured by using very high momentum and therefore very high energy . the reason i mention this is that at the energies we can currently reach ( 8tev at the lhc ) all the fundamental particles appear to be pointlike i.e. their radius is zero . composite particles like protons and mesons ( both made from quarks ) have a non-zero radius but the quarks themselves appear to be point like . so the electron radius is zero - as far as we can tell at the moment . though if string theory is the correct fundamental description of particles then electrons and all fundamental particles have a radius of the order of a planck length .
no , its temperature will not stop rising . substances have both a temperature , and a density . remember in chemistry you had two different heat capacities , constant pressure , and constant volume . the water will still heat up , although the heat capacity is the constant volume heat capacity . at some temperature , it becomes a gas , with the same density as it started with .
the situation is simpler than you think . basically you do not need to do clebsch-gordan at all ! the massless irreducible representations of the ( proper orthochronous ) lorentz group are $1$-dimensional , labelled by their helicity $h$ . since $h = \pm 1$ are related by parity we group them as the $\pm$ helicities of the photon . to compute the helicity of multiparticle states you must take a tensor product of the possible helicity configurations . then you should decompose these into irreducible representations , according to the clebsch-gordan procedure . but in this case , the $+1$ and $-1$ irreps are $1$-dimensional . therefore their tensor product is automatically an irrep ! so there is no clebsch-gordan decomposition to do . thus we are left with four possibilities , namely $$1 \otimes 1 , \ 1\otimes -1 , \ -1 \otimes 1 , \ -1 \otimes -1$$ considering the action of the helicity operator in each representation , you can verify that these have helicity $2,0,0 , -2$ respectively . combining the $1\otimes -1$ and $-1\otimes 1$ representations symmetrically and antisymmetrically , you reproduce the two $0$ helicity states of rob 's answer . let me know if you want some more explicit detail of the calculation !
old ways used schrodinger 's equation 's solutions for the atoms and mapped the square of the wave function . . since the solution fitted the spectrum of the atom it was accepted that the orbital was also correct . recently there has been an experiment that measured the orbitals of the hydrogen atom the abstract from the link : to describe the microscopic properties of matter , quantum mechanics uses wave functions , whose structure and time dependence is governed by the schrödinger equation . in atoms the charge distributions described by the wave function are rarely observed . the hydrogen atom is unique , since it only has one electron and , in a dc electric field , the stark hamiltonian is exactly separable in terms of parabolic coordinates ( η , ξ , φ ) . as a result , the microscopic wave function along the ξ coordinate that exists in the vicinity of the atom , and the projection of the continuum wave function measured at a macroscopic distance , share the same nodal structure . in this letter , we report photoionization microscopy experiments where this nodal structure is directly observed . the experiments provide a validation of theoretical predictions that have been made over the last three decades . . a popularization is here . after zapping the atom with laser pulses , ionized electrons escaped and followed a particular trajectory to a 2d detector ( a dual microchannel plate [ mcp ] detector placed perpendicular to the field itself ) . there are many trajectories that can be taken by the electrons to reach the same point on the detector , thus providing the researchers with a set of interference patterns — patterns that reflected the nodal structure of the wave function . and the researchers managed to do so by using an electrostatic lens that magnified the outgoing electron wave more than 20,000 times . please note that the orbitals are a probability distribution for finding an electron in a specific ( x , y , z ) around the nucleus , not a matter density in the classical sense . this experiment is for one electron and from the description it does not seem it would work for higher atomic numbers , at least not as simply .
this is a question for a biology board . it is because of the way the cells in our retina work . have you heard of the " after image " ? if one sees a bright enough objects and shuts ones eyes , an after image appears with eyes closed . the same biological mechanism keeps the sense of light , rather than dark and bridges over the gap . i am guessing at the physics here , but it must be due to the chemistry of the organic molecules and how it is used to transmit to the head the information on and off . obviously once excited to an on state , there is a lifetime to decay to the off , and depending on the frequency from a certain point onward it bridges the gap . something like that happens also with the change of frames in the film of movies and other optical motions .
the light you see as the image of the sun on the sky is basically undeflected . http://en.wikipedia.org/wiki/diffuse_sky_radiation says it is 75 % when the sun is high and the sky is clear . the frequency dependency is due to rayleigh scattering . for the cloudy sky the fraction is much smaller , up to many orders smaller than unity ( maybe 1 millionth part as a wild guess ) .
in zero electric field , the interactions energy between two rydberg atoms is $u=c_6 r^{-6}$ . if the rydberg transition is driven with an ( effective ) rabi frequency $\omega$ , then the blockade radius is $$r_b = \left ( \frac{\hbar\omega}{c_6} \right ) ^{-1/6}$$ if one atom is excited to the rydberg state , a second atom will be detuned by just enough to prevent a complete transition .
first note that \begin{align} ( a+\sigma^2 i ) and = \sum_i \lambda_i |u_i\rangle \langle u_i| + \sigma^2\sum_i|u_i\rangle \langle u_i|\\ and =\sum_i ( \lambda_i +\sigma^2 ) |u_i\rangle \langle u_i| \end{align} it is diagonal with respect to $|u_i\rangle$ , so the inverse is simply , \begin{align} ( a+\sigma^2 i ) ^{-1} and =\sum_i \frac{1}{ ( \lambda_i +\sigma^2 ) }|u_i\rangle \langle u_i| \end{align} using the vector decomposition $x = \sum_i \gamma_i|u_i\rangle$ , we have \begin{align} ( a+\sigma^2 i ) ^{-1}x = \sum_i \frac{\gamma_i}{ ( \lambda_i +\sigma^2 ) }|u_i\rangle \end{align} and so \begin{align} a ( a+\sigma^2 i ) ^{-1}x and =\sum_i \frac{\gamma_i}{ ( \lambda_i +\sigma^2 ) }a|u_i\rangle\\ and = \sum_i \frac{\gamma_i\lambda_i}{ ( \lambda_i +\sigma^2 ) }|u_i\rangle \end{align}
your solution using dirac notation is correct . the mistake in your first attempt is : if you define the transformation by this $u$ , then $$a'=u^\dagger a u = \begin{pmatrix} 0 and 1 \\ 1 and 0\end{pmatrix} \neq u a u^\dagger$$ . you can easily check this : for example if you apply $a$ to $b_1$ in basis of $a$ you get $b_2$ and vice versa ( exactly as $a'$ tells you ) .
( 1 ) the equation $pv=nrt$ is a " proportionality " type of equation , which necessarily has a linear dependence when only one variable is independent . the class of problems your are dealing with now has only one independent ( you control the change ) and one dependent ( the equation tells you the change ) variable , while the others are held constant . and from a pure math perspective , for it to be a non-linear dependence with only one independent variable there would need to be operators other than multiplication/division ( e . g . exponent , logarithm , sin , cos , etc . ) . ( 2 ) when you are equating two gas states you are really solving the equation : $$ \frac{p_1 v_1}{nrt_1}=\frac{p_2v_2}{nrt_2} $$ and for the case you are looking at $n_1=n_2 $ the variable $n$ is on both sides and cancels . so in this case you are not multiplying either side by $n$ . when you are only looking at one gas state , you are not setting equations equal to each other and there is no cancellation , so you still have the variable $n$ .
the problem is that your coordinates are not well defined at $\theta=0$ and $\phi=\pi/2$ . note in particular that $$ u|_{ ( 0 , \frac{\pi}{2} , \gamma ) } = \begin{pmatrix}1 and 0\\0 and 1\end{pmatrix} $$ for any value of $\gamma$ . a simpler choice is $$ \tilde{u} = \begin{pmatrix} x+iy and z+iw \\ -z+iw and x-iy \end{pmatrix} , $$ with $$ x = \sqrt{1 - y^2 - z^2 - w^2} . $$ differentiating this you find $$ d\tilde u = i\begin{pmatrix} dy and +i\ , dz + dw \\ -i\ , dz + dw and -dy \end{pmatrix} - \frac{y\ , dy+z\ , dz+w\ , dw}{\sqrt{1-y^2-z^2-w^2}}\begin{pmatrix}1 and 0\\0 and 1\end{pmatrix} $$ from which you can read off the pauli matrices at the point $ ( x , y , z , w ) = ( 1,0,0,0 ) $ .
as in physics in general , a suitable choice of coordinates makes our life so much better . time dilation in this problem is somewhat a more trivial effect , and the transformation of gravitational field is somewhat a more complicated phenomenon . with this in mind , let me reformulate slightly the two situations : case 2 . pendulum is at rest with respect to the earth ( and some observer moves with respect to them , observes time dilation etc etc ) case 1 . pendulum is set above the earth , which moves relativistically below it ( and some observer moves with the earth , observes time dilation etc ) so , let us settle the physics first , and the observer effects last . case 2: classical physics problem , nothing to settle . case 1: from the pendulum 's point of view , the gravitational field is generated by a moving body ( => the field is unknown ) . from the earth frame , a relativistic body moves in a gravity field ( => the equations of motion are unknown ) . one might transform the energy-momentum tensor of the earth from the earth rest frame to the pendulum frame , but special care should be taken about the fact that the earth ceases to be spherical in the new frame ( though its density does increase as $\gamma^2$ ) . additionally , it is not clear appriori that the motion of the earth does not cause any additional forces . i propose to use a straightforward yet more secure method of transforming the metric tensor from the earth frame to the pendulum frame , and hence obtain the gravity , acting on the pendulum . in the earth rest frame the metric tensor is known to be $$g_{\mu\nu}=\left ( \begin{array}{cccc} and 1-2u and 0 and 0 and 0 and \\ and 0 and 1-2u and 0 and 0 and \\ and 0 and 0 and 1-2u and 0 and \\ and 0 and 0 and 0 and -1-2u and \\ \end{array} \right ) , $$ where $u$ is the newtonian potential of the earth . this expression corresponds to the so called weak field limit , when the metric tensor is nearly flat . we use the standard notation of mtw ( $c=1$ , signature $ ( +++ - ) $ , einstein 's summation rule etc ) and refer to this book for further details on linearized gravity . transformation of the field to the pendulum frame : lorentz tranformation matrix is given by : $$ \lambda_{\mu&#39 ; }^{~\mu}=\left ( \begin{array}{cccc} and \gamma and 0 and 0 and \beta \gamma and \\ and 0 and 1 and 0 and 0 and \\ and 0 and 0 and 1 and 0 and \\ and \beta\gamma and 0 and 0 and \gamma and \\ \end{array} \right ) , $$ with $\beta=\dfrac{v}{c} , \gamma= ( 1-\beta^2 ) ^{-1/2}$ and $v$ being the relative velocity of the pendulum with respect to the earth rest frame . the transformed metric tensor is obtained by : $$g_{\mu&#39 ; \nu&#39 ; }=\lambda_{\mu&#39 ; }^{~\mu}\lambda_{\nu&#39 ; }^{~\nu} g_{\mu\nu}=\left ( \begin{array}{cccc} and 1-2u\dfrac{1+\beta^2}{1-\beta^2} and 0 and 0 and -\dfrac{4 u \beta}{1-\beta^2} and \\ and 0 and 1-2u and 0 and 0 and \\ and 0 and 0 and 1-2u and 0 and \\ and -\dfrac{4 u \beta}{1-\beta^2} and 0 and 0 and -1-2u\dfrac{1+\beta^2}{1-\beta^2} and \\ \end{array} \right ) $$ in the pendulum frame ( further primes in the indices are omitted ! ) : it is known that only the term $g_{44}$ determines the newtonian potential . one can see that by writing out the lagrangian for the pendulum : $$ \mathcal{l}=\dfrac{1}{2}g_{\mu\nu} u^\mu u^\nu=\\ =\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2- ( u^4 ) ^2 ) -\\ -u ( ( u^2 ) ^2+ ( u^3 ) ^2+4 u^1 u^4 \beta \gamma^2+ ( ( u^1 ) ^2+ ( u^4 ) ^2 ) \dfrac{1+\beta^2}{1-\beta^2} ) $$ here $u^\mu$ is the 4-velocity of the pendulum . as the latter moves non-relativistically ( in its own frame ) , we may consider $u^4\gg u^1 , u^2 , u^3$ and $u^4\approx \mathrm{const}$ , which leaves : $$ \mathcal{l}=\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2 ) -u ( u^4 ) ^2\dfrac{1+\beta^2}{1-\beta^2} $$ if the pendulum as a whole did not move with respect to the earth , we would have $\beta = 0$ and $$ \mathcal{l}_0=\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2 ) -u ( u^4 ) ^2 $$ effectively , therefore , the pendulum in its rest frame experiences the gravitational field magnified by the factor of $\dfrac{1+\beta^2}{1-\beta^2}$ . the pendulum frequency is thus magnified by $\dfrac{ ( 1+\beta^2 ) ^{1/2}}{ ( 1-\beta^2 ) ^{1/2}}$ . remarks : the neglected terms in the lagrangian are either $\dfrac{v}{c}$ or $ ( \dfrac{v}{c} ) ^2$ smaller than the kept leading terms . hence , up to $\dfrac{v}{c}$ accuracy the direction of motion does not affect the pendulum frequency . finally , lets add time dilations to get the final answers . let the period of the pendulum in the case when observer , the earth , and the pendulum do not move with respect to each other be $t_0$ . then : case 1: in the pendulum frame , as we have seen it has the period of $\dfrac{ ( 1-\beta^2 ) ^{1/2}}{ ( 1+\beta^2 ) ^{1/2}} t_0$ . then in the observer frame , due to time dilation , the period is $\dfrac{1}{ ( 1+\beta^2 ) ^{1/2}}t_0$ . case 2: in the pendulum frame the period is $t_0$ . in the observer frame the period is $\dfrac{t_0}{ ( 1-\beta^2 ) ^{1/2}}$ . to conclude , the two cases are quite different due to the different physics happening . in one case the observed period changes due to the change of the reference frame , whereas in the other there is an additional factor due to the fact that the gravity of a moving source is not the same as that of a still source .
as commenters have pointed out , it is german strecke . note that $s$ is for displacement , whereas $d$ is for distance . distance is the distance along the path traveled by a body , whereas displacement is the birds-eye distance traveled . displacement can also be negative in 1-d , depending upon your reference positive direction . for some reason , strecke actually means distance , not displacement , but its symbol is used for displacement . you might want to check out this paper , it is got an analysis of the naming , mainly for electrodynamic units . a few symbols from the table at the end of the paper : $c$ ( speed of light ) comes from latin celeritas ; $i$ ( current ) comes from " intensity of current " in french ( intensite du courant ) . the $\mathbf{a}$-potential , $\mathbf{b}$-field , $\mathbf{h}$-field got their symbols from the alphabetic order of the others .
you have said : if , for instance , the relative motion observed between two frames of reference is that of uniform acceleration , how can we determine which frame is the unaccelerated system ? it is obviously not possible . and another part of this very question is also : how can we call the occupied frame of reference as being inertial regardless of whether other frames of reference are accelerating with respect to the occupied frame of reference ? both these questions have been answered below . why would it not be possible ? if you are in a reference frame which is accelerating at all , then you will experience pseudo-forces ( forces whose source is not determined in that frame ) . that will tell you that your frame is accelerating . moreover , if the relative motion between two frames is that of uniform acceleration , then both are accelerating ! you do not have to determine which is accelerating ! the presence of acceleration ( uniform or not ) for any reference frame , guarantees that you will experience pseudo-force if you are in it . for example , if you throw a ball from a height , it seems to hit the ground after travelling a path perpendicular to ground . but the actual trajectory is not so . as the ball falls it is deflected due to coriolis force , which is a pseudo-force . so technically the earth is not an inertial frame of reference in any way since we can never point to a source who caused this coriolis force ! you have said : resnick states that the frame of reference he occupies is an unaccelerated one . with respect to what ? if accelerated motion were to be observed with respect to other frames of reference , how are we to determine that we occupy an inertial frame of reference at all ? according to resnick he occupies an inertial frame that means , in his frame , newton 's first law holds true . obviously you need a reference object . when we say a car travels at 75m/s then we actualy mean it travels 75m/s with respect to , say , a stationary tree . but it would travel at 50m/s with respect to another car travelling with 25m/s . so you need a reference object .
the first one is electrostatic potential energy , the second on is electric field . you can tell they are supposed to represent different physical quantities because they have different units . i am pretty sure that the way it is presented in your textbook , the second equation for the electric field is to be seen as justified by experiment and you will derive other things , for example , the first equation , from it .
your example has a tricky issue involving angular momentum ( see below ) , but i can address the spirit of the question using a much simpler example . let us imagine we have a chamber containing two gases , $a$ and $b$ , such that $a$-$a$ interaction terms are equal to both the $b$-$b$ interaction terms and the $a$-$b$ interaction terms . ( it does not hurt to imagine the molecules as red and blue spheres of the same size and weight . ) now we imagine swapping some of the $a$ molecules with $b$ molecules in such a way that the $a$ molecules all end up on one side of the chamber and all the $b$ molecules on the other . due to the assumption above , swapping two particles does not change the total energy , but by doing this we have created an ordered system from which work can be extracted . ( to extract work from this system you need a piston that is permeable to $a$ molecules but not $b$ molecules , and another piston that is only permeable to $b$ but not $a$ . see here for details on how , as well as some other relevant stuff about the relationship between work and entropy . ) now , since we can extract work from this system , will its weight change due to the $e=mc^2$ relation ? perhaps surprisingly , the answer is no . this is because the relevant $e$ is the internal energy of the system ( usually written $u$ in thermodynamics ) , and that has not changed . the work that can be extracted from a system at a constant temperature is given by $u-ts$ . by reordering the atoms we have reduced $s$ but kept $u$ constant . when we extract work from this ordered system , its internal energy $u$ also stays constant , but the energy of the environment reduces . effectively , we take heat out of the environment and convert it to work . normally this is not allowed by the second law , because converting heat into work would cause a reduction in entropy - but through a clever use of semi-permeable pistons we can offset that reduction in entropy by an increase in the entropy of the gas mixture . the point is that the $s$ term represents the disorder ( or , more correctly , it represents the opposite of information - see the paper linked above ) and the $u$ term represents the energy . the mass of an object depends only on the $u$ term and not on the $s$ term , so order and mass/energy are actually quite independent things . the tricky thing with your example is that although you keep the energy constant , changing the velocities in the way you describe adds angular momentum to the system . when momentum is involved , $e=mc^2$ is no longer strictly valid , and you have to use the full energy-momentum relation $$ e^2 = ( pc ) ^2 + ( mc^2 ) ^2 , $$ where $p$ is the relativistic momentum . i do not know how to do this for your example . it may or may not be the case that the effective mass would change . however , if it does change it is because the momentum has changed and not because the system has become more ordered .
i assume you are looking at a diode laser ( ld ) data sheet . in practical terms , spectral width is a measure of tunability of the ld as you vary injection current and temperature . this is quite useful in experiments ( say atomic physics with alkali atoms ) . the linewidth is related to the phase noise of laser . it is very complicated to derive the linewidth from first principles . agarwal is a standard reference if you wish to know more . a practical semi-conductor ld has a typical free running linewidth of 40mhz . this means , the uncertainty in the frequency is 40mhz . however , this entire frequency band can shift with changes in temperature and fluctuations in injection current . this rather broad linewidth can be narrowed by optical feedback to obtain the so called external/extended cavity diode laser ( ecdl ) . check out this seminal paper by weimann and hollberg ( pdf ) for more information . i have built many ecdl systems from the ground up with sub-mhz linewidths . it can be quite a tricky endeavor . measuring the linewidth can be quite tricky . the usual direct technique is to beat the unknown laser with a known standard reference and study the beat signal . the equipment necessary is very expensive . however , you can make rough estimates from say a simple saturation spectroscopy based frequency locking setup by studying the amplitude fluctuations of the " error signal " . granted that it is crude and limited to the natural linewidth of your atomic system , but it does not need super expensive equipment or complicated electronics . there are other ways to use atomic physics ( eit , cpt resonances etc ) to make reasonable estimates of laser linewidth and it all depends on what your goal is .
the true eigenstates , when they exist , do not decay . they sit and spin around in phase forever . but atomic eigenstates are not true eigenstates . the reason atomic states decay is because they are coupled to photon states , and the combined photon-atom hamiltonian does not have excited atom eigenstates . when you have an atom in a box of mirrors , there are true eigenstates of the combined photon-atom system inside the box . these are states where a quantum of energy is absorbed by the atom , remitted into the box , in a steady way , so that it is sometimes in the atom , sometimes in the photons of the box . but when you make the box big , the energy will be in the photons nearly all the time , and the atom will be in its ground state , just because there are infinitely many more photon states than atomic states . in the limit of no box , the excited atomic states are never true eigenstates , they always decay into photons irreversibly . this process was described by fermi , and the rate of irreversible decay is given by fermi 's golden rule . for atoms and radiation , the coupling is mostly by a term in the hamiltonian equal to $p\cdot a ( x ) $ , where p is the momentum of the electron and a is the vector potential at the position of the electron , plus a direct two-photon term $a ( x ) ^2$ which you can usually ignore . you evaluate the transition by expanding a in plane waves , the coefficients of which are photon creation operators , and approximating the exponential of the x operator by the first two terms of a taylor expansion . this is called the dipole approximation . the resulting hamiltonian describes transitions between the pure atom stationary states into states of the atom plus a photon , and for long times , the transitions conserve energy , so that the outgoing photon carries the energy difference that is lost by the decay . the dipole approximation is essentially exact for transitions which are dipole-allowed because the atomic motion is nonrelativistic , so that the wavelength of the light is enormous compared to the atom . the result is that there are small matrix elements for transitions between the states , accompanied by creating one photon , and these give the dipole atomic transitions . this is worked out in sakurai 's book , among others .
it sounds as if you are describing flow through an orifice plate . in that case see this wikipedia article for how to calculate the flow rate or google for something like " gas flow orifice " . there are various web sites with flow rate calculators e.g. this one . we are not supposed to just give links as an answer , but the formulae for calculating the flow are quite complicated and i am not sure what would be achieved by copying and pasting them here .
1 ) notice that by inserting a complete set of position states we can write $$ \hat p \psi ( x ) = \langle x|\hat p|\psi\rangle = \int dx'\langle x|\hat p|x'\rangle\langle x'|\psi\rangle =\int dx'\langle x|\hat p|x'\rangle \psi ( x' ) $$ so if we set $$ \langle x|\hat p|x'\rangle = -i\hbar \frac{\partial}{\partial x}\delta ( x-x' ) =i\hbar \frac{\partial}{\partial x'}\delta ( x-x' ) $$ then we can use integration by parts to obtain $$ \hat p \psi ( x ) =i\hbar \int dx'\frac{\partial}{\partial x'}\delta ( x-x' ) \psi ( x' ) = -i\hbar \int dx'\delta ( x-x' ) \frac{d \psi}{dx'} ( x' ) = -i\hbar \frac{d\psi}{dx} ( x ) $$ so your expression is correct . the derivative of a delta function is essentially defined by the integration by parts manipulation that i just performed ; in fact derivatives of distributions in general are defined in an analogous way . see this lecture for example . hope that helps ; let me know of any typos ! cheers !
there is no unique way to " further decompose " the deformation into the " rigid transformation " and " others " because whatever " rigid part " you choose , you may always calculate " others " as a simple difference ( that is because there is really no global constraint on the " other " part ) . so the " rigid part " may be anything you want .
volume increase in the system is due to work done by the system . therefore w is negative using your notation . think of it this way , work done on the system would push the system inwards , decreasing volume . therefore a volume increase is work done by the system . alternatively you could reason using the formula : $du = q - dw$ ( using your notation conventions , were $u$ is internal energy , $w$ is work and $q$ is heat added to the system ) $dw = pdv . $ therefore $du = q - pdv$ . therefore if $dv$ ( change in volume ) is positive , $du$ ( change in internal energy ) is negative .
in the representation $|\psi\rangle = a|0\rangle + b|1\rangle $ we must have $|a|^2+|b|^2=1$ , so that gives us one constraint . the second is that an overall global phase does not make any difference . we can use these two freedoms to chose $a$ and $b$ in a specific way . traditionally we choose them such that $$|\psi\rangle = \cos \theta|0\rangle+\exp ( i\phi ) \sin \theta|1\rangle $$ see this wiki article on the bloch sphere for details .
one way to compute the electrostatic energy stored into a plate capacitor is simply to calculate it as : $e_{el} \equiv \int d^3r \: \frac{1}{2}\vec{d}\cdot\vec{e}$ which would be simply : $e_{el} \equiv \int d^3r \: \frac{\epsilon_0}{2}\vec{e}^2$ in vacuum . i would just consider the energy difference between before and after insertion of the dielectric medium
let 's speak about 1d particles for simplicity . what should be understood first of all is that for indistinguishable particles configuration space is not the same as for distinguishable ones . for two distinguishable spinless 1d particles configuration space is a square : one side is for $x_1$ , another for $x_2$ . but if the particles appear indistinguishable , then half of the space is redundant : states obtained by exchange of particle coordinates are identical . so , for two spinless particles configuration space is really a triangle : here lower purple triangle is the config space . now , schrödinger equation for two indistinguishable spinless 1d particles becomes trivially to obtain from that for distinguishable particles : we just have to impose boundary conditions for $x_1=x_2$ line . for bosons wavefunction must be symmetric , and this implies homogeneous neumann conditions . for fermions wavefunction must be antisymmetric , so it has a node at the line of interparticle collision , and this means homogeneous dirichlet conditions . now for fermions this automatically gives us the correct eigenstates , which indeed obey pauli exclusion principle — they just can not have nonzero values on $x_1=x_2$ line . here're first several 1 : if we now want to return to our full ( square ) configuration space , we should just append zeroes to missing triangle , and then subtract the same wavefunction but with exchanged particles from it . it will automatically appear differentiable at $x_1=x_2$ line : now let 's include spin in our picture . thanks to spin being a discrete degree of freedom with finite range of values , for single particle we can just concatenate its configuration space parts corresponding to different spin values . for example , a state of a spin-$\frac12$ particle in infinite box in state $\left|1\uparrow\right\rangle+\left|3\downarrow\right\rangle$ could look like this : here left part corresponds to state of spin-up component of wavefunction , and right one is for spin-down component . note that the hamiltonian should not try to differentiate the wavefunction at the joint — as far as we neglect spin-dependent interactions , its matrix should just be a diagonal block matrix of two spinless hamiltonians . now for two indistinguishable particles nothing changes except we now use 4 times bigger full configuration space ( than in case without spin ) , or 2 times bigger correct triangle-shaped space . here 's how full configuration space would look : here spin states are denoted as $\left|s_1\right\rangle\left|s_2\right\rangle$ where $s_i$ is spin of particle $i$ . lines of interparticle collisions are denoted by slight dark color . now the truncated symmetrized configuration space : the upper-left rectangle is now merged with lower-right one . how — it will depend on the state . also note that there is still possibility of interparticle collision — inside this lower-right rectangle — which we have not symmetrized , and in fact must not do ( there is no reason to do it in this case ) . now it is easy to see that the states with identical spins are forced to have antisymmetric orbital — their orbital parts of configuration space are triangular . the states with non-equal spins may be symmetric or antisymmetric ( or asymmetric at all ) . this corresponds to known classification of two-particle states into spin-singlets and spin-triplets . just looking at the wavefunction , one could immediately say what type of state we have : triplet states will have a node along interparticle collision line — this means the orbital is antisymmetric . let 's now look at first several states : here we can see : spin-singlet states — numbers 1,2,6 , and triplets — numbers 3,4,5 . for more than two particles it seems quite straightforward to generalize : just truncate the configuration space so that it is no longer possible to exchange particles , and impose the correct boundary conditions on the hypersurfaces which appear after these cuts . for many particles this may even give some computational resource savings : hypervolume of configuration space would reduce by $n ! $ where $n$ is number of particles , as compared to space for distinguishable particles . 1: i used simplified way of computing these states , just putting very large potential barrier in the upper-left triangle . to decouple different spin parts of wavefunctions i also added thin barriers . in real computation one should , of course , use the opportunity to remove extra data from processing and more correctly define the hamiltonian matrix .
that is a really good question . you are right that measuring the tranverse velocity is a very difficult measurement , mostly due to andromeda 's distance from the sun . the problem can be tackled in two ways : directly , and indirectly . direct measurements mean actually tracking a positional change between andromeda and even more distant objects assumed to be essentially at rest , like quasars . the recent discovery of water masers mentioned above should make this possible ; a transverse velocity of ~100 km/s is an angular shift on the order of 10 microarcseconds per year . this is much smaller than is possible with optical telescopes ; the extreme baselines of radio telescopes like the very long baseline array , however , do make direct measurements feasible . these observations are currently taking place , and we should have a published measurement within a couple of years . indirect measurements of andromeda 's transverse velocity use a few different techniques . the loeb et al . ( 2005 ) paper made their estimate based on the fact that m33 , a neighboring galaxy to andromeda , shows no sign that its stellar population has been disturbed by passing nearby andromeda . this constrains the possible range of directions and speeds of andromeda 's velocity . they combine this with data on m33 's orbit , plus simulations of how close the galaxies would have to be to show an effect , and estimate both a direction ( mostly eastward ) and speed ( $100 \pm 20$ km/s ) of andromeda 's proper motion . a second indirect method was published by van der marel and guhathakurta in 2008 ; they used information on the orbits of satellite galaxies orbiting m31 to estimate the center of mass ( or barycentre ) of our local group . since the position and velocity of the local group barycentre depend partially on m31 's orbit , they also estimated a transverse velocity . their result is -78 km/s w , -38 km/s n . the upcoming direct measurement of m31 's proper motion should answer which ( if either ) of these other estimates are correct . in addition , we are looking forward to answering several interesting questions regarding both the past and future of our local group of galaxies . stay tuned !
yes . it turns out that your $t_l$ is equal to $-t/\omega$ , where $\omega$ is the angular velocity and $t$ is the usual temperature . we normally work with the reciprocals of such quantities , and in the language of non-equilibrium thermodynamics we say that a gradient in $-\omega/t$ is the " thermodynamic force conjugate to " a flow of angular momentum . within the formalism of thermodynamics itself there is indeed nothing special about energy . ( there is , however , quite a lot that is special about energy when it comes to mechanics . ) however , the usual terminology and notation obscures this quite a bit . we usually write the fundamental equation of thermodynamics with the energy on the left-hand-side , like this : $$ du = tds - pdv + \sum_i \mu_i dn_i . $$ this equation can be extended with many other terms , including $\phi dq$ ( electric potential times change in charge ) and $\omega dl$ ( angular velocity times change in angular momentum ) . however , the " special " quantity here is the entropy , $s$ , which is non-decreasing while all the other extensive quantities are conserved . we can rearrange this to put the special quantity on the left , and to get $$ ds = \frac{1}{t} du + \frac{p}{t}dv - \sum_i \frac{\mu_i}{t}dn_i + \dots - \frac{\phi}{t}dq - \frac{\omega}{t} dl . $$ this observation is the basis of non-equilibrium thermodynamics . it follows immediately from this that $$ \frac{\partial s}{\partial l} = -\frac{\omega}{t} . $$ it also follows that angular momentum cannot be spontaneously transferred from one body to another while keeping all other quantities constant unless the second body has greater $-{\omega}/{t}$ . however , that " while keeping other quantities constant " is a bit tricky . in just about any reasonable situation , adding angular momentum to a system will also change its energy . the same is true of changes in volume , chemical composition or charge : changing these things will , generally speaking , also change the energy . this is probably the main historical reason why energy is seen as special in thermodynamics : it is the only thing you can practially change while keeping everything else constant . ( we call this " heating up " or " cooling down " a system . ) so while it is quite possible to define angular-momentum analogues of heat , free energy and the carnot limit , these do not tend to have the same immediate practical applications as the energy-based versions . nevertheless , i think the existence of such quantities is an enlightening and often-overlooked observation . i would encourage you to keep on thinking along these lines , since understanding the symmetry between energy and the other conserved quantities leads to a deeper understanding of thermodynamics as a whole .
the rule is that the total momentum of an isolated system is constant . in the example with the car and the ferry , the isolated system is the system consisting of the car and the ferry . the system is isolated because it is assumed that there is no interaction ( such as drag ) between the ferry and the water . then you could imagine that the car and ferry are stationary with respect to each other . then as the car begins to move and gains momentum in one direction , the ferry gains momentum in the other direction . conservation of momentum guarantees that the car and ferry momenta add to zero so that the total momentum is zero . it is important to notice that the rule is not that total momentum is zero , but that total momentum is conserved . so if the car and ferry were initially drifting with some speed , then when the car starts moving across the ferry , the car will gain momentum and the ferry will lose momentum but the total momentum will remain at its initial non-zero value . also notice that in a more realistic model , you would include the drag force from the water . then the momentum of the car/ferry system would not be conserved ( which is allowed because it is no longer an isolated system ) . but if you add in the momentum of the water , you will find that still the total momentum is conserved .
you need to know the rate of heat given by the flame to the water . suppose the flame transfers $h$ kj/s to the water . the latent heat of evaporation of water is $2260$ kj/kg for energy balance , the heat given to the water must be equal to the amount of heat required to convert water into steam . $$ h = \dot{m} \times 2260 \\ \therefore \dot{m} = \frac{h}{2260} \text{ kg/sec} $$ if you say the rate of heat transfer does not matter ( ignore the flame and assume the water is at a certain temperature and stays at that temperature throughout the experiment ) , $$ \dot{m} = \frac{\theta a ( x_s - x ) }{3600} \text{ kg/s} $$ where $\theta = ( 25 + 19 v ) $ is the evaporation coefficient ( $kg/m^2\cdot hr$ ) . this is an empirical equation , so you can not derive it from first principles . $v$ is the velocity of air just above the surface of the water ( $m/s$ ) $a$ is the surface area of the water ( $m^2$ ) $x_s$ is the humidity ratio in saturated air at the same temperature as the water surface ( kg h2o in kg dry air ) $x$ is the humidity ratio in the air ( kg h2o in kg dry air ) it is fairly straightforward to find $x$ and $x_s$ from the relative humidity and the mollier chart
the term " rosenberg-coleman effect " originates from the article heliographic latitude dependence of the dominant polarity of the interplanetary magnetic field . it is also referred to as the " dominant polarity effect " . as the earth orbits the sun , the earth travels above and below the equator of the sun . according to rosenberg and coleman , the polarity of the interplanetary magnetic field ( imf ) at a given location in the solar system , such as earth , depends upon the corresponding latitude of the sun . according to this proposal , the imf at earth should be dominated by the southern solar pole from december 7th to june 6 , and the northern pole the other half of the year .
gold foil is quite easy to hold you just hang it from a paperclip . the only difficulty is if there is a lot of static electricity in the air which makes it stick to things . ( this is the main reason for the cold damp cambridge 's supremacy in early particle physics ) photographic film at the time was not sensitive and so in marsden and geiger 's experiments they used a tiny target coated with phosphorescent crystals which emitted a spark of light when hist by a particle and viewed them through a microscope . this involved sitting in a dark cupboard for hours straining to see and count rare pinpoints of light while staring down a primitive microscope - this is why the job was given to grad students . edit : there is a very good book , the fly in the cathedral which describes this period , the science and the experimental techniques . high vacuum stuff is a pain the ___ even with modern quick connects and helium leak detectors , doing it with string and sealing wax ( literally ) was quite a challenge .
in general physics course we assume maxwell 's equations as the result of many experiments . after that , in field theory we build the lagrangian which satisfies the maxwell equations . we also can build non-linear field theories of em interactions , but there is a requirement to getting of the maxwell equations in the limit of weak fields . so these methods are not connected with the derivation . but we can derive the equations by using some postulates , which generalize experimental facts . the number of postulates should be reduced to a minimum . maxwell 's equations can be earned from the coulomb 's law , special relativity theory and superposition principle ( more details you can see in my answer on this question ) . the other question is why do we need derivation of equations instead of postulating them . they satisfy the experiments , so that is enough for using them in practical cases . the postulating them is not much worse then deriving in this situation .
they will equalize pressure at the entrance to the tube between them . that pressure is the density of the fluid times the height from the bottom to the lowest point in the vortex , because the fluid at the lowest point has zero velocity and so is equivalent to standing fluid .
physics is independent of our choice of units and for something like a length plus a time , there is no way to uniquely specify a result that does not depend on the units you choose for the length or for the time . any measurable quantity belongs to some set $\mathcal{m}$ . often , this measurable quantity comes with some notion of " addition " or " concatenation " . for example , the length of a rod is $l \in \mathcal{l}$ a measurable quantity . you can define an addition operation $+$ on $\mathcal{l}$ by saying that $l_1 + l_2$ is the length of a the rod formed by sticking rods 1 and 2 end-to-end . the fact that we attach a real number to it means that we have an isomorphism $$ u_{\mathcal{m}} \colon \mathcal{m} \to \mathbb{r} , $$ in which $$ u_{\mathcal{m}} ( l_1 + l_2 ) = u_{\mathcal{m}} ( l_1 ) + u_{\mathcal{m}} ( l_2 ) . $$ a choice of units is essentially a choice of this isomorphism . recall that an isomorphism is invertible , so for any real number $x$ you have a possible measurement $u_{\mathcal{m}}^{-1} ( x ) $ . i am being fuzzy about whether $\mathbb{r}$ is the set of real numbers or just the positive numbers ; i.e. whether these are groups , monoids , or something else . i do not think it matters a lot for this post and , more importantly , i have not figured it all out . now , since physics should be independent of our choice of units , it should be independent of the particular isomorphisms $u_q$ , $u_r$ , $u_s$ , etc . that we use for our measurables $q$ , $r$ , $s$ , etc . a change of units is an automorphism of the real numbers ; given two units $u_q$ and $u'_q$ , the change of units is $$ \omega_{u , u'} \equiv u'_q \circ u_q^{-1}$$ or , equivalently , $$ \omega_{u , u'} \colon \mathbb{r} \to \mathbb{r} \ni \omega ( x ) = u'_q ( u_q^{-1} ( x ) ) . $$ therefore , $$ \omega ( x+y ) = u'_q ( u_q^{-1} ( x+y ) ) \\ = u'_q ( u_q^{-1} ( x ) +u_q^{-1} ( y ) ) \\ = u'_q ( u_q^{-1} ( x ) ) + u'_q ( u_q^{-1} ( y ) ) \\ = \omega ( x ) + \omega ( y ) . $$ so , since $\omega$ is an automorphism of the reals , it must be a rescaling $\omega ( x ) = \lambda x$ with some relative scale $\lambda$ . consider a typical physical formula , e.g. , $$ f \colon q \times r \to s \ni f ( q , r ) = s , $$ where $q$ , $r$ , and $s$ are all additive measurable in the sense defined above . give all three of these measurables units . then there is a function $$ f \colon \mathbb{r} \times \mathbb{r} \to \mathbb{r} $$ defined by $$ f ( x , y ) = u_s ( f ( u_q^{-1} ( x ) , u_r^{-1} ( y ) ) . $$ the requirement that physics must be independent of units means that if the units for $q$ and $r$ are scaled by some amounts $\lambda_q$ and $\lambda_r$ , then there must be a rescaling of $s$ , $\lambda_s$ , such that $$ f ( \lambda_q x , \lambda_r y ) = \lambda_s f ( x , y ) . $$ for example , imagine the momentum function taking a mass $m \in m$ and a velocity $v \in v$ to give a momentum $p \in p$ . choosing $\text{kg}$ for mass , $\text{m/s}$ for velocity , and $\text{kg}\ , \text{m/s}$ for momentum , this equation is $$ p ( m , v ) = m*v . $$ now , if the mass unit is changed to $\text{g}$ , it is scaled by $1000$ , and if the velocity is changed to $\text{cm/s}$ , it is scaled by $100$ . unit dependence requires that there be a rescaling of momentum such that $$ p ( 1000m , 100v ) = \lambda p ( m , v ) . $$ this is simple -- $10^5 mv = \lambda mv$ and so $\lambda = 10^5$ . in other words , $$ p [ \text{g} \ , \text{cm/s} ] = 10^5 p [ \text{kg} \ , \text{m/s} ] . $$ now , let 's consider a hypothetical situation where we have a quantity called " length plus time " , defined that when length is measured in meters and time in seconds , and " length plus time " in some hypothetical unit called " meter+second " , the equation for " length plus time " is $$ f ( l , t ) = l + t . $$ this is what you have said - $10 \text{ m} + 5 \text{ s} = 15 \text{ " m+s"}$ . now , is this equation invariant under a change of units ? change the length scale by $\lambda_l$ and the time scale by $\lambda_t$ . is there a number $\lambda$ such that $$ f ( \lambda_l l , \lambda_t t ) = \lambda_l l + \lambda_t t $$ is equal to $$ \lambda f ( l , t ) = \lambda ( l+t ) $$ for all lengths and times $l$ and $t$ ? no ! therefore , this equation $f = l + t$ cannot be a valid representation in real numbers of a physical formula .
the saturn v payload mass to leo was 118,000 kg . wikipedia has a decent comparison of super-heavy launch systems with a payload mass to leo of 50,000 kg or more . none are in current use , and only two systems are in development . there is also a " heavy " lift launch system list which includes the delta iv and ariane 5 you mentioned . the top operational system is the atlas v hlv with a mass to leo of 29,420 kg and a mass to gto of 13,000 . however , it has never been launched and the united launch alliance claims it needs a 30 month lead-time to produce the heavy launch vehicle variant of the atlas v . next on the list with mass to leo/gto : delta iv heavy : 22,950/12,980 kg , 3/4 successful launches . proton : 21,600/6,360 kg ( comparatively lower gto due to launch location ) , 295/335 successful launches ariane 5: 21,000/10,050 kg , 54/58 successful launches . so the answer is there are no currently operational launch systems which approach the saturn v mass to leo capability .
it is a matter of flux . two factors enter the ability to detect gammas . the flux of the gammas , i.e. . how many per meter square per second , and the crossection of interaction . the crossection is dependent on the energy , and for a given energy is the same for extraterrestrial and terrestrial gamma rays . the flux is not . gamma rays no matter how high was the flux when they were created reach us from large to enormous distances , the flux spreading like 1/r^2 from the distant point ( for us ) source . terrestrial gamma rays are close to the satellites in comparison and the flux much higher than the extra galactic one . the probability of finding terrestrial gammas is measurable , a substantial number survives the trip through the atmosphere . extra terrestrial ones are few and the probability of surviving to the surface very small to zero .
consider a tiny part of th conductor 's surface . then the field at this part is approximately uniform so this is like an infinite parallel plane : $e = \sigma/2\epsilon_0$ . whence , the surface charge density is $\sigma = 2\epsilon_0 e$ . since it is a conductor , there is no volumetric charges : everything is concentrated in the surface .
they are stating that as part of the question , not stating that it is necessarily true . you often see things like " ignoring air resistance " , a " frictionless plane " or " massless spring " as part of questions to allow a simple analytical answer . in reality for a car moving at 40 m/s air resistance is likely to be the major source of drag and this is proportional to velocity^2 , but this makes the resulting equations trickier
first of all , velocity has a sign . after rebounding the velocity is in the opposite direction so $\delta v = ( 25 - ( -22 ) ) = 47 m/s$ $47 m/s / 0.0035 s = 1,342.9 m/s^2$ [ correction $13,429 m/s^2$ ]
at the particle level the verb " charge " has no definition . charge cannot be added to a particle . a particle has charge ( noun ) ; it is a quantum number that characterizes the particle , and its charge may be 0 , +/-1/3 , +/-2/3 , +/-1 ( and some resonances +/-2 ) . a photon has charge 0 , spin 1 and mass 0 . that is why it is called a photon and not an electron . if it is possible to bend it than why not charge that it can change direction ( bend ) is a kinematic effect and controlled by the equations of motion . the quantum numbers are intrinsic and unchangeable in the definition of each particle .
any reshaping of the droplet will require flow of water inside the droplet and there will be viscous losses . presumably the energy would come from an increased torque on whatever motor was moving the droplet and substrate .
the determinant is fairly easy to calculate . you know already , essentially , the eigenvalues of the stiffness matrix ; more accurately , you know the eigenvalues of the matrix $\mathbf{m}^{-1}\mathbf{k}$ , because the $\omega_i$ are zeros of the equation $$0=\det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) . $$ ( the more aesthetically minded would replace $\mathbf{m}^{-1}\mathbf{k}$ with $\mathbf{m}^{-1/2}\mathbf{k}\ , \mathbf{m}^{-1/2}$ to get a hermitian matrix , but no matter . ) if you express the second determinant in the corresponding eigenbasis , you get $$ \det ( \mathbf{k}-\mathbf{m}\ , \omega^2 ) =\det ( \mathbf{m} ) \det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) =\frac{m^3}2\det\begin{pmatrix} \omega_1^2-\omega^2 and 0 and 0 \\ 0 and \omega_2^2-\omega^2 and 0 \\ 0 and 0 and \omega_3^2-\omega^2 \end{pmatrix} , $$ which gives your textbook 's expression . more generally , this is an expression of the principle that a matrix 's determinant is the product of its eigenvalues . the adjunct , on the other hand , does not ( to my knowledge ) satisfy any such nice relation ; in any case it is a nasty beast to deal with and i think few people judiciously substituting in the definition $k=m\omega_2^2/2$ of $\omega_2$ instead of $k$ .
here is a very basic estimation : the kinetic energy of a 1000 kg car moving at 60 km/h is $$e=\frac{mv^{2}}{2}=\frac{1000kg ( 16.7m/s ) ^{2}}{2}=138.9 kj$$ the heat of gasoline combustion is 47 mj/kg = 35000 kj/litre . assuming 10% efficiency of the car 's engine , you would need to burn $$\frac{138.9 kj}{0.1\cdot35000kj/l}=0.04 litre$$ of gasoline to accelerate the car to a given speed which would cost you 5 cents .
infinity is a mathematical term , very useful , but the history of physics has shown us that when we make mathematical extrapolations that lead to infinities of one sort or another , a different mathematical model will eliminate those infinities ( call me quantum mechanics ) . in thermodynamics the black body radiation leads to the ultraviolet catastrophe , and quantum mechanics saves the day . in classical electromagnetism , a point like electron would tend to an infinite potential at ( 0,0,0 ) as it goes with 1/r . quantum electrodynamics saves the day . that is because quantum mechanics has inherent probabilistic indeterminacies when sizes become of order of h ( the planck constant ) . even though elementary particles are postulated as point particles , they are not classical particles , the wave/particle duality saves the day , so the minimum volume would be of dimensions compatible with h in the variables examined and the measurement methods used . once gravity is quantized , the set will be complete , taking care of minimum black hole volumes too , in a similar way .
most of the subducted crust is sea floor , which does not normally contain oil or gas . at this point someone is bound to mention north sea oil/gas , but the north sea floor is actually more like continental rock than basalt sea floor , and it is not being subducted . anyhow , even if gas or oil was subducted it is not going to explode because there is not any oxygen available for it to burn . if it happened i would guess that the oil/gas would get dissolved in molten rock and eventually emerge from volcanos .
well , it can not ( float ) , since a black hole is not a solid object that has any kind of surface . when someone says that a super massive black hole has less density than water , one probably means that since the density goes like $\frac{m}{r^3}$ where m is the mass and r is the typical size of the object , then for a black hole the typical size is the schwarzschild radius which is $2m$ , which gives for the density the result $$\rho\propto m^{-2}$$ you can see from that , that for very massive black holes you can get very small densities ( all these are in units where the mass is also expressed in meters ) . but that doesn’t mean anything , since the black hole doesn’t have a surface at the schwarzschild radius . it is just curved empty space .
if there was no air in the bus you had be right ( but then what would the mosquito be flying in ? ) . when the bus travels , it pushes the air inside of the bus too so the air moves at the same speed as the bus . the mosquito is flying in this air so it is getting pushed by the air and it moves relative to the bus . this is just like when you are sitting in a car and it starts to accelerate . the seat behind and under you pushes you so you move at the same speed as the car .
the x axis there is not time , it is just six bins . each pair of bins is labeled by year , and can be assumed to represent all the data taken during that calendar year ( or to-date in the case of 2011 ) . that is going to be multiple periods of " running " each year , with varying beam intensities and both regularly scheduled and unexpected beam stops during the running periods . you can probably find the beam logs for the sps with enough digging around cern 's websites ( and possibly emailing ) .
yes , it is possible . a static setup like this will work as long as any small motion of the parts would increase the potential energy . in this case , it looks like there is only one possible motion - rotation of the entire ruler-hanger-hammer piece about the axis where the ruler touches the table . if the ruler were to rotate down a little bit , the entire hammer would go down some , decreasing its potential energy . however , it would also rotate , raising the head , where most of the mass is , and thus increasing the potential energy . when these two effects cancel , the system will be in equilibrium . to figure out when the effects will cancel , we could find the system 's center of mass . if the center of mass is directly under the contact point , the system 's in equilibrium . this is because as the system rotates , the center of mass moves in a circle . to be in equilibrium , it must be at the bottom of that circle . the center of the circle is at the contact point , so the bottom of the circle is directly beneath it . there are other , equivalent ways of doing the same problem . for examples check out these questions : why does the weighing balance restore when tilted and released equilibrium and movement of a cylinder with asymmetric mass centre on an inclined plane
this paper details the mathematical model behind what you are doing . by far the most difficult aspect is not the balloon itself but collisions with the environment : http://arxiv.org/pdf/physics/0407003.pdf
let me begin with the second question where you do not change the dimensionality , just the volume . the entropy never decreases when you actually compress gas . the compression means that the walls are mostly moving against the colliding molecules which means that they are recoiled backwards at higher velocities . the molecules ' kinetic energy increases so they occupy a larger volume in the momentum space ( in macroscopic language , a gas heats up while being compressed ) which at least compensates the decrease of the volume in the position space . the other answer is incorrect . the second laws says not only that systems exhibit some activity indicating that they do not like a decreasing entropy ; instead , it says that whatever activity physical systems display , they will never achieve a macroscopic decrease of the entropy . it is just impossible . to compress gas by 70% is possible , to decrease the entropy by a macroscopic amount is not . now , the interesting first question . if you could change the effective dimensionality , it would still be true in any consistent theory that the entropy can not decrease . so if your theory were just able to add dimensions like that while keeping a molecule in a sphere of the increasing dimension , the second law of thermodynamics would imply that such an addition of dimensions is not physically possible – it would be another , more sophisticated example of the perpetual motion machine of the second kind . in some sense , it is true that the second law encourages physical systems to lose the dimensions ( a way to increase the entropy , given your formula for the higher-dimensional spherical volumes ) . when the energy dissipates , the energy per degree of freedom effectively goes down which allows us to use a lower-dimensional " effective " description . for example , a gas full of kaluza-klein particles probing ( moving in ) extra dimensions will tend dissipate its energy and decay to many lower-energy quanta which are effectively living just in 3+1 dimensions .
you should not think of the schrödinger equation as a true wave equation . in electricity and magnetism , the wave equation is typically written as $$\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \frac{\partial^2 u}{\partial x^2}$$ with two temporal and two spatial derivatives . in particular , it puts time and space on ' equal footing ' , in other words , the equation is invariant under the lorentz transformations of special relativity . the one-dimensional time-dependent schrödinger equation for a free particle is $$ \mathrm{i} \hbar \frac{\partial \psi}{\partial t} = -\frac{\hbar^2}{2m} \frac{\partial^2 \psi}{\partial x^2}$$ which has one temporal derivative but two spatial derivatives , and so it is not lorentz invariant ( but it is galilean invariant ) . for a conservative potential , we usually add $v ( x ) \psi$ to the right hand side . now , you can solve the schrödinger equation is various situations , with potentials and boundary conditions , just like any other differential equation . you in general will solve for a complex ( analytic ) solution $\psi ( \vec r ) $: quantum mechanics demands complex functions , whereas in the ( classical , e and m ) wave equation complex solutions are simply shorthand for real ones . moreover , due to the probabilistic interpretation of $\psi ( \vec r ) $ , we make the demand that all solutions must be normalized such that $\int |\psi ( \vec r ) |^2 dr = 1$ . we are allowed to do that because it is linear ( think ' linear ' as in linear algebra ) , it just restricts the number of solutions you can have . this requirements , plus linearity , gives you the following properties : you can put any $\psi ( \vec r ) $ into schrödinger 's equation ( as long as it is normalized and ' nice' ) , and the time-dependence in the equation will predict how that state evolves . if $\psi$ is a solution to a linear equation , $a \psi$ is also a solution for some ( complex ) $a$ . however , we say all such states are ' the same ' , and anyway we only accept normalized solutions ( $\int |a\psi ( \vec r ) |^2 dr = 1$ ) . we say that solutions like $-\psi$ , and more generally $e^{i\theta}\psi$ , represent the same physical state . some special solutions $\psi_e$ are eigenstates of the right-hand-side of the time-dependent schrödinger equation , and therefore they can be written as $$-\frac{\hbar^2}{2m} \frac{\partial^2 \psi_e}{\partial x^2} = e \psi_e$$ and it can be shown that these solutions have the particular time dependence $\psi_e ( \vec r , t ) = \psi_e ( \vec r ) e^{-i e t/\hbar}$ . as you may know from linear algebra , the eigenstates decomposition is very useful . physically , these solutions are ' energy eigenstates ' and represent states of constant energy . if $\psi$ and $\phi$ are solutions , so is $a \psi + b \phi$ , as long as $|a|^2 + |b|^2 = 1$ to keep the solution normalized . this is what we call a ' superposition ' . a very important component here is that there are many ways to ' add ' two solutions with equal weights : $\frac{1}{\sqrt 2} ( \psi + e^{i \theta} \phi ) $ are solutions for all angles $\theta$ , hence we can combine states with plus or minus signs . this turns out to be critical in many quantum phenomena , especially interference phenomena such as rabi and ramsey oscillations that you will surely learn about in a quantum computing class . now , the connection to physics . if $\psi ( \vec r , t ) $ is a solution to the schrödinger 's equation at position $\vec r$ and time $t$ , then the probability of finding the particle in a specific region can be found by integrating $|\psi^2|$ around that region . for that reason , we identify $|\psi|^2$ as the probability solution for the particle . we expect the probability of finding a particle somewhere at any particular time $t$ . the schrödinger equation has the ( essential ) property that if $\int |\psi ( \vec r , t ) |^2 dr = 1$ at a given time , then the property holds at all times . in other words , the schrödinger equation conserves probability . this implies that there exists a continuity equation . if you want to know the mean value of an observable $a$ at a given time just integrate $$ &lt ; a&gt ; = \int \psi ( \vec r , t ) ^* \hat a \psi ( \vec r , t ) d\vec r$$ where $\hat a$ is the linear operator associated to the observable . in the position representation , the position operator is $\hat a = x$ , and the momentum operator , $\hat p = - i\hbar \partial / \partial x$ , which is a differential operator . the connection to de broglie is best thought of as historical . it is related to how schrödinger figured out the equation , but do not look for a rigorous connection . as for the hamiltonian , that is a very useful concept from classical mechanics . in this case , the hamiltonian is a measure of the total energy of the system and is defined classically as $h = \frac{p^2}{2m} + v ( \vec r ) $ . in many classical systems it is a conserved quantity . $h$ also lets you calculate classical equations of motion in terms of position and momentum . one big jump to quantum mechanics is that position and momentum are linked , so knowing ' everything ' about the position ( the wavefunction $\psi ( \vec r ) ) $ at one point in time tells you ' everything ' about momentum and evolution . in classical mechanics , that is not enough information , you must know both a particle 's position and momentum to predict its future motion .
well , the answer is yes and no . the band inversion between the $s$-like ( conduction ) band $\gamma_6$ and $p$-like ( valence ) band $\gamma_8$ in hgte is primarily responsible for its topologically nontrivial band structure . the bulk band structure of hgte with ( right ) and without ( left ) spin-orbit coupling is shown in the figure below . there are a total of eight bands ( including spin ) shown in both figures . since we’re interested in the physics close to the $\gamma$ point , we can approximately ignore bulk inversion asymmetry . under this assumption the spin up and down bands are degenerate as clearly seen from the figure . from this point on i will not consider spin explicitly when talking about bulk band structure ; i.e. there are a total of four bands ( ignoring spin ) in the figures below . note : please don’t focus on the quantitative details of the left figure . it is a hypothetical scenario introduced purely for pedagogical purposes . you can notice that , in the figure on the left ( without spin-orbit coupling ) , the heavy hole ( hh ) and light hole ( lh ) bands are degenerate . when you turn on spin-orbit , the $\gamma_6$ and $\gamma_8$ bands reverse their order , the $\gamma_8$ band splits its degeneracy , and the lh band gets inverted . the fermi energy sits at the intersection point of the lh and hh bands . but notice that , despite lh and hh acting as the conduction and valence bands ( right figure ) respectively , there is no gap between them ! you cannot get a topological insulator without a bulk gap . if you could somehow induce a gap between the lh and hh bands ( say ) by straining hgte then it could , in fact , be turned into a 3d topological insulator ! now , there were several ( experimental ) advantages in creating a cdte/hgte/cdte quantum well . first of all , since it’s a quantum well you would have sub-bands ( not bands unlike bulk materials ) due to quantum confinement in the out-of-plane ( say $z$ ) direction . as a result , a single band in the bulk will split up into several sub-bands , each corresponding to a different quantized $k_z$ , as you shrink the thickness of the material in the $z$-direction . now , you can notice ( in the figure below ) , unlike the bulk , the electron ( conduction ) and hole ( valence ) sub-bands do have an energy gap . this plot obviously shows the minima ( electron ) or maxima ( hole ) of these sub-bands ; they still disperse in k-space . and as you may know the inversion of the sub-bands will occur when you cross the critical thickness ( as shown in the figure below ) . another very important advantage of using a quantum well structure in doing your experiments is that , unlike a bulk sample , you can electrically tune your fermi energy using a gate . you could both tune your fermi energy to intersect the electron ( or hole ) sub-band or keep it in the gap , and observe the change in conductance . when you are in the quantum spin hall regime you will never stop conducting as your fermi energy goes from the electron ( or hole ) sub-band to the gap ; this is due to the topologically protected ( due to time-reversal symmetry ) edge states inside the bulk gap ( here bulk means not on the edge of the well ) . in a bulk sample ( bulk meaning not quantum confined ) you would probably have performed some sort of controlled doping ( assuming the gap has already been induced somehow ) to control your fermi energy . in that case you would probably have to fabricate different samples for different values of fermi energy ; that’s certainly very inconvenient . in summary , you need to somehow induce a gap in hgte , by either quantum confinement or induced strain to turn it into a 2d or 3d topological insulator . cdte is not responsible for the key physics , i.e. band inversion , which gives rise to a topologically nontrivial band structure in hgte . it is interesting to note that the hgte quantum well was not the first proposal by bernevig , hughes , and zhang . the experimental difficulty of working with strained hgte led them to revise their proposal and predict a topological insulator in the quantum well instead ! this was back in 2006 ; people have now managed to experimentally create 3d topological insulators out of strained hgte .
idea #2: build an electronic circuit that separates a periodic input signal ( e . g . a square wave ) into its component frequencies ( using an array of band-pass filters ) and then adds these signals back together to get an approximation of the original signal .
in order to do perturbation the expansion parameter needs to be small . otherwise the the system will be strongly coupled and you are in the non-perturbative regime . it is the same as for instance in qm : for perturbative calculations the pertubation must be small .
the most fundamental definition of temperature is derived from the zeroth law of thermodynamics . the zeroth law declares thermal equilibrium an equivalence relationship , and thus we can tag each equivalence class with a number that we call temperature . or in less mathematical term , temperature is a physical quantity tagged to each thermodynamic system such that any two systems with the same temperature would stay in thermal equilibrium when they contact . the exact way of assigning temperature to a system is called a temperature scale . there were multiple scales before , most based on thermal properties of a particular substance . then kelvin devised a scale based solely on thermodynamic principles , which we call " absolute scale " .
the hinge , which connects the " system " ( bullet and beam ) to the outside world does not exert any torque on the system , but it can and does exert both a vertical and horizontal force on the system at the moment of impact . consider for a moment if the bullet hit close to the hinge ; the hinge could fail and allow the beam and bullet to move in the direction of the bullet 's travel . i think that a consideration of " center of percussion " , http://en.wikipedia.org/wiki/center_of_percussion , could clarify the situation . . . not that in this article , the hinge is replaced with a sliding u-bolt , which cannot exert horizontal forces . . .
i can not explain qm here . it takes a lot of reading and working things out for yourself . for this particular question , however , an analogy might help ( this may be far below your level , in which case apologies ) . qm is very often about " simple harmonic oscillators " ( shos ) , for which the oldest prototype is the pendulum ( approximately , if the amplitude is small ) . for a pendulum , if we want to know how far it will go from the vertical , we can wait to see how far it goes on each cycle . an alternative way is to measure how fast the pendulum goes when it passes through the vertical . for any given speed , there is a corresponding farthest distance from the vertical . we can equate these two , in a notional sort of way , by choosing units just so , $s_0=d_1$ , the speed at its maximum is the same as its farthest distance from vertical . [ if you do not want to choose such helpful units , write $s_0=kd_1$ . ] now , suppose that we measure the speed and the distance at some intermediate point , for which we obtain $s_t , d_t$ . for a simple harmonic oscillator , and approximately for a pendulum if its oscillations are small , we obtain $\sqrt{s_t^2+d_t^2}=s_0=d_1$ . the square root $\sqrt{s_t^2+d_t^2}$ is an invariant quantity of the coordinates $ ( s_0,0 ) $ and $ ( 0 , d_1 ) $ , which in general are $ ( s_t , d_t ) $ . anything that is a function of the square root $\sqrt{s_t^2+d_t^2}$ is also a function of $s_t^2+d_t^2$ , so we can work with whichever is more convenient . the effects of a given sho on other systems ---or of a system that contains many shos on other systems--- are determined both the phases and by the amplitudes , but the amplitude often determines the more obvious properties , with differences of phase causing important but often more subtle effects , which we typically might call interference ( but there are many other words , such as " caustics " , or even , in a new age sort of way , " sympathetic vibrations " ! ) . the effects of a given quantum mechanical system are , at an elementary mathematical level , sui generis with a classical sho or system of shos , but quantum mechanics describes the ways that the probabilities of discrete events evolve over time , instead of describing the evolution of a trajectory . the introduction of probability as an essential property makes qm a discussion of a higher order mathematical object . especially different is the fact that we can no longer talk about velocities , because individual events do not have velocities ( if we are determined to talk in terms of particles we cannot in general be sure which individual events go with which particle ) , however it is useful to introduce a notional object that we call momentum , which allows us to model patterns that we observe in the evolution of the probabilities as interference effects ( whether that is what they are not , we can model the patterns in the probabilities using patterns of varying phases and amplitudes ) . the mathematical quantity that we call momentum is , however , sufficiently different from the classical momentum that is associated with a particle trajectory that the analogy breaks down in various mathematically significant ways . i can not see how to address the final aspect of this that occurs to me , for now , at least not well . the much touted linearity of quantum mechanics is a consequence of the fact that qm describes the evolution of probabilities of individual measurement events . the object we call momentum is closely related to the mathematics of fourier transforms of probability distributions , which is essentially associated with a squared modulus like $s_t^2+d_t^2$ . one consequence of that is noncommutativity of the algebra of observables . this is a quick and very vague writing down of a lot of experience , without much editing , so take it with a pinch of salt and with a lot of other reading of what other people have to say about the hard questions that quantum mechanics poses for us . i hope you find it more useful than confusing , but hey , i can take a few downvotes , and it is been oddly useful to me to write this down in this somewhat wild way . in fact , if you can see the ways in which this answer is related to your question , you understand qm pretty well already .
it would crash into the earth because the earth 's gravitational field is not uniform and , even if said ring were to be perfectly positioned , ignoring the effects of wind , strikes from cosmic debris ( not a lot that low in the atmosphere ) , change in mass of the ring ( e . g . corrosion ) , change in shape of the ring ( due to e.g. gravitational forces , heat deformation from sunlight ) , etc . , the earth 's gravitational field distribution changes over time and the ring would eventually fall out of its perfect position and crash into the surface .
a planet 's ir appearance depends on several factors including its temperature the amount and type of cloud cover the resolution of the observing telescope if you are observing with broadband filters and simply imaging , unless the planet has a complex cloud structure you should expect the planet to behave more or less like a black body at the mean surface temperature of the object . using narrowband filters or low resolution spectroscopy , you start to move more and more away from the ideal blackbody in appearance . especially if there is a complex cloud system on the planet ( like on the jovian planets ) . in this situation , you have a couple of effects . depending on the cloud composition , you will get absorption lines in the spectra that can be detected depending on where your narrowband filters are located or by the low resolution spectroscopy . the position and strength of these bands will depended on the cloud composition and concentration of materials if you have a mostly uniform cloud cover ( like venus ) the effect above will be the primary thing you observe . however , typically you will have gaps in the cloud system . this means that for certain portions of the planet , instead of seeing the cooler tops of the clouds , you see deeper into the planet 's surface or interior where it is warmer . this means that in those areas you will see emission from a warmer blackbody and it will have more flux than you would expect . in this situation you are effectively measuring bits of two ( or possibly more ) different blackbodies . again , depending on the placement of the filters ( if imaging ) or the resolution of your spectrograph you will be able to see these effects . finally , as your spectroscopy goes to higher and higher resolution , you will be able to resolve these effects more and more . here 's a picuture of the earth 's spectrum ( taken from this page . ) : a wave number of 1000 cm -1 corresponds to a wavelength of 10 microns . as you can see it generally follows the shape of a blackbody but there are several absorption bands due to the presence of various elements in the atmosphere . as the planets get bigger and more complex the spectrum differs even more from a black body to the point where what you have is peaks of emission in various bands that are coming from deep in the planet and huge absorptions features from the clouds higher up .
ok , it is probably a bad idea to exchange in comments . let me expand what i said in the comments . if my understanding is correct , the op wants to know , as the first step toward solving the whole problem , the ground state energy of the many-body hamiltonian $\mathcal{h}$ defined by $$ \mathcal{h} = \sum_{r , s}h_{rs}c^\dagger_r c_{s} , $$ for a given set of parameters $\{ h_{rs}\}$ . here $c^\dagger_{r}$ and $c_{r}$ are standard fermion creation and annihilation operators . the subscripts $r , s$ run over all lattice sites from 1 to $n$ . the hermiticity requires that $$ h_{rs} = h^\ast_{sr} . $$ in other words , the $n\times n$ matrix $h$ , whose $ ( r , s ) $ entry is defined to be $h_{rs}$ , must be hermitian . in some literature , $h$ is known as the " first-quantized hamiltonian " . note that the above $\mathcal{h}$ takes a slightly more general form than the one described by op . the first step is to diagonalize $\mathcal{h}$ . to this end , we introduce a new set of fermion operators : $$ c_{r} = \sum_{m}v_{rm}f_{m} ; \quad{}c^\dagger_{r}=\sum_{m}v^\ast_{rm}f^\dagger_{m} . $$ we demand that the new fermion operators obey the standard fermion algebra . it can be seen that this is amount to demand $$ \sum_{m}v_{rm}v^\ast_{sm}=\delta_{rs} , $$ or equivalently $vv^\dagger=1_n$ , i.e. $v$ is a unitary $n\times n$ matrix . substituting the above in , we find $\mathcal{h}$ written in terms of new fermion operators , $$ \mathcal{h} = \sum_{r , s , m , n}v^\ast_{rm}v_{sn}h_{rs}f^\dagger_m f_n = \sum_{m , n} ( v^\dagger hv ) _{mn}f^\dagger_m f_n . $$ since $h$ is hermitian , we can always find a unitary $v$ so that $h$ is diagonalized : $$ v^\dagger hv = \lambda . $$ here $\lambda = \textrm{diag} ( \lambda_1 , \lambda_2\cdots , \lambda_n ) $ . $\lambda_i\in\mathbb{r}$ are eigenvalues of $h$ . thus , $$ \mathcal{h} = \sum_{m}\lambda_m f^\dagger_m f_m . $$ this is the desired diagonalized form of $\mathcal{h}$ . the second step is to find the ground state energy of $\mathcal{h}$ . we see that all eigenstates of $\mathcal{h}$ are labeld by the occupation numbers $f^\dagger_mf_m$ . it is easy to see that the ground state of $\mathcal{h}$ is constructed by filling up all modes with negative energy . in other words , in the ground state , $$ f^\dagger_m f_m=\left\{\begin{array}{cc} 1 and \lambda_m&lt ; 0\\ 0 and \lambda_m&gt ; 0 \end{array} \right . . $$ there will be degeneracy if some $\lambda_m = 0$ . then , the ground state energy is $$ e_{g}=\sum_{m , \lambda_m&lt ; 0}\lambda_m . $$
for the photons that make up light to exist they have to be travelling at the speed of light . this means that to store them you have to put them in a container where they can move around at the speed of light until you want to let them out . you could build the container out of mirrors , but no mirror we can build is 100% reflective , or indeed can be 100% reflective . usually when a photon " hits " the mirror it is absorbed by one of the atoms in the mirror and then re-emitted back out into the container . however , occasionally the photon either will not get re-emitted ( leaving the atom in an excited state ) or it does not hit one of the atoms and makes it way through the mirror and out of the container . while the chances of this happening for an individual photon are low , there are lots of photons travelling very fast so it happens many times thus causing the light to " leak " or decay . building a near perfect mirror is hard , so it is easier to convert the light into something that can be stored and then convert that back into light when need it .
no , because even though the force that you exert on the earth is equal and opposite to the force it exerts back on you , you are not doing the same amount of work on the earth as the earth on you . your kinetic energy increases due to the work done by the earth on you . remember that $w = f \cdot d$ ; your bicycle moves a lot due to this force , but the earth does not really move much at all . another way to think about this is in terms of kinetic energy . $\mathrm{ke} = \frac{1}{2} mv^2$ , so if your velocity is high , so is your kinetic energy . the earth 's velocity is low , and so is its kinetic energy . so the forces are equal and opposite , and the impulse , or change in momentum , is too , but the kinetic energy stays mostly with you .
there are 3 actions of the galilean group on the free particle : on the configuration space , on the phase space and on the quantum state space ( wave functions ) . the galilean lie algebra is faithfully realized on the configuration space by means of vector fields , but its lifted action on poisson algebra of functions on the phase apace and on the wave functions ( by means of differential operators ) is the central extension of the galilean algebra , known as the bargmann algebra in which the commutator of boosts and momenta is proportional to the mass . the reasoning is given in the following arguments 1 ) the action on the configuration space : $q = \{x^1 , x^2 , x^3 , t\}$: here the translations and the boost operators act as vector fields and their commutator is zero : translation : $x^i \rightarrow x^i+c^i$ , generating vector $p_i = \frac{\partial}{\partial x^i}$ boost : $x^i \rightarrow x^i+v^i t$ , generating vector $g_i = t \frac{\partial}{\partial x^i}$ this is a faithful action of the galilean group : $ [ p_i , g_j ] = 0$ . 2 ) the lifted galilean action to the phase space $q = \{x^1 , x^2 , x^3 , p_1 , p_2 , p_3\}$ the meaning of lifting the action is to actually write the lagrangian and finding the noether charges of the above symmetry : the charges as functions on the phase space will generate the centrally extended version of the group . an application of the noether theorem , we obtain the following expressions of the noether charges : translation : $p_i = p_i$ boost : $ g_i = p_i t - m x^i$ . the canonical poisson brackets at $t=0$ ( because the phase space is the space of initial data ) : $\{p_i , g_j\} = m \delta_{ij}$ the reason that the lifted action is a central extension lies in the fact that that the poisson algebra of a manifold itself is a central extension of the space of hamiltonian vector fields , $$ 0\rightarrow \mathbb{r}\overset{i}{\rightarrow} c^{\infty} ( m ) \overset{x}{\rightarrow} \mathrm{ham} ( m ) \rightarrow 0$$ where the map $x$ generates a hamiltonian vector field from a given hamiltonian : $$x_h = \omega^{ij}\partial_{j}h$$ ( $\omega$ is the symplectic form . the exact sequence simply indicates that the hamiltonian vector fields of constant functions are all zero ) . thus if the lie algebra admits a nontrivial central extension , this extension may materialize in the poisson brackets ( the result of a poisson bracket may be a constant function ) . 3 ) the reason that the action is also extended is that in quantum mechanics the wave functions are sections of a line bundle over the configuration manifold . a line bundle itself is a $\mathbb{c}$ bundle over the manifold : $$ 0\rightarrow \mathbb{c}\overset{i}{\rightarrow} \mathcal{l}\overset{\pi}{\rightarrow} m\rightarrow 0$$ thus one would expect an extension in the lifted group action . line bundles can acquire a nontrivial phases upon a given transformation . in the case of the boosts , the schrödinger equation is not invariant under boosts unless the wave function transformation is of the form : $$ \psi ( x ) \rightarrow \psi' ( x ) = e^{\frac{im}{\hbar} ( vx+\frac{1}{2}v^2t ) }\psi ( x+vt ) $$ the infinitesimal boost generators : $$\hat{g}_i = im x_i + \hbar t \frac{\partial}{\partial x_i}$$ thus at $t=0$ , we get : $ [ \hat{g}_i , \hat{p}_j ] = -im \hbar\delta_{ij}$ thus in summary , the galilean group action on the free particle 's configuration space is not extended , while the action on the phase space poisson algebra and quantum line bundle is nontrivially central extended . the classification of group actions on line bundles and central extensions may be performed by means of lie group and lie algebra cohomology . a good reference on this subject is the book by azcárraga , and izquierdo . this book contains a detailed treatment of the galilean algebra cohomology . also , there are two readable articles by van holten : ( first , second ) . group actions on line bundles ( i.e. . quantum mechanics ) is classified by the first lie group cohomology group , while central extensions are classified by the second lie algebra cohomology group . the problem of finding central extensions to lie algebras can be reduced to a manageable algebraic construction . one can form a brst operator : $$ q = c^i t_i + f_{ij}^k c^i c^j b_k$$ where $b$ abd $c$ are anticommuting conjugate variables : $\{b_i , c_j \} = \delta_{ij}$ . $t_i$ are the lie algebra generators . it is not hard to verify that $q^2 = 0$ if we can find a constant solution to the equation $q \phi = 0$ with $\phi = \phi_{i j} c^i c^j$ which takes the following form in components , we have $$ f_{ [ ij|}^k \phi_{k|l ] } = 0$$ ( the brackets in the indices mean that the indices $i , j , l$ are anti-symmetrized . then the following central extension closes : $$ [ \hat{t}_i , \hat{t}_j ] = i f_{ij}^{k} \hat{t}_k + \phi_{ij}\mathbf{1}$$ the second lie algebra cohomology group of the poincaré group is vanishing , thus it does not have a nontrivial central extension . a hint for that can be found from the fact that the relativistic free particle action is invariant under poincaré transformations . ( however , this is not a full proof because it is for a specific realization ) . a general theorem in lie algebra cohomology asserts that semisimple lie algebras have a vanishing second cohomology group . semidirect products of vector spaces and semisimple lie algebras have also vanishing second cohomology provided that there are no invariant two forms on the vector space . this is the case of the poincaré group . of course , one can prove the special case of the poincaré group by the brst method described above .
tidal forces are residual forces , they are the consequence of gravitational forces acting more strongly on one part of an extended body than another . remember that gravity is proportional to $1/r^2$ . so one side of the earth ( the " nearby " side from the sun 's perspective ) feels a gravitational force $$f_g^{near} \propto 1/ ( d_{s-e}-r_e ) ^2$$ where $d_{s-e}$ is the distance between the center of the sun and the center of the earth and $r_e$ is the earth 's radius . contrarily , the " far " side of the earth feels a force $$f_g^{far} \propto 1/ ( d_{s-e}+r_e ) ^2 &lt ; f_g^{near}$$ so the far side of the earth is accelerated towards the sun less than the near side . this residual force ( the difference between the force on the near side and that on the far side ) is what we call a tidal force . notice that this discussion is incomplete , since the gravitational force is not only influenced by distance , it is also proportional to mass ( or mass density ) . because water and ( basically ) rocks do not have the same mass density , the effect will be different for the oceans and the ' solid ' chunk of ' rock ' they ' envelope ' . ( lots of quotes there ) this is not vital to our discussion here , though , since the tidal forces are not strong enough to significantly distort the shape of the rigid earth . but it is worth keeping in mind . furthermore , note that the tides on earth are mainly due to the gravitational effect of the moon on the earth , not the sun . the sun may be a lot heavier than the moon , but the moon is a lot closer . and gravitational forces scale only linearly with mass while they scale quadratically with inverse distance . as a consequence , the solar tides are about half as large as the lunar tides . ( see also this illustrative app and this link for some more information ) richard feynman actually briefly addressed the tides in his lectures on physics and it is fun and interesting to watch , so here 's a link . his discussion of the tides starts at around 25:00 . explicit calculation and comparison between solar tides and lunar tides let 's quickly crunch some rough numbers , shall we ? of course , we will need values for a few quantities . ( i will be using si units since you will probably be most familiar with those ) $$\begin{align} m_s and \approx 2\times10^{30}\ , \text{kg} \\ m_m and \approx 7.3\times10^{22}\ , \text{kg} \\ d_{se} and \approx 1.5\times10^{11}\ , \text{m} \\ d_{me} and \approx 3.8\times10^{8}\ , \text{m} \\ r_e and \approx 6.4\times10^{6}\ , \text{m} \\ g and \approx 6.7\times10^{-11}\ , \text{m}^3\ , \text{kg}^{-1}\ , \text{s}^{-2} \end{align}$$ with these numbers we can calculate the gravitational acceleration $a=f_g/m$ experienced by the near and far side of the earth . ( we do not calculate the force because the affected mass $m$ will in general be different ) the difference between the gravitational acceleration for the near and far side is a measure for the strength of the tides . for the sun we find $$a_s = gm_s\left ( \frac{1}{ ( d_{se}-r_e ) ^2} - \frac{1}{ ( d_{se}+r_e ) ^2}\right ) \approx 1\times10^{-6}\ , \text{m}\ , \text{s}^{-2}$$ and for the moon $$a_m = gm_m\left ( \frac{1}{ ( d_{me}-r_e ) ^2} - \frac{1}{ ( d_{me}+r_e ) ^2}\right ) \approx 2.3\times10^{-6}\ , \text{m}\ , \text{s}^{-2} . $$ so we see from this crude estimate that indeed the solar tides are about half as large as the lunar tides .
$pdv$ is boundary work . $vdp$ is isentropic shaft work in pumps ( as you have identified above ) , gas turbines , etc . now you must realize that even in a pump or turbine the mechanism of work is still $pdv$ , i.e. , the gas pushing on the blade out of its way . but , then there is work required to maintain the flow in and out of the device/control volume , which requires flow work $pv$ so the net reversible work from a steady-flow device turns out to be shaft $vdp$ . why flow work $pv$ ? to push a packet of fluid with volume $v$ forward into a device you have to do work against the pressure of the fluid already in the device , i.e. , overcome the back force of that fluid . this implies the work you do in pushing your new packet of length $l$ and cross-section area $a$ into the device is : \begin{align*} \int fdx = \int_{0}^{l}padx = pv \end{align*} it must be noted that in a steady-flow device ( unlike in a piston ) the back pressure $p$ is constant . now consider the device ( e . g . , turbine to be a control volume ) . the energy of the fluid going in is its internal energy and the work invested into the fluid to enter the device : $u_{entry}+p_{entry}v_{entry}=h_{entry}$ . similarly for exit from the device . the net change across the device is $\delta h$ . for a differential device ( or across a small change ) this is $dh$ . the work output from the shaft of then device is the $\delta w= dh$ . now if the device is isentropic , i.e. , adiabatic-reversible . the gibbs equation provides : \begin{align*} and dh=tds+vdp=vdp\\ and \delta w =dh=vdp \qquad ( \text{isentropic}\ ; ds=0 ) \end{align*} therefore $vdp$ is isentropic shaft work from a flowing device . important points : 1 ) both internal energy and enthalpy are state variables , therefore can be measured for a system static or flowing . this is why sometimes there is a tendency to use $u$ and $h$ incorrectly . the true purpose of $h$ is to capture the work required to push/maintain a flow against a back pressure , i.e. , it incorporates the $pv$ part . therefore when you write an energy balance with flows coming in and out , the energy crossing boundary is not just $u$ but $h$ and this distinction must be kept in mind . 2 ) $vdp$ is isentropic steady-flow shaft work . the isentropic is key here .
a supersymmetric supermultiplet - that is , a set of fields that are related by supersymmetry transformations - must contain a equal number of fermionic and bosonic degrees of freedom . a majorana fermion has two ( on-shell ) degrees of freedom . the majorana 's supersymmetric scalar partner must , therefore , be a complex scalar with two degrees of freedom . a complex scalar field can be neutral under a $u ( 1 ) $ symmetry . because the $u ( 1 ) $ transformation , $$ \phi\to\exp ( iq\theta ) \phi , $$ makes no sense if $\phi$ is a real field , all real scalar fields are necessarily neutral under a $u ( 1 ) $ symmetry ; however , not all scalar fields that are neutral under a $u ( 1 ) $ are necessarily real fields .
if dark matter interacts only gravitationally , then the cross section for producing it in the e+e- machines is inherently too low to be detected . i am discussing e+e- machines because those are the ones that can give a closed enough system to be able to detect missing mass and energy cleanly . the cross section at the y ( about 10gev in mass ) is something like 10^-2 millibarn . now the coupling constant in front of the calculations ( squared ) is the electromagnetic one , which is orders of magnitude larger than the gravitational one . this will affect to practically zero both the magnitude and the width of any reaction producing the hypothetical 7 gev particle , either in some pair production , or associated production . there was some talk of finding more positrons than electrons associated with the measurements reported . in that case there exists a coupling between electromagnetic fields and these proposed particles , but a specific model would be needed to say at what level the production of these would be excluded by the existing world data from e+e- machines . there are limits given assuming super symmetry is the valid theory . see this aleph thesis which gives limits over 40 gev .
this mistery has an easy answear : in absence of external currents the total current is the current induced by the electric field , which is just the derivative of the nanoparticle polarization . this holds in general in absence of external current : $$ \mathbf{j}_{pol} = \frac{\partial\mathbf{p}}{\partial t} $$ now in the case of a nanoparticle ( or a dielectric/metallic sphere ) , it is known that the response to an external field is dipole-like . the total electric dipole $\mathbf{d}$ is given by : $$ \mathbf{d} = \int \mathbf{p}\ , d\mathbf{r} $$ now , since the total field inside the sphere $\mathbf{e}_{inside}$ and the exciting field $\mathbf{e}_0$ are both uniform ( attention : the total field outside the sphere is not uniform ! ) , you can write : \begin{align} q_{abs} and = \frac{1}{2}\mathbf{re}\int \mathbf{j}_{pol}\cdot \mathbf{e}_{inside}^* = \frac{1}{2}\mathbf{re}\left\{ \mathbf{e}_{inside}^*\cdot \int \mathbf{j}_{pol}\ , d\mathbf{r} \right\}\\ and =\frac{1}{2}\mathbf{re}\left\{ \mathbf{e}_{inside}^*\cdot \left ( \frac{\partial}{\partial t}\int \mathbf{p}_{pol}\ , d\mathbf{r} \right ) \right\} = \frac{1}{2}\mathbf{re}\left\{\left ( \frac{\partial}{\partial t}\mathbf{d}\right ) \cdot\mathbf{e}_{inside}^*\right\} \end{align} if the external source is time harmonic , then all the time dependences can be assumed to be of the type $e^{-i\omega t}$ , thus $$ \partial_t \mathbf{d} = -i\omega \mathbf{d} $$ substituting into the above equation , you get : $$ q_{abs} = \frac{1}{2}\mathbf{re}\{-i\omega \mathbf{d}\cdot\mathbf{e}^*_{inside}\} = \frac{\omega}{2}\mathbf{im}\{\mathbf{d}\cdot\mathbf{e}^*_{inside}\} $$ and similarly for the $q_{ext}$ .
$$a=\pi r^2$$ $$\frac{da}{dr}=\pi\cdot2r$$ $$da=2\pi rdr$$ alternatively , you can write : $\lim_{\delta r\to 0}\frac{\delta a}{\delta r}=\lim_{\delta r\to 0}\frac{\pi\{ ( r+\delta r ) ^2-r^2\}}{\delta r}=\lim_{\delta r\to 0}\frac{2\pi r\delta r+\delta r^2}{\delta r}=2\pi r+0$ you have to ignore $ ( dr ) ^2$ as it is very small . why ? because you took the limit while taking infinitesimal rings .
saying that a splitting varies over the moduli space is not completely well defined : you have to say how to identify the total spaces at different points of the moduli i.e. to specify a flat connection on the bundle of total spaces . in the b-model , if you take the gauss-manin connection as the flat connection then the hodge splitting varies over the moduli space ( because the gauss-manin connection does not preserve the splitting in general ) . in the a-model , if you take the trivial connection as the flat connection then the splitting does not vary over the moduli space ( the trivial connection preserves the degree decomposition ) . but it is not the trivial connection which appears in mirror symmetry on the a-model side but a flat connexion which is the trivial one corrected by contributions of holomorphic world-sheet instantons ( i.e. . gromov-witten invariants ) and this connection does not preserve the degree decomposition in general . about the vacuum line bundle . on the a-model sigma model side , 1 in h^{0} gives a natural trivialization . but the sigma model description is generally only valid in some limit of the moduli space , some cusp which is topologically a punctured polydisk . in particular , any complex line bundle is trivial in restriction to this domain and this is also the case for the vacuum line bundle of the b-model . deep inside the moduli space , the topology can be complicated and the vacuum bundle of the b-model can be non-trivial but it is also the case for the a-model which has no longer a sigma model description and so no longer a "1" to trivialize $\mathcal{l}$ . ( remark : the genus g string amplitude is a section of $\mathcal{l}^{2-2g}$ and not $\mathcal{l}$ . )
i apologize , this is my third correction to my answer . this question is very subtle indeed . i hope this answer is the ultimate one ! first of all , if you want to take advantage of lie 's theorem you mention ( some time called third lie theorem ) , the lie algebra has to be real , as it must be the lie algebra of a real lie group . then , if you are interested in quantum mechanics applications , i mean if you wish that the given generators are also generators of a unitary representation of a lie group , the generators must be hermitian at least and a , a† are not . so you first have to pass to anti self-adjoint generators ( * ) , for instance , introducing two constants $\omega , m &gt ; 0$: $$-ii , -ih , -ip , -ik:= -imx\qquad ( 1 ) $$ where , up to real factors ( so without changing the real lie algebra ) x and p are given by $a+a^\dagger$ and $i ( a−a^\dagger ) $ as is well known . $m$ has the physical meaning of mass of the particle and $h = \hbar \omega ( a^\dagger a + \frac{1}{2}i ) $ can be re-arranged to : $$h = \frac{1}{2m}p^2 + \frac{m\omega^2}{2}x^2$$ operators ( 1 ) are , in fact essentially self-adjoint on the dense set made of finite linear combinations of vectors $|n\rangle$ , eigenstates of $h$ . classically , the galileo group in one dimension includes time translations , space translations along x , galileian boosts along x . if we think of the point $ ( x , p ) $ in the space of phases as the vector $ ( 1,1 , x , p ) \in \mathbb r^4$ , and the generic element of $g$ is denoted by a triple $ ( \tau , a , v ) $ ( time translation + space translation + boost ) $g$ acts on the system as $$ ( 1,1 , x , v ) ^t \mapsto a ( \tau , a , v ) ( 1,1 , x , p ) ^t$$ where , for the harmonic oscillator system $a ( \tau , a , v ) $ is ( barring errors in computations ) the $4\times 4$ real matrix $$a ( t , a , v ) = \begin{bmatrix} i and 0 \\ r_{t}t_{a , v} and r_t \end{bmatrix} $$ where $r_t$ and $t_{a , v}$ are $2\times 2$ respectively matrices defined as : $$r_t = \begin{bmatrix} \cos \omega t and -\frac{\sin \omega t}{m\omega} \\ m\omega \sin \omega t and \cos \omega t \end{bmatrix} $$ and $$t_{a , v} = \begin{bmatrix} a and 0 \\ 0 and mv \end{bmatrix} $$ in this way , we have $3$ generators $h , \pi , k$ obtaining by taking the derivative of $a ( t , a , v ) $ respectively in $t$ , $a$ and $v$ at ( 0,0,0 ) . the commutation relations of these generators are the same as for $$h , p , k$$ with the following exception : $$ [ \pi , k ] =0\quad \mbox{instead of}\quad [ \pi , k ] = m$$ to be compared with : $$ [ -ip , -ik ] = - m ( -ii ) $$ notice that this commutator is just a number , so that , when you exponentiate the generators it gives rise to a phase which commutes with all operators . in other words , if you wish to construct an unitary representation of $g$ acting in the hilbert space of the harmonic oscillator , you face a problem with the composition rule , as you find a so-called unitary-projective representation : $$u ( g ) u ( g' ) = e^{i\alpha ( g , g' ) }u_{gg'}\qquad ( 2 ) $$ the phase $e^{i\alpha ( g , g' ) }$ arises when $g$ and $g'$ includes transformations generated by the momentum $p$ and the boost $k$ . it is possible to compute $\alpha ( g , g' ) $ using several procedures , e.g. hausdorff-campbell-ecc . . . identity . notice that the mass $m$ explicitly shows up in $\alpha$ ( which has just the form $\alpha ( g , g' ) = m f ( g , g' ) $ ) and this is related to bargmann 's superselection rule . to obtain a true unitary representation of some lie group one can deals with as follows . start from the group $u ( 1 ) \times g$ ( a so called central extension of $g$ ) with the composition rule : $$ ( e^{ia} , g ) \circ ( e^{ia'} , g' ) = ( e^{i ( a+a'+ \alpha ( g , g' ) ) } , gg' ) $$ and define the map : $$u ( 1 ) \times g \ni ( e^{ia} , g ) \mapsto v_{ ( e^{ia} , g ) } := e^{ia}u_g\: . $$ just in view of ( 2 ) , this is a proper unitary representation of $u ( 1 ) \times g$ . notice that $u ( 1 ) \times g$ has now a further generator commuting with all the other generators in view of the fact that we have ``added'' $u ( 1 ) $ to the initial group $g$ . this generator , in the hilbert space , is proportional to $-ii$ . the anti-self-adjoint generators are just : $$-ii , -ih , -ip , -ik\: . $$ so , we can conclude that the considered generators are a representation of the lie algebra of a central extension of a group $g$ , representing the action of galileo group along the $x$ axis on the harmonic oscillator . there are some open issues . ( 1 ) $u ( 1 ) \times g$ is a lie group . what is the differential structure but also the topology on it ? this is a delicate problem solved by wigner . ( 2 ) in view of the commutation relations of $h$ and $p$ , the latter is not a conserved quantity along time evolution . this is a consequence of the fact that the system , obviously , is not invariant under space translations ( the location of the minimum value of the harmonic potential fixes a natural origin ) . nevertheless the system admits a conserved quantity associated with the generator $p$ . since $-ip$ belongs the the lie algebra of the representation , $$e^{-ith} ( -ip ) e^{ith}$$ still belongs to that lie algebra in view of the fact that $ e^{-ith}$ is a one-parameter subgroup of the representation . as a matter of fact ( barring trivial errors in computations ) $$e^{-ith} p e^{ith} = -\frac{\omega\sin ( \omega t ) }{m} k + \cos ( \omega t ) p\: . $$ therefore the explicitly depending on time observable in schroedinger picture : $$p ( t ) := -\frac{\omega\sin ( \omega t ) }{m} k + \cos ( \omega t ) p$$ turns out to be a constant in heisenberg picture : $$p ( t ) _h = e^{ith} p ( t ) e^{-ith} = p\: . $$ this is exactly the procedure exploited to associate a constant quantity ( always in heisenberg picture ) to the boost generator , even in relativistic theories . ( * ) when one unitarily represents lie groups , the lie algebra of the group is isomorphic to the corresponding lie algebra of anti self-adjoint generators of the unitary representation . it is true when identifying the lie algebra commutator with the operator commutator .
at the galactic center , there is an object called sagittarius a* which seems to be a black hole with 4 million solar masses . in 1998 , a wise instructor at rutgers made me make a presentation of this paper http://arxiv.org/abs/astro-ph/9706112 by narayan et al . that presented a successful 2-temperature plasma model for the region surrounding the object . the paper has over 300 citations today . the convincing agreement of the model with the x-ray observations is a strong piece of evidence that sgr a* is a black hole with an event horizon . in particular , even if you neglect the predictions for the x-rays , the object has an enormously low luminosity for its tremendously high accretion rate . the advecting energy is pretty " visibly " disappearing from sight . if the object had a surface , the surface would heat up and emit a thermal radiation - at a radiative efficiency of 10 percent or so which is pretty canonical . of course , you may be dissatisfied by their observation of the event horizon as a " deficit of something " . you may prefer an " excess " . however , the very point of the black hole is that it eats a lot but gives up very little , so it is sensible to expect that the observations of black holes will be via deficits . ; - )
since there are no specialists in depletion mass spectrometry , i will try to answer in a more general way . with depletion spectroscopy you look at a small variation of a large signal so what you need is not high sensitivity but signal stability and high dynamic range of the detector . i assume that you have a continuous stream of ions , otherwise pulse to pulse fluctuations of ion concentrations will be the major limiting factor . probably , you will not need superb mass resolution so you would want to buy a quadrupole mass spectrometer - these are cheap , compact and there are plenty of companies that make them . dynamic range depends on the ion detector and since you would be looking for a wide dynamic range and stability with time , the very best choice is a simple faraday cup . you can be sure that , whatever your experiment is , the sensitivity will be limited not by the detector but by fluctuations of your signal - most likely , by how stable your depleting factor is .
you get a rise in a capillary tube because it reduces the energy stored in the surface tension at the air-water and air-glass interface . the water rises until the reduction in the surface tension energy is balanced by the increase in the gravitational potential energy of the water . but it is not at all obvious how you could extract energy from this . if you evaporate water from the top of the tube then you will certainly pull up more water to replace the water lost by evaporation . i suppose this is analogous to a tree pulling up water , though my limited memory of biology i think the sap is driven up the tree by osmotic pressure in the roots as well as by capillary action . i suppose you could put a microturbine at the bottom of the capillary tube then heat the top and extract energy as the water rises up the tube to replace the water that is evaporated . however i doubt this would be as efficient as just using the same amount of heat in steam engine . were you wondering if there was a way to make the water rise up the tube , then fall back , then rise up again , generating energy with each cycle ? the only way you could do this was if there was some way to change the air-water or air-glass surface tension in some reversible way . you can easily reduce the air-water surface tension by adding surfactant , and this will make the water drop , but you had need to get the surfactant back out to make the water rise again .
can any solid material with a low heat capacity exist that feels closer to human body temperature than another solid material with a higher heat capacity ; where both materials were previously kept in either a mundane oven or freezer for a sustained period ? let me rephrase to : is there any solid which disobeys the inverse proportionality of thermal conductivity and specific heat capacity ? consider $1000kg$ of wood and $1000kg$ of aluminium , both at $320k$ ( very warm ) . at the instant you place a finger on such large thermal masses , your perception of temperature comparison is dependent on heat conductivity of the materials , not their heat capacity ( their masses are so large compared to your finger , their temperature is almost constant depsite losing heat to your finger ) . using such large masses and ( equal masses for that matter ) is necessary since otherwise i can instantly answer yes to your question by giving you 100g of wood and 1g of gold ( beaten to the same surface area of the wood ) just taken from the freezer and you would perceive gold being closer to body temperature than the wood after a second . so lets define the question by specific heat capacity , and instantaneous perception of heat transfer . to answer it though , there is in fact no metal which disobeys this relation due to the electron sea being the majority carrier of kinetic energy in the bulk metal . their having large mean free paths and low masses allow them to attain very high velocities ( which is a property of high temperature ) and therefore are able to transfer energy quickly in the bulk material . in other words , if metals used anything heavier to transmit heat , like their nuclei , it would not only take much more heat to accelerate them to the same velocities the electrons could attain ( resulting in higher heat capacity ) , but the rate at which that kinetic energy is transmitted across the material is accordingly slower ( lower thermal conductivity ) . in fact the lattice of metal nuclei do in fact contribute to both properties via phonons not translational kinetic energy like in gases , but phonons are still greatly superseded by the effect from electrons . therefore the inverse relation between thermal conductivity and heat capacity is valid for metals . what you are looking for is a non conductor with both higher heat capacity and thermal conductivity than a conductor . for that i give you diamond ( figuratively . . . i can not afford one ) , which has a specific heat capacity of $0.5 j/gk$ , higher than that of any metal denser than vanadium ( which is almost all of them ) , but has a thermal conductivity of $&gt ; 900w/mk$ , trumping silver 's $421w/mk$ which is tops for all pure metals . indeed , $1kg$ of silver would feel much closer to body temperature than $1kg$ of diamond ( that is alot of diamond ! ) despite diamond having a higher heat capacity .
on the electron ' cloud ' pictures : what is usually shown is a surface where the probability ( per unit volume ) of finding an electron ( the magnitude of the wave function squared ) is constant . ' inside ' the volume , the probability ( per unit volume ) is higher in this case , so indeed these shapes show the volume where the probability of finding an electron is greater than a certain value ( 'high' ) . concerning the zeeman effect , instead of saying that orbitals are ' split ' into sub-orbitals ( or subshells ) , one can also start from the fact that there are $n^2$ solutions ( 'steady states' ) for the $n$'th energy level ( without an external field ) . each of these solutions is characterized by the following quantum numbers : the principal quantum number $n$ , $n \ge 1$ the azimuthal quantum number $l$ ( sometimes also referred to as orbital angular momentum number ) , ranging from $0$ to $n-1$ the magnetic quantum number $m$ , ranging from $-l$ to $l$ , which is proportional to the projection of the electron orbital angular momentum onto the z-axis ( i am ignoring the intrinsic spin of the electron here ) without any external field , the energy levels of the wave functions only depend on $n$ , they are said to be degenerate ( multiple solutions / wave functions with the same energy ) . applying an external magnetic field , the energy of the wave functions changes . the additional energy depends on magnitude of the electrons angular momentum ( orbital momentum along the $z$ axis which is proportional to $m$ plus intrinsic spin ) and the strength of the magnetic field ( taken to be parallel to the $z$ axis ) . in this case , the solutions belonging to the same $n$ have different energies , now also depending on their value $m$ in addition to $n$ . in practice , if you were observing spectral lines ( differences of energies between solutions ) , you would see that when applying a magnetic field , the spectral lines suddenly split ( and that when increasing the strength of the magnetic field , the split also increases etc . ) . on your third question , if you want to get the probability as function of the radial distance to the center , you have to integrate over the other two variables , i.e. the polar angle $\theta$ and the azimuthal angle $\phi$ ( in spherical coordinates ) . multivariate calculus says that the coordinate transformation from cartesian $ ( x , y , z ) $ coordinates to spherical $ ( r , \theta , \phi ) $ is as follows : $$ \int_{x=0}^\infty \int_y \int_z |\psi|^2 dx dy dz = \int_{r=0}^\infty \int_{\theta=0}^{\pi} \int_{\phi = 0}^{2\pi} |\psi|^2 r^2 \sin\theta d\phi d\theta dr $$ at this link you can see that all the solutions to the schrödinger equation of the hydrogen atom can be written as the product of a radial part $r ( r ) $ ( depending on the quantum numbers $n$ and $l$ ) and a spherical harmonic $y ( \theta , \phi ) $ ( depending on the quantum numbers $n$ and $l$ ) , i.e. $\psi = r ( r ) \cdot y ( \theta , \phi ) $ . the fact that the wave functions can be written as such a product means that we can also write the integral as a product of an integral of $|y|^2$ over ( $\theta$ , $\phi$ ) and a integral of $r^2 ( r ) $ over $r$: $$ \left ( \int_{\theta=0}^{\pi} \int_{\phi = 0}^{2\pi} |y ( \theta , \phi ) |^2 \sin\theta d\phi d\theta \right ) \cdot \left ( \int_{r=0}^\infty r^2 ( r ) r^2 dr \right ) $$ the left integral is one ( this actually depends on the exact definition of $y$ ) while in the right part you can recognize the radial probability distribution function to be integrated over ( up to a constant factor , depending on how $y$ and $r$ are normalized ) .
if you want to switch from $x , y$ to $z , z^*$ , you need to invert your relationships because the opposite direction is needed . the inverse maps are $$ x = \frac{z+z^*}2 \quad y = \frac{z-z^*}{2i}$$ and $$\frac{\partial}{\partial x}\equiv\partial_x = \frac{\partial }{\partial z} + \frac{\partial }{ \partial z^*} , \quad \frac{\partial }{ \partial y}\equiv \partial_y = i ( \frac{\partial }{ \partial_z} -\frac{ \partial }{ \partial z^*} ) $$ note that one has to use the partial derivatives because we are talking about functions of several variables . when you substitute the identities above into the definition of the angular momentum ( note that it is $\hbar$ , hbar , and not $h$ over there ) , we get $$ l_z = -i\hbar ( x \partial_y - y\partial _x ) =-\frac{i\hbar}{2} ( 2iz \partial_z-2iz^*\partial_{z^*} ) = \hbar ( z\partial_z - z^*\partial_{z^*} ) $$ the factors of $i$ and $2$ cancel , much like the terms $z\partial_{z^*}$ and its complex conjugate $z^*\partial_z$ . the latter have to cancel because $l_z$ is symmetric under rotations around the $z$-axis so the $l_z$ $u ( 1 ) $ charges have to cancel in all terms . the " charge " of $z$ exactly cancels with the opposite charge of $\partial / \partial z$ ( and similarly for the complex conjugate term ) but the mixed terms would not . also note that there is no prefactor $i$ in the formula for $l_z$ in terms of $z , z^*$ and their derivatives . that is ok because the $l_z=m$ wave functions behave as $z^m$ or $z^{*-m}$ or some compromise and one may pick an $m$ factor ( eigenvalue ) by differentiating $z^m$ with respect to $z$ , getting $mz^{m-1}$ , and multiplying by $z$ again to get $mz^m$ . the $i$ is hidden in $z=x+iy$ , the relative phase of $x , y$ .
in the limit of very , very long time you can expect that situation to obtain , but the earth formed very violently and therefore started with a very high mean temperature . the earth is full of long lived radioactive materials ( u-238 , th-232 and k-40 ) whose decay introduces a steady heat flux in excess of 20 terawatts in this epoch . this heat input delays the cooling of the deep earth from it is fossil high temperature .
your statement is true . proof : let $\rho$ be the mass density of the rigid body . remember that the tensor of inertia $i$ is given by : $$ \vec{v}^t i \vec{w} = \int d^3b\ , \rho ( \vec{b} ) ( \vec{v} \cdot \vec{w} - ( \vec{v}\cdot \vec{b} ) ( \vec{w}\cdot \vec{b} ) ) $$ for all $\vec{v} , \vec{w} \in \mathbb{r}^3$ . now take an orthogonal matrix $o$ which represents a rotation around an axis $\mathsf{a}$ with direction vector $\vec{n}$ , i.e. $o\vec{n} = \vec{n}$ . your invariance means that $\rho$ is invariant under $o$ , i.e. $\rho ( o \vec{x} ) = \rho ( \vec{x} ) $ for all $\vec{x}$ . next show that the inertia tensor commutes with $o$: $io = oi$: $$ \begin{align*} \vec{v}^t i o \vec{w} and = \int d^3b\ , \rho ( \vec{b} ) ( \vec{v} \cdot o \vec{w} - ( \vec{v}\cdot \vec{b} ) ( o \vec{w}\cdot \vec{b} ) ) \\ and = \int d^3b\ , \rho ( \vec{b} ) ( o^t \vec{v} \cdot \vec{w} - ( \vec{v}\cdot \vec{b} ) ( \vec{w}\cdot o^t \vec{b} ) ) \\ and = \int d^3b\ , \rho ( o^t \vec{b} ) ( o^t \vec{v}\cdot \vec{w} - ( o^t \vec{v}\cdot o^t \vec{b} ) ( \vec{w}\cdot o^t \vec{b} ) ) \\ and = \int d^3b\ , \rho ( \vec{b} ) ( o^t \vec{v}\cdot \vec{w} - ( o^t \vec{v}\cdot \vec{b} ) ( \vec{w}\cdot \vec{b} ) ) \\ and = \int d^3b\ , \rho ( \vec{b} ) ( ( \vec{v}^t o ) ^t\cdot \vec{w} - ( ( \vec{v}^t o ) ^t \cdot \vec{b} ) ( \vec{w}\cdot \vec{b} ) ) \\ and = \vec{v}^t o i \vec{w} \end{align*} $$ for all $\vec{v} , \vec{w} \in \mathbb{r}^3$ then one sees that $i\vec{n}$ is again an eigenvector of $o$ because $o ( i\vec{n} ) = io\vec{n} = i\vec{n}$ . now $i$ has only one real eigenvalue $1$ with eigenspace $\mathbb{r}\vec{n}$ . this implies that there is a unique $\lambda$ with $i\vec{n} = \lambda \vec{n}$ . thus $\vec{n}$ is an eigenvector of $i$ i.e. $\vec{n}$ points along a principal axis of $i$ .
volumetric flow $\phi = a v$ . therefore the right expression would be $$v = \frac{ l/1000 }{ \pi ( d/2 ) ^2 } . $$ to get volumetric flow in si units $\mathrm{m}^3/\mathrm{s}$ you should divide $\mathrm{l}/\mathrm{s}$ by 1000 .
there is a very interesting story behind your question . in the early 1900 's ( after special relativity had been introduced ) the solution to wave equation in vacuum : $a\exp ( i ( kx-wt ) ) + b\exp ( -i ( kx-wt ) ) $ where $k$ is the wave vector and $w/k=v$ and v is the phase velocity . de broglie was the first to notice that the phase factors of the equation at every event remain invariant under lorentz transformations if $ ( k , w ) $ is considered a 4-vector . this meant invariant amplitude at every event . this is because the scalar product of the two 4-vectors $ ( k , w ) $ and $ ( x , t ) $ remains invariant under lorentz transformations . from this remarkable piece of insight , he deduced that $ ( k , w ) $ could represent the 4-momentum , $ ( p , e ) $ , of a massive particle . this is how wave-particle duality was first discovered . special relativity gave birth to quantum mechanics in its proper form ! the commutation relation discovered subsequently $ [ p , x ] =-i\hbar$ on solving gives ( setting the arbitrary phase factor in p to 1 ) : $p=-i\hbar \frac{∂}{∂x} $ . the uncertainty relation in momentum is derived from here . the fundamental form of schrodinger 's equation $i\hbar \frac{∂t}{∂t} = et$ where $e$ is the hamiltonian and $t ( t_0 , t ) $ is the unitary time evolution operator has its origins in the relationship between $x$ and $p$ being extended to $t$ and $e$ . when introducing the schrodinger equation in his book , dirac points out that its derivation comes mainly from considerations of relativity . in fact schrodinger 's original equation was actually relativistic , where $e$ was the relativistic energy . schrodinger was not sure whether to take the positive or negative root of $e^2$ . so he discarded it in favor of its widely known non-relativistic form . so in fact , other than the discovery of quantized energy levels in blackbody radiation and the photoelectric effect , every breakthrough in qm owes it is existence to special relativity . edit : i earlier said w/k = c for a massive particle , this is incorrect . w/k is equal to the phase velocity , which is proportional to 1/u . u is the group velocity which is equal to the classical velocity of a massive particle . i have fixed the offending sentences .
the claim is consistent with page 37 of this and page 11 of this . the $\beta$-function of $u ( 1 ) $ depends on the charge of the degrees of freedom , but since there are none below the scale $a$ , it is zero . as for hep-th/9707133 , i suppose that the coupling there is given for non-vanishing charges .
an interesting question indeed :- ) yes , you can flip the overall sign of the minkowski metric , and in fact a lot of physicists do this ! the sign choice $\operatorname{diag} ( -1 , 1 , 1 , 1 ) $ is conventional in fundamental quantum field theory and in quantum gravity , if i remember correctly , whereas $\operatorname{diag} ( 1 , -1 , -1 , -1 ) $ is conventional in particle physics . this does not affect the lorentz transform , though . if you apply the lorentz transform to a metric tensor , it computes as $g'_{\alpha\beta} = \lambda_\alpha^\mu \lambda_\beta^\nu g_{\mu\nu}$ , and so you will automatically come out with the same sign convention that you put in .
the darwin term can be obtained from the low energy approximation ( $|p|^2/m^2&lt ; &lt ; 1$ ) of the dirac equation of the electron in a central field . an elegant way to perform this task is by means of the foldy-wouthuysen transformation . the same approximation leads also to the other terms detected in the hydrogen atom fine structure including the spin-orbit term . however , the relation to the zitterbewegung is also correct , at least in the heuristic level . please see a derivation in the following work by klaus capelle ( equation 4 ) . the explanation is that due to the zitterbewegung high frequency motion , the central potential gets smeared on the average and acquires the extra darwin term . charles darwin together gordon , were the first to compute the exact energy levels of the hydrogen atom using the dirac equation in 1928 , just two years after dirac discovered his equation . darwin also performed the low energy approximation to understand the differences from the nonrelativistic treatment . a little piece of history about the darwin term can be found in footnote 7 of the following lecture notes which is quoted in the following : " sir charles galton darwin ( 1987-1962 ) , a british physicist , was a grandson of the charles darwin of evolution fame . sir charles was the first , with gordon , to work out the exact energy levels of hydrogen according to the dirac equation and thereby discovered the eponymous term in the levels . he worked out the lagrangian and hamiltonian for classical motion of several interacting charges correct to o ( $v^2/c^2$ ) . he also worked on statistical mechanics ( darwin-fowler method ) . later in life he took part in the manhattan project . " darwin 's work was referred to in the original foldy-wouthuysen article : phys rev . 78 no . 1 , 29-36 , 1950 .
coulomb repulsion it is . specifically , if a black hole has a lot of charge , then particles with a high charge-to-mass ratio will be repelled . anything that falls in will contribute " more mass than charge , " heuristically , keeping the charge-to-mass ratio of the black hole from getting too big .
such operators are ill-defined in an interacting theory because whatever counterterms we try to subtract , their expectation value in any finite-energy state will diverge . the closest operators that are well-defined are densities of charge – number operators with signs labeling antiparticles – because the divergent contributions naturally cancel for them . in free quantum field theories , you may define the number operator and write it as an integral but the integrand will not really be commuting with itself at other points so the attribution of the particles into different points will be misleading . in the non-relativistic limit of quantum field theory , all these problems go away under some extra assumptions .
entropy is a concept in thermodynamics and statistical physics but its value only becomes indisputable if one can talk in terms of thermodynamics , too . to do so in statistical physics , one needs to be in the thermodynamic limit i.e. the number of degrees of freedom must be much greater than one . in fact , we can say that the thermodynamic limit requires the entropy to be much greater than one ( times $k_b$ , if you insist on si units ) . in the thermodynamic limit , the concept of entropy becomes independent of the chosen ensembles - microcanonical vs canonical etc . - up to corrections that are negligible relatively to the overall entropy ( either of them ) . a single particle , much like any system , may be assigned the entropy of $\ln ( n ) $ where $n$ is the number of physically distinct but de facto indistinguishable states in which the particle may be . so if the particle is located in a box and its wave function may be written as a combination of $n$ small wave packets occupying appropriately large volumes , the entropy will be $\ln ( n ) $ . however , the concept of entropy is simply not a high-precision concept for systems away from the thermodynamic limit . entropy is not a strict function of the " pure state " of the system ; if you want to be precise about the value , it also depends on the exact ensemble of the other microstates that you consider indistinguishable . if you consider larger systems with $n$ particles , the entropy usually scales like $n$ , so each particle contributes something comparable to 1 bit to the entropy - if you equally divide the entropy . however , to calculate the actual coefficients , all the conceivable interactions between the particles etc . matter .
consider a patch of a world sheet and choose the coordinates so that it is extended in the $t$ and $z$ directions , setting $x=y=0$ - and similarly for the other transverse dimensions i omitted here . the spacetime dimensions is $d$ . the statement that the strings have tension equal to $t$ means that the energy density has to be $$t_{tt} = t \delta^{ ( d-2 ) } s ( x , y , \dots ) . $$ however , the world sheet is required to preserve the $so ( 1,1 ) $ symmetry rotating the $t , z$ axes , so it must be true that a part of the stress energy tensor , the components $$t_{tt} , t_{tz} , t_{zt} , t_{zz}$$ have to be proportional to the 1+1-dimensional metric tensor because multiples of the metric tensor are the only tensors that are invariant under the lorentz transformations , $so ( 1,1 ) $ in this case . it follows that the $t_{tz}$ and $t_{zt}$ components have to vanish while $$t_{zz} = -t \delta^{ ( d-2 ) } ( x , y , \dots ) . $$ the doubly spatial components of the stress-energy tensor represent the pressure but do not forget that the doubly transverse components such as $t_{xx}$ continue to vanish . if one has a gas of randomly oriented strings and he averages over all directions of the string , the average $t_{ii}$ will be $-t_{tt}/ ( d-1 ) $ where $ ( d-1 ) =3$ in our 4-dimensional spacetime so that $p=-\rho/3$ . that is the standard pressure from cosmic strings - domain walls would have $p=-2\rho/3$ for very similar reasons . ( the appearance of $1/3$ is not hard to understand : all the rotated strings will have the same trace over the spatial part of $t_{ii}$ . the $z$-oriented string has this trace equal to $-\rho/3$ so after the averaging over directions i.e. over $so ( d-1 ) =so ( 3 ) $ , when the spatial part becomes proportional to $\delta_{ij}$ , the coefficient of this kronecker delta inevitably has to become $-\rho/3$ to preserve the spatial trace . ) the negative pressure of strings is not too important if the strings remain small and compact but it is important e.g. in string gas cosmology http://scholar.google.com/scholar?hl=enq=string-gas-cosmology+negative-pressurebtng=searchas_sdt=0,5as_ylo=as_vis=0 so your number $p=-\rho$ is incorrect ; it should be $p=-\rho/ ( d-1 ) $ because the pressure only exists in the direction along the string , so the pressure in all directions get diluted by the averaging over directions . that is why the string gas has a different equation of state than the cosmological constant . like any gas , string gas picks a preferred reference frame , unlike the cosmological constant . the precision measurement are enough to exclude the possibility that dark energy boils down to a network of cosmic strings or domain walls because their $w$ is just too far from the " approximately observed " $w=-1$ . by the way , $w$ is defined as $p/\rho$ rather than $\rho/p$ .
there is no difference between those two operators you wrote , since $\frac{1}{i}=\frac{i}{i^2}=\frac{i}{-1}=-i . $ in qm , an operator is something that when acting on a state returns another state . so if $\hat{a}$ is an operator and $|\psi\rangle$ is a state , the quantity $\hat{a}|\psi\rangle$ is another state , which you could relabel with $|\phi\rangle \equiv \hat{a}|\psi\rangle . $ if this dirac notation looks unfamiliar , think of it as $\hat{a}$ acting on $\psi_1 ( x , t ) $ producing another state $\psi_2 ( x , t ) =\hat{a}\psi_1 ( x , t ) $ where $\psi_1$ and $\psi_2$ are just different states or wave functions . when you act on a state with your operator , you do not really " get " momentum ; you get another state . however , for particular states , it is possible to extract what a measurement of momentum would yield once you know what the resulting state is , but that is another question . the term eigenstate may help you in your discovery . but very briefly , if $\hat{a}\psi_1=a\psi_1$ ( note that its the same state on both sides ! ) , one can interpret the number $a$ as a physical observable if $\hat{a}$ meets certain requirements . this is probably what your textbook refers to .
the energy stored in the spring is equal to the work done in compressing it . the force needed to compress the spring to the maximum is only a small part of this calculation . suppose the spring is completely relaxed , and you apply a small force compressing the spring slightly . the energy stored is the product of the force and the distance , measured in newton-metres , or joules . now you increase the force slightly , and the spring compresses a short distance more . the additional energy stored in this new force multiplied by this next compression distance . this continues , until the spring is totally compressed . so to calculate the energy stored , you need to know how far the spring compresses , and how the compression force changes as the spring compresses . edit to rely to comment : suppose the spring is quite long in its relaxed state , with a spring constant of 40 lbf/in . the spring will need to compress by 50 in , to reach the winch maximum of 2000 lbf . . so the energy stored in the compressed spring would be ( with suitable conversion to si ) $$e_s=\frac12kx^2=\frac127005\times1.27^2=5650 \text{ joules}$$ so this would be the energy available to loft a projectile . the problem would be to transfer as much as possible of this energy to a light projectile . there are inherent , internal losses in the rapid extension of a spring that make it difficult to efficiently launch a missile . usually , the spring pushes slowly , with huge force on the short end of a lever , while a light mass accelerates quickly at the long end of the lever . google " trebuchet " , or even " punkin chuckin " . spelling is correct ! the equation in the comment would give the maximum height possible , with no losses in the spring .
the problem is with your first calculation and also with the somewhat misleading equation that you have found . it is true that $$\frac{i_2}{i_1}=\left ( \frac{d_1}{d_2}\right ) ^2$$ but units are important here . in that formula , $i_1$ and $i_2$ would properly be expressed as power values . to compute with decibels , which are logarithmic quantities , one would instead use $$i_1 + 10\log\left ( \frac{d_1}{d_2}\right ) ^2 = i_2$$ or equivalently , $$i_1 + 20\log\left ( \frac{d_1}{d_2}\right ) = i_2$$ , where $i_1$ and $i_2$ are decibels and $d_1$ and $d_2$ are in identical linear units ( feet or meters , for example ) . with your particular numbers we get $$\begin{eqnarray} i_2 and = and 213\text{ db} + 20 \log\left ( \frac{75}{45000}\right ) \\ and = and 213\text{ db} + 20\log\left ( \frac{1}{600}\right ) \\ and \approx and 213\text{ db} + 20 ( -2.78 ) \\ and \approx and 213\text{ db} - 55.56 \\ and \approx and 157.4\text{ db} \end{eqnarray}$$ estimating manually you have correctly remembered that -3db is half the power . that is , $$\frac{1}{2}p = -3\text{db}$$ . another easily remembered fact is $$\frac{1}{10}p = -10\text{db}$$ . both are very commonly used in engineering for rough estimations . so in this case , because it is an inverse square law , we have $$\begin{eqnarray} \left ( \frac{75}{45000}\right ) ^2 and = and \frac{1}{600^2} \\ and = and \frac{1}{360000} \\ and \approx and \frac{1}{400000} \\ and \approx and \frac{1}{2^2\cdot 10^5} \\ and \approx and -6\text{db} - 50\text{db} \\ and \approx and -56\text{db} \end{eqnarray}$$ so this would give $213\text{db} - 56\text{db} = 157\text{db}$
let 's start with the schwarzschild metric $$ \text{d}s^2 = \left ( 1 - \frac{r_\text{s}}{r}\right ) c^2\text{d}t^2- \left ( 1 - \frac{r_\text{s}}{r}\right ) ^{-1}\text{d}r^2 - r^2\text{d}\omega^2 , $$ with $$ \begin{align} r_\text{s} and = \frac{2gm}{c^2} , \\ \text{d}\omega^2 and = \text{d}\theta^2 +\sin^2\theta\ , \text{d}\varphi^2 . \end{align} $$ now let 's introduce a new radial coordinate $\bar{r}$ , defined as $$ r = \bar{r}\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^2 . $$ after some algebra , we find that $$ \begin{align} \text{d}r^2 and = \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^2\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^2\text{d}\bar{r}^2 , \\ \left ( 1 - \frac{r_\text{s}}{r}\right ) and = \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^2\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{-2} , \end{align} $$ so that the schwarzschild metric can be written in the form $$ \text{d}s^2 = \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^{2}\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{-2}c^2\text{d}t^2 - \left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{4}\left ( \text{d}\bar{r}^2 + \bar{r}^2\text{d}\omega^2\right ) . $$ now , in a local frame around a coordinate $ ( t , r , \theta , \varphi ) $ , we can treat $r$ in the coefficients as constant , so that $$ \begin{align} \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^{2}\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{-2} and \approx \alpha^2 , \\ \left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{4} and \approx \beta^2 , \end{align} $$ with $\alpha$ and $\beta$ constants , thus $$ \text{d}s^2 \approx c^2\alpha^2 \text{d}t^2 - \beta^2\left ( \text{d}\bar{r}^2 + \bar{r}^2\text{d}\omega^2\right ) . $$ finally , with the coordinate transformation $$ \begin{align} \tilde{t} and =\alpha\ , t , \\ \tilde{r} and =\beta\ , \bar{r} , \end{align} $$ we obtain the familiar minkowski metric $$ \text{d}s^2 \approx c^2\text{d}\tilde{t}{}^2 - \left ( \text{d}\tilde{r}^2 + \tilde{r}^2\text{d}\omega^2\right ) . $$ this transformation can be performed at any coordinate $ ( t , r , \theta , \varphi ) $ along the path , so the local metric is always of minkowski form .
dear dan , first of all , you should not use the term " uncertainty principle " if you are talking about " light sources " and light may be explained by ordinary - classical ( non-quantum ) - electrodynamics where no uncertainty principle applies . this is just an exercise in the propagation of waves . second , when you flip the switch , there may be temporary variations of the intensity , but they are not necessary , either . for example , you may find a minimum such that the number of wave peaks on the two trajectories ( coming from the two slits ) differs by 13.5 - one arm is 13.5 wavelengths longer than the other one . it will mean that the destructive interference only occurs when the beams from both slits are synchronized , and there will always be a period lasting about 13 periods after each flip of the switch when only one beam is coming to the detector . that will indeed eliminate the destructive interference , and give you the " apostrophes " in your ascii art . the precise shape of the graph depends on the character of the switches , geometry of the experiment , and other things .
it is difficult to compare becuase of different distance and time scales . but iodine 131 fallout from nevada tests caused doses of 10mgray - 200mgray ( 1-20rad ) across the central and north eastern us , 10mgray is about 8 x-rays or 1 ct-scan . generally bomb fallout contains more of the short lived highly active isotopes which are distributed further by the effects of the bomb . reactor fires generally disperse fallout more slowly and so many isotopes have decayed before they reach the environment
to close this post , i write an answer by myself though it turned out that i made just a simple calculation mistake . since the temperature increase should be monotonic and approach to zero at boiling point , it is reasonable to assume that the temperature increase $dt/dt$ is proportional to the difference $t - 100 \mathrm{^\circ c}$ , that is , $$ \frac{dt}{dt} = -k ( t - 100 \mathrm{^\circ c} ) $$ holds for some positive constant $k$ . solving this equation gives $$ t = 100 \mathrm{^\circ c} + ( t ( t_0 ) - 100 \mathrm{^\circ c} ) e^{-k ( t - t_0 ) } . $$ let 's determine coefficient $k$ from $n$ measurements by linear regression . let $c$ be a time interval of experiments and $x_n$ the temperature of the water on $t_n = cn$ . then estimate the slope of the tangent line by $$ y_n = \mathrm{mean}\big ( \frac{t_{n + 1} - t_{n}}{c} , \frac{t_{n} - t_{n - 1}}{c}\big ) = \frac{t_{n + 1} - t_{n - 1}}{2c} $$ for $0 &lt ; n &lt ; n$ . from above equation , there should be a relation of the form $$ y_n = -k ( x_n - 100 \mathrm{^\circ c} ) + \varepsilon_n $$ where $\varepsilon_n$ stands for experimental errors . i denote this equation by $$ y = -kx +\varepsilon $$ as shorthand . the best estimator $\hat{k}$ is given if $x$ and $\varepsilon$ are orthogonal to each other . therefore $$ \hat{k} = -\frac{ ( x , y ) }{ ( x , x ) } \approx 0.00667 \mathrm{s^{-1}} $$ from calculations . and this value fits the experimental data well . note : i completely rewrite this answer . here , i would like to review where my answer was inappropriate . it seems not a problem of physic but a problem of statistics . last time , i solved de first and took logarithm to make it linear . however the experimental errors also transformed . especially , $\ln ( 100 - x_n ) \to -\infty$ as $x_n \to 100$ . so this seems to cause overfitting at higher temperature and bad fitting at lower temperature . ( considering a effect of the pot looks a good idea but everything i tried fails . it will not fit data and still open though i already got a reasonable approximation . ) thank you so much for helping me , chris , stefan bischof , michael brown and christoph .
monopoles are still created in inflationary models . they are just created before ( or during ) inflation , so that the rapid expansion thereafter dilutes their density to unobservably low levels . at the time when the monopoles are created , they are created at a density of order 1 per hubble volume -- that is , there is one in each " observable universe " at that time . in general , when a symmetry breaks , topological defects form that are separated on a length scale of order ( speed of propagation of the field ) ( time scale over which the symmetry breaks ) . the first is of order $c$ , and the second is of order the hubble time , so monopoles are separated by a distance of order the hubble length . you should take " of order " here very liberally -- i do not actually care if i am off by factors of $10^5$ or $10^{10}$ or anything measly like that ! after all , inflation blows up lengths by something like $10^{20}$ or more . so one monopole per horizon volume becomes one per $10^{60}$ horizon volumes . ( also , the horizon volume continues to change after inflation is over , but not by anything like this sort of factor . ) with densities like that , we certainly would not expect to see any monopoles . problem solved .
you assume a magnetic field as static ? magnetic fields may represent and even present at level 1x as standing wave functions , which has led classical physical sciences to regard them as static . however , basic researches have shown magnetic fields to be " consistently dynamic " and " coherently interactive " ( distributable both algebraically and geometrically like fluids ) ; magnetic fields " work " either with external impingement or without .
your equations ( 1 ) ( 2 ) , saying $\delta \psi =-\frac{1}{4}\lambda ^{\mu \nu}\gamma _{\mu \nu}\psi$ with or without $^c$ , just says that both $\psi$ and $\psi^c$ are in the same representation , namely $ ( 1/2,0 ) + ( 0,1/2 ) $ . the third equation ( 3 ) , saying $ ( p_l\psi ) ^c=p_r \psi^c$ , just says that the charge conjugation swaps the two irreducible components of the reducible representation that is the dirac spinor .
the wave function itself can never be discontinuous . it is the derivative what it is discontinuous at $x=0$ , and that discontinuity can be calculated integrating the schrödinger equation between $ ( +\epsilon , -\epsilon ) $ and taking the limit $\epsilon \to 0$ . all terms but the proportional to the delta vanish , giving you $$\left . \frac{d\psi ( x ) }{dx}\right|_{\epsilon=0^+} -\left . \frac{d\psi ( x ) }{dx} \right|_{\epsilon=0^-}=-2\hbar \alpha\psi ( 0 ) $$
ok , i have found this : http://www.cv.nrao.edu/course/astr534/brightness.html i proves that it is not possible to build such optical system . the conservation of brightness also applies to any lossless optical system , a system of lenses and mirrors for example , that can change the direction of a ray . no passive optical system can increase the specific intensity or total intensity of radiation . if you look at the moon through a large telescope , the moon will appear bigger ( in angular size ) but not brighter . many people are disappointed when they see a large , nearby galaxy ( e . g . , andromeda ) through a telescope because it looks so dim ; they expected to see a brilliantly glowing disk of stars , as in the photograph below . the difference is not in the telescope ; it is in the detector—the photograph appears brighter only because the photograph has summed the light over a long exposure time . though the andromedia example is not necessarily correct . . . because it consists of many stars which have huge surface brightness . i think if we could have large enough aperture can could resolve sirius as a disk it would be more eye damaging sight than the sun . . .
first of all , this is a really amazing piece of technology , in particular because it has achieved something that optical engineers have dreamed of for a long time 1 , and done so with underlying techniques that we have had for quite a while . just to give you some impression of how cool this is , i am an optical engineer and the first time i heard about this i was certain that it was either a hoax , a massive exaggerated description of something less impressive ( like an unsharp filter in photoshop or something ) , or simply a piece of godawful technology journalism . however , once i found the doctoral dissertation of the guy who developed this technology dr . ren ng ) , i realized how it works , and really how clever it is . ( note : i will probably add on to this answer a couple times , because i do not have time to give a good and thorough summary right now . if you want a really thorough description , check out the thesis i linked above . it may be a little over your head if you do not have background in optics though , which is what my summary here should help with . ) hardware this technique depends on some very clever data processing techniques , and on the unusual type of camera ( ren ng calls it " plenoptic " which is not a term i have heard before ) that is used to capture the data . this camera has an array of very small lenses ( a " microlenslet array" ) at its image plane , where a normal camera would just have the image sensor . the image sensor is positioned slightly behind this . the microlens array alters the incoming light before it hits the sensor . if you were to look at this raw data as it is captured by the camera , it would look similar to the image you would expect from a normal camera , but it would be composed of thousands of little blobs rather than being a nice continuous image ( i am not talking about pixels , these dots are many pixels across ) . if you zoom in on this image , you would see that each dot is actually a very small , possibly blurry image of a portion of the scene being photographed . on its own , this image is ugly and not really good for anything , but because we know exactly how it was altered by the lenslet array , we actually have more information about the light that entered the camera 2 . with some clever data processing algorithms , we can retrieve this information and use it to determine the focus condition of each part of the scene being imaged . algorithms ( this section will be expand when i have more time ) in the most basic sense , the job of an imaging system is to produce an image where each point records the color and brightness of a corresponding point in the scene being photographed . a plenoptic camera also aims to determine , for each point in the image , how the light from the corresponding point in the scene was focused . this amounts to figuring out not only where that light hits your sensor ( which a normal camera measures ) but what path it took to get there . the data processing done after the image is captured is able to reconstruct this information because instead of having only one piece of information about each ray of light -- which pixel on our sensor it hit -- we now have a second piece of information -- which lenslet in the array did that ray pass through . once we have figured out the path of each bundle of light that we captured from the scene , we can calculate the image we would have gotten from a normal camera for any focus setting over a large range . 1: there are actually other way to achieve this , but the methods developed by ren ng are impressive and novel in that they are reliable and do not require insane amounts of computing power/time . this is what makes his technology marketable to consumers . 2: actually , we did not really get extra information , we just traded a little of one type of information for a little of another type . a normal camera would be able to produce one pixel in the output image for each pixel in its sensor , but this camera will produce an output image with about one pixel for each microlens in the array . the details of the algorithm may raise or lower that ratio a little , but the basic idea about trading one type of information for the other will always hold . this is , by the way , on reason that this technology did not happen sooner -- it was not until recently that we could produce high quality lenslet arrays with enough lenses to produce a good picture .
every ordinary star we are able to individually observe is a part of the milky way . well , except for stars in a small number of very nearby galaxies but even galaxies such as andromeda look like a " continuum " so we are not observing the stars individually although we see that the galaxy is not just a point . only if a star goes nova ( a lethal nuclear explosion of a white dwarf star ) of supernova ( a similar explosion but stronger ) , it may be observed outside the milky way . in all such cases we have experienced , one may always identify a galaxy at the same location that was known before the nova/supernova explosion . so the star going nova/supernova clearly belongs to that galaxy . please note that distant galaxies look like dots – pretty much visually indistinguishable from stars in the milky way . a star going nova has 50,000-100,000 times higher luminosity than the sun ; the number is even higher for a supernova . that is a sufficient increase of the luminosity for an exploding star in a distant galaxy to become " almost as bright " as the whole galaxy , well , not quite .
if a net force is towards the positive x direction , then the $x$ component of velocity of the object is increasing ; period . now , knowing this , are any of the statements ( a ) through ( d ) true ? a ) it can be moving in the negative x direction sure , it could be moving in the negative $x$ direction . all that is required is that the velocity is becoming less negative ( increasing ) , i.e. , that the acceleration is in the positive $x$ direction . b ) it can be speeding up sure , speed is the magnitude of the velocity . so , it could be speeding up if it were moving in the positive $x$ direction . c ) it can be slowing down sure , speed is the magnitude of the velocity . so , it could be slowing down up if it were moving in the negative $x$ direction . d ) it can be moving in the positive y direction sure and this motion would be completely unaffected by the net force in the positive x direction . acceleration is the rate of change in velocity . if one tells you that the velocity is changing but does not specify what the velocity is , you do not know anything about the velocity , only its rate of change .
so . . . the delta function in time is defined as : $\delta ( t - t_0 ) = \int _0^\infty \delta ( t ) dt = \int_t^{t_0} \delta ( t ) dt \equiv 1$ which means that the delta function has the unit of $sec^{-1}$ hence i should divide by the time step .
i cannot tell you if 18kv is correct but yes - it must be a high voltage to create such a spark . addressing the current : it is not simply a u=ri behavior since you have a capacity in your circuit which means , that the current get less over time . this is what the circuit looks like : which means , that the current would behave like : the current behaves like : the current does not equal the charge on the plate but they are dependent on each other by the following equation : keep in mind that if the voltage gets too low for a spark on a certain distance the discharge process stops . i hoped that this answer helped you and contains the information you liked to know .
the much larger mass of the rotating cylinder than that of the bullet , together with the friction between the cylinder and chamber position mechanism , would make the downward bias negligible .
no , it is not a boolean property . entanglement between two quantum systems ( could be particles , or anything else ) could be partial , and can be quantified using different measures . in the specific example of bell states , the two systems ( each of them with 2 states $|0\rangle$ and $|1\rangle$ ) are said to be maximally entangled with the entanglement entropy being 1 qubit .
i will recommand you a marvellous little book : supersymmetric quantum mechanics , ( asim gangopadhyaya , jeffry v mallow , constantin raisnariu ) , world scientific the first idea is to factorize an hamiltonian , using generalized creation/anihilation operators $a^-$ , $a^+$ , coming from a superpotential $w ( x ) $ , $a^\pm = w ( x ) \mp \frac{d}{dx}$ this could be done in two ways : $h^+ =a^- a^+$ and $h^-= a^+ a^-$ , and this leads to two potentials $v_\pm ( x ) = w^2 ( x ) \pm w' ( x ) $ . the fundamental interest of the above factorization of the hamiltonians is that they are positive definite ( $a^-$ and $a^+$ are hermitian conjugate each other ) these relations establish a link between the 2 spectra of the 2 potentials : except the ground state of $h^-$ , the spectra are the same , and the eigenfunctions are related each other by the operators $a^-$ , $a^+$ . so , surprinsingly , two very different potentials could have the same spectra ( for instance posch-teller potential and infinite one-dimensional square well potential ) moreover , with a supplementary property called translational shape invariance , one may calculate the all spectra of each of the 2 potential very easily , for instance , in the case of the radial part of coulomb potential , you may obtain , with only very basic and simple calculus ( no differential equations ! ! ) , the eigenvalues and the eigenfunctions , which is quite amazing . the relation with the supersymmetry is writing supersymmetric charges $q^-= \begin{pmatrix} 0 and 0\\ a^- and 0\end{pmatrix}$ , $q^+= \begin{pmatrix} 0 and a^+\\ 0 and 0\end{pmatrix}$ an hamiltonian is $h= \{q^- , q^+\} = \begin{pmatrix} h^+ and 0\\ 0 and h^-\end{pmatrix}$ . we have $ ( q^- ) ^2 = ( q^+ ) ^2=0$ , and $ [ q^\pm , h ] =0$ . the 2-dimensional space of the eigenfunctions may be understood as a boson+ fermion space .
the available methods depend on what you know about the cft ( or another quantum field theory ) . for example , there are nice classes of two-dimensional conformal field theories ( i mean " minimal models " etc . ) that are almost uniquely ( up to several integer-valued labels ) determined by the conformal symmetry . all the dynamics – correlators etc . – of these cfts are fully encoded by the opes which means that all this dynamics is fully encoded in the coefficients $c_{ijk}$ . these coefficients may be calculated as solutions to the constraints equivalent to the conformal symmetry . once you calculate them , you should view them as a major part of the definition of the cft – they are the most fundamental numbers defining the identity of the cft so it is futile to try to calculate them from something more fundamental . if you have more constructive cfts , the cfts may be essentially " free fermions " or " free bosons " . in such a case , the opes may be calculated by " wick contractions " of some kind . for more general ( but non-free ) cfts , it is helpful to notice that the operators $a_j ( w ) $ are in one-to-one correspondence to the states in the cft ( quantized on a circular space times infinite time ) and the structure coefficients $c_{ijk}$ may also be calculated from the action of the operator $a_i$ on the state corresponding to the equally named operator $|a_w\rangle$ .
bosonic fields are " more significant " than fermionic fields because they may get large vacuum expectation values – from a condensate of many bosons in the same state . consequently , there may exist a meaningful classical field theory limit . massless fields are " more significant " than massive fields because the massive fields in string/m-theory because the massive ones have masses of order the string scale or the planck scale which is huge and at these short distances or high energies , the classical reasoning breaks down , anyway . so we apply the classical effective equations of motion only at distances much longer than the string or planck scale and at these low energies , only the massless fields are visible ( the massive fields can not be excited so they are " integrated out " and do not appear in the action ) . becker-becker-schwarz try to jump to the truly consistent full theory , which is the supersymmetric one , as quickly as they can so the general bosonic string theory 's effective action may be absent in the book . but the corresponding action for the superstring theory is on page 311 etc . – type ii supergravity . borrow another textbook such as polchinski if your primary interest is bosonic string theory . there are kinetic and potential terms for the tachyon , some kinetic terms for the dilaton , the einstein-hilbert action for the metric tensor , and some natural " squared field strength " from the $b$-field . strictly speaking , the tachyon terms should be removed if we talk about " massless fields " because the tachyon field is not massless . but because of its negative squared mass , it is even " less massive " than the ordinary massive states as well as the massless states – it is " below " the massless level – so we usually do include it , too .
okay , so i gather from the link that $g^{ ( k ) }$ in your notation refers to the correlation between field values at $2k$ points , with $\varepsilon^+$ inserted at half of them and $\varepsilon^-$ inserted at the other half . this concept of an $n$-point correlation function is very similar to the $n$th moment of a random variable or statistical distribution . for simplicity , consider an example with 1 dimension : a field over 1 time dimension ( at a single point in space ) , described by a random variable . now , we can consider the probability distribution of this random variable and talk of it is moments . the mean value would be called the 1st moment and the variance would be related to the second moment ( it is in fact called the second central moment ) . similarly , you can generalize to higher order moments which help characerize the distribution . the moments help you characterize the distribution and also give an intuitive feel for the function . generalize this concept to random variables which are fields over many-dimensional spacetime . that is what your correlation functions are . btw , for a gaussian distribution ( non-interacting fields i.e. quadratic action ) , all odd moments vanish . ( that might be the motivation for $g^{ ( k ) }$ to be defined as the correlation between field values at $2k$ points . . . even though the actual physical theory you are considering will probably be interacting , else all correlation functions are fairly trivial ) . also , all even moments beyond the 2nd-moment are completely specified by the 1st and the 2nd moment . ref1 and ref2 if you had an interaction term in the hamiltonian/lagrangian involving 4 fields , then the 4-point correlation function would have 2 kinds of contributions : 2 sets of 2-point correlation functions between pairs of points among those 4 points a nontrivial contribution from the interaction term with one of it is field insertions at each of the 4 points . so you can see that higher order correlations functions give you very important ( an unique ) information in an interacting physical theory . update : the ( many ) answers to this se question might also shed some light on the discussion .
where you went wrong was $$-mgk \times \frac{1}{2}at^2=-mgk \times \frac{1}{2}\frac{\delta v}{t}t^2$$ . instead it should be $$-mgk \times \frac{1}{2}\frac{\delta v}{\delta t}t^2$$ anyways i would do it as $$f_{friction}s = mgk\frac{1}{2}at^2 = \frac{v-u}{2t}mgkt^2 = \frac{v-u}{2}mgkt = \frac{27.7ms^{-1}}{2} \times 540kg \times {10ms^{-2}} \times 0.6 \times t = ( 44874kgm^2s^{-3} ) t$$ so $$60kw \times t = ( 44874kgm^2s^{-3} ) t + 207000j$$ and solving gives $$t=13.7s$$
yes . there is a set of metric tensors that describe flat spacetime--that is , the spacetime of special relativity . general relativity allows us to consider many kinds of metrics , but limiting ourselves only to those that are flat reproduces all the basic predictions of special relativity . a big thing that separates sr from gr is that gr demands that matter and energy couple to the underlying curvature of spacetime . in the absence of such coupling , spacetime would simply be flat .
the lamp is not a point source . the smaller the angular size of the source , the narrower is the penumbral shadow region .
there were historically several systems of units ( ancestors to modern si , cgs electric , cgs magnetic , cgs gaussian , cgs by heaviside ) , and the ultimate choice in favour of gaussian cgs was made when special relativity has united electric and magnetic fields into one electromagnetic field tensor . only in gaussian ( and heavisidian ) versions , these fields take no additional factors and make components of the field tensor immediately . any other choice just looks ugly . for the reference , the electromagnetic field tensor in cgs has a form $$f_{\mu\nu}=\left ( \begin{array}{cccc}\hphantom{-}0 and \hphantom{-}e_x and \hphantom{-}e_y and \hphantom{-}e_z\\-e_x and \hphantom{-}0 and -b_z and \hphantom{-}b_y\\-e_y and \hphantom{-}b_z and \hphantom{-}0 and -b_x\\-e_z and -b_y and \hphantom{-}b_x and \hphantom{-}0\end{array}\right ) $$
the very claim that there are " four fources " is an approximation . we know that the electromagnetic and the weak force have to be unified to an electroweak theory . so counting the electroweak theory as one force , there are just three known elementary forces . the electroweak theory is based on the $su ( 2 ) \times u ( 1 ) $ group which has two factors , but these two factors are not in one-to-one correspondence with the electromagnetism and the weak force , respectively . the strong force with its $su ( 3 ) $ group is another seemingly independent factors , except that there is evidence that all three non-gravitational forces get unified into a grand unified force of a gut theory at high energies . string theory unifies the non-gravitational forces with gravity , too . every vacuum of string theory predicts gravity described by gr plus extra non-gravitational forces . the number of factors and their higgs-like breaking patterns are essentially random properties of the string vacua . according to the anthropic picture of the world , the number of low-energy forces is an accidental property of our world that could be different in different parts of the multiverse . according to non-anthropic reasoning , the precise selection of our vacuum - including the fact that it has 4 low-energy forces - could be derivable from some more unique theoretical principles . however , this research program remains a wishful thinking as of 2011 .
we can . but they do not scale well : to get grid-level storage , you need to be able to scale to gigawatts of power , and gigawatt-hours of energy . and to be able to cycle hundreds , or thousands , of times . to date , we have one technology which will do that , which is pumped storage hydro , which typically has a round-trip efficiency of 75% . so even though that is worse than a decent battery 's round-trip efficiency , its scalability means that it dominates grid storage . but that is just one small part of the picture . behind the question of storage , is the physics question of how do you balance an electricity grid . the grid typically has very low capacitance , so electricity in and electricity out must balance at every second . to manage that balance , you can either adjust the amount going in , or the amount going out , or both . there are lots of ways to integrate high wind penetrations into the grid : this is a solved problem technically . see , for example , the work in energy policy by delucchi and jacobson ; or the book by gregor czisch on renewable scenarios . there are more ways to do virtual storage than direct storage . for example , delaying consumption of 1gwh of electrical energy at 1gw power , for one hour , is equivalent to storing it at 100% efficiency for one hour . in the uk , a lot of energy is used for domestic hot water use . so thermal storage and delayed heating of that thermal storage , can act as a virtual storage for electrical heating of water . to put some numbers on that , uk domestic hot water storage is currently about equivalent to 60gwh @ 30gw for 24 hours . similarly , with 20 million cars , if they were all electrified , you might have 20 million 50kwh batteries , which is 1twh of electrical storage . v2g ( vehicle to grid ) studies often refer to a round-trip efficiency of about 75% , which comes from 10% loss on charge , 10% loss on discharge , and a few percent on transmission . to find out more , there is a wealth of literature on integrating renewables into the grid . the paper " energy-storage technologies and electricity generation " , hall and bain , energy policy 2008 available as a pdf here , sets out some of the issues of batteries and the grid .
if you go to this link you will see that the lifetime of the pi0 is orders of magnitude shorter than of the charged pions . 8.4 ± 0.6 × 10^−17 seconds , a time characteristic of electromagnetic reactions . it decays to two photons , which can be measured in the laboratory . if it is produced with some energy in the laboratory system , its speed can be estimated by measuring the four momenta of the photons and equating the sum to the four momentum of the pi0 . its speed then can be found for that individual measurement . there is no general " speed " of the pi0 , as there is no general speed of any elementary particle , their four momenta being dependent of the interaction that produced them and very variable . to have a speed a fraction of the speed of light any pion or other elementary particle should have an energy given by the relativistic formulae . have a look here where they calculate the energy necessary for a velocity 1% of the velocity of light for various particles .
you would have to use the fact that the momentum operator in position space is $\vec{p} = -i\hbar\vec{\nabla}$ and use the definition of the gradient operator in spherical coordinates : $$\vec{\nabla} = \hat{r}\frac{\partial}{\partial r} + \hat{\theta}\frac{1}{r}\frac{\partial}{\partial\theta} + \hat{\phi}\frac{1}{r\sin\theta}\frac{\partial}{\partial\phi}$$ so the radial component of momentum is $$p_r = -i\hbar\hat{r}\frac{\partial}{\partial r}$$ however : after a bit of investigation prompted by the comments , i found that in practice this is not used very much . it is more useful to have an operator $p_r&#39 ; $ that satisfies $$-\frac{\hbar^2}{2m}\nabla^2 r ( r ) = \frac{p_r&#39 ; ^2}{2m} r ( r ) $$ this lets you write the radial component of the time-independent schrödinger equation as $$\biggl ( \frac{p_r&#39 ; ^2}{2m} + v ( r ) \biggr ) r ( r ) = e r ( r ) $$ the action of the radial component of the laplacian in 3d is $$\nabla^2 r ( r ) = \frac{1}{r^2}\frac{\partial}{\partial r}\biggl ( r^2\frac{\partial r ( r ) }{\partial r}\biggr ) $$ and if you solve for the operator $p&#39 ; _r$ that satisfies the definition above , you wind up with $$p&#39 ; _r = -i\hbar\biggl ( \frac{\partial}{\partial r} + \frac{1}{r}\biggr ) $$ this is called the " radial momentum operator . " strictly speaking , it is different from the " radial component of the momentum operator , " which is , by definition , $p_r$ as i wrote it above , although i would not be surprised to find people mixing up the terminology relatively often .
trigonometric functions do not " preserve " units . the expression under a trigonometric function must be dimensionless and so is the value of a trigonometric function . thus , c 2 in your equations is in units of frequency : hz or 1/s . there is an error in one of the equations , perhaps a missing constant .
escape velocity depends on what you are trying to escape from and how far away from it you are . that wikipedia reference makes that very clear . as sachin shekhar pointed out , if you are in the vicinity of the sun , and you are trying to escape the galazy , you need 525 km/s . if you are trying to escape the solar system , and you are at neptune 's distance from the sun , you only need 7.7 km/s . when did voyager 2 achieve escape velocity from the solar system ? according to this diagram from wikipedia , it occurred during its gravity assist from jupiter . voyager 1 was probably not too much different .
by the word classical we will mean $\hbar=0$ , and we will use the conventions of ref . 1 . the lagrangian density for maxwell theory with various matter content is $$\tag{1} {\cal l} ~=~{\cal l}_{\rm maxwell} + {\cal l}_{\rm matter} , $$ $$\tag{2} {\cal l}_{\rm maxwell}~=~ -\frac{1}{4}f_{\mu\nu}f^{\mu\nu} , $$ $$\tag{3} {\cal l}_{\rm matter}~=~{\cal l}_{\rm matter}^{\rm qed}+{\cal l}_{\rm matter}^{\rm scalar qed} + \ldots , $$ $$\tag{4} {\cal l}_{\rm matter}^{\rm qed} ~:=~ \overline{\psi} ( i\gamma^{\mu} d_{\mu}-m ) \psi , $$ $$\tag{5} {\cal l}_{\rm matter}^{\rm scalar qed}~:=~ - ( d_{\mu}\phi ) ^{\dagger} d^{\mu}\phi -m^2\phi^{\dagger}\phi -\frac{\lambda}{4} ( \phi^{\dagger}\phi ) ^2 , $$ with covariant derivative $$ \tag{6} d_{\mu}~=~d_{\mu}-iea_{\mu} . $$ ( here we are too lazy to denote various matter masses $m$ and charges $e$ differently . ) the matter equations of motion ( eom ) are $$ \tag{7} ( i\gamma^{\mu} d_{\mu}-m ) \psi ~\approx~0 , \qquad d_{\mu}d^{\mu}\phi~\approx~m^2\phi+\frac{\lambda}{2} \phi^{\dagger}\phi^2 , \qquad \ldots . $$ ( the $\approx$ symbol means equality modulo eom , i.e. an on-shell equality . ) the infinitesimal global off-shell gauge transformation is $$ \delta a_{\mu} ~=~0 , \qquad \delta\psi~=~-i\epsilon \psi , \qquad \delta\overline{\psi}~=~i\epsilon \overline{\psi} , $$ $$ \tag{8} \delta\phi~=~-i\epsilon \phi , \qquad \delta\phi^{\dagger}~=~i\epsilon \phi^{\dagger} , \qquad \ldots , \qquad\delta {\cal l} ~=~0 , $$ where the infinitesimal parameter $\epsilon$ does not depend on $x$ . the noether current is the electric $4$-current$^1$ $$ \tag{9} j^{\mu}~=~e\overline{\psi}\gamma^{\mu}\psi - ie\{\phi^{\dagger} d^{\mu}\phi- ( d^{\mu}\phi ) ^{\dagger}\phi\}+\ldots . $$ noether 's theorem is a theorem about classical field theory . it yields an on-shell conservation law $$ \tag{10} d_{\mu}j^{\mu}~\approx~0 . $$ hence the electric charge $$\tag{11} q~=~\int\ ! d^3x~ j^0$$ is conserved on-shell . references : m . srednicki , qft . -- $^1$ interestingly , the electric $4$-current $j^{\mu}$ depends on the gauge potential $a_{\mu}$ in case of scalar qed matter .
i recommend you chapter 5 ( page 150+ ) of the ads bible , http://arxiv.org/abs/hep-th/9905111 concerning your individual questions , which are mostly answered at the beginning of that chapter , the additional virasoro generators correspond to bulk coordinate reparametrizations that preserve the metric at infinity , but they do map the ground state to excited states yes , the cfts in ads/cft typically have a nonzero central charge which is directly related to the $ads_3$ curvature radius in the planck units ; there is no reason for $c=0$ here because the boundary cft is not really coupled to gravity ( which is what the world sheet cft is doing ) for the same reason , you can not directly interpret the cft as string theory ; the full string theory needs $c=0$ in total , so extra ghosts must be added ; also , the interpretation of " winding/twisted " sectors is different in boundary cfts and string cfts . of course , this does not eliminate the fact that similar " building blocks of cfts " are used in both kinds of cfts . . .
there are a lot of methods to calculate free surface or multiphase flows , and most of them have some implementation of surface tension forces . a small list : volume of fluid method level set method marker and cell method moving mesh techniques
i will use a much simpler notation for starters , going to drop $\langle$ and $\rangle$ . so the first term in line a is $\sum_{i}\sum_{j}a_ia_j$ and if you write it explicitly you have $\sum_{i}\sum_{j}a_ia_j= ( a_1a_1+a_2a_2+\dots+a_na_n ) + ( a_1a_2+a_1a_3+\dots+a_1a_n ) +\dots+ ( a_na_1+a_na_2+\dots+a_na_{n-1} ) =\sum_ia_{i}^2+a_1\sum_{i\ne1}a_i+a_2\sum_{i\ne2}a_i+\dots+a_n\sum_{i\ne n}a_i=\sum_i a_{i}^2+\sum_i\sum_{j\ne i}a_ia_j$ so this gives you term one and two in line b . the third term in line b stays the same . now for the last line when you take the following difference $\sum_{j=1}^{n}\sum_{k=1 ( k\neq j ) ) }^{n} &lt ; f_{j}&gt ; &lt ; f_{k}&gt ; - \sum_{j=1}^{n} &lt ; f_ {j}&gt ; \sum_{k=1}^{n}&lt ; f_{k}&gt ; $ you get $\sum_{j=1}^{n} &lt ; f_{j}&gt ; ^2$ this is because the first double sum contains only terms like $f_if_j$ and the second sum cointains terms like $f_if_i$ and $f_if_j$ . so when you take the difference all the terms $f_if_j$ will cancel out and your left with $\langle f_i\rangle\langle f_i\rangle=\langle f_i\rangle^2$ . thus you find your final result .
what you are talking about is called a combined cycle engine . they are commonplace in stationary power generation , i.e. utility-scale electricity generation . there has even been some talk of combined cycle engines in cars . as pointed out in the answer by dmckee , the reason this has not been widely applied in cars is that no one has demonstrated an economically competitive combined-cycle car . i promise you , if such a thing can pay for itself in gas savings then it will eventually be built and sold , unless some better technology makes it irrelevant . in general there are many reasonable ideas that are physically permissible but economically or technically difficult or nonviable . you are effectively suggesting to add a steam engine to a car , which is quite a difficult proposal . i would suggest that a hybrid gas-electric car is more economical than what you suggest , and even they have had a hard time catching on . in electric power generation it matters much less that the combined cycle engine has a larger sunk cost than a normal engine , is heavier , etc . , so the economic balance works out . bringing the question back to physics , no matter what you use for heat scavenging , your engine including all of its " subengines " cannot exceed the carnot efficiency corresponding to the largest temperature difference in the engine . adding additional heat engines will help to approach the carnot limit . in order to beat carnot , you can not use heat as an intermediate step between chemical energy ( fuel ) and mechanical work .
the moment of inertia is merely a generalisation/application of the ‘usual’ inertia to rotations . since translations and rotations are different kinds of motion , it appears sensible ( to me ) to have different kinds of inertia associated with them . regarding your second question : imagine a particle at position $ ( x , 0,0 ) $ which you would like to rotate with angular velocity $\omega$ about the $ ( 0,0 , z ) $ axis . to do so , you have to initially accelerate the particle along the $ ( 0 , y , 0 ) $ axis to velocity $v_y = \omega x , v_{x , z} = 0$ , as this is the velocity the particle would have at this point if it were already rotating . as you can clearly see , the momentum $p$ associated with this velocity is proportional to $r$ ( $p_y = m v_y = m \omega x$ ) , hence it takes more energy to accelerate a particle to angular velocity $\omega$ if it is further away from the centre of rotation . as this is exactly the quantity described by the ‘moment of inertia’ , the moment of inertia depends on the radial distance of the mass .
you should read the wikipedia article on nuclear reactions for a start . while the number of possible nuclear reactions is immense , there are several types which are more common , or otherwise notable . some examples include : fusion reactions — two light nuclei join to form a heavier one , with additional particles ( usually protons or neutrons ) thrown off to conserve momentum . spallation — a nucleus is hit by a particle with sufficient energy and momentum to knock out several small fragments or , smash it into many fragments . induced gamma emission belongs to a class in which only photons were involved in creating and destroying states of nuclear excitation . alpha decay - though driven by the same underlying forces as spontaneous fission , α decay is usually considered to be separate from the latter . the often-quoted idea that " nuclear reactions " are confined to induced processes is incorrect . " radioactive decays " are a subgroup of " nuclear reactions " that are spontaneous rather than induced . for example , so-called " hot alpha particles " with unusually high energies may actually be produced in induced ternary fission , which is an induced nuclear reaction ( contrasting with spontaneous fission ) . such alphas occur from spontaneous ternary fission as well . neutron-induced nuclear fission reactions – a very heavy nucleus , spontaneously or after absorbing additional light particles ( usually neutrons ) , splits into two or sometimes three pieces . this is an induced nuclear reaction . spontaneous fission , which occurs without assistance of the neutron , is usually not considered a nuclear reaction . at most , it is not an induced nuclear reaction . direct reactions : an intermediate energy projectile transfers energy or picks up or loses nucleons to the nucleus in a single quick ( 10−21 second ) event . energy and momentum transfer are relatively small . these are particularly useful in experimental nuclear physics , because the reaction mechanisms are often simple enough to calculate with sufficient accuracy to probe the structure of the target nucleus . from this list one can see that " direct reactions " are specific scattering reactions with the purpose of studying a particular nucleus . thus the former list of nuclear reactions cannot be described as a combination or a series of " direct reactions " .
in case of walking on horizontal plane chemical energy is turned into heat . ( muscles are constantly contracting and expanding and in this way your body 's temperature increases . ) moving your limbs is not very efficient way of moving , this is why there is a room for improvement . e.g. if you are cycling ( or skating . . . ) , you use the same quantity of chemical energy to make much larger distance . if one could make a perfect bycicle ( without any friction in the wheels and friction between the wheel and the ground ) , you could make miles of horizontal distance without any effort/energy at all !
i would guess you mean self diffusion : see http://en.wikipedia.org/wiki/self-diffusion for details . suppose you take an aqueous solution of ( for example ) salt that is uniform so there are no concentration gradients . there is no net diffusion , but the sodium and chloride ions wander around due to random thermal motion , so if you watch a particular sodium atom it will " diffuse " around in a random walk motion .
wikipedia to the rescue ! fj dyson and a lenard : stability of matter , parts i and ii ( j . math . phys . , 8 , 423-434 ( 1967 ) ; j . math . phys . , 9 , 698-711 ( 1968 ) ) ; fj dyson : ground-state energy of a finite system of charged particles ( j . math . phys . 8 , 1538-1545 ( 1967 ) ) i found the reference in ref 6 of http://en.wikipedia.org/wiki/pauli_exclusion_principle
do not get too worried about fine meanings of the word : it is ultimately a little imprecise , and when you are thinking about real , physical problems , you are going to be working with equations . the more fundamental concept is interference , which is simply a manifestation of the linear superposition principle . amplitudes add , so magnitude and phase is important when summing up contributions to a field from different sources . diffraction works like this . suppose you know a monochromatic field 's values on one transverse plane . now fourier transform the values , to express the field on a transverse plane as a sum of plane waves . plane waves running nearly orthogonal to the transverse plane have almost the same phase over wide transverse regions . so they show themselves as low spatial frequencies in the transverse plane field pattern . plane waves running at steep angles to the transverse plane beget high spatial frequency components in that plane . so we have resolved our field into a linear superposition of plane waves . because these waves are propagating in different directions , they undergo different delays in reaching another transverse plane . the fourier co-efficients take on different phases , so the same constituent plane waves interfere together to make a different field configuration on other transverse planes . diffraction is thus the interference of a field 's ( e . g . an electromagnetic field following the linear maxwell equations ) plane wave constituents . these constituent plane waves beat differently on different transverse planes because they undergo different phase delays by dint of their different directions . see my answer here and also here for more info . another equivalent ( in the larger propagation distance limit ) is huygens 's principle . think of a single slit field . diffraction is the interference on a farfield plane between the different fields arising from the different huygens point sources at different positions in the slit .
k.y. tang is a geophysicist who is known for work on the allais effect , which is pathological science dating back to the 1950 's , when allais claimed anomalous effects on a foucault pendulum during an eclipse . a google scholar search shows no citations yet to tang et al . ' s february 2013 paper claiming to have measured the speed of gravity . as is often the case with pathological science , there seems to be a certain set of people who take the subject seriously and cite each other 's papers , while people outside their circle can not be bothered to debunk them . this particular subgroup includes kooks like van flandern , who has claimed , for example , that light propagates faster than $c$ . as discussed in the answers to this question , we have strong indirect confirmation from binary pulsars of gr 's prediction that gravity propagates at $c$ , whereas attempts at a direct measurement have been thwarted by the lack of any test theory that predicts any other speed for gravity . as with the previous bogus claim by kopeikin , tang et al . seem to have made no effort to seek the involvement of anyone competent in general relativity to help with analyzing and interpreting their data . a google scholar search shows a couple of papers , amador 2008 and duif 2004 , that reference tang 's previous work on the allais effect . amador , " review on possible gravitational anomalies , " 2008 , http://arxiv.org/abs/gr-qc/0604069 duif , " a review of conventional explanations of anomalous observations during solar eclipses , " 2004 , http://arxiv.org/abs/gr-qc/0408023
the fact that $p = \large \frac{\partial l}{\partial \dot{q}} = 0$ introduces a problem in the equivalence between lagrangian and hamiltonian representations . the idea is that the hamiltonian representation plus the constraint $p = 0$ is equivalent to the lagrangian representation the lagrangian $l$ is a function of $q$ and $\dot q$ , that is $l ( q , \dot q ) $ if we work with the lagrangian , we will apply the euler-lagrange equations which are : $$\frac{\partial l}{\partial q} = \frac{d}{dt} ( \frac{\partial l}{\partial \dot{q}} ) $$ because $\large \frac{\partial l}{\partial \dot{q}} = 0$ , the equation is simply $\large \frac{\partial l}{\partial q} = 0$ , that is $ \frac{1}{q} - 2\lambda = 0$ , so $q = \frac{1}{2 \lambda}$ now try to work with the hamiltonian . the hamiltonian $h$ is a function of $q$ and $p$ , that is $h ( q , p ) $ the link between the two is the legendre transformation : $$h=\dot{q}\frac{\partial l}{\partial \dot{q}}-l$$ because your lagrangian does not depends of $\dot q$ , then $p = \frac{\partial l}{\partial \dot{q}} = 0$ , and so : $$h ( q , p ) = - l ( q , \dot q ) = - \ln ( q ) + ( 2q-10 ) \lambda$$ from this hamiltonian , you get the equations of movement : $$\dot q = \frac{\partial h}{\partial p} ~ , ~\dot p = - \frac{\partial h}{\partial q}$$ so we have : $$\dot q = 0~ , ~\dot p = \frac{1}{q} - 2\lambda \tag{1}$$ from this , we cannot recover the equation obtained from euler-lagrange equations , we have to add the constraint $p = 0$ . if $p = 0$ , it means that $\dot p = 0$ , and so : $$q = \frac{1}{2 \lambda}\tag{2}$$ this is coherent with the fact that $\dot q = 0$
only with this , as the waves have different wavelengths , i guess there can not be any interference , we will only see the difraction pattern , the two functions of the form sin2 ( x ) /x2 , with the principal maximums separated a distance d . am i right here ? sort of . the diffraction pattern is visible " at infinity " , which is in fact your case #3 . i will explain there . 1 . - in the first one , the system is configured such that the slits are far away from the lens . here , we can approximate the wave that arrives as a planar wave , and therefore the lens will perform the fourier transform in the focal plane of the screen . the diffraction of the slits also performs the fourier transform , so this configuration should lead to having only two bars of light in the screen , centered in the focus . am i right ? sort of . your lens has a limited diameter , so if you place it far from the slits , it will capture only the central portion of the diffraction pattern , i.e. the top of the sinx/x function . in other words , you will loose the fringe pattern and reconstruct slits without fringes . 2 . - the slits are in the focal plane on the lens , such that the lens is in the middle of slits-screen . here , the same thing should happen , right ? as the light comes from the focal plane , the lens must do the fourier transform with no extra things , and we should get the two bars , again both of them in the same line ( center of the screen ) . am i right here ? this will be somewhat different because you do the image of the slits at infinity , i.e. a blurry image at close distances . depending on how close is the lens , you may only be taking the " no-fringe " portion of the diffraction pattern . 3 . - the last one , i can not see . . . the lens is just behind the slits so the distance between slits and lens is 0 . that is exactly the typical school case . the diffraction pattern is , before the lens , located at infinity , or in other words , the fringes are defined as angles and not position ( $\sin\theta/\theta$ ) . the role of the lens is to bring these fringes at a finite distance ( the focal length ) . you probably learned that a lens makes an image , initially located at infinity , located at the focal length . that is the same with the diffraction fringes . now , as the two slits are both close to the lens , you will not do an image of them . that means that you should not see separated slit images , but instead , you should see two superimposed diffraction patterns , centered at the same point .
the rule is : with a diagonal metrics , the character of the coordinate $x^i$ , is given by the sign of $g_{ii}$ setting $t= \frac{u + v}{2}$ and $z= \frac{u - v}{2}$ , your metrics becomes ( with the constraint $z &gt ; 0$ ) : $$ds^2= -dt^2 + dz^2+ ( 1-t-z ) ^2dx^2+ ( 1+t+z ) ^2dy^2$$ so , the character of $t$ as a time-like coordinate and $z$ as a space-like coordinate ( with the constraint $z &gt ; 0$ ) appears clearly . and $x$ and $y$ are space-like coordinates too . the variables $u = t + z$ and $v = t - z$ are called light-cone coordinates , you could say that they are light-like coordinates . they are very often used in general relativity and string theory . they are also called null coordinates , because for instance , if $dx=dy=0$ , then you have $ds^2 = - 2dudv$ , so each particle with $u=$constant or $v=$constant corresponds to $ds^2=0$ , that is a light-like interval ( a light ray ) . but you cannot say that one of the coordinates $u , v$ is a time-like coordinate , and the other a space-like coordinate . in schwarzchild metric , it is not correct to say that $r$ and $t$ could change their sign . the schwarzchild metric describes a gravitational field only for $r&gt ; r_s$ . it is a limited description ( which does not cover the entire manifold ) , and you have to use kruskal-szekeres coordinates to have a glbal view of the black hole . mathematically , " these coordinates have the advantage that they cover the entire spacetime manifold of the maximally extended schwarzschild solution and are well-behaved everywhere outside the physical singularity . "
when you derive a huygen-fresnel propagator ( which is how actual wavefronts propagate according to maxwell 's equations ) a fresnel zone is really the difference ( in phase ) between surfaces of equal phase on the propagating wavefront and a plane slicing or tangent to that surface of equal phase . these fresnel zones are defined when propagating a plane wave incident on some circular aperture . this picture shows the concept , except the fresnel zones are shown for a finite conjugate ( i.e. . phase difference between a propagating wave and a point that we want to use the zone plate act as a " lens" ) . basically , the fresnel zone is the phase difference map between something like a spherical wavefront , and a plane tangent to that wave front . that is exactly what it is physically and conceptually . a zone plate ( manufactured on a plane ) can then be used to change the propagation properties of the light wave . i say something like a spherical wavefront above because that is a good approximation , the actual green 's function for the propagator is the derivative of a spherical wavelet , which is required to satisfy the boundary conditions properly . the figure above shows how canceling the additive and subtractive parts of the wavefront can result in " lens type " behavior .
the details of your analysis are not quite right - that is not what the electric field of a moving charge looks like , for example . this is probably because you have not learned all the rules of electromagnetism yet . still , the spirit of your question is hitting at an important point . charges do not conserve momentum and do not obey newton 's third law . you have to include the momentum of the electromagnetic field to see conservation laws hold . there is an accessible discussion in section 8.2 of griffiths " introduction to electrodynamics " if you would like a little more math .
i think user689 's answer is basically correct , so you should regard this as just an extension/clarification of their answer . if you place a small volume of hot tea in contact with your tongue then the large thermal capacity of your tongue will cool the tea a lot and your tongue will heat up a little . this is essentially what happens when you sip tea . you pull in a certain volume of tea , this tea is spread out over the tongue and stays in contact with it for a second or so . in this time the tea cools and your tongue heats . however the heating of the tongue is relatively small because the heat from the tea is spread over a large area . if you suck the same volume of tea through a straw , then because the cross sectional area of the straw is small the velocity of the tea in the straw is high ( as user689 points out ) so the whole volume of the tea rapidly hits , and heats , a small area on the tongue . this causes much greater heating of that small area and scalds the tongue . exactly what the sensory effects of the scald are i will leave to our medical/biological colleagues . this also suggests a reason why drinking cool fluids through a straw is pleasant . the same effect means that when drunk through a straw a cold drink causes more intense local cooling than if it was drunk without a straw .
i think there is some interesting physics to be had here . the rate of change of temperature depends on the rate of heat flow in from the electric heating element and the rate of heat flow out as heat is lost to the air . if we write the heat capacity of the hotplate as $c$ ( $c$ is the traditional symbol for heat capacity ) then : $$ \frac{dt}{dt} = c \left ( \frac{dh_{in}}{dt} - \frac{dh_{out}}{dt} \right ) $$ where $dh_{in}/dt$ is the rate of heat flow in and $dh_{out}/dt$ is the rate of heat loss . $dh_{in}/dt$ is simply the power being supplied to the hotplate . you can measure the current the hotplate draws from the mains and calculate the power that way , or you could simply use a power meter . the power in watts , call this $w$ , is simply the energy in joules per second , so it is exactly what you need for $dh_{in}/dt$ . the rate of heat loss , $dh_{out}/dt$ is harder because it depends on how the hotplate is cooled . if the cooling is dominated by convention ( it probably is ) then the cooling will obey newton 's law of cooling and the heat loss will be given by : $$ \frac{dh_{out}}{dt} = a \space ( t - t_0 ) $$ where $t_0$ is the ambient temperature and $a$ is some constant to be determined experimentally . put all this together and you will get : $$ \frac{dt}{dt} = c \left ( w - a \space ( t - t_0 ) \right ) $$ you will need to measure $c$ and $a$ experimentally . if you have a copy of excel to hand you can use its solver to fit values of $c$ and $a$ . alternatively , you can get $c$ from the initial rate of temperature rise . when $t \approx t_0$ the heat loss is small and : $$ \frac{dt}{dt} \approx cw $$ so if you know $w$ you can calculate c . you can calculate $a$ by heating the hotplate then turning the power off and letting it cool . as it cools the temperature variation is : $$ \frac{dt}{dt} = -c a \space ( t - t_0 ) $$ so if you know $c$ you can calculate $a$ .
an electromagnetic wave with a well-defined frequency and direction , i.e. $\vec k$ , only has two possible truly physical i.e. transverse polarizations , i.e. the linearly polarized waves in the $x$ and $y$ direction ( or the two circularly polarized ones ) . that implies that a truly physical counting of polarizations gives you 2 , more generally $d-2$ in $d$ spacetime dimensions . starting from the $a_\mu$ potential fields , one component is unphysical because it is pure gauge , $a_\mu\sim\partial_\mu\lambda$ , and one of them is forbidden due to gauss ' constraint $\rm div\ , \vec d=0$ etc . that already constrains the allowed initial state of the electromagnetic field . both of these killed polarizations are ultimately linked to the $u ( 1 ) $ gauge symmetry . if one is allowed to count off-shell and unphysical fields , there may be many more components than two . but it is always possible to deduce that there are two physical polarizations at the end . for example , when we view $\vec b , \vec e$ as basic fields , there are six components , a lot . but these fields only enter maxwell 's equations through first derivatives , and not second as expected for " normal " bosonic fields , so these fields are simultaneously the canonical momenta for themselves . this brings us to three polarizations but one of them is killed by the constraints , the maxwell 's equations that do not contain time derivatives . the hertz vector is just the most famous " non-standard " example how to write the electromagnetic field as a combination of derivatives of some other fields . one must understand that the room for mathematical redefinitions etc . is unlimited and it is a matter of pure maths . all these descriptions may describe the same physics . at the end , the only " truly invariant because measurable " number of " fields " that all these approaches must agree about is the number of linearly independent physical polarizations of a wave/photon with a given $\vec k$ . if you can analyze any mathematical formulation of electromagnetism or another field theory and derive that there are $d-2$ physical polarizations ( this usually boils down to the difference of the number of a priori fields minus the number of independent constraints and the number of parameters defining identifications i.e. gauge symmetries – but the independence is sometimes hard to see and requires you to make many steps of the counting ) , then you have proved everything that is " really forced to be true " . various formalisms may offer you other ways to count the number of off-shell fields ( with different answers ) and they may be useful ( because they satisfy certain conditions or enter some laws ) but to discuss them , one has to know what the laws where they enter actually are . a truly physical approach is only one that counts the physical polarizations . the gauge symmetry is just a redundancy , a mathematical trick to get the right theory with 2 physical polarizations out of a greater number of fields with certain extra constraints or identifications . the precise number of constraints or identifications may depend on the chosen mathematical formalism and it is not a physically meaningful question – it is a question of a subjectively preferred mathematical formalism because the physics is equivalent for all of them .
at the moment of the lunar eclipse both the earth and moon are moving tangentially to the line joining them to the sun , and their velocities are parallel . the diagram above shows the moon just before , during and just after the lunar eclipse . i am guessing the question is asking whether the velocity of the moon , $v_m$ , is greater or less than the velocity of the earth , $v_e$ . if so , then you just have to see what direction the earth 's shadow moves across the moon . if $v_m &gt ; v_e$ then the moon starts at the lower position and moves up past the earth , so the shadow starts at the top edge of the moon and moves down . if $v_m &lt ; v_e$ then the moon starts at the top position and moves down , so the shadow starts at the bottom edge and moves up . note that my diagram shows the top view of the solar system i.e. looking down on the north pole , so the top edge is the east edge and the bottom edge is the west edge .
why is that the case ? why does all the heat go towards its kinetic energy per unit mass ? that is my one question basically there are two places for the energy to go in the output stream : thermal energy kinetic energy the problem through ( a ) is a direct energy balance problem . in this part , we are treating $t$ as independent . if $t$ is the average of the inlet temperatures , then the velocity will be zero because all energy is accounted for , and $\delta q$ will be zero . mentally , i see a thermal cycle with the $t_1$ feeds the boiler and $t_2$ feeds the condenser . that cycle outputs useful work . the more useful work it produces , the more the average temperature of the streams is lowered . this is similar to a thermal power plant . the boiler produces more heat than the condenser rejects . typical efficiency is 33% , so the condenser removes only 2/3rd of the boiler 's heat . the rest of the heat went to turning the turbine because heat is a form of energy and energy is the ability to do work . in your case , the stream might have been accelerated by a pump that feeds into a cavity that has a nozzle where the stream is accelerated . the more work the pump does , the more the temperature of the water has to decrease because this is a closed system . practically , either the temperature change would be miniscule or the velocity would be gigantic . the reason is that thermal vibrations are fast compared to speeds we are used to . you can look at it this way in terms of your problem too ! the average kinetic energy of the molecules is the same going in and going out - it is just that some of that kinetic energy going out is from the bulk motion of the stream so we do not count it as temperature . that temperature would have to be measured by a thermometer moving along with the stream .
i can´t fully come up with an explanation from more basic principles , but in the case you describe you will have kinetic friction . or at least that is what all engineering books say . . . there are a number of situations where this effect is clearly demonstrated : pulling a cork out of a bottle , using a basic corkscrew such as this : if you simply pull on the corkscrew , it is harder to pull it out than if you first get it rotating and then pull . while not entirely the same , a similar situation arises when a car rolling down a road with a strong side wind brakes and locks the wheels . while the wheels were rolling , there is no relative motion between the road and the tire , so there is static friction in effect , and unless the wind is really strong , as in a hurricane , the force will not overcome friction and the car will not skid sideways . once the brakes are locked , the car starts skidding forward , because the force due to the inertia of the car overcomes friction . once this happens , the car will also start skidding sideways , due to two factors : the lesser important is that the coefficient of friction in effect once there is relative motion is the kinetic , not the static one . the main effect is due to the fact that the direction of movement does not really matter at all : at the contact point you have a force due to inertia and a force due to the side wind , and once their combined magnitude exceeds friction , you will start having movement in the direction of that combined force , so forward but also to the side .
spinless non-identical particles . ground state : $ ( 0,0 ) \implies \text{non-degenerate}$ first excited state : $ ( 0,1 ) \text{ and } ( 1,0 ) \implies \text{doubly degenerate}$ spinless identical particles . ground state : $ ( 0,0 ) \implies \text{non-degenerate}$ first excited state : $ ( 0,1 ) + ( 1,0 ) \implies \text{non degenerate}$
feynman in multiple writings suggested thinking about " exchanging particles " in terms of exchanging them as they move through time . that is , they can either move in two parallel paths as they move forward , or they can cross paths ( exchange roles ) . the antisymmetric cancellation applies to the latter , but not to the former . now if you think that through , it means that the parallel path remains strong even as the crossover paths cancel out , resulting in the two particles avoiding each other and maintaining unique paths ( wave functions ) . the net result is not full cancellation , but cancellation at the edges , where the particles would cross . ( feynman goes into a lot more detail about rotations , but frankly that part can get you sidetracked a bit ; it is the " anti-crossover " part that counts in terms of actual outcomes . ) another consequence of identical fermions cancelling each other out is that packing more fermions into a tight space forces their space-filling wavelengths to become shorter also . since in quantum mechanics the spatial wavelength of a particle defines its momentum , particles that are squeezed in this fashion also get very , very hot . a neutron star is a good example . pauli exclusion -- the " constriction of space because crossover cancels but parallel does not " -- allows neutrons to pack together very densely indeed . there are limits , however . when gravity gets too monumental , even pauli exclusion is unable to keep up with the pace , and the entire star collapses , very quickly . thus is born a stellar-sized black hole , or at least this is one example of how one can form .
the acceleration of uniform circular motion is a very basic computation that we do for first year students . $$ a = \frac{v^2}{r} $$ which for someone standing on the earth 's equator comes to $$ a_\text{equator} \approx \frac{\left ( 465\text{ m/s}\right ) ^2}{6400\text{ km}} = 0.03\text{ m/s}^2$$ or less than 1% of g . that is a measurable quantity , but not very significant . indeed fluxuations of local $g$ at that level can ( and do ) occur simple due to local deposits of heavy ore . mining and oil companies use precise gravitation maps in surveys for exactly this kind of reason .
you will need to include a vertical and horizontal force at point c due to ladder ac ( since we should not assume a direction for the total force -- even though our intuition may prove to be correct ) . the vertical forces on ladder bc are then related by $f_v + n - w = 0$ and you have already established that $n = \frac{3w}{4}$ . there are now five forces acting on ladder bc , but if we look at the force moments about b , we can neglect the torque due to n and the static friction acting on the foot of the ladder . we can then sum the torques due to w , $f_v$ , and $f_h$ to zero . you should indeed find that the total force f at c ( using the pythagorean theorem ) comes to $\frac{w}{2}$ and the direction is 30º above the horizontal ( i find it pointing to the right ) .
it is because the atoms are arranged in a long chain that interacts mostly with itself , and very little ( at least for the electrons of interest ) with atoms in other chains . a better term might be " quasi-1d " since of course the atoms themselves are 3d , but 1d does convey the key idea that the parts of interest are interacting along a single dimension of space . quantum mechanics does very odd things when you insist that the wavelike properties of matter be limited to lines , planes , or for that matter points ( quantum does , atoms ) . you can see one reason by thinking about waves in tunnels : they do not dissipate ! a blast deep within a tunnel has nearly the same force when it exits it does when it happens deep in the tunnel , which is why explosive trucks are banned from long tunnels . in quantum mechanics your waves are further constrained by the need to arrive at a resonant , repeating pattern , which somewhat ironically is called a " stationary " solution since whatever it is does not appear to be moving when examined from our classical perspective . for long chains , that means that any long waves ( e . g . , conduction electrons in a metal ) must stabilize into solutions that are topologically similar to ordinary skip ropes , ones that can have one , two , three , or many more loops . for a semiconductor such as cdse you have more complicated electron configurations and energy levels , but you still maintain that need to settle into nicely resonant solutions . that in turn can lead to really interesting electronic and optical behaviors , which is why the fields of 1d and 2d ( and 0d , quantum dots ) have had and continue to have a lot of interesting materials research going on in them .
the van de graaff generator itself has stored potential energy , even without the extra charged object . like a capacitor , there will be energy stored in the electric field . when you turn the generator off , this energy does not vanish by itself but either dissipates slowly through corona discharge , or quickly by arc discharge . the potential energy in moving a charged object close to the generator would go through the same pathway . by bringing the object close to the generator , you will alter the electric field , increasing the electric potential on the generator . more energy will then be dissipated in any arc or corona discharge .
on the bortle dark-sky scale , it takes the very darkest skies - class 1 . these will be very , very dark . the " seeing " does not have to be especially good , of course , since the gegenschein is an extended feature and is essentially unaffected by atmospheric refraction . poor seeing degrades the resolution of observations , but it has no effect on the brightness .
you start out getting a bachelor of science in a related field . this could be physics , astronomy , mathematics , or possibly chemistry . depending on which country you are planning to go to grad school in , specializing at this stage may not be as important as in later stages . however , note that in the uk , for example , it is almost unheard of for a student without a bachelors in physics ( almost always with a minor in astrophysics ) to gain a place in grad school for astronomy . after doing the bsc , you would go on to get a masters degree in astronomy , which would set the stage for a phd in astronomy . the phd ( and to some extent , the msc ) will have you specialize in a specific area in astronomy , such as star formation , planetary studies , or cosmology . after doing the phd , you would get a post-doc at a university or research institute . this is typically a three-year job where you do research in your chosen area . most astronomers do two or three post-docs . after this , you could become a professor or research associate at a university or an in-house research astronomer at an observatory . universities or observatories are typically the only places to be a " professional astronomer " , and depending on the position , would give you time to do your own research in conjunction with other duties like teaching .
generally the fifth equation is pressure conservation : $$ \frac{\partial p}{\partial t}+\mathbf v\cdot\nabla p+\gamma p\nabla\cdot\mathbf v=0 $$ which , depending on your particular subfield , is usually written in terms of the total energy : $$ \frac{\partial e}{\partial t}+\nabla\cdot\left [ \left ( e+p\right ) \mathbf v\right ] =0 $$ where $e=\frac12\rho v^2+p\left ( \gamma-1\right ) ^{-1}$ . however , if you want to do this in terms of the entropy and you are working with an ideal gas , then we can define a variable $$ s\equiv p\rho^{-\gamma} $$ as a measure for entropy ( not entropy itself because it is missing certain constants , e.g. heat capacity at constant volume $c_v$ ) . then you can use the total derivative to evolve $s$: $$ \frac{ds}{dt}=\frac{\partial s}{\partial t}+\mathbf v\cdot\nabla s=0 $$ the connection between this measure of entropy , $s$ , and the entropy density , $s$ , is $$ s=c_v\ln ( s ) =c_v\ln\left ( p\rho^{-\gamma}\right ) =c_p\ln\left ( p^{1/\gamma}\rho^{-1}\right ) $$ where we used $\gamma=c_p/c_v$ between the latter two equalities . the above relation can be found in section 83 of landau 's fluid dynamics text .
the only force which works is gravity$^1$ . so , change in gravitational potential energy equals final kinetic energy ( assume initial is zero ) . $$mgh=mv^2/2$$ $$v=\sqrt{2gh}$$ here $h$ is vertical height traversed . see the velocity does not depend on angle of string , mass of body too . . let 's see the kinematics of body . the length of string is $h cosec\theta$ ( $\theta $ being angle with horizontal assumed $\pi/6$ ) acceleration of body along the string=$g\sin\theta$ now $\text{using} : v^2=u^2+2as$ $$v^2=0+2\times h cosec\theta\times g \sin\theta$$ $$v=\sqrt{2gh}$$ working in differentials for $v$ along the rope . $$dv/dt=v\dfrac{dv}{dx}=a$$ $$\int_0^{v_f} v . dv=\int_0^{hcosec\theta} a . dx=ax\bigg|_0^{hsosec\theta}$$ $$\dfrac{v_f^2}2=gsin\theta . hcosec\theta \ \ ; \ \ a=gsin\theta$$ $1 ) $assuming the pulley being used to slide to be friction less . though not possible . also the rope is assumed to be in-extensible and straight .
according to most sources such as this university page on albedo and this modeling paper on albedo versus wavelength , the typical albedo of snow is virtually 1 throughout the visible region , so it seems unlikely that it is the snow itself which is causing this effect . also , i have never personally observed this as john rennie stated , so perhaps it is caused by something else in your case , such as an atypical source of illumination .
the up quark has a charge of $+2/3$ , the down has a charge of $-1/3$ . if you have a bound state of charged particles , the total charge is just the charge of the elementary constituents . the neutron consists of one up quark and two down quarks , so the total charge $q$ is : $$q = 2/3 + 2 \times ( -1/3 ) = 0$$
both " perfectly open " ( zero acoustic impedance ) and " perfectly closed " ( infinite acoustic impedance ) boundary conditions are only idealizations that never occur in practice . for the case of the human vocal tract , they are not even very good approximations . the " bottom end " of the resonating cavity is not , in fact , the lungs , but the vocal folds ( as georg pointed out ) . this end has some acoustic impedance that is neither extremely low nor extremely high . i am sure the impedance also changes somewhat with the pitch and volume of the phonation . the " top end " of the cavity is of course the mouth , and its impedance changes with the vowel sound you are pronouncing . for aptly-named " open " vowels such as " ah " , " oh " , and so on , the impedance is actually low enough that it might be a good approximation to say it is perfectly open . but for closed vowels ( "ee " , " oo " , . . . ) and especially for humming with closed lips , the acoustic impedance is much higher ( but still far from infinite ) .
objects are not damaged by momentum $\vec{p}$ they are damaged by force $\vec{f}$ . when two objects collide their momenta changes because of forces they apply each other while being in contact . according to 2nd newton 's law the force can be calculated as follows : $$ \vec{f} = \frac{d\vec{p}}{dt} $$ so the force is determined by the time of interaction . when two objects contact their surfaces are flexed . the bigger is the flex the higher is the force which changes the momentum . at some moment of time the relative normal momentum ( and velocity ) becomes zero and the objects start move backward . at this moment the force and the flex is maximal . if the surface ( armor ) is not strong enough for this flex the object is damaged . this can happen long before the relative normal velocity become zero . in that case the interaction between internal parts of the objects started . momentum hence damage is not determined only by momentum . it is determined by the force and the ability of the object resist it . the force is not constant during the collision and its maximal value depends on momentum and the time of interaction of the surfaces . the time of interaction depends on both flexibility and strength . when a basketball hits the floor its momentum changes from $\vec{p}$ to almost $-\vec{p}$ so the total change is almost $2p$ . when a glass hits the floor its momentum becomes zero so the change is $p$ . the ball has higher flexibility and strength and is not destroyed even though the momentum change is two times higher . another example is a bullet that hits a door . it makes hole before the door opens . the force is huge but the momentum of the door is almost not changed because the time necessary to reach critical flex is too short . when one pushes the door with his finger the force is small and does not destroy the surface before the flex stop the finger . the door gets enough momentum to start move . energy when the surface is flexed in irreversible way or damaged some part of kinetic energy of the objects turns into kinetic energy of their parts , heat , sound , light etc . this is called inelastic collision . elastic collision means no damage . if you need a good model this should be taken into account .
photons go on for ever unless they hit something and space is pretty empty . so unless there is a grain of dust , or a star in the way a photon will travel across the universe . paradoxically it is harder for high energy photons such as x-rays to travel large distances in space . because of their energy they can be effected by passing close to even something as small as a single electron . the other reason we still do not see x-ray objects at the same vast distances that we see infrared sources is that , even with chandra , x-ray telescopes are smaller and less sensitive than optical or radio telescopes and so we need more photons from the source , so it needs to be brighter or closer . the reason distant objects are fainter is not so much that photons are blocked - it is that the photons spread out into a sphere . so at 2x the distance away they are spread over an area 4x as big and so are diluted .
assume for simplicity that the speed of light $c=1$ . the existence of the gauge $4$-potential $a^{\mu}= ( \phi , \vec{a} ) $ alone implies that the source-free maxwell equations $$\vec{\nabla} \cdot \vec{b} ~=~ 0 \qquad ``\text{no magnetic monopole"}$$ $$ \vec{\nabla} \times \vec{e} + \frac{\partial \vec{b}}{\partial t} ~=~ \vec{0}\qquad ``\text{faraday 's law"}$$ are already identically satisfied . to prove them , just use the definition of the electric field $$\vec{e}~:=~-\vec{\nabla}\phi-\frac{\partial \vec{a}}{\partial t} , $$ and the magnetic field $$\vec{b}~:=~\vec{\nabla}\times\vec{a}$$ in terms of the gauge $4$-potential $a^{\mu}= ( \phi , \vec{a} ) $ . the above is more naturally discussed in a manifestly lorentz-covariant notation . op might also find this phys . se post interesting . thus , to repeat , even before starting varying the maxwell action $s [ a ] $ , the fact that the action $s [ a ] $ is formulated in terms the gauge $4$-potential $a^{\mu}$ means that the source-free maxwell equations are identically satisfied . phrased differently , since the source-free maxwell equations are manifestly implemented from the very beginning in this approach , varying the maxwell action $s [ a ] $ will not affect the status of the source-free maxwell equations whatsoever .
maybe an example : a particle moving in 2 dimensions has a lagrangian $$l = \frac{\dot{x}^2 +\dot{y}^2}{2} $$ so $$p_x = \frac{\partial l}{\partial \dot{x}} = \dot{x}$$ $$p_y = \frac{\partial l}{\partial \dot{y}}=\dot{y}$$ suppose it is constrained to move on a circle $x^2+y^2=r^2$ now there is a constraint between the p 's which you can get from differentiating the constraining circle , namely $$x\dot{x}+y\dot{y}=0$$ this is a constraint , but not of the type you are talking about , since the lagrangian is still regular . to obtain a lagrangian which is singular rather than regular , we require c onstraints which result in the vanishing of the hessian matrix $\frac{\partial^2l}{\partial \dot{q}_i \partial \dot{q}_j}$ . this means that the legendre transform ( sometimes called the floer map ) from the tangent bundle to the cotangent bundle ( phase space ) $$\mathcal{fl} : tq \rightarrow t^{*}q$$ given by $$ ( q_i , \dot{q}_i ) \rightarrow ( q_i , p_i=\frac{\partial l}{\partial \dot{q}_i} ) $$ is not invertible . it is image is restricted by a bunch of constraint functions . ( caveat , assuming we are restricted to a neighbourhood where rank of hessian is constant ) . for example , for the following lagrangian $$l=\frac{1}{2} ( \dot{x}^2+\dot{y}^2 ) +\dot{x}\dot{y}+4x\dot{y}+2x^2+4xy$$ the hessian determinant is easily seen to vanish . the generalized momenta are $$p_x=\dot{x}+\dot{y}$$ $$p_y=\dot{x}+\dot{y}+4x$$ you can then eliminate $\dot{x}$ and $\dot{y}$ from these relations to find your constraint equation . ( edited to provide example appropriate to the op 's question )
this question is very difficult to answer , and in the end , the answer is going to be more semantic than it is going to be physcial . the reason for this is that it is very difficult to pull apart what is done by " gravity " and what is done by the matter content of the universe . the safest answer to this is to say that during inflation , the matter content of the universe has a predominantly negative pressure , which causes objects to expand . during a truly inflationary period , the density of this substance does not decrease as the universe expands , so the rate of expansion is approximately exponential . there are various ways to create a scenario like this if you assume different properties for the quantum field theoretic vacuum or various types of matter coupled to gravity .
this question is answered by nima arkani-hamed in his simons center talk , at about 112 minutes in . his answer is that the structure of the amplituhedron itself does not directly use integrability of the theory in any way . it is only when you come to do the integrals themselves that integrability makes it possible . the amplituhedron itself is more linked to locality and unitarity conditions , while the positivity of the grassmannian is linked to the planar limit . since they only have the amplituhedron fully working for the n=4 theory at present it is impossible to say that integrability is not necessary to make the structure work , but he would be very surprised if it is not possible to extend the amplituhedron itself to n &lt ; 4 and perhaps even to ordinary n=0 yang-mills . the part which should change with n is the form defined on the amplituhedron which is integrated to give its " volume " . this form is simple for n=4 with just logarithmic singularities on the boundary . for n &lt ; 4 you need to multiply by another factor that has singularities elsewhere corresponding to unltraviolet divergences , although this extension of the theory is not in such a complete state as the n=4 case , they are optimistic that it still works without the integrability so there is no reason to think that integrability is hidden in all qfts by the way they also hope to go beyond the planar limit by replacing positivity with some more general structure and it sounds like that work is progressing well .
the wikipedia is showing a standing wave while the second link has a wave propagating along the string and reflecting from a wall . these are 2 distinct types of waves . both these types are possible and valid .
exchange interaction is an addition to other interactions between identical particles caused by permutation symmetry . this addition is a result of specific form of multi-particle wave function . it gives no contribution to hamiltonian unlike " usual " interactions but appears as an additional term in equations for single -particle wave functions ( e . g . hartree-fock equation ) . interaction usually associated with energy and forces . we could find the exchange correction as a force added to coulomb forces , but we should understand first what is force in quantum system . let 's consider two fermions with single-particle coordinate wave functions $\psi_a ( x ) $ and $\psi_b ( x ) $ and spin wave fucntions $\phi_a ( s ) $ and $\phi_b ( s ) $ . the possible two-particle wave fucntions are singlet with symmetric coordinate part $$ \psi_s ( x_1 , x_2 ) = \frac{1}{\sqrt{2}}\left [ \psi_a ( x_1 ) \psi_b ( x_2 ) + \psi_a ( x_2 ) \psi_b ( x_1 ) \right ] $$ and triplet with antisymmetric coordinate part $$ \psi_a ( x_1 , x_2 ) = \frac{1}{\sqrt{2}}\left [ \psi_a ( x_1 ) \psi_b ( x_2 ) - \psi_a ( x_2 ) \psi_b ( x_1 ) \right ] $$ let the two-particle hamiltonian do not depend on spins : $$ \hat{h} = \frac{\hat{\mathbf{p}}_1 + \hat{\mathbf{p}}_2}{2m} + v ( x_1 , x_2 ) $$ then the average energy of the interaction will be : $$ u_s = \left&lt ; \psi_s\right|v\left|\psi_s\right&gt ; = u + u_\text{ex} $$ $$ = \left&lt ; \psi_a ( x_1 ) \psi_b ( x_2 ) \right|v\left|\psi_a ( x_1 ) \psi_b ( x_2 ) \right&gt ; + \left&lt ; \psi_a ( x_1 ) \psi_b ( x_2 ) \right|v\left|\psi_a ( x_2 ) \psi_b ( x_1 ) \right&gt ; $$ $$ u_a = \left&lt ; \psi_a\right|v\left|\psi_a\right&gt ; = u - u_\text{ex} $$ $$ = \left&lt ; \psi_a ( x_1 ) \psi_b ( x_2 ) \right|v\left|\psi_a ( x_1 ) \psi_b ( x_2 ) \right&gt ; - \left&lt ; \psi_a ( x_1 ) \psi_b ( x_2 ) \right|v\left|\psi_a ( x_2 ) \psi_b ( x_1 ) \right&gt ; $$ the term $u_\text{ex}$ is not zero only if the particles are close enough to each other and their wave functions overlap ( see picture below ) . in classical limit when distance $l$ is big the overlapping is zero and $u_s=u_a=u$ let 's assume that $\psi_a$ and $\psi_b$ are non-negative everywhere anv $v$ acts as coulomb interaction ( i.e. . positive and decreases when the distance increases ) . then $u$ and $u_\text{ex}$ are positive and energy of symmetric coordinate state ( opposite spines ) is higher than energy of antisymmetric coordinate state ( similar spines ) . if the average positions of the particles are fixed the exchange interaction will put the spins same direction . the force of interaction between the particles can be defined as the generalized force corresponding to the parameter l : $$ f = -\frac{\partial u}{\partial l} $$ within our assumptions concerning $\psi_a$ , $\psi_b$ and $v$ the derivative of both $u$ and $u_\text{ex}$ are negative . hence the " usual " force is positive ( repulsion ) and the exchange force is positive for symmetric coordinate state and negative for antisymmetric coordinate state ( attraction ) . so the exchange interaction for the case of two particles can be considered as additional force depending on spin configuration . for multiple particles this is more complicated .
without seeing the quote/context i can only imagine that it means something like : if you take , say , a cube moving at close to c in the z direction , then ( in the frame in which it is moving ) its z extent gets lorentz contracted to virtually zero , so it is effectively now a square in the xy plane and has only the degrees of freedom that a square in the xy plane has .
there is a related question does brown but transparent swimming pool water heat significantly faster than western style highly chlorinated pools ? . the question is not a duplicate , though the answers there are relevant . at the equator the intensity of sunlight at the ground is about 1kw/m$^{2}$ , of which about half is visible and half is ir ( plus a few per cent in the uv ) . the as fffred says in the comment , water absorbs ir radiation so about half the energy is absorbed directly by the water . the visible light will pass through water unabsorbed and will heat the walls of the swimming pool . because water absorbs ir you get a greenhouse effect that keeps the walls warm , and the walls then warm the water by conduction and convection . estimating exactly how fast the water heats will be hard because the swimming pools walls tend to painted a light colour that will reflect a lot of the light . we had have to know the reflectance to calculate how much light the walls absorb and hence how fast they heat . presumably the walls will lose some heat to the ground , though i would guess this will be slow . finally need to take into effect the fact that the sunlight intensity falls with latitude , and of course depends on atmospheric conditions . assuming maximum absorption and a pool on the equator sunlight will heat a 2m deep pool at slightly under half a degree per hour .
when we say that the spin of a silver atom ( in a magnetic field ) is $+1/2$ or $-1/2$ we mean its component in the direction of the magnetic field ( referred to as $s_z$ ) is $+1/2$ or $-1/2$ . the magnitude of the spin is the same in both cases , it is just the direction that is different . hund 's rule just tells you what the magnitude of the total spin is , and does not say anything about the direction that spin is pointing . for example with two unpaired electrons hund 's rule tells us the total spin will be $s = 1$ . however put that atom in a magnetic field and you can have $s_z = 1$ , $0$ or $-1$ . the two blobs in the stern-gerlach experiment correspond to the electrons with $s_z = +1/2$ and $-1/2$ , but all the electrons have the same total spin $s = 1/2$ . the two unpaired electron system with $s = 1$ would give us three blobs corresponding to $s_z = 1$ , $0$ and $-1$ .
contrary to what queueoverflow says , you do not actually need to perform any integration here ; a pretty cool symmetry argument will give you the answer . let the cube we are considering in the problem have side length $\ell$ . the trick is to consider putting the charge at the center of an imaginary cube of side length $2\ell$ . the flux through the surface of this cube is just $q/\epsilon_0$ by gauss 's law since it is a closed surface containing the charge . now imagine dividing each face of this larger cube into four squares of side length $\ell$ . by symmetry , the flux through each of these squares is the same , but there are a total of 24 such squares since there are six faces , so the flux through each of these squares is $q/ ( 24\epsilon_0$ ) . now , simply notice that if we were to cut the larger cube into eight cubes of side length $\ell$ , then each of these squares mentioned in the last paragraph would be a face of one of these cubes at which the charge is at a corner . qed . regarding your confusion . notice that a bunch of the flux coming from the charge does not even pass through the faces of the cube when the charge is at its corner . only one eighth of its total flux goes through the faces of the cube as illustrated by the argument above . then , each of the three faces opposite the charge get one third of this flux because the other three faces next to the charge are parallel to the electric field , so there is no flux through them .
given a vector $$\vec{v} = \begin{pmatrix} x \\ y \\ z \end{pmatrix} $$ and a general rotation axis $$ \vec{k} = \begin{pmatrix} k_x \\ k_y \\ k_z \end{pmatrix} $$ then the resulting vector after a rotation by $\theta$ is $$\vec{u} = \vec{v}\cos\theta + \left ( \vec{k} \times \vec{v} \right ) \sin \theta + \vec{k} \left ( \vec{k} \cdot \vec{v} \right ) ( 1-\cos \theta ) $$ where $\times$ is the vector cross product and $\cdot$ the dot product . if you are looking for the angle to rotate , then look up angle between two vectors . the the rotation axis is defined by the cross product of the original vector and the target vector . i am not sure how familiar you are with C# but there is the code to do what i think you are asking .
when i use the linear equation , i have always written it as $r_{2}=r_{1} ( 1+\alpha \ \delta t ) $ . for this problem , $r_{1}=100\omega$ and $r_{2}=200\omega$ . also substituting the value for $\alpha$ will give you the change in temperature from 100 degrees . so remember to add the 100 to the $\delta t$ for the complete answer . regarding why you got different answers , it is because linear approximations are linearized about a point and do not always hold . the question indicates that you are to linearize about $100^{\circ}$ . keep in mind that a slope of $0.005\ /^{\circ}c$ is 50% per $100\ ^{\circ}c$ , which equates to $50\ \omega\ per\ 100\ ^{\circ}c$ . if you start at $100\omega$ at $100^{\circ}$ , if you get $100^{\circ}$ colder you will lose $50\omega$ . this gives a different result at 0 than what you had .
whether the conformal symmetry is local or global depends on the theory ! more precisely , the symmetry that may be local is not really conformal symmetry but ${\rm diff}\times {\rm weyl}$ . for example , in all the cfts we use in the ads/cft correspondence , for example the famous ${\mathcal n}=4$ gauge theory in $d=4$ , the conformal symmetry is global – and , correspondingly , it is a physical symmetry with nonzero values of generators . this is related to the fact that the cft side of the holographic duality is a non-gravitational theory so it avoids all local symmetries related to spacetime geometry . the previous paragraph holds even if the dimension of the cft world volume is $d=2$ . in $d=2$ , it may happen that the global conformal symmetry is extended to the infinite-dimensional local symmetry where $\omega ( x ) $ depends on the location . however , such an enhancement looks " automatic " only classically . quantum mechanically , a nonzero central charge $c\neq 0$ prevents one from defining the general local conformal transformations . in all the cfts from ads/cft , we have $c\geq 0$ . such a nonzero $c$ leads to the " conformal anomaly " ( proportional to the world sheet ricci scalar and $c$ ) . on the contrary , the world sheet $d=2$ cft theories used to describe perturbative string theory always have a local diffeomorphism and local weyl symmetry . this is needed to decouple all the unphysical components of the world sheet metric tensor ; and a necessary condition is the incorporation of the conformal ( and other ) ghosts so that in the critical dimension , we have the necessary $c=0$ . we say that the world sheet cft is " coupled to gravity " as we add the world sheet metric tensor , the diff symmetry , and the weyl symmetry . the weyl symmetry is the symmetry under a general scaling of the world sheet metric by $\omega ( x ) $ that depends on the location on the world sheet . one may gauge-fix this local weyl symmetry along with the 2-dimensional diffeomorphism symmetry , e.g. by demanding the $\delta_{ij}$ form of the metric tensor . this gauge-fixing still preserves some residual symmetry , a subgroup of the originally infinite-dimensional " diff times weyl " symmetry . this residual symmetry is nothing else than the infinite-dimensional conformal symmetry generated by $l_n$ and $\tilde l_n$ . because its being infinite-dimensional , we may call it a local conformal symmetry but it is really just a residual symmetry from " diff times weyl " . the global $sl ( 2 , c ) \sim so ( 3,1 ) $ global subgroup is the mobius group generated by $l_{0 , \pm 1}$ and those with tildes , too . as far as i know , this local conformal symmetry is a special case of some $d=2$ theories . in higher dimensions , the weyl and diff are not enough to kill all the components of the metric tensor and the " partially killed " theories with a dynamical metric are still inconsistent as the usual naively quantized versions of general relativity . in all the cases above and others , it is true that the local symmetries – where the parameter $\omega ( x ) $ is allowed to depend on time and space coordinates ( if the latter exist ) – are gauge symmetries ( in the sense that the generators are obliged to annihilate physical states ) while the global symmetries are always " physical " in your sense of the charge 's being nonzero . these equivalences follow from some easy logical argument . when you have infinitely many generators of the ( space ) time-dependent symmetry transformations , it follows that all the quanta associated with these generators exactly decouple – have vanishing interactions – with the gauge-invariant degrees of freedom . so we always study the physical part of the theory only , and it is the theory composed of the gauge symmetry 's singlets . greetings to david .
it depends what you call " one state " . with only one species , the fock state basis is of the form $|n_1 , n_2 , n_3\rangle$ which gives the number of particle on the sites $1$ , $2$ , $3$ . this is one state of the system ( even though it is not a eigenstate ) . in the case with two species , one can trivially generalize the notation , with a basis $|n^a_1 , n^b_1 , n^a_2 , n^b_2 , n^a_3 , n^b_3\rangle$ . so your notation $ [ 1,0,1 ] $ is ambiguous if you do not explain what the number are , even in with one species ( it could be $ [ n_2 , n_1 , n_3 ] $ , etc . ) . once you have chosen a convention , there is no ambiguity left .
in fluids , conservation of mass and momentum are still applicable , only not that easily , because there are internal forces in a fluid , i.e. caused by viscosity . in liquids , conservation of mass is described by the continuity equation , while the conservation of momentum is described by the navier-stokes equations of course , you can incorporate the effect the solid has in these equation , but this easier said than done , and large depends on the properties of the solid ,
the notation $t_{n1}$ may come from the fact that the mean is also referred to as the " first moment " , this is what the number $1$ as a lower label might stand for . for reference , see the wikipedia article on moments in mathematics . regarding the derivative of the energy : you are supposed to formally take the derivative of some expression with respect to some variable , even if it is a discrete one .
the form of your book 's value $\frac{\hbar^2}{2ma^2}$ makes it clear that that is a fully classical ( non-relativistic ) limit ( $\frac{p^2}{2m}$ , right ? ) , while yours is a fully relativistic one ( after all $e , p \gg m_e$ ) . the argument about the nuclear confinement is simply that both the gravitational and elctromagnetic potentials on the electron due to the nucleus are many orders of magnitude smaller than 40 mev ( the weak nuclear force too , but you may not know how to compute this ) , and the electron is not affected by the strong nuclear force .
i ) first of all , one should never use the dirac bra-ket notation ( in its ultimate version where an operator acts to the right on kets and to the left on bras ) to consider the definition of adjointness , since the notation was designed to make the adjointness property look like a mathematical triviality , which it is not . see also this phys . se post . ii ) op 's question ( v1 ) about the existence of the adjoint of an antilinear operator is an interesting mathematical question , which is rarely treated in textbooks , because they usually start by assuming that operators are $\mathbb{c}$-linear . iii ) let us next recall the mathematical definition of the adjoint of a linear operator . let there be a hilbert space $h$ over a field $\mathbb{f}$ , which in principle could be either real or complex numbers , $\mathbb{f}=\mathbb{r}$ or $\mathbb{f}=\mathbb{c}$ . of course in quantum mechanics , $\mathbb{f}=\mathbb{c}$ . in the complex case , we will use the standard physicist 's convention that the inner product/sequilinear form $\langle \cdot | \cdot \rangle$ is conjugated $\mathbb{c}$-linear in the first entry , and $\mathbb{c}$-linear in the second entry . recall riesz ' representation theorem : for each continuous $\mathbb{f}$-linear functional $f : h \to \mathbb{f}$ there exists a unique vector $u\in h$ such that $$\tag{1} f ( \cdot ) ~=~\langle u | \cdot \rangle . $$ let $a:h\to h$ be a continuous$^1$ $\mathbb{f}$-linear operator . let $v\in h$ be a vector . consider the continuous $\mathbb{f}$-linear functional $$\tag{2} f ( \cdot ) ~=~\langle v | a ( \cdot ) \rangle . $$ the value $a^{\dagger}v\in h$ of the adjoint operator $a^{\dagger}$ at the vector $v\in h$ is by definition the unique vector $u\in h$ , guaranteed by riesz ' representation theorem , such that $$\tag{3} f ( \cdot ) ~=~\langle u | \cdot \rangle . $$ in other words , $$\tag{4} \langle a^{\dagger}v | w \rangle~=~\langle u | w \rangle~=~f ( w ) =\langle v | aw \rangle . $$ it is straightforward to check that the adjoint operator $a^{\dagger}:h\to h$ defined this way becomes an $\mathbb{f}$-linear operator as well . iv ) finally , let us return to op 's question and consider the definition of the adjoint of an antilinear operator . the definition will rely on the complex version of riesz ' representation theorem . let $h$ be given a complex hilbert space , and let $a:h\to h$ be an antilinear continuous operator . in this case , the above equations ( 2 ) and ( 4 ) should be replaced with $$\tag{2'} f ( \cdot ) ~=~\overline{\langle v | a ( \cdot ) \rangle} , $$ and $$\tag{4'} \langle a^{\dagger}v | w \rangle~=~\langle u | w \rangle~=~f ( w ) =\overline{\langle v | aw \rangle} , $$ respectively . note that $f$ is a $\mathbb{c}$-linear functional . it is straightforward to check that the adjoint operator $a^{\dagger}:h\to h$ defined this way becomes an antilinear operator as well . -- $^{1}$we will ignore subtleties with discontinuous/unbounded operators , domains , selfadjoint extensions , etc . , in this answer .
i love manly arguments ! from this hail mary pass , it looks like , with initial angle 45deg and velocity $\sim 30$ m/s , the best football pass can go 70 yards in 4 sec hang time . if he had thrown it straight up , the maximum height would have been about 50 yards , just barely making it . give him a running start and no 300-lb defenders to worry about , and i say it is possible ! best nfl punts have hang time of 6-8 secs , so they also would have just made it .
you may just not bother to use a test function , here . this problem is so easy you can work it all just using the properties of the commutator . $$ [ xp_y , x ] =x [ p_y , x ] + [ x , x ] p_y$$ now $ [ p_y , x ] $ vanishes because of the fundamental commutation relation between $p_i$ and $x_i$ which is $$ [ p_i , x_j ] = -i\hbar \delta_{ij}$$ on the other hand $ [ x , x ] =0$ because anything commmutes with itself .
no . you can add an arbitrary constant shift ( or an arbitrary operator commuting with $x$ ) without affecting the ccr . for 1-dimensional qm , the general solution of the ccr with $\hat x$ represented as multiplication by $x$ on wave functions with argument $x$ is $\hat p=\hat p_0-a ( \hat x ) ~~$ , where $\hat p_0$ is the canonical momentum operator , and $a ( x ) $ is an arbitrary function of $x$ . proof . the difference $\hat a:=\hat p_0-\hat p~$ commutes with $\hat x$ , hence is a function of $\hat x$ .
the most authoritative solar system ephemeris system is the jpl horizons system . there is a web interface that will let you perform computations and extract solar system data from the jpl development ephemerides , the international standard .
assuming 1 , you live somewhere that is colder outside than in 2 , the curtain has finite thermal resistance ( ie some insulating value ) 3 , the curtain is close enough to the window to reduce convection then yes . try measuring the air temperature on the window side of the curtain , it should be lower than the room .
i am happy to know that you are interested in space and the nature of time , your questions indicate that you dont have a very clear understanding of what is already understood about the subject , but show an interested student and potentially a good one . people talk about the expansion of the universe and black holes and all these things , but to talk about them correctly you need to learn how to talk about them . i would highly recommend einstein 's paper , " on the electrodynamics of moving bodies " . ( available online ) please look into it , if you have questions post them on this forum . saying that " is time the rate at which one moves through space " is not correct . one definition that can serves our purposes is that time is what is measured by clocks . to understand the behaviour of moving clocks in relation to stationary clocks , you have to understand this concept called the proper time . the concept of proper time is a generalization of the distance between 2 points you learn in high school . implication of the theory of relativity is that clocks measure proper time . definition of proper time is $$tp = ( c^2 dt^2-dx^2 ) ^{1/2}$$ where dt is the time elapsed and dx is the distance moved . ( it must be summed by dividing path into small segments but it does not matter if you move with constant velocity ) . now we know for something moving at the speed of light $$x=ct . $$ implies that $$tp = ( c^2t^2 - c^2 t^2 ) ^{1/2} = 0$$ which is why clocks moving at the speed of light dont show the movement of time . another way to understand this , is a clock moving at the speed of light shows the same time when you see it .
if you search the iter site , iter being the international prototype fusion reactor which will demonstrated the possibility of getting megawat useful energy from fusion , one sees that their main aim is to demonstrate this feasibility : the main carrier of energy out of the plasma is the neutron , and methods to efficiently use this energy have not been developed yet , but wait for the commercial prototype . the helium nucleus carries an electric charge which will respond to the magnetic fields of the tokamak and remain confined within the plasma . however , some 80 percent of the energy produced is carried away from the plasma by the neutron which has no electrical charge and is therefore unaffected by magnetic fields . the neutrons will be absorbed by the surrounding walls of the tokamak , transferring their energy to the walls as heat . in iter , this heat will be dispersed through cooling towers . in the subsequent fusion plant prototype demo and in future industrial fusion installations , the heat will be used to produce steam and—by way of turbines and alternators—electricity . they have developed methods for cooling the system and dissipating the energy to the environment .
in theories with spontaneous symmetry breaking , the phase transition can usually be characterized by a local order parameter $\delta ( x ) $ , which is not invariant under the relevant symmetry group $g$ of the hamiltonian . the expectation value of this field has to be zero outside the ordered phase $\langle\delta ( x ) \rangle = 0$ , but non-zero in the phase $\langle\delta ( x ) \rangle \neq 0$ . this shows that there has been a spontaneous breaking of $g$ to a subgroup $h\subset g$ ( where $h$ is the subgroup that leaves $\delta ( x ) $ invariant ) . what local means in this context , is usually that $\delta ( x ) $ at point $x$ , can be constructed by looking at a small neighborhood around the point $x$ . here $\delta ( x ) $ can be dependent on $x$ and need not be homogeneous . this happens for example when you have topological defects , such as vortices or hedgehogs . one powerful feature of these landau-type phases , is that there will generically be gapless excitations in the system corresponding to fluctuations of $\delta ( x ) $ around its expectation value $\langle\delta ( x ) \rangle$ in the direction where the symmetry is not broken ( unless there is a higgs mechanism ) . these are called goldstone modes and their dynamics are described by a non-linear $\sigma$-model with target manifold $g/h$ . an example is the order parameter for s-wave superconductors $\langle\delta ( x ) \rangle = \langle c_{\uparrow} ( x ) c_{\downarrow} ( x ) \rangle$ , which breaks a $u ( 1 ) $ symmetry down to $\mathbb z_2$ . but there are no goldstone modes due to the higgs mechanism , the massive amplitude fluctuations are however there ( the " higgs boson" ) . [ edit : see edit2 for correction . ] a non-local order parameter does not depend on $x$ ( which is local ) , but on something non-local . for example , a non-local ( gauge-invariant ) object in gauge theories are the wilson loops $w_r [ \mathcal c ] = \text{tr}_r{\left ( \mathcal pe^{i\oint_{\mathcal c}a_\mu\text dx^\mu}\right ) } , $ where $\mathcal c$ is some closed curve . the wilson loop thus depends on the whole loop $\mathcal c$ ( and a representation $r$ of the gauge group ) and cannot be constructed locally . it can also contain global information if $\mathcal c$ is a non-trivial cycle ( non-contractible ) . it is true that topological order cannot be described by a local order parameter , as in superconductors or magnets , but conversely a system described by a non-local order parameter does not mean it has topological order ( i think ) . the above mentioned wilson loops ( and similar order parameters , such a the polyakov and ' t hooft loop ) , is actually a order parameter in gauge theories which probe the spontaneous breaking of a certain center-symmetry . this characterizes the deconfinement/confinement transition of quarks in qcd : in the deconfined phase $w_r [ \mathcal c ] $ satisfies a perimeter law and quarks interact with a massive/yukawa type potential $v ( r ) \sim \frac{e^{-mr}}r$ , while in the confined phase it satisfy an area law and the potential is linear $v ( r ) \sim \sigma r$ ( $\sigma$ is some string tension ) . there might be other examples of spontaneous symmetry breaking phases with non-local order parameter . [ edit : see edit2 . ] let me just make a few comments about topological order . in theories with with spontanous symmetry breaking , long-range correlations are very important . in topological order the systems are gapped by definition , and there is only short-range correlation . the main point is that in topological order , entanglement plays the important role not correlations . one can define the notion of long-range entanglement ( lre ) and short-range entanglement ( sre ) . given a state $\psi$ in the hilbert space , loosely speaking $\psi$ is sre if it can de deformed to a product state ( zero entanglement entropy ) by locally removing entanglement , if this is not possible then $\psi$ is lre . a system which has a ground state with lre is called topological order , otherwise its called the trivial phase . these phases have many characteristic features which are generally non-local/global in nature such as , anyonic excitations/non-zero entanglement entropy , low-energy tqft 's , and are characterized by so-called modular $s$ and $t$ matrices ( projective representations of the modular group $sl ( 2 , \mathbb z ) $ ) . note that , unlike popular belief , topological insulators and superconductors are sre and are not examples of topological order ! if one requires that the system must preserve some symmetry $g$ , then not all sre states can be deformed to the product state while respecting $g$ . this means that sre states can have non-trivial topological phases which are protected by the symmetry $g$ . these are called symmetry protected topological states ( spt ) . topological insulators/superconductors are a very small subset of spt states , corresponding to restricting to free fermionic systems . unlike systems with lre and thus intrinsic topological order , spt states are only protected as long as the symmetry is not broken . these systems typically have interesting boundary physics , such as gapless modes or gapped topological order on the boundary . characterizing them usually requires global quantities too and cannot be done by local order parameters . edit : this is a response to the question in the comment section . i am not sure whether there are any reference which discuss this point explicitly . but the point is that you can continuously deform/perturb the hamiltonian of a topological insulator ( while preserving the gap ) into the trivial insulator by breaking the symmetry along the way ( they are only protected if the symmetry is respected ) . this is equivalent to locally deforming the ground state into the product state , which is the definition of short range entanglement . you can find the statement in many papers and talks . see for example the first few slides here . or even better , see this ( slide with title " compare topological order and topological insulator " + the final slide ) . let me make another comment regarding the distinction between intrinsic topological order and topological superconductors , which at first seems puzzling and contrary to what i just said . as was shown by levin-wen and kitaev-preskill , the entanglement entropy of ground state for a gapped system in 2+1d has the form $s = \alpha a - \gamma + \mathcal o ( \tfrac 1a ) $ , where $a$ is the boundary area ( this is called the area law , not the same area law i mentioned in the case of confinement ) , $\alpha$ is a non-universal number and $\gamma$ is universal and called the topological entanglement entropy ( tee ) . what was shown in the above papers is that the tee is equal to $\gamma = \log\mathcal d$ , where $\mathcal d\geq 1$ is the total quantum dimension and is only strictly $\mathcal d&gt ; 1$ ( $\gamma\neq 0$ ) if the system supports anyonic excitations . modulo some subtleties , lre states always have $\gamma\neq 0$ , which in turn means that they have anyonic excitations . conversely for sre states $\gamma = 0$ and there are no anyons present . this seems to be at odds with the existence of ' majorana fermions ' ( non-abelian anyons ) in topological superconductors . the difference is that , in the case of topological order you have intrinsic finite-energy excitations which are anyonic and the anyons correspond to linear representations of the braid group . while in the case of topological superconductors , you only have non-abelian anyons if there is an extrinsic defect ( vortex , domain wall etc . ) which the zero-modes can bind to , and they correspond to projective representation of the braid group . the latter type anyons from extrinsic defects can also exist in topological order , but intrinsic finite-energy ones only exist in topological order . for more details , see the recent set of papers from barkeshli , jian and qi . edit2: please see my comments below for some corrections and subtleties . such as , it is in a sense not correct that superconductors are described by a local order parameter . it only appears local in a particular gauge . superconductors are actually examples of topological order , which is rather surprising .
it is important to understand the context in which statements like " there must be a singularity in a black hole " are made . this context is provided by the model used to derive the results . in this case , it was classical ( meaning " non quantum" ) general relativity theory that was used to predict the existence of singularities in spacetime . hawking and penrose proved that , under certain reasonable assumptions , there would be curves in spacetime that represented the paths of bodies freely falling under gravity that just " came to an end " . for these curves , spacetime behaved like it had a boundary or an " edge " . this was the singularity the theory predicted . the results were proved rigorously mathematically , using certain properties of differential equations and topology . now in this framework , spacetime is assumed to be smooth - it is a manifold - it does not have any granularity or minimum length . as soon as you start to include the possibilities of granular spacetime , you have moved outside the framework for which the original hawking penrose theorems apply , and you have to come up with new proofs for or against the existence of singularities .
let 's look at the relationship between momentum and energy . as you know , for a mass $m$ kinetic energy is $\frac12mv^2$ and momentum is $mv$ - in other words energy is $\frac{p^2}{2m}$ now to counter the force of gravity we need to transfer momentum to the air : $f\delta t = \delta ( mv ) $ the same momentum can be achieved with a large mass , low velocity as with small mass , high velocity . but while the momentum of these two is the same , the energy is not . and therein lies the rub . a large wing can " move a lot of air a little bit " - meaning less kinetic energy is imparted to the air . this means it is a more efficient way to stay in the air . this is also the reason that long thin wings are more efficient : they " lightly touch a lot of air " , moving none of it very much . trying to replicate this efficiency with an engine is very hard : you need compressors for it to work at all ( so you can mix air with fuel and have the thrust come out the back ) and this means you will have a small volume of high velocity gas to develop thrust . that means a lot of energy is carried away by the gas . think about the noise of an engine - that is mostly that high velocity gas . now think of a glider : why is it so silent ? because a lot of air moves very gently . i tried to stay away from the math but hope the principle is clear from this .
there are various possible reasons why the normal force of the floor on the box might not equal the weight of the box . for example : there could be another box on top of the box being asked about . the floor could be the floor of an elevator that is accelerating ( so the net force on the box would not be zero ) . the " floor " could be a board , with a c-clamp pressing the box to the board . the " floor " could be the bottom of a tank of water ( so there would be a buoyant force on the box ) . and of course you could put these together in combinations .
the plane of the earth 's orbit is extremely stable . of course , the earth 's orbit is affected by the other planets , especially jupiter , but all the planets orbit in approximately the same plane , so the forces pulling the earth 's orbit out of its plane are small . we can see that the planes of the planets ' orbit are stable , because all the planets are in roughly the same plane after 4.5 billion years , and it is approximately the same plane as the sun 's rotation , so it was determined by the angular momentum of the cloud from which the entire solar system formed . a big change in the orbital plane of a planet could be caused by a close encounter with jupiter . this may have happened early on - indeed planets may have been ejected from the solar system altogether . will this change in orbital plane have any visual or physical effect for an observer on the earth ? let 's imagine that some slow and non-catastrophic process causes the earth 's orbital plane to rotate by 90º . we need to ask : what happens to the earth 's rotation axis ? if it stays the same ( and why would it change ? ) , then the effects would be dramatic , as the axis is now approximately in the plane of the orbit , pointing almost directly towards the sun twice a year . the whole of northern hemisphere would be in daylight for several months in the " summer " , then in darkness during the " winter " .
the effective gravity inside the iss is very close to zero , because the station is in free fall . the effective gravity is a combination of gravity and acceleration . if you are standing on the surface of the earth , you feel gravity ( 1g , 9.8 m/s 2 ) because you are not in free fall . your feet press down against the ground , and the ground presses up against your feet . inside the iss , there is a downward gravitational pull of about 0.89g , but the station itself is simultaneously accelerating downward at 0.89g -- because of the gravitational pull . everyone and everything inside the station experiences the same gravity and acceleration , and the sum is close to zero . imagine taking the iss and putting it a mile above the earth 's surface . it would experience about the same 1.0g gravity you have standing on the surface , but in addition the station would accelerate downward at 1.0g . again , you will have free fall inside the station , since everything inside it experiences the same gravity and acceleration ( at least until it hits the ground ) . the big difference , of course , is that the iss never hits the ground . its horizontal speed means that by the time it is fallen , say , 1 meter , the ground is 1 meter farther down , because the earth 's surface is curved . in effect , the station is perpetually falling , but never getting any closer to the ground . that is what an orbit is . ( as douglas adams said , the secret of flying is to throw yourself at the ground and miss . ) but it is not quite that simple . there is still a little bit of atmosphere even at the height at which the iss orbits , and that causes some drag . every now and then they have to re-boost the station , using rockets . during a re-boost , the station is not in free fall ; instead . the result is , in effect , a very small " gravitational " pull inside the station -- which you can see in this fascinating video .
we start with the definition $$\tag{1} s^{\alpha \beta}~:=~u^\alpha v^\beta-u^\beta v^\alpha . $$ indices are raised and lowered with the metric . up to an overall factor , one has $$\tag{2} \bar{s}_{\alpha \beta}~\propto~ \epsilon_{\alpha \beta \gamma \delta} s^{\gamma \delta} , $$ so that the matrix trace $$ \mathrm {tr} ( \mathbf{\bar{s}\cdot s } ) ~=~ \bar{s}_{\alpha \beta} s^{\beta\alpha} ~\stackrel{ ( 2 ) }{\propto}~ \epsilon_{\alpha \beta \gamma \delta} s^{\gamma \delta}s^{\beta\alpha} ~\stackrel{ ( 1 ) }{\propto}~ \epsilon_{\alpha \beta \gamma \delta} u^{\gamma}v^{\delta}u^{\beta}v^{\alpha}$$ $$\tag{3} ~\stackrel{ ( 4 ) }\propto~ \det [ \mathbf{u , v , u , v} ] ~=~0$$ is just the determinant of the $4\times 4$ matrix with column vectors $\mathbf{u , v , u , v}$ . this is zero , because the determinant is totally antisymmetric in its column vector entries . -- ( 4 ) : see e.g. wikipedia .
you are very nearly there . you are correct to say that $power = fv$ , and that the velocity at the moment the train reaches the top of the slope is given by $v_{top} = 350/ ( 6 + 150gsin2 ) $ so the force at the top of the slope is $f_{top} = 1000 ( 6 + 150gsin2 ) $ . but the acceleration is the net force divided by the mass , and the net force is $f_{top}$ minus the 6kn frictional force i.e. $$ a = \frac{f_{top} - 6000}{m} = \frac {1000 ( 6 + 150gsin2 ) - 6000}{150000} = g sin2$$ which using $g = 9.81m/sec^2$ i get as 0.342 .
the most general relationship is $$c ( b ) = \frac{\int_0^b \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b}{\int_0^\infty \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b} = \frac{1}{\sigma_\text{inel}}\int_0^b \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b\tag{1}$$ ( source , one of many ) . in practice , we usually use the glauber model to describe heavy ion collisions , and this model predicts an impact parameter dependence of the differential cross section which can be ( very roughly ) approximated as $$\frac{\mathrm{d}\sigma}{\mathrm{d}b} \approx \begin{cases}2\pi b , and b \le b_\text{max} \\ 0 , and b &gt ; b_\text{max}\end{cases}$$ where $\pi b_\text{max}^2 = \sigma_\text{inel}$ . that reduces equation ( 1 ) to $$c ( b ) = \frac{\pi b^2}{\sigma_\text{inel}}$$ for $b &lt ; b_\text{max}$ . you do have to be careful because sometimes ( rarely ) a different definition is used , $c ( b ) = 1 - \pi b^2/\sigma_\text{inel}$ . just pay attention to whether large centrality values correspond to peripheral ( the former definition ) or central ( the latter ) collisions . in practice , this is all somewhat approximate anyway , because you can not definitively identify the centrality of a collision from the information collected by a detector . all you can do is estimate the centrality based on how many particles come out and how strongly they are scattered . if you get a lot of particles coming out roughly perpendicular to the beamline ( pseudorapidity $\eta\sim 0$ ) , then that means a lot of nucleons were involved in the collision , and thus it is characterized as central . if there are few particles coming out perpendicular to the beamline , then few nucleons were scattered , meaning the collision was peripheral .
the simple answer is that the chemical properties of oil and water result in them having smaller coefficients of friction than other surfaces like concrete . the frictional force exerted on a skidding body is given by $f=\mu_k n$ where $n=mg$ is the normal force of the body , and $\mu_k$ is the coefficient of kinetic friction . assuming the same object skids on both concrete and water , $n$ will be constant while $\mu_k$ will be smaller for water , thus resulting in a smaller frictional force ( allowing the object to skid further on water ) . now you may be wondering why $\mu_k$ is smaller for water than concrete . kinetic friction is primarily caused by chemical bonding between a surface and the object skidding on that surface . in classical mechanics however , it is not necessary to understand the chemical properties of the surfaces being studied . it is just accepted that some surfaces bond stronger than others , with $\mu_k$ being measured for different materials by experiment .
$x_0$ is proportional to $\sqrt{e_{initial}}$ square root of initial energy in the system . for example suppose we have dumped motion of a mass on a spring and $x$ is distance from equilibrium position . initial energy of the system can be fully in the tension of the spring ( zero initial velocity ) . initial energy is $e_{initial} = \frac12k\ , x_{initial}^2 = \frac12k\ , x_0^2$ generaly for system in the example : $x_0=\sqrt{2\frac{e_{initial}}{k}}$ we can change distribution of initial energy between potential and kinetic energy ( changing $\phi$ ) but if sum of initial energy is the same then $x_0$ is also the same .
circularly polarized light should create a circle on the oscilloscope , which is a type of lissajous curve . any polarization of light produces a lissajous curve with the restriction that the two frequencies of the x and y parameters are the same . so the only possibilities are a circular , elliptical , and linear polarization . if your oscilloscope does not show a circle for circularly polarized light , something has gone wrong in the experiment . edit : this does assume your laser is a simple harmonic wave , so that the electric field varies sinusoidally and we therefore have in-phase x and y components . also , see polarization .
i do not think this is possible . suppose that your unaided eye can focus on objects over some range of distanced from $d_1$ ( closest ) to $d_2$ ( furthest ) . putting a converging lens in front of your eye reduces both $d_1$ and $d_2$ . that is , it shifts the range of distances over which you can focus closer to you , not further . here 's the geometric-optics proof of this . varying the distance at which you can focus is equivalent to varying the effective focal length of your eye . if $d$ is the diameter of your eye , and $d$ is the distance of the object you are focusing on , then the focal length is given by $$ {1\over f}={1\over d}+{1\over d} . $$ when you put on your reading glasses , the effective $f$ is decreased , or to put it another way , the effective power of the lens , $1/f$ , is increased . ( to be specific , $1/f_{\rm new}=1/f_{\rm old}+1/f_{\rm lens}$ . ) so the left side of the equation becomes larger . $d$ does not change , so $1/d$ must become larger . so $d$ , the distance to which you focus , becomes less .
no , maxwell did not invent surface or line integrals . see history of stokes ' theorem , which explains surface integrals were in use earlier .
in my opinion the presentation you linked is a weird description that conflates different phenomena in a confusing way . the following is a totally wrong explanation of the the results of that referenced paper ( non-paywall link ) : " the front part of the pulse has the higher-energy part of the light , which is more likely to tunnel . " both parts of this sentence are wrong : ( 1 ) the frequency profile at the front and back of the pulse is essentially the same in this experiment ( the light pulse is not substantially " chirped" ) ; ( 2 ) it is not true that higher-energy ( higher-frequency ) light tunnels with higher probability through this particular structure . ( see fig . 1 in the paper ; the photon is around 702nm , where the transmission curve is almost flat . technically , the transmission minimum is at 692nm , so the lower -frequency parts are transmitted with very slightly higher probability . however , this slight difference is not important for the effect--see the theoretical curves in fig . 1 . ) it is certainly true that the front part of the pulse is reduced a little bit while the back part is reduced a lot , which moves the center of the pulse forward . but the reason for this differential reduction is not that sentence above . i think it is a more subtle kind of wave-interference effect , which is not necessarily easy to explain intuitively .
newton 's cradle fights air resistance and restitution ( efficiency of rebound ) . you then want the highest density , hardest balls with the highest restitution . cobalt-sintered tungsten carbide is magnetic . polished hardened tool steel ball bearings are a good start . http://www.wired.com/wiredscience/2011/10/what-went-wrong-with-the-mythbusters-newton-cradle/ platinum plus gallium and indium alloys heat treat to a very hard and springy state , density around 19 g/cm^3 versus less than 8 g/cm^3 for tool steel ( steve kretchmer , niessing co . , eastern smelting ; platinum sk ( tm ) alloys ) . for more shallow wallets , tungsten steel , thoriated tungsten . 95.5% pt , 3.0% ga , 1.5% in ; 95.2% platinum , 4.8% ga , in , cu ; 1550-1650 c melt . 700 c for 30 minutes and slow cool to harden ( not reducing atmospheres ) . vickers hardness 318/rockwell a 76/rockwell c 32 . 125,000 psi tensile , 104,000 psi yield .
if you say that earth 's velocity around the sun is 67,000 mi/h , your reference point is the sun itself , which makes the aeroplane 's velocity 68,000 mi/h , not 1000 . using special relativity only , and ( a ) observing from the sun , a clock on the plane would seem to run slower than a clock on earth . a person ( b ) on earth would measure also measure an aeroplane 's clock to be slower , but by a different factor . time dilation , $ \delta t ' = \frac{\delta t}{\sqrt{1-\frac{v^2}{c^2}}} $ ( a ) observation from the sun : time dilation of aeroplane clock vs earth clock , ( i ) $\frac{\delta t}{\sqrt{1-\frac{68000^2}{c^2}}}$ and ( ii ) $\frac{\delta t}{\sqrt{1-\frac{67000^2}{c^2}}}$ ( b ) observation from the earth : time dilation of aeroplane clock vs earth clock , ( iii ) $\frac{\delta t}{\sqrt{1-\frac{1000^2}{c^2}}}$ these are all different values due to different velocities as measured from different observation locations . this is what relativity is all about . ( side note , this equations are correct iff $c$ is in mi/h . ) up to this point we have ignored general relativity , which takes into account time dilation due to gravitational acceleration ( clocks are measured to be faster in lower g ) . this is an opposite effect from the time dilation due to sr . as it turns out , aeroplanes are travelling way too slow to have their clocks observed to be slowed at all --in fact they measure to be faster than earth clocks , because the difference in gravity outcompetes the difference in velocity in this case . if you are wondering if there is a sweet spot where time dilation due to sr and gr cancel out , there is . you can find out more by searching for time dilation due to gravitation and motion . an important thing to note is that the effect of time dilation are observational effects , and are due different conditions at the observation point and the point being observed . when two objects have relative velocity , they both measure the other 's clock to be slower than their own but a third object with the same velocity as one of the original two will clearly not measure those two as equally slow .
the above posters seem to have missed the fact that $\psi$ is not an eigenfunction , but an arbitrary wavefunction . the types of wavefunctions we normally see when we calculate things are usually expressed in terms of eigenfunctions of things like energy or momentum operators , and have little to do , if anything , with classical behaviour ( e . g . look at the probability density of the energy eigenstates for the quantum harmonic oscillator and try to imagine it as describing a mass connected to a spring ) . what you might want to do is construct coherent states which are states where position and momentum are treated democratically ( uncertainty is shared equally between position and momentum ) . then , the quantum number that labels your state might be thought of as the level of excitation of the state . for the harmonic oscillator , this is roughly the magnitude of the amount of energy in the state in that $e = \langle n \rangle \hbar= |\alpha^2| \hbar$ . if you naively take $\hbar \to 0$ then everything vanishes . but if you keep , say , the energy finite , while taking $\hbar \to 0$ , then you can recover meaningful , classical answers ( that do not depend on $\alpha$ or $\hbar$ ) .
the wave function is antisymmetric under exchange of ( all ) the coordinates of each electron ( we will just call them electrons since that is shorter than " two electrically charged spin 1/2 fermions " and equivalent ) . we will write the wave function as : \begin{align} \psi ( 1,2 ) and = \psi_1 ( \mathbf{r}_1 ) \psi_2 ( \mathbf{r}_2 ) |s_1s_2\rangle -\psi_1 ( \mathbf{r}_2 ) \psi_2 ( \mathbf{r}_1 ) |s_2s_1\rangle . \end{align} ( check that this is antisymmetric under $\mathbf{r}_1 \leftrightarrow \mathbf{r}_2$ and $s_1 \leftrightarrow s_2$ . ) now let 's calculate the force between these due to some two-body operator $v ( \mathbf{r}_1 , \mathbf{r}_2 ) $ ( that might depend on spin ) . it is proportional to ( i am ignoring normalization ) : \begin{align} and \int d^3r_1 d^3r_2 \psi^\dagger ( 1,2 ) v ( \mathbf{r}_1 , \mathbf{r}_2 ) \psi ( 1,2 ) \\ and = 2\int d^3r_1 d^3r_2 \big\{ |\psi ( \mathbf{r}_1 ) |^2|\psi ( \mathbf{r}_2 ) |^2 \langle s_1 s_2|v ( \mathbf{r}_1 , \mathbf{r}_2 ) |s_1 s_2\rangle\\ and - \mbox{re} [ \psi_1^* ( \mathbf{r}_1 ) \psi_2 ( \mathbf{r}_1 ) \psi_2^* ( \mathbf{r}_2 ) \psi_1 ( \mathbf{r}_2 ) \langle s_1 s_2|v ( \mathbf{r}_1 , \mathbf{r}_2 ) |s_2 s_1\rangle ] \big\} \end{align} the second term is the exchange term . note that the square of the wave functions in the first term are proportional to particle densities at $\mathbf{r}_1$ and $\mathbf{r}_2$ . while the second term has different wave functions , $\psi^*_1 ( \mathbf{r}_1 ) \psi_2 ( \mathbf{r}_1 ) $ at the point $\mathbf{r}_1$ . the short answer is : " the origin of the exchange interaction is the property of definite parity under coordinate exchange of the wave function . " ( often the answer is given that it is the pauli principle . but this is incomplete . systems of bosons with internal coordinates ( flavor , spin , etc . ) enjoy the exchange interaction too . prove this . ) note too that this holds for any two-body operator .
i think you are misunderstanding the article . i also think that the name ' flying saucer ' is a bit misleading . i think referring to it with the actual project name : ' low density supersonic decelerator ' ( ldsd ) fits better . the concept of this project is that a spacecraft uses a inflatable saucer-shaped balloon to increase its reference area ( the surface area used in the drag equation ) to increase its atmospheric drag during atmospheric entry . during nasa 's test flight they want to test whether this system would suffice to land big spacecrafts safely on to mars . to simulate the atmosphere of mars they lift the test craft with a helium balloon high up into the atmosphere of earth such that the density is similar to that of mars at its surface . they than use a rocket to get up to speeds comparable with a atmospheric entry at mars . this video also gives a good explanation of the ldsd , and this video give a little longer and more detailed explanation .
unlike the case in psychology and sociology , in physics we are capable of repeating the same experiment/measurement in the same conditions over and over . any valuable , unexpected results that reflect reality should repeat also , so there is no such problem . this is true unless the experiment is not repeated enough to filter unexpected results out of noise . this is why the lhc collides particles over and over , to catch any such repeated unexpected events over the background noise with big certainty .
1 ) photons are bosons and can exist with the same quantum numbers with an indefinite number of photons . 2 ) photons have zero mass but have energy $e=h\times \nu$ , increasing their number and frequency increases the energy per cubic centimeter . 3 ) photons move with the velocity of light , and trapping them presupposes reflectors of one kind or another . from 1 ) the answer is " no limit " from 2 ) and 3 ) the limit would come from the melting of the reflectors due to the high energy density . the number would be large and would depend on the frequencies present .
one should always keep in mind that the marvelous wikipedia is constantly being updated by different users . most of the time an update constitute an overall improvement , but there is always a danger that previous coherence gets lost when a new user change something in one place but not in the rest of the text . so concretely , the sentence we have assumed units such that the speed of light $c=1$ should of course have been removed after $c$ was restored in $\box$ . also note that in the current version , wikipedia mixes notations $g_{\mu\nu}$ and $\eta_{\mu\nu}$ for the flat metric . such minor flaws should always be expected of wikipedia until a page has matured .
this is much to do with the possible eigenvalues of the operators . normal operators on a hilbert space are closely analogous to complex numbers , with the adjoint taking the role of the conjugate ; these relations are typically inherited directly to the operator 's eigenvalues . thus , if a linear operator $l$ has an eigenfunction $f$ with eigenvalue $\lambda$ , $$lf=\lambda f , $$ then saying "$l$ is self-adjoint " means that $l^\dagger=l$ which translates to $\lambda^\ast=\lambda$ , i.e. that $\lambda$ be real . similarly , $l$ being nonpositive implies that $\lambda\leq0$ . in your case , the behaviour can be reduced to an equation of the form $$ \frac{\partial^2 p}{\partial t^2} ( x , t ) =\hat lp ( x , t ) , $$ where $\hat l$ is some differential operator . in general the solution will not be of this form , but you can take a first stab of the problem by inserting in an eigenfunction of the differential operator for the spatial dependence . that is , you use the trial solution $p ( x , t ) =p_0 ( x ) t ( t ) $ , where $\hat lp_0=\lambda p_0$ . this hugely simplifies the time-propagation equation , which reduces to the solvable form $$ \frac{\partial^2 }{\partial t^2}t=\lambda t . $$ while the solutions of this equation are formally all the same ( i.e. . $t ( t ) =t_+e^{\sqrt{\lambda}t}+t_-e^{-\sqrt{\lambda}t}$ ) regardless of what $\lambda$ is , the behaviour will be very different and depend , sometimes sensitively , on $\lambda$: if $\lambda&gt ; 0$ , then at least one of the exponentials $e^{\pm\sqrt{\lambda}t}$ will have a blow-up . if $\lambda$ has an imaginary part , however small , then one of the two square roots $\pm\sqrt{\lambda}$ will have a positive real part , and the corresponding contribution to $t ( t ) $ will oscillate at a blowing-up amplitude . if $\lambda$ is negative or zero , then both roots $\pm\sqrt{\lambda}$ will be imaginary or zero , and both exponentials will be completely oscillatory and have bounded amplitude for all time . it is clear that only the third case is consistent with conservation of energy . in terms of the differential operator , it corresponds to a condition of self-adjointness ( i.e. . $\lambda\in\mathbb r$ ) and non-negativity of the operator .
on the upper end , coulomb 's law has not been observed to break for any large collection of charge that can be put together . in principle , if you tried to put more and more charge together then there would be a lot of energy stored in the field , and if the mass equivalent of this energy density got too high , there would be general relativistic effects to consider . in practice , unless you are dealing with a charged black hole , the charge distribution will tear itself apart many orders of magnitude before that . there is no corresponding breakdown as such for " very small " charges , on the other hand , because charge is quantized : no free charge smaller than the electron charge , $e=1.6\times10^{-19}\textrm{ c}$ , has ever been observed . coulomb 's law also breaks down if charges are moving , and particularly if they are moving fast ( comparably to the speed of light ) or there are charge movements in an otherwise neutral conductor . this is fixed by extending the electrostatic case into the full electromagnetic theory , as developed by maxwell , which is fully compatible with special relativity . in the domain of the small , the electrostatic force remains unaltered for standard quantum mechanics . it does change for the relativistic case , in which case you should use quantum electrodynamics ( qed ) , which describes a bunch of nonclassical phenomena that occur when charged elementary particles go fast . there is , however , one very interesting application of qed to stationary charges , and it happens in the short distance limit : as you get in closer , the electron looks like it has more charge , and the force goes up faster than $1/r^2$ . this is called charge screening and results from a cloud of virtual particle pairs that momentarily pop into and out of existence .
simply because our final goal is a set of laws of physics that describes any part of the universe equally well . let 's say a physicist jumped into a black hole and saw that the interior of the black hole was composed entirely of john lennon clones . his last thoughts before getting spaghettified would be " why ? " . from his perspective , physics is incomplete . sure , we probably can not use it to predict anything -- but modern physics is much less about predictions and much more about having a beautiful , mathematically rigorous model of the universe . mathematical models with discontinuities usually are not " beautiful " , and john lennon in black holes counts as a discontinuity if we take general relativity as our mathematical model of the universe . which would mean that we will eventually have to replace our model ( which is why knowing as much as we can about the inside is important ) . besides , if our current theories partially fail inside a black hole , we need to patch that up .
the world wide web was born with the previous collider , lep , which was using the same tunnel as the lhc . it was born because for the first time there were many countries and tens of institutes and hundreds of physicists in each of the four experiments and there was great need to communicate clearly and easily and fast , bypassing group e-mails . the lhc physics groups are an order of magnitude larger than the lep ones . the data gathering and the computer needs both during the ten years of development and during the runs are enormous . a new computer system was developed for handling large data samples and monte carlo samples , in a distributed manner , called the grid . this is at the frontier of large data handling and the methods developed will certainly find applications world wide . there will also be knowledge gathered on radiation hardness , on superconducting technologies and magnetic fields . in my opinion no new mathematics is coming out of the lhc per se . the data analysis from the experiments at lhc may show confirmation of existing theoretical predictions/models , but the mathematics of these are already developed . an unexpected discovery might force new theories to form , that might need new mathematical tools , but i would consider it rather improbable .
the candela is the si base unit of luminous intensity ; that is , power emitted by a light source in a particular direction , weighted by the luminosity function ( a standardized model of the sensitivity of the human eye to different wavelengths ( wikipedia ) therefore , 7500 cd ( 3x2500 cd ) is more visible than 6000 cd .
in a complex hilbert space ${\cal h}$ , an operator $a : d ( a ) \to {\cal h}$ , with $d ( a ) \subset {\cal h}$ a ( not necessarily dense ) subspace , is said to be hermitian if $$\langle a \psi | \phi \rangle = \langle \psi| a \phi\rangle \quad \forall \psi , \phi \in d ( a ) \: . \quad ( 1 ) $$ it seems to be worth stressing that , to check ( 1 ) , it is not necessary to exploit the definition of adjoint operator , $a^\dagger$ that , generally , does not exist when $d ( a ) $ is not dense . if $d ( a ) $ is dense , the hermitian operator $a$ is said to be symmetric . in your case ( s ) $a:=t_n$ and $d ( t_n ) = s ( {\mathbb r} ) $ , the hilbert space ${\cal h}$ being $l^2 ( \mathbb r ) $ . just using ( a ) integration by parts , ( b ) the definition of scalar product in $l^2 ( \mathbb r ) $ and ( c ) the fact that the elements of $s ( \mathbb r ) $ rapidly vanishes for $|x|\to \infty$ with all derivatives , you immediately establish that ( 1 ) is valid for $t_n$ . actually these operators are symmetric because $s ( \mathbb r ) $ is dense in $l^2 ( \mathbb r ) $ . existence of self-adjoint extensions can be studied examining deficiency indices of symmetric operators . it is not necessary to prove that $t_n$ is closed , even if this condition is assumed in several textbooks as reed and simon , as stated in dunford schwartz books ( vol ii corollary 13 ch . xii . 4.13 ) . however , if $n$ is even , self-adjoint extensions do exist in view of the fact that $t_n$ commute with the anti unitary involutive operator given by the complex conjugation of fourier transforms ( exploiting theorem 18 ch . xii . 4.13 ) which establishes that the deficiency indices coincide .
in most of the cases , the charge in dielectric material is created by imbalance in the charges existing in the dielectric . the dielectric is initially neutral . the balance is broken by certain “charging effects” such as friction , heat and pressure . the answer to which charging effect is responsible for the charge imbalance depends on the material and the surrounding conditions . also in most cases the generation of a charge on a dielectric requires another body to be involved . with respect to the rest of your questions : 1 . the charge does not go anywhere ; it stays where it was generated . charges can’t move in a dielectric as you know . 2 . the moving charges are mostly electrons , for electrons to move they need the necessary energy to overcome what is called the band gap . for dielectrics the band gap is large such that the electrons require a lot of energy to overcome the gap to arrive to conduction band where they can move freely . see the picture 3 . yes they are bounded to certain atoms , what makes those atoms specific is that they are the ones who experienced the imbalance of charges by the charging effects i mentioned above . the same things can be said for positive or negative charges . some dielectric materials have a tendency toward having negative charge when they are charged ; some have a tendency toward having a positive charge . that tendency is determined by materials properties . you can find a lot of information on this topic here
on the topic of the actual consequences of qm , here is answers with a few things that cannot be explained without qm . that aside . . . there is a golden rule that one should recite before all theoretical physics studies : " it all adds up to normality " . - greg egan , quarantine the thing with quantum mechanics and " logic " is that humans are really bad at qm and really good at " logic " . in fact , " logic " is an actual area of study in psychology : naïve physics . naive physics ( forgive me not using umlauts ) is an all-right approximation of anthropically scaled physical phenomena : object permanence , an exclusion principle based on volume , absolute time , a primitive notion of gravity . . . this is what humans think with , every day , on an instinctive level . over the times , you see refinements of naive physical concepts in works of aristotle and newton . object permanence , volume exclusion , gravity , absolute time . then relativity comes along and throws absolute time out the window . believe me , people cried " logic is meaningless " when relativity was new too . then comes quantum mechanics : throw away volume exclusion and even to a degree the intuitive notion of object permanence . there is one other concept of naive physics which qm seemingly violates : looking is a free action . suddenly you chance things by " looking " . so people do indeed cry out " logic is broken . " but it is not , because logic has nothing to do with qm . logic , and indeed all of mathematics is about axioms and theorems and the steps of inference in between . qm 's apparent weirdness has no effect on me using the peano axioms to say : $$ \begin{array}{l l}\vdash and 0 = 0 \\ \vdash and \forall x , y : s ( x ) = s ( y ) \iff x = y \\ \vdash and \forall x : x + 0 = 0 + x = x \\ \vdash and \forall x , y : x + s ( y ) = s ( x ) + y \\ \vdash and 2 = s ( s ( 0 ) ) \\ \vdash and 5 = s ( s ( s ( s ( s ( 0 ) ) ) ) ) \\ \vdash and 2 + 2 = 2 + s ( s ( 0 ) ) = s ( 2 ) + s ( 0 ) = s ( s ( 2 ) ) + 0 = s ( s ( 2 ) ) \\ \vdash and 2 + 2 \not = 5 \\ and \iff s ( s ( s ( s ( 0 ) ) ) ) \not = s ( s ( s ( s ( s ( 0 ) ) ) ) ) \\ and \iff s ( s ( s ( 0 ) ) ) \not = s ( s ( s ( s ( 0 ) ) ) ) \\ and \iff s ( s ( 0 ) ) \not = s ( s ( s ( 0 ) ) ) \\ and \iff s ( 0 ) \not = s ( s ( 0 ) ) \\ and \iff 0 \not = s ( 0 ) \\ \end{array} $$ captial l logic is set in mathematical stone and to say that qm 's " mysteries " imply $2 + 2 = 5$ is juvenile , trivially untrue , and shows that the speaker has not really understood anything at all . it is like saying : boy am i bad at abstracting from my everyday life and interactions , that i cannot even sit down and actually learn qm . i am going to put that on a t-shirt !
given boundary conditions allow for a unique solution of the wave equation . if you do it your way and immediately guess the retarded green 's function you are fine , but in principle there is an infinite amount of solutions which have to be fixed by boundary conditions . by imposing sommerfeld conditions , you make sure that only the retarded solution survives , which is the only physical solution : there is no incoming radiation from infinity . $ct+r=constant$ assures that you are actually treating past null infinity ( which is not clear from the first condition alone ) . furthermore , $r\bar{h}_{\mu\nu}$ and $r\partial_\rho\bar{h}_{\mu\nu}$ have to be bounded in order to assure that the limit at $-\infty$ is well defined .
you are on the right track . hints : recall that under a diffeomorphism $f$ , the tensor transformation law tells us that the metric transforms as $g\to g_f$ where \begin{align} ( g_f ) _{\mu\nu} ( f ( x ) ) = g_{\alpha\beta} ( x ) \partial_\mu ( f^{-1} ) ^\alpha ( f ( x ) ) \partial_\nu ( f^{-1} ) ^\beta ( f ( x ) ) \end{align} which , sending $x\to f^{-1} ( x ) $ can be re-written as \begin{align} ( g_f ) _{\mu\nu} ( x ) = g_{\alpha\beta} ( f^{-1} ( x ) ) \partial_\mu ( f^{-1} ) ^\alpha ( x ) \partial_\nu ( f^{-1} ) ^\beta ( x ) . \tag{$\star$} \end{align} consider an infinitesimal diffeomorphism ( physics speak for a smooth , one-parameter family of diffeomorphisms that starts at the identity ) \begin{align} f ( x ) =x - \xi ( x ) + o ( \xi^2 ) \end{align} notice that \begin{align} f^{-1} ( x ) = x+\xi ( x ) +o ( \xi^2 ) \end{align} plug this into the right hand side of $ ( \star ) $ and taylor expand about $\xi=0$ to first order . recall that $\delta g = g_f - g + o ( \xi^2 ) $ , and compare to the expression you wrote down . addendum . ( 2 april 2014 ) notice that the first transformation law i wrote down is a more mathematically explicit version of \begin{align} ( g' ) _{\mu\nu} ( x' ) = g_{\alpha\beta} ( x ) \frac{\partial x^\alpha}{\partial x'^\mu} ( x' ) \frac{\partial x^\beta}{\partial x'^\nu} ( x' ) \end{align} since if we write $x ' = f ( x ) $ then $x = f^{-1} ( x' ) $ so in particular \begin{align} \frac{\partial x^\alpha}{\partial x'^\mu} ( x' ) = \frac{\partial ( f^{-1} ) ^\alpha}{\partial x'^\mu} ( f ( x ) ) = \partial_\mu ( f^{-1} ) ^\alpha ( f ( x ) ) \end{align} where in the last equality , i have simply suppressed the prime in the derivative notation ; a derivative $\partial_0$ for example simply means " take the derivative with respect to the $0^\mathrm{th}$ argument of the function . " while we usually label the zeroth argument of $f^{-1}$ with the letter $x'^0$ because we are thinking of $f^{-1}$ as the transformation that maps us from the " primed " coordinates to the " unprimed " coordinates , but this is just a dummy label , and we do not strictly need it as long as the derivative tells us which argument of the function we are differentiating with respect to .
this creates a point of extremely focused energy at the middle point where the bubble collapses . in theory , this point focuses enough energy to trigger nuclear fusion . it is not currently accepted mainstream science to say that collapsing bubbles focus energy enough to cause nuclear fusion . temperatures over 10,000k can be acheived , but are still well below the millions of degrees needed for fusion . see the extensive review article single-bubble sonoluminescence for detailed information .
yes , it is a normal field theory , so you may derive the equations of motion . they will be the ordinary maxwell 's equations for the electromagnetic field $$ \partial_\nu f^{\mu\nu} = j^\nu $$ with $j^\mu$ calculated as the sum of the conserved currents for the dirac field and for the higgs fields , combined with the dirac equation coupled to the electromagnetic field ( with some yukawa interaction $y\cdot \phi\psi$ terms ) , and the klein-gordon equation for a charged scalar field with some $v' ( \phi ) $ and $\psi \psi$ terms added in the right hand side etc .
if you are looking at radio waves then the mirror will have to be made of thicker metal , because as you increase the wavelength you also have to increase the thickness of the metal to get the same reflectivity . that is actually how satellite dishes work . they are basically a big curved mirror that concentrates all of the microwaves coming down from the satellite . they are often full of holes to keep the weight down , and this does not matter because the wavelength of the waves is larger than the holes . this is the same principle as seeing a light on in your microwave . you can see the light escaping through the door but the microwaves are not escaping because they are too long . once you get beyond visible light into the shorter wavelengths ; ultraviolet light is easy to make mirrors for , but x-rays are very difficult . and so making x-ray telescopes is very difficult . sometimes they do it by using a bag of gas to act like a lens rather than mirrors or by using a metal mirror but at a very grazing angle which makes the mirror very large . mobile phone waves are at the microwave end of radio waves , and a sheet of aluminium would work nicely as a mirror for those . source : the naked scientists one can see that for making lenses , materials of different refractive indices can be used based on required convergence , divervence and dimensions can be compared to those of the above described mirrors , although there may a problem that making enormous lenses for radio waves require materials/machinery that we may not have at the moment , mirror though as you see are the big satellite dishes that we have seen many times .
since there is positive and negative plate on your question , the field you want to say to us might be an electric field . suppose that the positive plate on the left and the negative plate on the right creates an uniform electric field and the velocity of the particle is downward , it will make a projectile depending on the charged of particle . if the particle is proton , the trajectory will go to the right since it will be attracted by negative plate . if the particle is electron , the trajectory go to the left since it will be attracted by positive plate .
there is a rigorous formal analysis which lets you do this . the true problem , of course allows both the proton and the electron to move . the corresponding schrödinger equation thus has the coordinates of both as variables . to simplify things , one usually transforms those variables to the relative separation and the centre-of-mass position . it turns out that the problem then separates ( for a central force ) into a " stationary proton " equation and a free particle equation for the com . there is a small price to pay for this : the mass for the centre of mass motion is the total mass - as you had expect - but the radial equation has a mass given by the reduced mass $$\mu=\frac {mm}{m+m}=\frac{m}{1+m/m} , $$ which is close to the electron mass $m$ since the proton mass $m$ is much greater . it is important to note that an exactly analogous separation holds for the classical treatment of the kepler problem . regarding self-interactions , these are very hard to deal with without invoking the full machinery of quantum electrodynamics . fortunately , in the low-energy limits where hydrogen atoms can form , it turns out you can completely neglect them .
there are two definitions for the wavenumber , one is $k=\frac{2\pi}{\lambda}$ , the number of wavelengths per $2\pi$ units of distance ( i.e. . on the circle ) , the other one is $k=\frac{1}{\lambda}$ , which is the number of wavelengths per unit distance . the formula for the raman shift is refering to the latter definition .
this is a two-part answer based on my own ideas and on the specifically referenced book . i am quite confident about the first part , but less so about the second . 1 ) the radiated power is indeed refractive index dependent . the question arises because of the refractive index dependence of the radiated power . this dependence is hardly mentioned anywhere online or in books , and it is not obvious that it is true ( indeed john implied that it is not true in his answer ) . so first we establish the claim : the power radiated per unit surface are of a black body is proportional to $n^2$ , where $n$ is the index of refraction of the medium it is radiating into . this follows from stefan 's law , when the speed of light $c$ in stefan 's constant is replaced by $c_0/n$ , where $c_0$ is the speed of light in vacuum . the $n^2$ dependence of the radiated power can also be derived by a modification of the normal derivation of planck 's law . the bose-einstein factor is constant for constant $e$ , but the density of states $g ( e ) de$ is directly wavelength-dependent via the mode-number . the wavelength dependence of $g ( e ) de$ gives an $n^2$ term for fixed $e$ , which integrates to give an $n^2$ term in the total radiated power . the $n^2$ dependence of the radiated power is also discussed explicitly in the book by r . siegel and john reid howell : " thermal radiation heat transfer " , 4th ed . : " the . . . and total emissive power of a blackbody into a medium with constant index of refraction $n$ are given by the stefan-boltzmann law : $\pi i_b = e_b = n^2 \sigma t^4$ . " the above reference could be wrong , although i have only heard good things about that book , but if it is wrong then we will need a good reference or derivation . 2 ) the resolution of the apparent paradox in the question . i just found it and have not gone through it myself , but siegel and howell give a possible resolution in their book : " section 18-5 shows how a portion of blackbody radiation from within a medium with $n&gt ; 1$ cannot pass through an interface into an adjacent medium with smaller $n$ . when entering into vacuum , there is a $1/n^2$ interface-reflection factor that removes the $n^2$ provided by [ stefan 's law ] , so the maximum energy passing into the vacuum is $\sigma t^4$ . "
the worldsheet fermions have to do with internal degrees of freedom , namely the spin -- therefore better name for the superstring is the more old-fashioned " spinning string " ( since worldsheet susy should not be confused with spacetime susy ) . the worldsheet fermions generate multiplets of some internal symmetry group . if you want those internal degrees of freedom generated by ws fermions to transform under spacetime lorentz transformations , rather than an independent internal symmetry , you need to correlate the lorentz transformations of the worldsheet bosons and fermions . this is what worldsheet susy does for you . all of this is not specific to string theory . if you want to first-quantize a field theory , a " bosonic " worldline theory will give you a ( free ) scalar field theory . adding fermions and the corresponding worldline supersymmetries will generate ( free ) higher spin fields . it is probably a useful exercise to get e.g. classical ( free ) maxwell field from a ( n=2 susy ) worldline theory in order to appreciate precisely what the worldsheet structures mean precisely . wish i had a good reference , but maybe someone can help me out .
with $$u ( n ) =u_0e^{-i ( \omega t+k n a ) }$$ we have $$p=\sum_{n=1}^nm\frac{d}{dt}u ( n ) =i\omega m u_0e^{-i\omega t}\left ( \frac{1-e^{-iakn}}{1-e^{iak}}\right ) . $$ with cyclic boundary conditions we have $$u ( n+1 ) =u ( 1 ) \rightarrow k=\frac{2\pi j}{na}\text{ for }j\in\mathbb{z}$$ and inserting $k$ into $p$ gives $$p=i\omega m u_0e^{-i\omega t}\left ( \frac{1-e^{-2 i \pi j}}{1-e^{\frac{2 i \pi j}{n}}}\right ) =0 . $$ your method uses a slightly different boundary condition , but i think it is still valid .
the field and its conjugate momentum are operators . they act on the hilbert space of the theory , which is not the space of square-integrable functions of position , as it is in single particle quantum mechanics . rather , the configuration space of the theory is the set of all field configurations and the wavefunction is actually a functional , which ascribes an amplitude $\psi [ \phi ( x^\mu ) ] $ for the field to be in a configuration $\phi ( x^\mu ) $ . qft is just quantum mechanics applied to an infinite collection of degrees of freedom labeled by positions $x$ . once you get used to the switch it comes to seem pretty simple , but the set of all field configurations makes a huge space so naturally it is very difficult to make things mathematically rigorous . for the most part physicists can live without rigourously defining the configuration space . the dictionary from ordinary quantum mechanics to quantum field theory is $$ \begin{array}{lcl} \mathrm{qm} and and \mathrm{qft}\\ t , i and \to and \left ( t , x , y , z\right ) \equiv\left ( t , {\bf x}\right ) \equiv x^{\mu}\\ q_{i}\left ( t\right ) and \to and \phi\left ( x^{\mu}\right ) \\ p_{i}\left ( t\right ) and \to and \pi\left ( x^{\mu}\right ) \\ \psi\left ( t , q_{1} , q_{2} , \cdots , q_{n}\right ) and \to and \psi\left [ \phi\left ( x^{\mu}\right ) \right ] \\ \left [ q_{i}\left ( t\right ) , \ p_{j}\left ( t\right ) \right ] ~=~i\hbar\delta_{ij} and \to and \left [ \phi\left ( t , {\bf x}\right ) , \ \pi\left ( t , {\bf y} \right ) \right ] =i\hbar\delta^3\left ( {\bf x}-{\bf y}\right ) \\ \hat{h}\left ( t\right ) and \to and \hat{h}\left ( t\right ) =\int\mathrm{d}^{3}x\ \hat{\mathcal{h}}\left ( x^{\mu}\right ) \end{array} $$ you can represent the field momenta explicitly by functional derivatives : $$ \pi ( x^\mu ) ~=~ - i\hbar \frac{\delta}{\delta \phi ( x^\mu ) } , $$ and check that this satisfies the commutation relationship .
there is the basic confusion here , in my opinion , of the wave/particle identity . when we speak of photons we are in the quantum mechanical regime , it is an elementary " particle " . the quotation marks are necessary because it is not a particle like a billiard ball , and it is not a wave like an acoustic wave , or even a classical electromagnetic wave . it is a mathematically described " entity " which , depending on the experiment will act either like a billiard ball , i.e. a point in four dimensional space but with specific quantum numbers , ( in the case of the photon spin , polarization and zero mass ) or like a probability wave . probability wave in bold to emphasize that the energy of the entity when appearing as a wave is not distributed , as the energy in sound waves , in space . the entity will appear always with a specific ( x , y , z , t ) ( within the heisenberg uncertainty principle ) but the probability of finding it there will display the properties of waves , interference patterns . . the experiment you are setting up does not have the ability to detect the wave nature of the probability distribution of the photon . if it is done in vacuum the distance will play no role to the efficiency of detection . it will take a little longer due to the velocity of light but the material of the detector will not make a difference : the photon will either be there or not . in the delayed choice experiment the description makes the same mistake . the individual photon does not take both paths . the interference pattern appears because of the statistical accumulation of many photons which then display the probability wave aspect of the photon wave function . each individual photon takes a specific path , but the probability of taking it is affected by the interference setup .
the short answer ( i hope someone gives a longer , more precise one ) is that not all photons are equal when you consider this question . you need a photon with a short enough wavelength ( high enough energy ) to measure the position of the electron precisely enough to make the uncertainty in the position less than the spacing between the slits . most photons are not that high energy , so electrons are not forced to behave like particles most of the time . please note that i am being very loose in my language ( at about the level of the question , i think ) . a much more precise answer is possible , if you are interested , but it requires a lot of clarifying of concepts covered in the original question .
in order to know with certainty where every particle was going to be you would need to know its mass , velocity and state very accurately - as any rounding errors or uncertainty will be magnified over the timespans involved . unfortunately it is impossible to know to 100% precision where anything is , so we are already at strike 1 . if you could somehow hold all this information , you would need at least the size of the universe to store it ( plus another couple of universes to manage and read that information ) basically the answer is a very big no - as you cannot have the prerequisites you defined in the question .
i have always found the cornell to be a good source of info ( and also the former home of carl sagan ) : blue stragglers are stars which stay on the main sequence ( the normal , hydrogen-burning phase of a star 's lifetime ) longer than they are expected to . the color of a star is a measure of its temperature and its mass - blue stars are hotter and more massive than red ones . the more massive a star is , the faster it burns up its hydrogen , so blue stars are expected to spend less time on the main sequence than red stars . therefore , when you look at a color-magnitude diagram of a globular cluster ( whose member stars all formed around the same time ) you expect to see an orderly transition ; stars which are bluer than a certain value ( known as the " turnoff " point ) will have already left the main sequence , while those which are redder will still be on it . the location of the turnoff point can be used to estimate the age of the cluster . but , it is usually the case that several stars in a cluster are observed along the main sequence past the turnoff point , and these are referred to as blue stragglers . the most likely explanation for blue stragglers seems to be that they are the result of stellar collisions or mass transfer from another star . that way , a star which is red , cool and already somewhat old can get extra mass and turn bluer . it spent most of its life as a red star and therefore burnt its hydrogen at a slow enough rate to still be on the main sequence , but then at a certain point it gets extra mass and effectively " disguises " itself as a blue star , which makes us think it is younger than it really is . i think the key takeaway is that we really are not 100% sure . however , that in no way means that anyone can simply assert that their pet theory ( guess , lie , whatever ) is the answer either . especially if they do not bring any evidence , testable hypotheses , or data to examine to the table . solstation also has quite a bit of good info on blue stragglers and what we know about them . of particular interest is this graphic : this paper from cornell also has some additional info . abstract : in open star clusters , where all members formed at about the same time , blue straggler stars are typically observed to be brighter and bluer than hydrogen-burning main-sequence stars , and therefore should already have evolved into giant stars and stellar remnants . correlations between blue straggler frequency and cluster binary star fraction , core mass and radial position suggest that mass transfer or mergers in binary stars dominates the production of blue stragglers in open clusters . analytic models , detailed observations and sophisticated n-body simulations , however , argue in favour of stellar collisions . here we report that the blue stragglers in long-period binaries in the old ( 7 × 109-year ) open cluster ngc 188 have companions with masses of about half a solar mass , with a surprisingly narrow mass distribution . this conclusively rules out a collisional origin , as the collision hypothesis predicts a companion mass distribution with significantly higher masses . mergers in hierarchical triple stars are marginally permitted by the data , but the observations do not favour this hypothesis . the data are highly consistent with a mass transfer origin for the long-period blue straggler binaries in ngc 188 , in which the companions would be white dwarfs of about half a solar mass .
warning : students , stay away from antiquities . the aim to learn is to survive . there is usually reasons that old materials are not cited . in this case , nakanishi is obsolete . nakanishi is not wrong , but unsatisfactory in two points . first , what happened to the gauge symmetry ? and it is limited to qed , and not applicable to non-abelian symmetries . the answer is known since late 70s , given by brst . the gauge symmetry is conserved even after gauge fixing , where the seemingly lost symmetry lies in zero-norm or ghosts , and they are unphysical i.e. harmless . this also means the lorentz covariance is preserved too in non-covariant gauges . you know , you definitely need ghosts ( negative-norm states ) for practical calculation for non-abelian gauges , unlike qed where ghosts are detached . i recommend weinberg 's qft book , vol 2 , sec 15.7 for brst introduction . even if you do not need non-abelian gauge , the cited section of weinberg is not difficult at all . ( there you encounter the structure constant $c^{\alpha\beta\gamma}$ . you can safely think of it as the levi-civita symbol $\epsilon^{ijk}$ , and $t_{\alpha}$ as pauli matrices . ) or rather , you will be stricken to find how easy it is in the functional quantization , compared to nakanishi 's machinery , ( fourier expansion of the field and heavy use of $\delta ( x ) $ and its derivative . ) - brst wins also in pragmatism . as a free material , see for example sredinicki 's qft book , sec 74 , but prerequisites sections may be a bit more cumbersome than weinberg . peskin and schroeder does not help .
the speed of sound in a liquid is given by : $$ v = \sqrt{\frac{k}{\rho}} $$ where $k$ is the bulk modulus and $\rho$ is the density . the bulk modulus of mercury is $2.85 \times 10^{10}$ pa and the density is $13534$ kg/m$^3$ , so the equation gives $v = 1451$ m/sec . the speed of sound in solids is given by : $$ v = \sqrt{\frac{k + \tfrac{4}{3}g}{\rho}} $$ where k and g are the bulk modulus and shear modulus respectively . the bulk modulus of iron is $1.7 \times 10^{11}$ pa , the shear modulus is $8.2 \times 10^{10}$ pa and the density is $7874$ kg/m$^3$ , so the equation gives $v = 5956$ m/sec . you give a slightly different figure for the speed of sound in iron , but the speed does depend on the shape and the figure you give , $5130$ m/sec , is the speed in a long thin rod . there are more details in the wikipedia article i have linked .
try pyrex , vycor , fused silica - in that order .
" for example , the value 5 kpa is spelled out as “five kilopascals , ” although “five kilopascal” is acceptable . if in such a single-unit case the number is less than one , the unit is always singular when spelled out ; for example , 0.5 kpa is spelled out as “five-tenths kilopascal . ”$^{ [ 1 ] }$ but then it is always preferred to use all units in singular only for simplicity . $ [ 1 ] $: http://physics.nist.gov/pubs/sp811/sec09.html#9.7 ( see section 9.7 )
i guess ron is saying that by definition the bulk modulus is the value measured in frame at rest relative to the bar , and therefore it makes no sense to ask how the bulk modulus depends on speed . it is a bit like relativistic mass : few physicists assign any significance to relativistic mass - we just use rest mass , which is defined in the rest frame . having said this , i think your question is interesting , though i doubt it exposes any fundamental insights into relativity . you ask : is it due to relativistic dilation of the bar 's mass and volume , and hence its density ? is there some other effect ? i started doing a few back of the envelope calculations and quickly concluded that the answer is " all of the above " . however i also concluded that it would take more time than i have available in my coffee break to give you a detailed answer . the problem with sr is that it is easy to throw around ideas like velocity addition or time dilation , but doing this casually is to tread a dangerous path . to really work out what is going on you need to take your system , i.e. the bar , and apply the lorentz transforms to calculate exactly how it looks in your frame . if you do this you will find that the atomic motion , interatomic forces and interatomic spacing all change , so the bulk modulus and the density both change , and unsurprisingly the speed of sound changes as well . i did google to see if anyone had done this calculation , but without any success . re your second point , the speed of shear and compression waves are usually different . in fact they are different in steel even without raising the spectre of relativistic motion . it is unsurprising that they would appear to be different when viewed from a relativistic frame . i hope this helps ; i feel it is a bit of a cop out since i have not actually given you a straight answer . i think the answer would be a lot of work ( if straightforward maths ) and would not reveal any fundamental insights . it is sort of the physics equivalent of a crossword puzzle .
there is only one parameter in the system . you have several choices , but you might use the scattering angle , $\theta$ , of the photon in the electrons initial rest frame . now conserve 4-momentum , and write the final state energy of the electron as a function of $\theta$ and maximize .
the sort of rheometers normally used for industrial research work by applying a controlled stress then measuring the resulting shear rate . typically you program them to start at a high stress and end at a low stress , and the rheometer will go away and automatically take readings at intervals between the two . when it is finished it spits out a table of stress-strain readings . the software running the rheometer can display this in any way you want e.g. as a graph of viscosity vs shear rate . you can put whatever type of fluid you want in your rheometer . you just need to bear in mind that with strongly shear thinning fluids it can take a very long time to obtain the low stress readings because the rheometer has to wait a long time for the reading to stabilise . you also generally need a trial run to get a rough guide as to what initial and final stress to specify .
a nature news article ( entangled photons make a picture from a paradox , 27 august 2014 ) the news article refers to the article " quantum imaging with undetected photons " , gabriela barreto lemos et al . and anton zeilinger , nature 512 , 409–412 , ( 28 august 2014 ) . there is a corresponding arxiv article ( 1401.4318 ) with the same figures available . a way to form an image of an object out of photons that have not interacted with the object , but which are entangled with photons that have [ . . . ] have i misunderstood something about the setup [ . . . ] ? a key element of the schematic of the experiment ( i.e. . figure 1 ) is that the photons which have interacted with the object ( marked red ) are subsequently directed to a nonlinear crystal nl2 where in turn a photon originates ( marked yellow ) which eventually contributes to forming the image . ( i would say that the description of the news article about " certain pairs of photons being recombined " is a bit misleading on this point ; and there is apparently no mentioning of " recombination " in the arxiv article . ) the schematic of the experiment requires that " an effective light beam " runs , through several optical elements , from the object to the " screen" ; even though there are different photons ( marked red vs . marked yellow ) contributing to different segments of that " effective light beam " . in other words : any event at which " the image is made on the screen " is within , or at least on , the future light cone of the corresponding event at the " the object had been illuminated " . and this requirement stands in the way of paradoxial implementations . in [ other ] experiments you have to combine all the beams back again to make the image in the end , so there is no way to construct a paradox . for the experiment considered here it is also true that the two ( yellow ) signal pulses must be coincident at the final ( combining ) beam splitter bs2 ; after corresponding two ( green ) laser pulses had been generated ( in coincidence ) at beam splitter bs1 . but here the passages through the nonlinear crystals ( nl1 and nl2 ) effectively allows photons to be substituted ( "yellow for red" ) along the way . consequently , the frequency of photons ( red ) illuminating the object and the frequency of photons ( yellow ) which make the image may be different from each other ; which may be used to " practical/technical " advantage .
the answer lies in the selection bias towards brighter stars . there are two reasons this makes the sun look relatively dense . the first is in martin 's answer . looking at a list of brightest stars , many ( e . g . betelgeuse , aldebaran , antares ) are red giants . these are stars that have finished burning hydrogen into helium in their cores and are much larger in size than main-sequence stars like the sun . as a result , their mean densities are small . the second effect is that the more massive a main-sequence star is , the smaller its mean density but the greater its luminosity . so again , more massive stars on the main-sequence ( e . g . rigel ) are easier to see but also have lower mean densities . if you compare the sun to stars from a list of sun-like stars , you will find it is not unusual .
according to lenz 's law , polarity of induced emf will be such that it produces the current which opposes the change in magnetic flux which produced it . it directly follows from the law of conservation of energy . let 's say you have a circular loop of conductor , if you bring the north pole of the magnet towards the loop , current induced in the loop will be anti-clockwise when viewed from the side of the magnet . remember if current is in the anti-clockwise direction it acts same as the north pole . so , north pole-north pole repel each other . thus , induced emf is opposing the change in magnetic flux . therefore , work has to be done in order change the magnetic flux linked with the coil . this work done will be converted into electrical energy . when you take the magnet away , when viewed from the side of magnet , current in the loop will be clockwise ( acts as south pole ) whereas magnet side near the loop will be south pole . so , even here south pole-south pole repel each other . thus , induced emf is opposing the change in magnetic flux . therefore , work has to be done even here in order to change the magnetic flux linked with the coil . as said above , this work done will be converted into electrical energy . thus , in the above two cases energy is conserved . let 's say that you bring the north pole of the magnet towards the same circular loop , now let current induced be clock-wise ( acts as south pole ) when viewed from the magnet side . north pole- south pole attract each other , so no work is needed to change the flux linked with the coil . but when flux linked with the coil changes , there is emf induced . here , electrical energy is produced without any work being done . it violates the law of conservation of energy . similarly you can consider the other case . therefore , from lenz 's law it follows that electrical energy is produced by the expense of mechanical energy . for this to happen , induced emf should oppose change in magnetic flux . you asked to explain with out using law of conservation of energy , but i felt the above explanation would explain your questions better .
your second approach is conceptually very wrong . . . the equation you wrote , $f = m\cdot a$ , holds for the acceleration of the center of mass of the whole disk , not for individual points of it . and since your disk is not going anywhere when you apply $f_f$ with the axe , we can assume that the axis of rotation is applying a force $-f_f$ that exactly compensates the other one , so that the center of mass remains at rest . note that while this pair of forces are parallel , they are not co-linear , so they still generate the torque you calculated before . i do not think there is a way of solving your problem without dealing with the moment of inertia . . .
their angular momentum stays nearly constant . you might be thinking of their angular velocity . there is a lot of simulation-based work out there on the inspiral of two compact supernova remnants ( nss or bhs ) , done partly to determine what the gravitational wave signals would look like for the ligo experiment . two nss would merge to form a bh , because their combined mass would be above the threshold for the fermi pressure of the ns to resist gravitational collapse . part of the ns material would briefly orbit the newly formed bh as an accretion disk , and some of the energy would be beamed out along the rotational axis , visible to us ( if we happen to be in the line-of-sight of the axis ) as a gamma ray burst .
it is 45 degrees with respect to a vertical plane containing the pendulum ball 's instantaneous direction of motion but you need to consider the angle of the tension force with respect to the ball 's motion . that plane is spanned by the pendulum 's length and the instantaneous direction of motion of the ball and you can see that these are orthogonal , 90 degrees , therefore no work is done .
in flat space the surface area of a sphere is $4\pi r^2$ . in positively curved space the surface area of a sphere is less than $4\pi r^2$ and in negatively curved space the surface area of a sphere is greater than $4\pi r^2$ . by $r$ i mean that if you start at the centre of the sphere with your trusty ( infinitesimal ) ruler and measure the distance to the sphere you will be measuring the quantity $r$ ( this is known as the proper distance ) . the area of the sphere is then measured by crawling over it and measuring it with the same ruler . if you are looking for an intuitive way to visualise the different curvatures i can not help - please let me know if you find one ! re your question 2: this is a somewhat vexed issue and different commentators have different views on whether it makes any sense to talk about the total energy of the universe . see for example the many and varied answers to total energy of the universe . you specifically ask about dark energy : you need to appreciate that ordinary matter , dark matter and dark energy all have a positive energy density and you can add them all together to get the total energy density . dark matter causes a repulsion because it has an unusual equation of state , not because it is exotic in any way . the zero energy universe idea is that the negative gravitational potential energy exactly balances out the positive combined energy density of matter and dark energy so the net energy is zero .
not only is it possible to remove the nucleus from an atom , but the rhic does it every day ! the rhic collides heavy nuclei like gold to measure the properties of nuclear matter at high densities . gold atoms have their electrons stripped off in the tandem van de graaff accelerator . the atoms are subjected to such strong electric fields that the positive nuclei and negative electrons are pulled apart . response to comment : see http://isnap.nd.edu/html/research_fn.html for a few more details on how the atoms can have their electrons stripped off ( this is a different accelerator from the one at the rhic ) . you start with singly ionised atoms . these are easily made e.g. by shining ultraviolet light on the atoms . the singly ionised atoms are accelerated to a high speed than crashed into a very thin carbon sheet . the heavy nuclei plough straight through while the electrons are scattered , and the nuclei are then accelerated away with a second electric field .
the key is . . . the closest the mass to the axis of rotation , the easiest to add angular velocity to the body . for instance a figure skater rotates faster when she puts her limbs closer to her body . let 's see how it works from a more intuitive fashion : for instance , in the figure bellow , trying to lift up the table ( a ) would be easier compared with the table ( b ) . in both cases the mass of each individual box is the same , but in ( a ) you have a better leaver by means of the distance from the border where the force is being applied , to each box . therefore , the table ( b ) would be harder to lift up , even when r ( length of the table ) and m ( total mass of the four boxes ) are the same . now let 's see how it works in the case with the spheres : lets make the sphere a disk , and then divide it in slides . make fixed the center of mass of the disc and move all the slides to one side . now we have a similar scenerario to the one with the tables . both spheres , the solid and hollow one , rotate around their center of mass in the same way that the table rotates around the legs at the opposite side where the force is being applied . to make sence of the step 2 where the mass of all slides is collapsed into one slide , think on a merry-go-round where all the kids move to one side keeping fixed their distance to the axis of rotation .
i think that the paper is completely wrong and the conclusions are preposterous . the paper argues that when one models the vicinity of the electron as a rotating black hole , he will get new effects . however , the black hole corresponding to the electron mass – which is much lighter than the planck mass – would have a much smaller radius than the planck length . it really means that the einstein-hilbert action can not be trusted and all the quantum corrections are important . it also implies that the typical distance scale in any hypothetical electric quadrupole moment of the electron would be much shorter than the planck scale – surely not a femtometer . also , the black holes with masses , charges , and spins similar to those of electrons would heavily violate the extremality bound – something that is not a problem because the classical general theory of relativity can not be trusted for such small systems . the facts in the previous paragraphs are just different perspectives on the universal facts that gravity may be neglected in any observable particle physics , a fact that the author of the paper tries to deny . proof of the vanishing of the quadrupole moment more seriously , one may prove from quantum mechanics that the quadrupole moment for an electron , a spin-1/2 particle , has to vanish because of the rotational symmetry . the quadrupole moment is a traceless symmetric tensor and because the electron 's spin is the only quantum number of the particle that breaks the rotational symmetry , one would have to express the quadrupole moment as a function of the spin , i.e. as $$ q_{ij} = \gamma\cdot ( 3s_i s_j+3s_j s_i - 2s^2 \delta_{ij} ) $$ however , in the rest frame , $s_i$ simply act as multiples of pauli matrices ( with respect to the up/down basis vectors of the electron 's spin ) and the anticommutator $\{s_i , s_j\}$ above – needed for the symmetry of the tensor – is nothing else than the multiple of the kronecker delta symbol , so it cancels against the last term . $q_{ij}=0$ for all spin-1/2 objects ( and similarly , of course , for all spin-0 objects ) . only particles ( nuclei ) with the spin at least equal to $j=1$ ( the case of deuteron ) may have a nonzero electric quadrupole moment . this simple group-theoretical selection rule is the reason why you will not find any experiments trying to measure the electron 's ( or proton 's or neutron 's or other spin-1/2 particles' ) electric quadrupole moment . such experiments would be as nonsensical as the paper quoted by the op . note that unlike the case of the electron 's dipole moment , one does not have to rely on any c , p , or cp-symmetry ( which are broken ) to show that the quadrupole vanishes . to deny the vanishing , one would have to reject the rotational symmetry . let me wrap by saying that the quadrupole moment may always be interpreted as some " elliptical shape " of the object or particle . this ellipsoid would be stretched along some axes and shrunk along other axes . however , the electron 's spin-up and spin-down state really pick the same preferred axis in space – the sign does not matter for the quadrupole – so they can not have different values of the quadrupole moment . in other words , the quadrupole moment does not depend on the spin , and because the spin is the only rotational-symmetry-breaking quantum number that the electron has , the quadrupole moment has to be zero . ( a pauli-matrix-free proof . )
dear sb1 , a good question . well , beauty is a good guide in physics research but only for those whose sense of beauty is aligned with nature 's sense of beauty . ; - ) dirac was among them , at least when he was writing down his beautiful equation , but many others have a different sense of beauty that can easily lead them off the track . the right sense of beauty is linked to the equations ' rigidity and uniqueness . if a woman is beautiful , you may think that not a single thing could be improved about her . every correction would damage this beauty . the same thing holds for the beautiful theories and equations in physics that simply " fit together " . an important characteristic that can make a theory more rigid and constrained is symmetry - but it is not the only characteristic that can do so . for example , the nontrivial cancellation of various a priori conceivable theoretical problems - such as anomalies - also constrains theories and makes them " prettier " relatively to theories that have not had to pass any similar theoretical tests . why are the equations and theories that " fit together " more likely to be the right description of nature ? well , unless they are already falsified , they have many fewer parameters waiting to be adjusted than the competing - so far unfalsified - theories that are not so beautiful . the " posterior " ( after the comparison with the reality ) probability that the " not so beautiful " candidate theory is valid is , by the bayesian logic , multiplied by the probability $p ( g=g_0 ) $ that the parameters $g$ take the right values to agree with the reality . if the overall prior probability for the " beautiful " and " ugly " classes of theories are chosen to be equal , then the " ugly " theory is punished by the extra factor of $p ( g=g_0 ) $ , so it becomes less likely that it is the right theory that describes the observations . a more constrained point in the space of theories ( constrained by symmetries and special consistency advantages ) gets a " higher weight " because it is qualitatively different from the more " generic " or " uglier " points . nature has apparently chosen some repeatable laws that apply everything in the universe ( and maybe beyond ) and that predict millions of phenomena from a very small amount of information about the laws that has to be known in advance . so it makes sense to extrapolate this observation and assume that the laws of physics are as constrained as possible , and in this sense , they must " fit together " and be " beautiful " . but again , one has to be very careful about this method to look for theories that becomes very unscientific unless the " beauty " of the mathematical structures may be justified by some technical arguments .
the hierarchy problem is not only about big numbers , such as $m_{pl}/m_{ew}$ , per se ' . in fact in qcd there is no hierarchy problem associated to the ratio $m_{pl}/\lambda_{qcd}$ . the problem is actually about the quantum numbers of certain operators in a wilsonian eft . the point is that we understand the sm as an effective low-energy description of the dynamics associated to relatively light degrees of freedom . because of qm , the heavy degrees of freedom that one has integrated out actually leak into the effective description by changing the couplings of the local operators of the eft . it is pretty simple , by means of dimensional analysis , to see what operators are strongly affected by the uv degrees of freedom that live at the scale $\lambda$ or above : $\delta\mathcal{l}=\sum_{\mathcal{o}}c_\mathcal{o}\lambda^{4-\delta_{\mathcal{o}}}\mathcal{o}$ , where $\delta_{\mathcal{o}}$ is the scaling dimension of the operator $\mathcal{o}$ . it is thus clear that relevant operators , i.e. with $\delta&lt ; 4$ , are very sensitive to the scale of uv physics . marginal ( $\delta=4$ ) or almost marginal ( $\delta\simeq 4$ ) are pretty much insensitive to the scale of uv physics whereas irrelevant operators ( $\delta&gt ; 4$ ) are suppressed by large $\lambda$ . notice that in the sm the smallness of neutrino masses , and the conservation of b and l quantum numbers , follow from the irrelevance of the operators associated to those operators . however , in the sm , the operator $|h|^2$ is relevant and one would expect its coefficient to scale with $\lambda^2$: a light higgs and an hierarchically small vev ( especially if compared to $\lambda\sim m_{pl}$ ) , are hard to accommodate without finely tuning some cancellation in the uv . one could solve the hierarchy problem by introducing new degrees of freedom that enforce such cancellation to occur as a symmetry requirement ( rather than accidentally ) which suppresses the couplings of the relevant operator . example : susy . bdw , qcd does not have the hierarchy problem ( apart for the cp-problem . . . ) because of two facts : 1 ) the qcd gauge coupling is almost marginal so tha the actual scale $\lambda_{qcd}$ is generated only when the coupling has ran for for very long ( that is , for very a large energy range ) to get into a strong coupling regime which allows for strong bound states to form ; 2 ) there are no relevant operators that are not forbidden by symmetry .
i do not have the book you mentioned , but i could find something in my notes from last year 's classical mechanics lecture . firstly , if your projectile reaches escape velocity , then it will of course travel away from the earth forever , which answers your question with $\infty$ . when does this happen ? the potential energy of the projectile is given by : $v ( r ) = -\frac{gmm}{r}$ we will escape the earth 's potential , if we have enough energy to reach the potential at $r=\infty$ , which obviously corresponds to $v=0$ . so we need at least the potential at the earth 's radius in kinetic energy : $k = \frac{1}{2}mv_0^2$ therefore $v_{escape}=\sqrt{\frac{2gm}{r_e}}$ where $g$ is the gravitational constant , $m$ the mass of the earth , $m$ the mass of your projectile and $r_e$ the radius of the earth ( which is the current distance of your projectile from the earth 's center ) . so if the projectile 's velocity is higher than this we have an unbound orbit , and there is no maximum height . if it is less than this we have a bound orbit , whose trajectory we need to figure out . now gravitation is a radial inverse-square law force of the form $\textbf{f}=\frac{k}{r^2}\textbf{r}$ where $\textbf{r}$ is a unit vector pointing radially outwards . since it is attractive $k&lt ; 0$ ( in fact $k=-gmm$ ) . for these forces a bound orbit will be elliptical , with the center of the earth at one of the ellipse 's foci . the equations i could turn up let you determine the eccentricity $e$ and the semi-latus rectum $h$ , which define the ellipse , from the angular momentum and the total energy of the projectile . $e = k + v = \frac{1}{2}mv_0^2-\frac{gmm}{r_e} \\ \textbf{l} = m\textbf{r}_e\times\textbf{v}_0=mr_ev_0\sin ( \theta_0 ) \\ h = -\frac{l^2}{mk} = \frac{l^2}{gmm^2} \\ e = \sqrt{1+\frac{2el^2}{mk^2}}=\sqrt{1+\frac{2el^2}{g^2m^2m^3}}$ using $e$ and $h$ we can figure out $r_a$ and $r_b$ , the distance from the earth 's center to the " perigee " and " apogee " ( the points that are nearest and farthest from the earth 's center , respectively ) . we do this using standard ellipses formulae : $r_a = \frac{h}{1+e} \\ r_b = \frac{h}{1-e} \\$ since $e$ is between $0$ and $1$ , we obviously have $r_b &gt ; r_a$ as expected . so the largest height above the earth 's surface would then be $r_b - r_e$ . we would be done here . . . if there was not the possibility that the projectile hits the earth 's surface before reaching the apogee ( for example because $\theta_0&gt ; \pi/2$ ) . i am honestly at a loss when it comes to figuring out whether the projectile reaches the apogee , the perigee or both , but i have a feeling that if it reaches any of the two , it would be the apogee as required . but i cannot figure it out properly right now . when i do , i will edit the answer .
the acceleration is a vector $\mathbf{g}$ throughout the motion , and $\mathbf{g}$ is always pointing downward . since you choose positive $x$ to be vertically downward , so $\mathbf{g}$ is along positive $x$ if we draw out the cartesian coordinate , then $\mathbf{g}$ must have positive value , $\mathbf{g}=g\ , \hat{\mathbf{x}}$ . if you choose vertically upward to be $x&gt ; 0$ , then acceleration $\mathbf{g}$ has negative sign , $\mathbf{g}=-g\ , \hat{\mathbf{x}}$ . it is just the matter how you choose the $x&gt ; 0 , y&gt ; 0$ directions . draw a diagram of $v$ , $g$ , force on the ball with $ ( x , y ) $ coordinates .
for $p=1$ , ctc 's do not exist in minkowski spacetime . in other $1+3$ spacetimes , in principle they are admitted in the absence of further requirements ( like globally hyperbolicity ) on the causal structure of the spacetime . they must be present if the spacetime is compact , for instance . for $p\geq 2$ , the answer is obviously yes . consider a manifold $m$ with metric $g$ with signature ( p , q ) and $p \geq 2$ . in a $p+q$-dimensional neighbourhood $u$ of any point $s\in m$ , using the exponential map at $s$ starting from a $p$ dimensional subspace generated by $p$ timelike vectors in $t_sm$ , you can construct an embedded $p$-dimensional submanifold $n$ passing through $s$ and whose metric ( induced by $g$ ) has signature $ ( p , 0 ) $ . this means that every vector tangent to a point in $n$ , considered as a manifold on its own right , is timelike . in local coordinates on $n$ around $s\in n$ , any circle surrounding $s$ is a closed timelike curve .
i think it would be useful to migrate questions 1,3 to math . se ; as for the mathematical side of the story , i think that all gravitates around the idea of moduli space ( a modulus being -as far as i know- the old name for " the set of values a certain variable takes as soon as it runs over a certain set ( often a continuum ) " ) . the idea of a " moduli problem " is central to modern algebraic geometry , and it is deeply intertwined with physics : i am not an expert on the second field , but i can give a ( fairly heuristic ) idea of the first . you will realize in a while that the topic is really huge ; all that follows is not intended to strive for rigor . the rough idea is fairly simple , indeed . as a starting example consider the generic conic in the projective plane $\mathbb p^2 ( k ) $ ; as you may know , such a geometric locus is the zero-set of a polynomial which depends on a certain set of coefficients : more precisely , you have to consider the polynomial $$ a x_0^2 + 2 b x_0 x_1 + 2 c x_0 x_2 + d x_1^2 + 2 e x_1x_2 + f x_2^2=0 $$ where $ ( a , b , c , d , e , f ) $ can on its own right be identified to a point of the projective space $\mathbb p^5 ( k ) $ . and non-degenerate conics , i.e. those conics whose matrix has nonzero determinant , can be identified with an hypersurface in $\mathbb p^5 ( k ) $ . this identification is the key-step in understanding the definition of moduli space : a set of geometric objects can often be regarded as a geometric object of the same kind . now that you have your moduli-glasses on , you will notice that lots of things are indeed moduli spaces : the set of nondegenerate conics in the projective plane happens to be a cubic hypersurface ; the set of iso classes of vector bundles over a given manifold can be thought as a space on its own right ; and in the same vein , for example , there are cases when the set of degree-zero complex line bundles over a smooth projective curve is not only a group ( the picard group $pic^0 ( \mathcal c ) $ of the curve $\cal c$ ) but also a complex torus ( this is an old theorem due to jacobi ) . the formalism of moduli problems offers you a way to express this in full generality : to formulate a " moduli problem " you need the following gadgets . a class of geometric objects ( if this is not painful , think of it as a category $\cal a$ of spaces -manifolds , bundles , lie groups , . . . - ) a rule which assigns to every space $s$ an $s$-parametric family of objects , i.e. a bundle of $\cal a$-objects . in other words , you want to give a function $\rho\colon s\in s\mapsto f_s\in \cal a$: in the case where $\cal a=$manifolds , any space $x$ gives rise to such a structure , taking as $\rho$ a rule which assigns to every $s\in s$ a bundle $f_s\to x$ over $x$ . the assignment $\rho=\rho_s$ has to be functorial and contravariant in $s$ , namely any transformation $s\to t$ must induce a morphism between the bundle $f'$ of $t$-parametric objects and the bundle $f$ of $s$-parametric object ; again in the case of bundles over a manifold , this is given by pull-back operation . to package all this stuff in a single , nifty request : we would like to have a contravariant functor ${\cal a}\to set$ which sends an object $s$ to the set of all $s$-parametric objects $f_*=\{f_s\}_s$ of $\cal a$: understanding this functor is your moduli problem . this is the place where a key step for the solution of your moduli problem comes into play : denote this functor as $\phi$ and suppose you can find a space $m\in\cal a$ such that $$\phi ( s ) \cong {\rm map} ( s , m ) ; $$ this request is not so absurd , since for example you can impose $m=ob ( \cal a ) $ endowed with a suitable topology , and define a map of sets $f_*\mapsto ( s\mapsto f_s ) $ . this map is natural in $s$ , so that if $m$ exists , then it is unique up to a unique isomorphism in $\cal a$ . in a more categorical jargon , the functor $\phi$ is represented by the space $m$ , which is called the ( fine ) moduli space of your moduli problem . the problem is that moduli problems are seldom solvable ( if all of them were , algebraic geometry would be far more easy ; it is up to you to decide if this is a good thing ) ! in most cases of interest no moduli space exists , in the fine sense . to workaround this problem , one needs to define a coarse notion of solution for a moduli problem , but this would turn out to be too technical , and i think i have been far too verbose for a single response . instead i would try to sum up : often in geometry you have to cope with families of spaces which can be turned into another space , called the moduli space of the family : each point in the moduli space corresponds to a space on its own right . the geometry of the moduli space tells you things about the geometry of the spaces/points in it . this is precisely what i read when i see the space of vacua for the quantum field theory is a manifold ( or orbifold ) , usually called the vacuum manifold . this manifold is often called the moduli space of vacua , or just the moduli space , for short .
just " plug into the equation " is always a bad idea . so here is a short overview : given a hamiltonian , the possible energy levels correspond to the eigenvalues of the hamiltonian ( no " plugging in " needed ) . more precisely , we have $h|\psi\rangle=e|\psi\rangle$ for every eigenvector . given a normalized eigenvector , you can find the probability by $\langle \psi |h|\psi \rangle$ , otherwise you have to normalize ( divide by $\langle \psi|\psi \rangle$ ) . in the density matrix formalism , this means that given a state $\rho$ ( positive semidefinite matrix with trace one - otherwise normalize the trace ) the probability is given by $\operatorname{tr} ( \rho h ) $ gives you the probability . so the question is : what do you mean " i am not given a traditional wavefunction to normalize " ? edit : to me , it seems that you are given a perfectly reasonable wave function ( in matrix formulation , though ) . an electron that sits just at site $j$ will have corresponding wave function $|e_j\rangle$ , where $e_j$ denotes the $j$-th basis vector ( i.e. . $|e_j\rangle= ( 0 , \ldots , 0,1,0 , \ldots 0 ) ^t$ with the $1$ at position j . following your assignment , this tells you the wave function of your particle looks like : $$ |a\rangle=\sum_{i=1}^n a|e_i\rangle$$ where $a$ is a complex number and $n|a|^2=1$ for normalization . in order to find the probabilities , you can now either compute the eigenvectors of $h$ and then decompose $|a\rangle$ in terms of these eigenvectors , or you can compute the spectral decomposition of $h$ , i.e. the eigenvalues $\lambda_i$ and projectors $p_i$ such that $h=\sum_{i=1}^n \lambda_i p_i$ and compute $\langle a|p_i|a\rangle$ to obtain the probability of measuring $\lambda_i$ .
for the entire curved part of the path the second ball has higher velocity in x direction than the other ball on the level path . on the curved part there is acceleration $a_{slope}=g\sin\theta$ where $\theta$ the slope angle to the horizontal . acceleration in the x axis $a_x=a_{slope}\cos\theta$ . velocity cannot be constant as long as there is a slope . furthermore the rate of change in this velocity is also dependent on that slope angle .
the easiest way to think about it is that $\exp ( \dots ) $ is just a number and does not affect the dimension . however , you still have $3n$ factors of the momenta and the position lying around that will give you dimensions of [ length x momentum ] ${}^{3n}$ . planck 's constant has the units of length x momentum , so the $3n$ factors of $h$ cancel the $3n$ factors coming from the integral .
it is a good question . the answer is that the bound on the density is given by the requirement that the interactions between the bosons have to remain weak for the bose-einstein condensate to exist . in practice , the helium-4 atoms have to be further away from each other than their radius . why it is so ? well , if you are talking about the bosons occupying the " same state " , it really means that you are constructing a multi-particle state in the multi-particle theory . if you want the energy of this state to be simply given by the sum of the energies of the individual bosons - i.e. $n$ times the energy of the one-particle state - you must guarantee that you have the right hamiltonian which is essentially the hamiltonian for the bosons in an external potential , without any significant interaction term in between the bosons . a sufficiently strongly interacting hamiltonian for the bosons could not be solved that easily . if you try to push the composite bosons really close to each other , i.e. by lowering the temperature extremely close to the absolute zero , the interactions between them will start to matter which will prevent you from approximating the hamiltonian by a sum of many one-particle terms . consequently , the right description is in terms of the component particles - which are often fermions . it is believed by many condensed matter physicists that the ultimate state of any bound matter very near the absolute zero is a superconductor or a fermi liquid - and i do not know . consider helium-3 as an example . i am actually not sure what one gets at superextremely low temperatures .
great question . a little background first . note that any force $\boldsymbol{f}$ moment $\boldsymbol{m}$ system on a point a can be equipollently translated into the screw axis s leaving only the components of $\boldsymbol{m}$ that are parallel to $\boldsymbol{f}$ . the location is found by $$ \boldsymbol{r} = \frac{\boldsymbol{f} \times \boldsymbol{m}}{\boldsymbol{f}\cdot\boldsymbol{f}} $$ also the moment components parallel to $\boldsymbol{f}$ are described by a scalar pitch value $h$ found by $$ h = \frac{ \boldsymbol{m} \cdot \boldsymbol{f}}{\boldsymbol{f} \cdot \boldsymbol{f}} $$ in reverse , a moment is defined by a force vector $\boldsymbol{f}$ passing through an axis located at $\boldsymbol{r}$ with pitch $h$ $$ \boldsymbol{m} = \boldsymbol{r} \times \boldsymbol{f} + h \boldsymbol{f} $$ have you noticed how difficult it is to apply a pure moment on a rigid body , without applying a force ? this is because you cannot have one without the other . a moment is really a result of the line of action of forces . so the scalar potential of a moment is really the same as the one for forces with $$ \boldsymbol{m} = - \boldsymbol{r} \times \nabla v - h \nabla v = -\left ( \left [ 1\right ] h + \boldsymbol{r}\times \right ) \nabla v $$ the problem is that in rigid body mechanics forces are not treated as scalar fields , but spatially constant , and temporally varying . furthermore , i cannot think of a case where spatially varying moments arise that are not due to a force at a distance . i suppose you can come up with a tensor pitch $h$ instead of a scalar which is spatially varying for a definition like $\boldsymbol{m} = -\left ( h + \boldsymbol{r}\times \right ) \nabla v$ , but then you will be making things up that do not have any physical meaning that i know of .
there are many models of solar cookers relying on various phenomena . two that come to mind are : greenhouse cooker : the cooker is a box close by transparent material that will let in solar radiation but will not let out radiation produced by the heated material inside the box . so energy accumulates . same as greenhouse for growing food in winter or cold climate . mirror cooker : the radiation from the sun is concentrated by several mirrors . you can also get a parabolic mirror , but they are harder to come by , though metallic foldable ones exist . you could also use a lense to concentrate radiation , but there is probably loss in the lens , and it is heavy as soon as it is large . it should often be usable in other places than desert as long as the sun shines , and makes for more ecological barbecues . if you meant to cook without any equipment at all , you might try to bury the food in very hot sand , preferably dark . i am not sure whether it works or how fast . i read you can cook an egg at 55 celsius , which is very hot on the feet , but still a temperature found in desertic grounds ( sometimes even in cities ) .
yes , subsystems of an entangled state – if this subsystem is entangled with the rest – is always in a mixed state or " statistical mixture " which is used as a synonym in your discussion ( or elsewhere ) . if we are only interested in predictions for a subsystem $a$ in a system composed of $a , b$ , then $a$ is described by a density matrix $\rho_a$ calculable by " tracing over " the indices of the hilbert space for $b$: $$\rho_a = {\rm tr}_{i_b} \rho_{ab}$$ note that if the whole system $ab$ is in a pure state , $$\rho_{ab}= |\psi_{ab}\rangle\langle \psi_{ab}| $$ if $\psi_{ab}$ is an entangled i.e. not separable state , i.e. if it cannot be written as $|\psi_a\rangle\otimes |\psi_b\rangle$ for any states $|\psi_a\rangle$ and $|\psi_b\rangle$ , then the tracing over has the effect of picking all the terms in $|\psi_{ab}\rangle$ , forgetting about their dependence on the $b$ degrees of freedom , and writing their probabilities on the diagonal of $\rho_{ab}$ . that is why the von neumann entropy will be nonzero – the density matrix will be a diagonal one in a basis and there will be at least two entries that are neither $0$ nor $1$ . take a system of two qubits . we have qubit $a$ and qubit $b$ . there are 4 natural basis vectors for the two qubits , $|00\rangle$ , $|01\rangle$ , $|10\rangle$ , and $|11\rangle$ where the first digit refers to the value of $a$ and the second digit to $b$ . a general pure state is a superposition of these four states with four coefficients $\alpha_{ab}$ where $a , b$ are $0,1$ , matched to the corresponding values . if $\alpha_{ab}$ may be written as $\beta_a\gamma_b$ i.e. factorized in this way , the pure state is separable . $|01\rangle$ is separable , for example . if it is not , then it is entangled . for example , $|00\rangle+|11\rangle$ is not separable so it is entangled . the mixed state is a more general state than a pure state . in this case , it is given by a $4\times 4$ hermitian matrix $\rho$ . the matrix entries are $\rho_{ab , a'b'}$ where the unprimed and primed indices refer to the values of qubits $ab$ in the bra and ket vectors , respectively . if these matrix entries may be factorized to $$\rho_{ab , a'b'} = \alpha^*_{ab}\alpha_{a'b'}$$ for some coefficients $\alpha_{a'b'}$ and their complex conjugates that specify a pure state $|\psi_{ab}\rangle$ , then the density matrix $\rho$ is equivalent to the pure state $|\psi_{ab}\rangle$ and we say that the system is in a pure state . in the more general case , $\rho$ can not be written as this factorized product but only as a sum of similar products . if you need at least two terms like that to write $\rho$ , then the state is mixed and the von neumann entropy is therefore nonzero .
first , no , " radio propagation " is not " via atmosphere " . different wavelengths get absorbed , reflected , or simply passed by different parts of the atmosphere . there is no one general rule . many of our radio communications within the atmosphere are pretty much like they would be in free space , for example . second , all radio waves propagate infinitely in free space . there is no finite end to the propagation . what does matter in a practical sense is signal to noise ratio . below some signal to noise ratio for whatever information encoding scheme is used , that information can not be recovered . or more accurately , the error rate goes up as the signal to noise ratio goes down . at some point the errors in the information make is useless or " unreceivable " in a practical sense . added : you are now asking specifically about " commercial fm " , which i take to mean radio at around 100 mhz ( 3 meter wavelength ) . the primary mechanism limiting reception distance of such commercial fm stations is the curvature of the earth . the wavelength is too short for significant refraction around the earth , as happens with commercial am at around 1 mhz ( 300 meters ) . note that when you are in a car listening to a fm station at the fringe of its range , it will come and go as you get to tops of hills or dip between them . another factor is that here on earth there is significant interference from all kinds of unintentional broad-band radiators . eventually at some distance the intended signal becomes too small relative to this background noise for the signal to be picked up well enough for your liking . in the end , there are no hard limits and reception is about signal to noise ratio . that means you can extend the useful range of a transmission by reducing noise or selectively amplifying the signal . note that spaceprobes usually emit only a few watts or tens of watts , whereas commercial fm stations usually a few kw . one important difference is that spaceprobe signals are picked up with highly directional antennas and extra low noise receivers . you would not want to pay for one of those receivers in your car . the narrow beam of the receiving antenna greatly increases signal to noise ratio . since the signal is coming from a point , the same signal is still picked up along a narrow beam . but , only a tiny fraction of the noise that is coming from all around is picked up along that same beam .
to a very good approximation the transmission of a metal film falls exponentially with thickness i.e. : $$ t = e^{-\alpha t}$$ where $\alpha$ is the absorption coefficient given on the web site alexander mentioned , http://refractiveindex.info/?group=metalsmaterial=copper, and at 500nm wavelength this gives $\alpha = 6.4297\times 10^5/cm$ . so you just have to solve for $t = 0.5$ . if you want to do the calculation properly it turns into a bit of a nightmare . by one of those strange co-incidences i did exactly this calculation as part of my phd , and even more amazingly i have my thesis to hand . the reference i used for the calculation was o . s . heavens , optical properties of thin solid films , butterworths scientific publications , london 1955 . it is on google books here , but annoyingly has not been scanned so you can not see the contents . i will copy the equation for the optical transmission from my thesis , but i imagine one look will make you run for cover . i compared the full calculation to the simple exponential formula and agreement was basically perfect except at very small film thicknesses ( below about 5nm ) but in any case the metal films break up into islands at these thicknesses so the equation does not really apply . $$ t= n_s \frac{ ( ( 1 + g_1 ) ^2 + h_1^2 ) ( ( 1 + g_2 ) ^2 + h_2^2 ) }{e^{2\alpha} + ( g_1^2 + h_1^2 ) ( g_2^2 + h_2^2 ) e^{-2\alpha} + ccos ( 2\gamma ) + dsin ( 2\gamma ) }$$ where : $$ c = 2 ( g_1g_2 - h_1h_2 ) $$ $$ d = 2 ( g_1h_2 + g_2h_1 ) $$ $$ g_1 = \frac{1 - n^2 - k^2}{ ( 1+n ) ^2 + k^2} $$ $$ g_2 = \frac{n^2 - n_s^2 + k^2}{ ( n+n_s ) ^2 + k^2} $$ $$ h_1 = \frac{2k}{ ( 1+n ) ^2 + k^2} $$ $$ h_2 = \frac{-2n_sk}{ ( n+n_s ) ^2 + k^2} $$ $$ a = \frac{2\pi k d}{\lambda} $$ $$ \gamma = \frac{2\pi n d}{\lambda} $$ where $k$ and $n$ are the extinction co-efficient and refractive index of the metal film and $n_s$ is the refractive index of the glass substrate . the film thickness is $d$ and the light wavelength is $\lambda$ . if you do a sample calculation for some test film thickness you will probably find most of the terms are approximately zero or unity , which is why it approximates to an exponential equation in the film thickness . bear in mind that i have hand copied this from my thesis , so there may be transcription errors lurking as traps for the unwary .
searching on google there is nothing new . considering the plethora of arxiv papers coming out with theoretical comments on the superluminal result i would think that if the lqg model had something to say , it would have said it , particularly if it were vindicated . so the answer is " no " . for the nonce . because if one reads the wiki article there exists the cryptic : led lee smolin and others to suggest that spin network states must break lorentz invariance . lee smolin and joao magueijo then went on to study doubly special relativity , in which not only there is a constant velocity c but also a constant distance l . they showed that there are nonlinear representations of the lorentz lie algebra with these properties ( the usual lorentz group being obtained from a linear representation ) . doubly special relativity predicts deviations from the special relativity dispersion relation at large energies ( corresponding to small wavelengths of the order of the constant length l in the doubly special theory ) it may be that lqg might be able to accommodate the opera result , though again , from not having jumped at the opportunity i would not hold my breath . p.s. i am an experimentalist and am treating theories statistically : ) .
here we will only consider the leading semi-classical approximation of a $1$-dimensional problem with hamiltonian $$ h ( x , p ) ~=~ \frac{p^2}{2m} + \phi ( x ) , $$ where $\phi ( x ) &lt ; 0$ is the potential function . let us for simplicity assume that the potential $\phi ( x ) =\phi ( -x ) $ is an even function and strongly monotonically increasing for $x\geq 0$ with limit $\lim_{|x|\to \infty}\phi ( x ) =0$ . let $e_n&lt ; 0$ denote the energy of the $n$'th bound state , ordered increasingly $e_1 &lt ; e_2&lt ; e_3&lt ; \ldots$ , so that $e_1$ denotes the ground state energy . there is also a positive continuous unbounded spectrum $e\geq0$ , which we shall not discuss further . we shall here construct a counterexample of two potentials $\phi^{\prime} ( x ) $ and $\phi^{\prime\prime} ( x ) $ such that the quotient of potentials satisfies the limit $$ \lim_{|x|\to \infty} \frac{\phi^{\prime\prime} ( x ) }{\phi^{\prime} ( x ) } ~=~1 , \qquad ( 1 ) $$ but where the corresponding quotient of bound state energies satisfies $$ \lim_{n\to \infty} \frac{e_n^{\prime\prime}}{e_n^{\prime}} ~\neq~1 . \qquad ( 2 ) $$ the idea is to seek for a potential $\phi$ that would generate a bound state spectrum of the form $$e_n= -r e^{-\mu n} , \qquad ( 3 ) $$ where $r&gt ; 0$ is a rydberg-like constant of dimension energy , and $\mu&gt ; 0$ is a dimensionless positive constant . [ the hydrogen atom is for comparison $e_n= -r/n^2$ . ] thus the number of states $n ( e ) $ below energy-level $e$ should roughly satisfy $$e~\approx~ -r e^{-\mu n ( e ) } \qquad \leftrightarrow \qquad n ( e ) ~\approx~\frac{1}{\mu}\ln ( -\frac{e}{r} ) . $$ this answer provides a semi-classical inversion formula for the potential $\phi$ that we will use . the length $\ell ( v ) $ of the classically accessible region of the potential well at potential energy-level $v$ becomes $$ 2\phi^{-1} ( v ) ~=~\ell ( v ) ~\approx ~\hbar\sqrt{\frac{2}{m}} \frac{d}{dv}\int_{v_{0}}^v \frac{n ( e ) ~de}{\sqrt{v-e}} $$ $$~\approx~\frac{\hbar}{\mu}\sqrt{\frac{2}{m}} \left ( \frac{2\arctan\sqrt{\frac{v_0}{v}-1}}{\sqrt{-v}} - \frac{\ln ( -\frac{v_0}{r} ) }{\sqrt{v-v_0}}\right ) . \qquad ( 4 ) $$ one may check that the accessible length function $\ell ( v ) $ is a monotonically increasing function for $v\in [ v_1,0 [ $ for some choice of the constants $v_0$ and $v_1$ with $v_0&lt ; v_1&lt ; 0$ . asymptotically , such potential $\phi ( x ) $ behaves as an inverse square potential $-c/x^2$ for $|x|\to\infty$ , where $c&gt ; 0$ is a positive constant . we now construct the potential functions $\phi^{\prime} ( x ) $ and $\phi^{\prime\prime} ( x ) $ such that they are given by formula ( 4 ) in the outer region $x\geq x_1$ , where $x_1:=\ell ( v_1 ) /2&gt ; 0$ ; and arbitrarily monotonically increasing in the inner region $0\leq x\leq x_1$ . this implies that the quotient of potentials satisfies $$ \frac{\phi^{\prime\prime} ( x ) }{\phi^{\prime} ( x ) } ~=~1 \qquad {\rm for}\qquad |x|\geq x_1 , $$ so that condition ( 1 ) is satisfied . for large enough states $n\geq n_1$ , after the inner potential well is filled , the spectrum $e_n$ eventually becomes exponentially a la ( 3 ) . we now choose the inner potentials $\phi^{\prime} ( x ) $ and $\phi^{\prime\prime} ( x ) $ such that there fits one more bound state into the profile $\phi^{\prime\prime} ( x ) $ than $\phi^{\prime} ( x ) $ for $|x|\leq x_1$ . then the labeling of states $e_{n+1}^{\prime\prime}\approx e_n^{\prime}$ would be off by one for $n\geq n_1$ , yielding the inequality ( 2 ) , $$ \frac{e_n^{\prime\prime}}{e_n^{\prime}} ~\approx~e^{\mu} ~\neq~1 . \qquad {\rm for}\qquad n\geq n_1 . $$
good question . assume we have one cube of ice in a glass of water . the ice displaces some of that water , raising the height of the water by an amount we will call $h$ . archimedes principles states that the weight of water displaced will equal the upward buoyancy force provided by that water . in this case , $$\text{weight of water displaced} = m_\text{water displaced}g = \rho vg = \rho ahg$$ where $v$ is volume of water displaced , $\rho$ is density of water , $a$ is the surface area of the glass and $g$ is acceleration due to gravity . therefore the upward buoyancy force acting on the ice is $\rho ahg$ . now the downward weight of ice is $m_\text{ice}g$ . now because the ice is neither sinking nor floating , these must balance . that is : $$\rho ahg = m_\text{ice}g$$ therefore , $$h = \frac{m_\text{ice}}{\rho a}$$ now when the ice melts , this height difference due to buyoancy goes to 0 . but now an addition mass $m_\text{ice}$ of water has been added to the cup in the form of water . since mass is conserved , the mass of ice that has melted has been turned into an equivalent mass of water . the volume of such water added to the cup is thus : $$v = \frac{m_\text{ice}}{\rho}$$ and therefore , $$ah = \frac{m_\text{ice}}{\rho}$$ so , $$h = \frac{m_\text{ice}}{\rho a}$$ that is , the height the water has increased due to the melted ice is exactly the same as the height increase due to buoyancy before the ice had melted .
if velocity does not change direction , the object will travel in a straight line . in the zero initial velocity scenario , with constant force ( and therefore acceleration ) from wind in the horizontal direction , and constant force ( and therefore acceleration ) from gravity in the downward direction , there is net constant acceleration in a particular direction ( the vector sum of the two accelerations ) . starting from zero velocity and accelerating in a particular direction means velocity never changes direction and motion is in a straight line . instead , if there is an initial velocity in the downward direction , velocity and acceleration are not in the same direction . initially , motion would be downward , but become increasingly in the same direction as acceleration .
the electric field due a uniformly charged sphere with its radius increasing at a constant rate at any point outside the sphere is the same as if the radius were not changing at all . in this scenario , you can still invoke the radial symmetry argument and use the gauss 's law along with invariance of electric charge to obtain the electric field which turns out to be the same as that of a point charge at the centre of the sphere . any change in the rate of increase of radius of the sphere does not lead to a change in the electric field at any point farther from the sphere than the maximum radius the sphere can attain while it is pulsating . so the electric field at all points whose distance from the centre of the sphere is larger that the maximum radius that can be attained by the pulsating sphere is constant over time . so beyond the maximum attainable radius there is no electromagnetic radiation . however , i think there will be some electromagnetic radiation in the region between the minimum attainable radius and the maximum attainable radius . this could be in the form of standing waves confined to that region . at any point whose distance from the centre of the sphere is less than the minimum attainable radius , i think you can again apply the above mentioned logic and conclude that there is to electromagnetic radiation in that region . so , to conclude , i feel there should be no net electromagnetic radiation emanating from the sphere . p.s. i have not done a detailed study of electromagnetic radiation . so , i am not sure if the above analysis is correct . i would be glad if somebody were to post the correct analysis of the scenario in question .
in principle , the gravitational potential energy should be included into total internal energy , but in practice , most often it is not . i know of two reasons . because for systems that are discussed in thermodynamics , it is believed that gravitational energy is negligible compared to electromagnetic potential energy of the constituting particles ; because it is difficult to include $1/r^2$ forces such as electromagnetic or gravitational force to calculations based on standard statistical physics in a unique and convincing way .
as the voltage between the capacitor 's plates decreases , so should the current flowing through the circuit . i do not follow your reasoning here . recall that , for an ideal capacitor , we have : $$i_c = c\frac{dv_c}{dt}$$ in words , the current through the capacitor is proportional to the rate of change of the voltage across , not the instantaneous value of the voltage . so , for example , if the voltage across the capacitor is sinusoidal $$v_c = v \sin\omega t$$ the current is $$i_c = \omega v \cos \omega t$$ which means ( 1 ) that the maximum current ( magnitude ) occurs when the voltage is zero and ( 2 ) that the maximum voltage ( magnitude ) occurs when the current is zero . now , for this simple lc circuit , the voltage across the capacitor is identical to the voltage across the inductor : $$v_c = v_l$$ thus , $$i_c = c\frac{dv_l}{dt}$$ for an ideal inductor , we have : $$v_l = l\frac{di_l}{dt}$$ but , the inductor current is $$i_l = - i_c$$ thus , $$i_c = -lc\dfrac{d^2i_c}{dt^2}$$ which means that the current is sinusoidal $$i_c = a \sin \omega t + b \cos \omega t $$ where $$\omega = \frac{1}{\sqrt{lc}}$$ since , in your example , the initial current is zero and the initial voltage is $v$ , we have $$i_c ( t ) = -\frac{v}{\omega l} \sin \omega t$$
openmc ( github ) the openmc project aims to provide a fully-featured monte carlo particle transport code based on modern methods . it is a constructive solid geometry , continuous-energy transport code that uses ace format cross sections . the project started under the computational reactor physics group at mit . you can find more background information in this paper ( sciencedirect )
( lubos just posted an answer , but i think this is sufficiently orthogonal to post too ) . the usual wavefunction for a bosonic field is a complex number for each field configuration at all points of space : $$\psi ( \phi ( x ) ) $$ this wavefuntion ( al ) obeys the schrodinger equation with the field hamiltonian , where the field momentum is a variational derivative operator acting on $\psi$ . this formulation is fine in principle , but it is not useful to work with this object directly under usual circumstances for the following reasons : you need to regulate the field theory for this wavefunctional to make mathematical sense . if you try to set up the theory in the continuum right from the beginning , to specify a wavefunction over each field configuration you need to work just as hard as to do a rigorous definition for the field theory . for example , just to normalize the wavefunction over all constant time slice field values , you need to do a path integral over all the constant time field configurations . this is a path integral in one dimension less , but the thing you are integrating is no longer a local action , so there is no gain in simplicity . even after you normalize , the expectation value of operators in the wavefunctional is a field theory problem in itself , in one dimension less , but with a nonlocal action . once you regulate on a lattice , the field wavefunctional is just an ordinary wavefunction of all the field values at all positions . but even when you put it on an infinite volume lattice , a typical wavefunctional in infinite volume will have a divergent energy , because you will have a certain energy density at each point when the wavefuntional is not the vacuum , a finite energy density . infinite energy configurations of the field theory , those with a finite energy density , are very complicated , because they do not decompose into free particles at asymptotic times , but keep knocking around forever . the actual equations of motion for the wavefuntional are not particularly illuminating , and do not have the manifest lorentz symmetry , because you chose a time-slice to define the wavefunction relative to . these problems are overcome by working with the path integral . in the path integral , if you are adamant that you want the wavefuntion , you can get it by doing the path integral imposing a boundary condition on the fields at a certain time . but a path integral monte-carlo simulation , or even with just a little bit of wick rotation , will make the wavefunction settle to be the vacuum , and insertions will generally only perturb to finite energy configurations , so you get the things you care about for scattering problems . still the wavefunction of fields is used in a few places for special purposes , although , with one very notable exception , the papers tend to be on the obscure side . there are 1980s papers which attempted to find the string formulation of gauge theories which tried to work with the field hamiltonian in the schordinger representation , and these were by famous authors , but the name escapes me ( somebody will know , maybe lubos knows immediately ) . the best example of where this approach bears fruit is when the reduction in dimension gives a field theory which has a relationship with known solvable models . this is the example of the 2+1 gauge vacuum , which was analyzed in the schrodinger representation by nair and collaborators in the past decade . a recent paper which reviews and extends the results is here : http://arxiv.org/ps_cache/arxiv/pdf/1109/1109.6376v1.pdf . this is , by far , the most significant use of schrodinger wavefunctions in field theory to date .
he just means that an object which has a three-dimensional structure with no symmetries cannot be turned from a left-handed version to a right-handed version using rotations alone . you can not rotate a left-hand glove to be a right-hand glove . a vector can be inverted by rotating it , but this does not invert a general rigid body , because a vector only has one axis , not three . the proof of the statement that rotations cannot invert is by the continuity of the determinant function . the determinant of a rotation is always 1 , and of a reflection-rotation is -1 . the determinant cannot smoothly go from 1 to -1 .
re : i am looking for an ohm 's " law " like equation that relates spring constant to resistivity with a few physical constants thrown into a constant somewhere in the equation . does such a thing exist ? no . for a specific material with a specific dopant , you can get a relationship between the two , but there is no general relationship . while a component of the spring constant is related to the electron " pressure " -- and hence related to the fermi energy -- the exact nature of the relationship ( and how big a role it would play in the overall spring constant ) will depend on the specific material . the resistivity is also related to the fermi energy , but how much and in what manner will depend on the specific material .
it will work just as well provided the part of the coil making contact with the supports is not insulated . the rest of it can be insulated . the important property is that current has the ability to run through the coil because it is this current that experiences a force due to the magnetic field of the permanent magnets causing the coil to rotate . for a bit more detail and pictures : http://hyperphysics.phy-astr.gsu.edu/hbase/magnetic/mothow.html
false . to quote from the specific section in the list of common misconceptions : glass does not flow at room temperature as a high-viscosity liquid . although glass shares some molecular properties found in liquids , glass at room temperature is an " amorphous solid " that only begins to flow above the glass transition temperature , though the exact nature of the glass transition is not considered settled among theorists and scientists . panes of stained glass windows are often thicker at the bottom than at the top , and this has been cited as an example of the slow flow of glass over centuries . however , this unevenness is due to the window manufacturing processes used at the time . normally the thick end of glass would be installed at the bottom of the frame , but it is also common to find old windows where the thicker end has been installed to the sides or the top . no such distortion is observed in other glass objects , such as sculptures or optical instruments , that are of similar or even greater age . one researcher estimated in 1998 that for glass to actually " flow " at room temperatures would take many times the age of the earth .
the comparison is viable , here 's why : let 's choose the positive $x$-direction to point upward , perpendicular to the water 's surface . by archimedes ' principle , the magnitude of the buoyant force on an object of volume $v$ equals the weight of the displaced water ; $f_b = \rho_w v g$ where $\rho_w$ here denotes the density of water . the buoyant force points in the positive $x$-direction . in addition to the buoyant force , an object submerged in water will experience a gravitational force $f_g = \rho v g$ pointing in the negative $x$-direction , where $\rho$ is the density of the object . it follows that the net vertical force on the water is $f_x = f_b - f_g = ( \rho_w-\rho ) v g$ let 's assume that the object is rigid , so that its volume changes negligibly with depth , and let 's make the approximation that the density of water varies negligibly as a function of depth as well , then the work done in moving such an object to a depth $d$ below the water 's surface along the $x$-axis will simply be $w = f\delta x = ( \rho_w-\rho ) v g ( 0-d ) = \boxed{ ( \rho-\rho_w ) v g d}$ notice how similar this looks to $mgd$ , the change in potential energy of moving an object upwards under the influence of gravity by an amount $d$ . in fact , we could think of the water as effectively decreasing the mass of the object so that it has an effective mass $m_\mathrm{eff} = ( \rho- \rho_w ) v$ , and then the analogy becomes clear an important caveat to all of this is that water is viscous , so they will be an additional drag force you have to contend with that will change the answer in the event that you submerge the object quickly . however , by moving the object sufficiently slowly , you can make this contribution as small as you had like . you can also relax the constant water density and constant object volume assumptions if you know how these things change with depth . if you had like more details on this , then let me know ! cheers !
